CR Number                     : 13406
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Unexpected behavior of "stat" against an LFS file
Reported Date                 : 3/25/96
Found in Baseline             : 1.2.1
Found Date                    : 3/25/96
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[3/25/96 public]
(for Hitachi...)
 
               The DFAM engineer who has investigated the DFAM failure in
        48 CHO (i.e., DFAM can run only 9 hours) reported that he found a 
        possible cause which logically explains the DFAM failure.
 
               He looked into the DFAM trace file to find that an unexpected
        behavior of the stat system call seems to have triggered the DFAM
        failure.  The DFAM trace file showed that the stat system call was
        issued to a DFS (LFS) file (not a UNIX file), and the issued system
        call sometimes waited for 2 hours before it stopped and reported a 
        connection time out error.  The previous spec of the stat system 
        call, however, was that it waits only for 5 minutes, then issues a 
        connection time out error.
 
               DFAM generates a process in every 2 to 3 seconds.  If the 
        stat system call waits for 2 hours, the number of the generated 
        process goes up to 3,000, and this can easily cause the shortage 
        of memory, which stops the DFAM 48 CHO.

[3/27/96 public]
I wanted to fill this in with some additional e-mail traffic.
 
What I sent to the pm list:
  
The ``dfstrace'' output from the DFS cache manager would be useful in
determining just what was going on here.  Without that I can only
speculate as to the possible causes.
 
The DFS CM sets a 10-minute timeout for each RPC that it makes to the
file server, so that if a call does not complete in that interval, the
calling thread will continue with the timeout signal.  Furthermore, if
the CM does not receive a response from the dfsbind helper process
within three minutes, the CM thread will continue with an error
indication.  If a second client holds conflicting tokens but is
unreachable due to a network partition, after two minutes the tokens
will be preemptively revoked and our client will be able to continue.
 
However, there are several circumstances that will cause a thread to
retry an operation for an unbounded interval; these are in general
controlled by the result of the cm_Analyze() routine in
src/file/cm/cm_rrequest.c.  The usual causes of these long (but
interruptible) retry sequences are that the fileset on which the
operation is being performed is involved in some long-lived
administrative activity.  It is conceivable, for instance, that moving a
very large, actively-updated fileset from server to server across a slow
link might delay a client this much.  Initial replica propagation might
conceivably take this long, as well.  Of course, these delays and
retries should have resulted in the eventual success of the call, not in
a connection timeout.
 
In any case, I am unaware of any specification for the stat() system
call that claims it will wait for only five minutes before declaring a
failure.  If there was a two-hour delay, it is worth trying to
understand why that delay was occurring, particularly if it ultimately
fails.  I would recommend the output of dfstrace and the console log as
being relevant to this further diagnosis.
  
>             DFAM generates a process in every 2 to 3 seconds.  If the
>      stat system call waits for 2 hours, the number of the generated
>      process goes up to 3,000, and this can easily cause the shortage
>      of memory, which stops the DFAM 48 CHO.
>             The questions we would like to ask the DFS engineers are:
>      Q1: Has the spec of the stat system call issued against DFS files
>          been changed so that the system call waits for 2 hours ?
 
As above: I am unaware of any such specification.  However, the RPCs
that the DFS CM generates are made with two timeouts:
	- the comm_timeout of (default+2), or about 2.5 minutes
	- the call_timeout of ten minutes
Either of these could result in an ETIMEDOUT value in errno.
 
>      Q2: If yes, Why ?
 
>      Q3: If no, could you please check the system call ?
 
I would be happy to investigate the trace and console output....
 
There was then some follow-up on getting dfstrace to generate correct output
(finding its message catalog files, I presume).  I then recommended:
 
Ideally, the result log would show how an error condition would evolve,
ultimately seeing both the beginning of the CM's vnode operation to the
end where it returns the error code that is passed to your test as an
errno value that you do not expect.
Tsuneo Yamaura then replied:
             Thank you very much for your quick suggestions.
     Now the trace command works.  We are trying to reproduce
     the memory shortage.  We will send you the trace file 
     when we catch the problem.
 
This is the up-to-date story as I understand it.

[3/28/96 public]
For the record, here is the Hitachi test program that they believe may
be able to reproduce the problem.
 /*
 * Test program for stat()
 * Repeatedly fork and have child processes issue stat().
 * Get dfstrace if child process didn't return in TIMEOUT seconds.
 * Target files must be prepared in the place name defined in STATFILE
 * before this program can be executed.  Number of the target files should
 * be same as* SETITER.
 */
#include <sys/stat.h>
#include <signal.h>
#include <sys/wait.h>
#include <syslog.h>
#include <errno.h>
#include <unistd.h>
#include <math.h>
#include <time.h>
 
#ifdef DEBUG
#define	SETITER		3000			/* number of spawn child in one set */
#define	DFSTRACE	"/bin/echo"
#define	STATFILE	"/tmp/stattest/statfile%04d"
#define	TIMEOUT		1
#else
#define	SETITER		3000			/* number of spawn child in one set */
#define	DFSTRACE	"/bin/dfstrace"
#define	STATFILE	"/:/stattest/statfile%04d"	/* target file on DFS */
#define	TIMEOUT		600				/* seconds to wait child to die */
#endif
 
#define	LOGFILE1	"/tmp/statlog1"	/* when stat() blocked */
#define	LOGFILE2	"/tmp/statlog2"	/* when blocked stat() returned */
 
char	statfile[1024];
int		alarm_flag, iteration;
extern int	errno;
 
void sig_alrm();
void child_stat();
 
main()
{
	pid_t pid;
	int status;
 
	alarm_flag = 0;
	openlog("STATTEST",(LOG_PID|LOG_CONS|LOG_NDELAY|LOG_NOWAIT),LOG_USER);
	syslog(LOG_ERR,"***STATTEST START***");
	if (signal(SIGALRM,sig_alrm) == -1){
		syslog(LOG_ERR,"***signal system call error***\n");
		exit(0);
	}
 
	while(1)
	{
		sprintf(statfile, STATFILE, iteration);
		if (iteration == SETITER){
			syslog(LOG_ERR,"***STATTEST PATH:%ld***\n",iteration);
			iteration = 0;
		} 
		if ((pid = fork()) < 0){
			syslog(LOG_ERR,"***fork failed***\n");
			exit(1);
		}
		else if (pid == 0){		/* child */
			signal(SIGALRM,SIG_IGN);
		 	child_stat();
		}
		/* parent */
		alarm(TIMEOUT);	
		
do_wait:
		if (wait(&status) != pid){
			switch(errno){
				case EINTR:
					goto do_wait;
				default:
					syslog(LOG_ERR,"***wait system call failed:%d***\n",errno);
					exit(1);
			}
		}
 
		alarm(0);
 
		if (alarm_flag == 1){
			if (execl(DFSTRACE,"dfstrace","dump","-set","cm","-file",LOGFILE2,0) == -1){
				syslog(LOG_ERR,"***execl error:%d***\n",errno);
				exit(1);
			}
		}
		iteration++;
	}		
}
void child_stat()
{
	struct stat buf;
 
#ifdef DEBUG
	printf("%s\n", statfile);
	if (iteration == 3) {
		sleep(TIMEOUT + 1);
	} else {
		stat(statfile,&buf);
	}
#else
	stat(statfile,&buf);
#endif
	exit(0);
}
 
void sig_alrm()
{
	pid_t pid;
	int status;
 
	if ((pid = fork()) < 0){
		syslog(LOG_ERR,"***fork system call error:%d***\n",errno);
	}
	else if (pid == 0){			/* child */
		if (execl(DFSTRACE,"dfstrace","dump","-set","cm","-file",LOGFILE1,0) == -1){
			syslog(LOG_ERR,"***execl system call error:%d***\n",errno);
		}
	}
	
	syslog(LOG_ERR,"***sig_alrm PATH***");
	if (wait(&status) != pid){
		syslog(LOG_ERR,"***wait system call error:%d***\n",errno);
		exit(0);
	}
 
	alarm_flag = 1;
	alarm(0);
	signal(SIGALRM,sig_alrm);
} 
 
I (CFE) have been running three copies of this for several hours on my test
machine, but have seen no failures.  I will update this OT (or try to) if
any failures occur.

[4/1/96 public]
Over the weekend, Hitachi was able to reproduce the problem and obtain
good dfstrace results, which were sent to Vikram and Susan as well as
to me.  Both sets arrived in good order.
 
They represent almost a continuous log.  The thread (pid 14551) that
first returned ETIMEDOUT after about two hours had in fact been
suspended from 3:39:28am (all times Monday 1 April 1996 JST) until
5:19:28am in a call to rpc_binding_set_auth_info(); the call was necessary
because a fresh PAG had apparently just been allocated and authenticated,
e.g. with dce_login.  When rpc_binding_set_auth_info() returned, it then
made an AFS_SetContext call to the file server (since it was a new
connection), which timed out after 600 seconds.  Since no authenticated
call to the file server had completed correctly in the previous few
minutes, the CM declared that the FX was ``down'' and returned ETIMEDOUT
to the application making the stat() call.
 
In looking at other activity to the same server, though, it seems that the
FX (the file server) started having trouble responding to RPCs after about
3:44:05am, when it responded to an unauthenticated call.  After that, calls
either to the file server itself or to the security service would hang or
fail, until about 5:19:27am, when one of the many pending AFS_SetContext
calls received a response indicating that the file server had crashed
and was returning to service, as normal, with new token grants temporarily
disabled until post-crash TSR should complete.  Various threads started
being unblocked for about eight more seconds, after which the server again
failed to respond to RPCs--before the CM had recovered completely.  At about
5:22:42am, the server started to accept RPCs again, and the CM threads started
to make progress again, but only for another 15 seconds.  Finally, at
5:30:53am, the server returned to service and stayed up.  After each of
these three suspensions in service, the server told the CM to re-negotiate
its token state because the file server was in its post-crash token
recovery phase.
 
Since the first two server up-times were so short and the CM so heavily
loaded, the CM was apparently unable to recover its state fully.  The actual
error returned to the application doing the stat() system call was due to
the ten-minute call timeout applied to an RPC call made to a server that
was down for the last several minutes of that interval.  The initial long
delay was entirely within the rpc_binding_set_auth_info() call, and I cannot
speak for why that call's behavior, why it would delay for an hour and forty
minutes and eventually succeed. 
 
There was some anomalous-seeming behavior on the part of the file server,
where it may not have been running reference-platform 1.2.1 file exporter
code.  Might this have been the case?
 
In any case, the delays and anomalous behavior are apparently due to server
crashes and reboots.  I do not know why rpc_binding_set_auth_info() might or
might not wait for the recorded hour and forty minutes before succeeding,
rather than failing earlier.  And the final failure is due to a ten-minute
timeout on the AFS_SetContext RPC call made to a server that apparently was
in the process of crashing when the call was made.

[4/26/96 public]
Cancelling.  It should have been handled by Mike Burati's code drop of
April 16 (or so).  I couldn't determine whether that drop was done under
an OT, so I didn't know if this should simply be dup'ed to that.



CR Number                     : 13372
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: LIBDCEDFS_LIBRARY_MAKEFILE not set
Reported Date                 : 2/25/96
Found in Baseline             : 1.2.1
Found Date                    : 2/25/96
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 
Affected File(s)              : src/file/fsint/Makefile
Sensitivity                   : public

[2/25/96 public]
LIBDCEDFS_LIBRARY_MAKEFILE not set

[2/26/96 public]
The build needed an export of afs4int_s2c.h

[3/6/96 public]
This change was backed out in ot 13380. See this ot for details.



CR Number                     : 13149
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fshs
Short Description             : fshs_InqContext() has old redundant cruft hanging around
Reported Date                 : 10/10/95
Found in Baseline             : 1.1
Found Date                    : 10/10/95
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : file/fshost/fshs_prutils.c
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[11/25/96 public]
Not reproducible by R1.2.2.  No part of dfs testing has encountered this
problem.

[10/10/95 public]
 
Bill just noticed that fshs_InqContext() calls both rpc_binding_inq_auth_client
and rpc_binding_inq_auth_caller() (the latter obsoletes the former in 1.1).
 
I took a look at the source and sure enough, it looks like the old code was
copied and pasted into the same routine and the function name changed, but
the old code never removed when the new code was finished.  The result is
that it does the same thing twice for no good reason.

[10/16/95 public]
Carl Burnett noticed the same thing.  I named the resulting Transarc delta in
his honor: cburnett-db7000-clean-up-delegation .



CR Number                     : 13145
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_server.ext
Short Description             : Improper parameter passing in
px_invokeserver()
Reported Date                 : 10/5/95
Found in Baseline             : 1.1
Found Date                    : 10/5/95
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : px_init.c
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]
HP platform specific.  Hasn't been seen in R1.2.

[10/5/95 public]
It appears that fxd makes
an afs_syscall() as (in file/pxd/pxd.c),

       error=afs_syscall(AFSCALL_PX, PXOP_RPCDAEMON, Flags, mainprocs,
                   tokenprocs, (int)&ioBuf);

and ioBuf is a stack variable.  Its address is passed in as above
through (in file/sys/HPUX/lclcalls_hpux.c)

        errcode = syscall(hpux_dfs_syscall, asyscall,
                          parm1, parm2, parm3, parm4, parm5);

and (in file/kutils/HPUX/syscall_hpux.c)

    osi_setuerror((*fun)(uap->parm1, uap->parm2,
                         uap->parm3, uap->parm4,
                         uap->parm5, retval));

It ends up (in file/px/px_init.c)

int afscall_exporter(parm, parm2, parm3, parm4, parm5, retvalP)
    long parm, parm2, parm3, parm4, parm5;
    int * retvalP;

calling (in file/px/px_init.c)

static px_invokeserver(maxCalls, tknCalls, parm5)
    long maxCalls, tknCalls, parm5;

in which parm5 is handled by

        struct afs_ioctl *ioctlPtr = (struct afs_ioctl *) parm5;

This is no good, since parm5 is on the user stack, and cannot be
dereferenced.  Indeed, the panic occurs later when

            if ( bufSize > ioctlPtr->out_size )  {

Osi_copyin() should be used to copy parm5 to ioctlPtr.

Following is a diff of our change:
********************************
<<< file 1: /vob/dce.src.file/src/file/px/px_init.c@@/main/HPDCE02/3
>>> file 2: px_init.c
********************************
-----[after 10 inserted 11-13]-----
>  * Revision /main/HPDCE02/4  1995/10/04  20:55 UTC  mort
>  * 	CHFts16484: Fxd causes panic.
>  * 
-----[83 changed to 86]-----
< RCSID("$Header: /vob/dce.src.file/src/file/px/px_init.c,v /main/HPDCE02/3
1995/08/12 22:46 UTC kissel Exp $")
---
> RCSID("$Header: /vob/dce.src.file/src/file/px/px_init.c,v /main/HPDCE02/4
1995/10/04 20:55 UTC mort Exp $")
-----[after 90 inserted 94]-----
> struct afs_ioctl ioctl;
-----[deleted 109 after 112]-----
< 	struct afs_ioctl *ioctlPtr = (struct afs_ioctl *) parm5;
-----[215 changed to 218-225]-----
< 	if (ioctlPtr)  { 
---
> 	if (parm5)  { 
> 	    /* Copyin parm5 */
> 	    if (code = osi_copyin((char *) parm5, (char *) &ioctl,
> 				  sizeof(struct afs_ioctl))) {
> 		uprintf("fx: can't copyin ioctl (code %d)\n", code);
> 		goto cleanup_server;
> 	    }
> 
-----[221 changed to 231]-----
< 	    if ( bufSize > ioctlPtr->out_size )  {
---
> 	    if ( bufSize > ioctl.out_size )  {
-----[226-227 changed to 236-237]-----
< 	    ioctlPtr->out_size = bufSize;
< 	    if (code = osi_copyout(lp->rpcStringBuf, ioctlPtr->out,
bufSize)) {
---
> 	    ioctl.out_size = bufSize;
> 	    if (code = osi_copyout(lp->rpcStringBuf, ioctl.out, bufSize)) {
-----[231 changed to 241]-----
< 	    if (code = osi_copyin(ioctlPtr->in, (char *) lp->principalName,
---
> 	    if (code = osi_copyin(ioctl.in, (char *) lp->principalName,



CR Number                     : 13091
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Test of OT
Reported Date                 : 9/1/95
Found in Baseline             : 1.1
Found Date                    : 9/1/95
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 
Affected File(s)              : src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/1/95 public]
This is just a test of the emergency Open Track system.  Should this actually
work, we will be amazed.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/1/95 public]
Willie, I'm cancelling this because it was just a test.  Thanks.
Changed Status from `open' to `cancel'



CR Number                     : 12999
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : Panic in reservation doing truncation
Reported Date                 : 8/2/95
Found in Baseline             : 1.0.3
Found Date                    : 8/2/95
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/2/95 public]
We encountered a panic running the fx stress tests on a pair of
machines, asserting that there should never be a reservation after
vnvm_truncate has executed.  Apparently, though, Solaris can allow a
pvn_vpzero() call that will start asynchronous activity, which can
change the reservation.
More....
Delta: dimitris-db4870-fix-reservation-in-vnm_Truncate
The idea here is to (a) remove the assertion that there is no
reservation and simply re-try the truncation to assure that it
happened, as well as to call vnvm_purgereserved after calling
vnvm_truncate.
More....
Regression test information:
- backed by build: 09.030
- tests passed: all kinds of functional tests and system tests;
                in particular, the fx stress tests now pass.
- test configuration
        Solaris 2.3, both UFS and LFS, many of each

[8/3/95 public]
Delta: dimitris-db4870-fix-reservation-in-vnm_Truncate
Change: file/episode/vnops/efs_misc.c from 4.410 to 4.422
*** file/episode/vnops/efs_misc.c
--- 4.422	1994/01/19 21:46:15
***************
*** 76,82 ****
  
  #define DEBUG_THIS_FILE EFS_DEBUG_MISC
  
! RCSID ("$Header: /project/ot/dce/d01/d29/RCS/c012999,v 1.3 95/12/21 11:03:33 root Exp $")
  
  /* Flags to pass to epia_Truncate */
  #if	defined(AFS_SUNOS5_ENV) || defined(AFS_AIX31_VM)
--- 76,82 ----
  
  #define DEBUG_THIS_FILE EFS_DEBUG_MISC
  
! RCSID ("$Header: /project/ot/dce/d01/d29/RCS/c012999,v 1.3 95/12/21 11:03:33 root Exp $")
  
  /* Flags to pass to epia_Truncate */
  #if	defined(AFS_SUNOS5_ENV) || defined(AFS_AIX31_VM)
***************
*** 424,444 ****
      }
  
      /* Flush VM */
      code = vnvm_truncate (EVTOV (evp), len, credp);
      MBZ(code);
! 
      /*
       * vnvm_truncate may have dropped the vnode lock and re-obtained it.
       * So we must re-check that there is still no reservation.
       */
       if ((evp->vd_flags & VD_RWAITERS) || EV_HAS_RESERVED (evp)) {
- #ifdef AFS_SUNOS5_ENV
- 	/* we should never get here because in Solaris vnvm_truncate doesn't
- 	 * drop the vnode lock */
- 	osi_assert(0);
- #endif
  	goto loop;
      }
  #endif	/* AFS_AIX31_ENV, AFS_SUNOS5_ENV */
  
      /* Really truncate (calling the anode layer) */
--- 424,453 ----
      }
  
      /* Flush VM */
+ 
      code = vnvm_truncate (EVTOV (evp), len, credp);
      MBZ(code);
! #ifdef AFS_SUNOS5_ENV
!     /* 
!      * Although vnvm_trucate will not drop the lock in Solaris it
!      * might cause a pvn_vpzero() which does the putpage in async
!      * mode so we can't assert that the reservation is still 0
!      * So we call vnvm_purgereserved() which on Solaris will ensure
!      * that the reservation is 0 by the time it comes back
!      */
!     vnvm_purgereserved (EVTOV (evp),
! 			evp->vd_reservation.firstReserved,
! 			evp->vd_reservation.lastReserved,
! 			credp);
! #else /*AFS_SUNOS5_ENV*/
      /*
       * vnvm_truncate may have dropped the vnode lock and re-obtained it.
       * So we must re-check that there is still no reservation.
       */
       if ((evp->vd_flags & VD_RWAITERS) || EV_HAS_RESERVED (evp)) {
  	goto loop;
      }
+ #endif /*AFS_SUNOS5_ENV*/
  #endif	/* AFS_AIX31_ENV, AFS_SUNOS5_ENV */
  
      /* Really truncate (calling the anode layer) */

[12/21/95 public]
This is a Solaris-only bug.  The fix is contained in this report, but
it doesn't make sense to include it separately in the code base at the
OSF (until the big submission).



CR Number                     : 12993
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : undo fopen()/fclose() libc.so.1 work around
Reported Date                 : 8/2/95
Found in Baseline             : 1.0.3
Found Date                    : 8/2/95
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/2/95 public]
From Transarc:
SYNOPSIS: undo fopen()/fclose() libc.so.1 work around
  The delta srinivas-4647-dfsbind_core_dump removes fcalls.o from list of
object files used to build dfsbind. fcalls.o had fopen()/fclose() wrapper for
libc.so.1

[8/3/95 public]
Delta: srinivas-4647-dfsbind_core_dump
Change: file/dfsbind/SUNOS5/machdep.mk from 1.3 to 1.4
*** file/dfsbind/SUNOS5/machdep.mk
--- 1.4	1993/11/09 15:09:37
***************
*** 1,6 ****
  VPATH				= ./${TARGET_OS}
  .if !defined(NO_LICENSE_MANAGEMENT)
! dfsbind_OFILES     	+= license.o fcalls.o 
  LIC_CFLAGS		= -D_EPISODE_LIC_MGMT_ 
  LIC_LIBS        	= -llic -lnetls -ldl
  main_helper.o_CFLAGS	+=  ${CFLAGS} ${LIC_CFLAGS}
--- 1,6 ----
  VPATH				= ./${TARGET_OS}
  .if !defined(NO_LICENSE_MANAGEMENT)
! dfsbind_OFILES     	+= license.o 
  LIC_CFLAGS		= -D_EPISODE_LIC_MGMT_ 
  LIC_LIBS        	= -llic -lnetls -ldl
  main_helper.o_CFLAGS	+=  ${CFLAGS} ${LIC_CFLAGS}

[12/8/95 public]
Applies ONLY to the Solaris port of DCE.  The OSF doesn't support that
port, or even have the files to which this port applies.



CR Number                     : 12839
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : acl
Short Description             : inconsistant acl include files
Reported Date                 : 4/11/95
Found in Baseline             : 1.1
Found Date                    : 4/11/95
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[11/25/96 public]

Cannot reproduce this problem with R1.2.2.  Not part of dfs testing has
encountered this problem.


Transarc Deltas                      [text]: 
Transarc Status                      [text]:

[4/11/95 public]
Report from cutosmer:
     1. symptom of bug
            EINVAL is returned when afs_syscall(AFSCALL_VNODE_OPS,
     VNX_OPCODE_GETACL,...) is called.
     2. cause
            In the application world, afs_syscall() refers to 
     dce/aclbase.h, while it refers to dcedfs/dacl.h in the processing
     world of the DFS kernel.  Since the actual values of (A) and (B)
     shown below do not match, EINVAL is returned to the processing for
     the ACL entry in (A).
     (1) contents of dce/aclbase.h (a header referred by an application)
	     typedef enum {sec_acl_e_type_user_obj,  /*  0 */
	     sec_acl_e_type_group_obj,               /*  1 */ 
	     sec_acl_e_type_other_obj,               /*  2 */
	     sec_acl_e_type_user,                    /*  3 */
	     sec_acl_e_type_group,                   /*  4 */
	     sec_acl_e_type_mask_obj,                /*  5 */
	     sec_acl_e_type_foreign_user,            /*  6 */   ------
	     sec_acl_e_type_foreign_group,           /*  7 */        |
	     sec_acl_e_type_foreign_other,           /*  8 */        | (A)
	     sec_acl_e_type_unauthenticated,         /*  9 */        |
	     sec_acl_e_type_extended,                /* 10 */        |
	     sec_acl_e_type_any_other                /* 11 */   ------
		     :
	     } sec_acl_entry_type_t;
     (2) contents of "dcedfs/dacl.h" (a header referred by the DFS kernel)
	     typedef u_int32 dacl_entry_type_t;
	     #define dacl_entry_type_user_obj        0
	     #define dacl_entry_type_group_obj       1
	     #define dacl_entry_type_other_obj       2
	     #define dacl_entry_type_user            3
	     #define dacl_entry_type_group           4
	     #define dacl_entry_type_mask_obj        5
	     /*                                             -------
	     #define dacl_entry_type_class_group     5            |
	     #define dacl_entry_type_class_owner     6            |
	     #define dacl_entry_type_class_other     7            |
	     */                                                   |
	     #define dacl_entry_type_foreign_user    8            | (B)
	     #define dacl_entry_type_foreign_group   9            |
	     #define dacl_entry_type_foreign_other   10           |
	     #define dacl_entry_type_unauth          11           |
	     #define dacl_entry_type_extended        12           |
	     #define dacl_entry_type_anyother        13     -------

[1/24/96 public]
The cause described above has no relation to the symptom you are
seeing.  Nowhere in the getacl syscall, do you indicated type of entry
you are desiring to look at. What you do specify is
which ACL you want object or IO or IC acl.  For the afs syscall, you
need to use one of the following defined in src/file/sysacl/aclint.h:
#define VNX_ACL_REGULAR_ACL	0	/* regular */
#define VNX_ACL_DEFAULT_ACL	1	/* default for (sub)directories */
#define VNX_ACL_INITIAL_ACL	2	/* default for (sub)files */
You get the whole acl in *DFS ACL format*, not in the DCE security ACL
format.  Why, You do not want to do ACL format conversions in kernel
space.
You do not make it clear the end objective you are seeking. Taking a
reasonable guess, if you desire to use the system call to get the DFS
acl and use it in application program that understands the DCE
security acl format, you need to convert the DFS acl to the DCE acl
format on the getacl and viceversa on the setacl. There are two
functions in the src/file/security/dacl/dacl_sec_acl.c that will do
this for you sec_acl_ParseAcl to convert DFS acl to DCE acl and
sec_acl_FlattenAcl for the reverse.
Hope this helps
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `'



CR Number                     : 12704
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfs acl getset test fails
Reported Date                 : 10/21/94
Found in Baseline             : 1.1b22
Found Date                    : 10/21/94
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[10/21/94 public]

Here is the log file for the acl getset test that was run in an episode
file system:

... lines deleted
+ expr /.../zeugma.test.cell/fs/acl/scratch/getset : ^\/\.\.\.\/.*
PATH_TYPE=cmpath
+ export ALLPERMS
+ export CUR ROOT RDIR LOGFILE EXECUTION_DIRECTORY
+ export TESTACL_RGY_DATABASE_CMD
+ export TESTACL_GROUP_LIST TESTACL_USER_LIST TESTACL_GROUP_MEMBERS
+ export TESTACL_GROUP_COUNT TESTACL_USER_COUNT
+ export TESTACL_RGY_SETUP TESTACL_RGY_CLEANUP
+ export TESTACL_CREDS_FILE
+ export SETUP_RGY_DATABASE
+ export CELL_ADMIN_PASSWD COMMON_USER_PASSWD TESTACL_COMMON_USER_PASSWD
+ export DCE_LOGIN
+ export COM ACL_EDIT_FLAGS FLAGS
+ export AWK
+ export EXECTRACE trace_only
+ export PATH_TYPE
+ common_setup
+ setup_rgy_database
+ [ off = off ]
+ [ done = not_done ]
+ [ -d /.../zeugma.test.cell/fs/acl/results ]
+ [ -d /.../zeugma.test.cell/fs/acl/scratch ]
+ [ getset != top_level_driver ]
+ [ ! -d /.../zeugma.test.cell/fs/acl/results/getset ]
+ rm -fr /.../zeugma.test.cell/fs/acl/results/getset/ERROR /.../zeugma.test.cell
/fs/acl/results/getset/STATUS
+ [ -d /.../zeugma.test.cell/fs/acl/scratch/getset ]
+ rm -fr /.../zeugma.test.cell/fs/acl/scratch/getset
rm: 0653-602 Cannot read /.../zeugma.test.cell/fs/acl/scratch/getset/testdir3.
rm: 0653-611 Directory /.../zeugma.test.cell/fs/acl/scratch/getset is not empty.
+ [ 2 -ne 0 ]
+ error_exit Unable to remove execution dir /.../zeugma.test.cell/fs/acl/scratch
/getset
+ echo Unable to remove execution dir /.../zeugma.test.cell/fs/acl/scratch/getse
t
+ exit 1

[10/24/94 public]
Bob, I've seen something like this before.  It looks like the acl 
test was just trying to clean up from a previous (possibly aborted) 
run and couldn't remove one of the directories because of permission 
problems.  This could happen because cell_admin != root.  If the 
permissions on the testdir3 directory didn't allow cell_admin to 
read the directory, you might get this error.  Try doing whatever
chmods are necessary then removing everything under ../scratch/getset 
by hand and re-running the test.

[10/25/94 public]
I was able to rerun the tests without any failures. I am certain the
reason the test failed because of a permission problem.



CR Number                     : 12686
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : osi
Short Description             : Remove superfluous dacl i/f's from libdce
Reported Date                 : 10/20/94
Found in Baseline             : 1.1
Found Date                    : 10/20/94
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]
HP platform specific.  Hasn't been seen in R1.2.

[10/20/94 public]

dce1.1 libdce.sl has the unresolved symbol:

$ show_remaining_imports /opt/dcelocal/bin/dced
- --------- Imports That Must Be Resolved Elsewhere
      osi_GetNGroups     Code /usr/lib/libdce.sl

This is coming from file/security/dacl/dacl_pac.c and the following objects
have the reference to osi_GetNGroups .

  ./security/dacl2acl/dacl_pac.o
  ./security/dacl_lfs/dacl_pac.o
  ./security/dacl_lfs.klib/dacl_pac.o

I think that this is caused by the wrong version of osi_cred_mach.h
exported by file/osi/Makefile. There are HP800/osi_cred_mach.h and
HPUX/osi_cred_mach.h and HPUX/osi_cred_mach.h gets exported to the export
tree because of VPATH=${TARGET_OS} in HPUX/Makefile .

BTW, both HPUX and HP800 directories have:
  osi_buf_mach.h
  osi_cred_mach.h
  osi_net_mach.h
  osi_port_mach.h
  osi_uio_mach.h
One of them must be wrong.

[10/24/94 public]
Yes, there are two copies of those files and you might be
able to delete the HP800 version (I don't know, I haven't
done an exhaustive search).

However, I think the real problem is that the functions 
dacl_PacFromUcred and dacl_PacListFromUcred are being
included in libdce.  These functions in turn call
osi_GetNGroups() which is undefined in libdce.  These 
functions should only called from within the kernel, and 
in certain user level test environments.  They are not
referenced anywhere in libdce and they have no business 
being included there.  So, I think the solution is to
put a #if !defined(DACL2ACL) around those two functions
to remove them from dacl_pac.o when built in the dacl2acl
directory since the version in this directory is solely
for libdce consumption.  This is low priority since it
causes no test failures and only serves to increase the
size of libdce slightly, so I'm deferring it to 1.2.

[10/24/94 public]

Just FYI: You are so lucky that the unresolved symbol doesn't prevent the
process startup. :-) The -Bnonfatal ld option used for the DCE build allows
it. (But, I don't think that this option is necessary anymore. It's there
for a historical reason...) I wonder what happens if linking with the
archived library, libdce.a ...



CR Number                     : 12621
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : gateway
Short Description             : functional tests fail
Reported Date                 : 10/13/94
Found in Baseline             : 1.1b21
Found Date                    : 10/13/94
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]
HP platform specific.  Hasn't been seen in R1.2.

[10/13/94 public]

 The functional tests in .../file/gateway/dfs_login/test3 fail although
the same commands succede when entered individually at the command line.



CR Number                     : 12620
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : gateway
Short Description             : bad dfs_logout exit value
Reported Date                 : 10/13/94
Found in Baseline             : 1.1b21
Found Date                    : 10/13/94
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]

HP platform specific.  Hasn't been seen in R1.2.

[10/13/94 public]

 the command line

 > dfs_logout

 gives an error of

  dfs_logout: cannot determine default host

 but returns an exit value of 0 to the shell. This causes a failure in
 .../test/file/gateway/dfs_login/test2



CR Number                     : 12369
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : DFS leaving cred files behind
Reported Date                 : 9/27/94
Found in Baseline             : 1.1b18
Found Date                    : 9/27/94
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/27/94 public]

After running for several days it becomes apparent that DFS is leaving
expired credential files in the /opt/dcelocal/var/security/creds
directory.  I am not sure which DFS server is causing this, but by
using strings on the .data files it can be seen that the dfs-server
principal is responsible.

If left alone this causes the partition that
/opt/dcelocal/var/security/creds is on to run out of space and then
all of DCE will start having problems.

[9/27/94 public]
Assigned to me.  Mike, this looks related to OT12179.  Do you agree?

[10/3/94 public]
No, I don't think it's related.  I have no idea why it would be doing this.
Are you sure it's DFS leaving them behind, or is it tests running on that
system leaving them behind?

[10/24/94 public]
It serves no useful purpose to leave this open for 1.2, so it is
cancelled.  The problem may have been related to test programs leaving
creds behind, or maybe there simply wasn't enough room allocated for
creds.  In any case more data needs to be collected next time it
occurs for useful progress to be made (data on usage history, number
of cred files, size of cred files, total space used, etc.).



CR Number                     : 12358
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 12315
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : dfs.read_write_all test fails w/ Conn timeouts
Reported Date                 : 9/27/94
Found in Baseline             : 1.1b18
Found Date                    : 9/27/94
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/27/94 public]

BUILD:	18.2
CONFIG:DFS on, AUDITING off - 9 hour dfs.read_write_all FAILED 
	plus DTS COMM FAILURES - unclear if any correlation exists
dce11 (h):	sec dts flserver (dfs)
dce8 (h):	cds dts flserver (dfs)
wolfboy (r):	dts flserver (dfs)

Attempted to run dfs.read_write_all.main w/ 3 users for 9 hours
overnight, only 1 user (on dce8) completed successfully. The
wolfboy user only lasted 6 hours, the dce11 user 8 hours.

wolfboy user:
tar: etc/zoneinfo/SystemV/AST4ADT: Cannot write extracted data: Connection timed out
check_CR[11]: dfs_1.tmplog: cannot create
tar_and_compress[27]: /:/dfsusers/dfs_1/utils/etc: bad directory
check_CR[11]: dfs_1.tmplog: cannot create
dfs_1 on wolfboy: FAILED - check the logs. Exiting ...

dce11 user:
check_CR[11]: dfs_2.tmplog: cannot create
dfs.test.Z: Connection timed out
tar: cannot open dfs.test
check_CR[11]: dfs_2.tmplog: cannot create
tar_and_compress[27]: /:/dfsusers/dfs_2/utils/etc: bad directory
dfs_2 on dce11: FAILED - check the logs. Exiting ...

DTS ERRORS:
dce11
1994-09-26-22:19:00.771-04:00I----- dtsd ERROR dts dtsdate time_request.c 685 0x
401bf508 RPC call ClerkRequstTime() to remote server failed: Communications fail
ure (dce / rpc)
1994-09-26-22:19:09.427-04:00I----- dtsd ERROR dts dtserror dtss_service_global_
set.c 478 0x7aff8388 Couldn't get remote server's principal name: Communications
 failure (dce / rpc)
1994-09-27-07:54:04.016-04:00I----- dtsd ERROR dts dtserror dtss_service_global_
set.c 478 0x7aff8388 Couldn't get remote server's principal name: Communications
 failure (dce / rpc)

dce8
1994-09-26-20:13:09.010-04:00I----- dtsd ERROR dts dtsdate time_request.c 685 0x
401bd458 RPC call ClerkRequstTime() to remote server failed: Communications fail
ure (dce / rpc)
1994-09-27-00:11:28.605-04:00I----- dtsd ERROR dts dtsdate time_request.c 685 0x
401bf158 RPC call ClerkRequstTime() to remote server failed: Communications fail
ure (dce / rpc)
1994-09-27-00:22:05.964-04:00I----- dtsd ERROR dts dtserror dtss_service_global_
set.c 478 0x7aff8388 Couldn't get remote server's principal name: Communications
 failure (dce / rpc)
(next 2 AFTER the test completed)
1994-09-27-05:52:27.982-04:00I----- dtsd ERROR dts dtsdate time_request.c 685 0x
401bf158 RPC call ClerkRequstTime() to remote server failed: Communications fail
ure (dce / rpc)
1994-09-27-05:52:34.633-04:00I----- dtsd ERROR dts dtserror dtss_service_global_
set.c 478 0x7aff8388 Couldn't get remote server's principal name: Communications
 failure (dce / rpc)

wolfboy
NO DTS ERRORS DURING OR AFTER TEST

Assigning to myself so I can rerun this (I had not rebooted/restarted
since a successful 48 hour cthon run so memory leaks may be involved)
in this cell AND in an HP-only cell.

[9/27/94 public]
Noticed that the dfsbind processes had clocked a lot of time so this
may be just another instance of the performance problems blocking
the fts tests, causing dfsbind to spin etc. Queueing this behind
verification of the 10766 fix.

[9/30/94 public]
All 3 users are still running after 15 hours on BL-20! There have only
been warnings during the test, no DTS errors! Test needs to make the
48 hour mark to PASS but it's well on its way. The dfsbind process
on all machines is not clocking anywhere near the amount of time it
had been.

[10/3/94 public]
Test ran successfully for 42.5 hours - at which point 1 of the users
got a Conn timeout failure - the other users completed with no problems ...
pretty darn close eh? Investigating to see if I can find out what
happened during hour # 42 ...

[10/4/94 public]
Because it made it beyond the 40 hour mark, lowering priority to 1.
If Gail Marie gets more data on the failure, there is always the
potential for bumping this up again.

[10/4/94 public]
Whoops.  Really update the priority.

[4/24/96 public]
This defect is no longer reproducible, so I am cancelling the ot.
In any case, I have recently made changes to this test that enable
it to run smoothly on IBM and Solaris platforms. In particular, I
have run 48 hour CHO (including this test) on IBM platform. The
revised test code will be included in 1.2.2.



CR Number                     : 12281
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Something is slowing down DFS...
Reported Date                 : 9/21/94
Found in Baseline             : 1.1b18
Found Date                    : 9/21/94
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/21/94 public]

This is a place holder.

We are seeing severe slow downs of DFS functional and system tests.
I am investigating.....



CR Number                     : 12239
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm fvt test3 fails to set cache size
Reported Date                 : 9/19/94
Found in Baseline             : 1.1b18
Found Date                    : 9/19/94
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/19/94 public]

The following failures occur when running the cm fvt tests as root/cell_admin

cm setcachesize 12345
cm: Invalid argument.
- failed (test3 E13)

cm getcachesize | grep 12345
- failed to set cachesize properly (test3 E14)

cm setcachesize 0
cm: Invalid argument.
- failed to set cachesize properly (test3 E15)

cm setcachesize 1
cm: Invalid argument.
- failed (test3 E19)

cm setcachesize -reset
cm: Invalid argument.
- failed (test3 E22)

[9/27/94 public]

Canceled.  My mistake.  I was trying to run the tests against a memory
cache.  These operations are only valid for disk caches.



CR Number                     : 11779
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : i486
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : /file/ftutil
Short Description             : don't know how to make ftu_errs.ms
Reported Date                 : 8/19/94
Found in Baseline             : 1.1
Found Date                    : 8/19/94
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/19/94 public]

[ /file/ftutil at 12:24 (PM) Friday ]
makepath ftutil/. && cd ftutil &&  exec make MAKEFILE_PASS=AUTOGEN     build_all
make: don't know how to make ftu_errs.msf (continuing)
[ /file/ncscompat at 12:24 (PM) Friday ]

[8/19/94 public]
DFS is not supported on the OSF/1 platform.  Cancel.



CR Number                     : 11662
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : run_commands.dfs
Short Description             : cd /: FAILED!
Reported Date                 : 8/12/94
Found in Baseline             : 1.1
Found Date                    : 8/12/94
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/12/94 public]

fts lsfldb
+ /u1/opt/dcelocal/bin/fts lsfldb 

root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               budapest.osf.org    RW       lfs_aggr1 0:00:00 hosts/budapest <nil>               
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed; 0 wrong aggr type)
+ [ 0 != 0 ] 
+ sleep 180 
+ echo cd /: 
cd /:
+ cd /: 
/u1/RAT_tools/run_commands.dfs: /:: bad directory

[8/12/94 public]
I couldn't reproduce this problem on the smoketest platform.
I've suggested the RATs modify thier script to sleep 240
seconds rather than 180 since the TSR interval is really 
3 minutes 20 seconds.  Peter is attempting to reproduce now
on the smoketest platform.

[8/12/94 public]
Peter ran the smoketest again with the larger TSR sleep time and 
couldn't reproduce the problem (the smoke was successful).  I'm 
taking the drastic step of cancelling this bug.  If it occurs 
again, Peter can re-open this.

[8/15/94 public]

This error complains about servers not being up 
but upon checking budapest and sif, secd, dced, cdsd
were all running with the exception of dcecp

fts lsfldb
+ /u1/opt/dcelocal/bin/fts lsfldb 
Could not access the FLDB for attributes
Error: no servers appear to be up (dfs / ubk)
+ [ 1 != 0 ] 
+ echo fts lsfldb FAILED! 
fts lsfldb FAILED!
+ sleep 240 
+ echo cd /: 
cd /:
+ cd /: 
/u1/RAT_tools/run_commands.dfs: /:: bad directory

[8/15/94 public]
Peter, a change was made to the dfs_config script last week.  Have
your scripts been updated to deal with this?  Talk to Andy McKeen
for details.

[8/15/94 public]

Yes!  All scripts have been updated last week.



CR Number                     : 11355
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : dfs.lock loses command args
Reported Date                 : 7/19/94
Found in Baseline             : 1.1
Found Date                    : 7/19/94
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[7/19/94 public]

dfs.lock cannot parse the command line args because they are being
lost by the HP-UX shell when a function is called.

[8/25/94 public]

This turns out to be a known bug with the HP-UX shell. The symptoms
when running the test are that only the default data file (lock.data)
is used, regardless of the value you pass with -f. The workarounds are
to use either the HP-UX /bin/posix/sh OR to make sure lock.data contains
the values you want (which is what I do).

Cancelling as this is not a bug with the test. Sending info to dce-relnotes.



CR Number                     : 11326
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : file
Short Description             : multi-machine smoke test failed
Reported Date                 : 7/15/94
Found in Baseline             : 1.1
Found Date                    : 7/15/94
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[7/15/94 public]

The multi-machine smoke test failed.  The baseline used was 
DCE1.1-bl12.

The failure in the log file is:

    ... Exiting wait_and_see
    dfs.read_write_all.main FAILED
    Exiting /dcetest/dcelocal/test/systest/file/dfs.read_write_all.main with 1 - program terminated at Fri Jul 15 13:45:18 EDT 1994

The smoke machines are:
    sif, budapest and mudslide

The whole test will take ~ 3hours to recreate, we do not have
the time to recreate this again today.  So, if you want to debug
this, then be aware that the machines will be rebooted at 3 am 
daily.  

We have proven that the problem is NOT with our process.  The 
set up is:

    1) reboot sif
    2) mount bl-12 on /mnt on sif
    3) run core dce_config on sif

    4) reboot budapest
    5) mount bl-12 on /mnt on budapest
    6) run core dce_config on budapest

    7) reboot mudslide
    8) mount bl-12 on /mnt/tailor on mudslide
    9) run core dce_config on muslide

   10) run do_test.dfs on budapest

   11) run do_test.dfs on mudslide 

Let me know if you need more information.  

thanks
annie

[7/15/94 public]
Yes, I need more information. Can you please tail the /tmp/logs/*.log files
on the machine where dfs.read_write_all.main is run? Thanks.

[7/15/94 public]
I believe this may be a machine setup problem,unrelated to DCE or
DFS. I had to add each machine to each other's /.rhosts file
so that remote shells that the test relies on could work ... I'm
still having trouble doing a remote shell/dce_login -c command
from mudslide to budapest ...

[7/29/94 public]
The multi-machine smoketest passed yesterday once the workaround
for CR 11423 was done by hand. Unclear what the problem was that
caused the mult-machine smoketest to fail before 11423 was introduced
but I'm canceling this since we've had a successful run.



CR Number                     : 11216
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Document that root receives self identity/permissions
Reported Date                 : 7/8/94
Found in Baseline             : 1.0.3
Found Date                    : 7/8/94
Severity                      : B
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : aclgroup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[7/8/94 public]
Users who log into a machine as root without authenticating to DCE assume
the identity of the machine's self principal.  They receive the permissions
associated with the self idenity when they access data stored in DFS via its
DCE pathname.  Details related to this need to be worked into the docs.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[7/9/94 public]
I'm canceling this defect, which I mistekenly opened just yesterday.  The
change it describes is not supported in OSF DCE 1.1.  Sorry for the confusion;
I misunderstood the nature of the change.
 
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 10952
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : possible runtime error
Reported Date                 : 6/15/94
Found in Baseline             : 1.1
Found Date                    : 6/15/94
Severity                      : A
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]

HP platform specific.  Hasn't been seen in R1.2.

[6/15/94 public]

While running the DFS core test on budapest, and mudslide
they both crashed during the test. It is not clear as to
what caused the crash, whether it was from the DFS test
or some other circumstance. However the following is output
from the test log which is not very helpfull. I have
also included output from the console at the time of the crash
We reran on budapest with noproblems encountered.

******************** ERROR LOG **********************************


Current site is: registry server at /.../sif_cell/subsys/dce/sec/master
Domain changed to: group
Current site is: registry server at /.../sif_cell/subsys/dce/sec/master
Domain changed to: group
S:****** Starting bosserver...
1994-06-14-12:26:28.594-04:00I----- dtsd WARNING dts config 0x7aff8460
Too fews servers (2), need 3 servers
Checking for a Ubik sync site in  hosts/budapest
Host /.:/hosts/budapest is now the sync site
*** /dev/rdsk/c201d2s0 ALREADY CONTAINS AN EPISODE FILE SYSTEM
*** Using default initialempty value of 1.
*** Using default number of (8192-byte) blocks: 81867
*** Defaulting to 818 log blocks                              (maximum of 90 con
current transactions).
Done.  /dev/rdsk/c201d2s0 is now an Episode aggregate.
/dev/rdsk/c201d2s0: Marked as not a BSD file system any more.
S:****** Partition /dev/dsk/c201d2s0 successfully initialized.
1994-06-14-12:28:56.096-04:00I----- dtsd WARNING dts config 0x7aff8460
Too fews servers (2), need 3 servers
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
budapest            RW       lfs_aggr1 0:00:00 hosts/budapest <nil>

Fileset 0,,1 created on aggregate lfs_aggr1 of budapest
S:****** Starting dfsbind...
S:****** Starting fxd...


********************** CONSOLE OUTPUT ********************************

15:43  Tue Jun 14 1994.  Reboot after panic: @(#)B2352A HP-UX (C.09.01) #1: Mon Jun 28 15:27:29 EDT 1993 with HP DFS/9000 1.0@
17:00  Tue Jun 14, 1994.  Reboot:

[6/6/94 public]

This is a long standing (and apparently infrequent) problem.  fxd creates threads
for rpc's via pthread_create().  This in turn calls newproc(), which calls
growpreg() in the child process.  The growpreg() fails for unknown reasons (we 
can't tell why without kernel source).  

Rick Kissel tells me HP is aware of this problem, but it happens so
infrequently that they haven't really adressed it.  I've lowered the
bug to B2 until we see how often it happens.

[7/13/94 public]
I just hit this, single machine HP cell.  The telltale panic message
is: "pthread_create: could not grow user stack".



CR Number                     : 10715
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : file/ncscompat
					     /file/ncsubik
					     /file/flserver
					     /file/px
Short Description             : rpc_s_unknown_status_code" undefined
Reported Date                 : 5/20/94
Found in Baseline             : 1.1
Found Date                    : 5/20/94
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : file/ncscompat/compat_err.
Sensitivity                   : public

[5/20/94 public]

c89 -c    -D_SHARED_LIBRARIES  +z  -Dunix -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCA\
L_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DH\
PUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/ncscompat\
 -I/project/dce/build/dce1.1-snap/src/file/ncscompat  -I/u3/devobj/sb/nb_ux/export/hp800/usr/\
include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.\
1-snap/src/file/ncscompat/compat_err.c
cc: "/project/dce/build/dce1.1-snap/src/file/ncscompat/compat_err.c", line 95: error 1588: "r\
pc_s_unknown_status_code" undefined
/file/ncsubik
rpc_s_mod" undefined
/file/flserver
rpc_s_mod" undefined
 /file/px
rpc_s_mod" undefined

[5/25/94 public]
This bug is garbage from the rpc drop.
Canceling.



CR Number                     : 10626
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfs should not DIE when server is not responding
Reported Date                 : 5/12/94
Found in Baseline             : 1.1a
Found Date                    : 5/12/94
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[5/12/94 public]

We have discussed this problem in the DFS meeting.  (OSF dfs engineers,
please execuse/pardon me if I am NOT using the 'correct' term to phrase
what I am trying to log here.  Feel free to change or add to this.)

Occassionally, during the complete build of DCE1.1a in a self-hosting
cell, the process dies and kills the build.  

Prior to the dead of the process, I got a bunch of messages from the
kernel (I know this is from the kernel because I have redirected both
the standard output and errors to my build log and there were NO other
processes running except mine.) as followed:

1  root@sofia $ tail /:/project/dce/build/nb1_1a_hp800/logs/build.log.full.root
2  dfs: rpc errors (code 382312711) from the fx server 130.105.4.200 in cell sisyphus.osf.org
3  tail: cannot open input
4  Memory fault
5  root@sofia $ ls
6  . not found
7  root@sofia $ rm core
8  rm: cannot stat core: Connection timed out
9  root@sofia $ ls /:/project/dce/build/nb1_1a_hp800/logs/build.log.full.root
10  /:/project/dce/build/nb1_1a_hp800/logs/build.log.full.root not found
11  root@sofia $ ls /:/project/dce
12  /:/project/dce not found
13  root@sofia $ ls /:
14  /: unreadable
15  root@sofia $ Pid 8399 killed due to text modification or page I/O error
16  dfs: file server 130.105.5.69 in cell sisyphus.osf.org back up!
17  Pid 23637 killed due to text modification or page I/O erro
18
19  root@sofia $ Pid 8441 killed due to text modification or page I/O error
20  l
21  cd, bsubmit.log, build.cache_fix.out, build.ksh, build.log,
22   Pid 8399 killed due to text modification or page I/O error
23  dfs: file server 130.105.5.69 in cell sisyphus.osf.org back up!
24  Pid 23637 killed due to text modification or page I/O error
25
26  root@sofia $ Pid 8441 killed due to text modification or page I/O error
27  l
28  cd, bsubmit.log, build.cache_fix.out, build.ksh, build.log,
29  build.new.out, build.out, build.out., build.out.home, build.out.nocache,
30  build.out.pfy, build.self_host.ksh, dfs-table.template, driver.self_host.sh,
31  driver.sh, parse, parse.ksh, path.save, test.sh
32  root@sofia $ /build/nb1_1a_hp800/logs/JOBS_DIE_05-11 
<
33  root@sofia $ jobs
34  [1] +  Running                 /:/project/dce/build/nb1_1a_hp800/scripts/driver.self_host.sh > /:/project/dce/build/nb1_1a_hp800/logs/build
35  Done                    /:/project/dce/build/nb1_1a_hp800/scripts/driver.self_host.sh > /:/project/dce/build/nb1_1a_hp800/logs/build.log.full.root 2>&1 &
36  root@sofia $ ==> cd usr/include/dce
37  ksh: ==:  not found
38  root@sofia $ tail /:/project/dce/build/nb1_1a_hp800/logs/build.log.full.root
39  + return
40  + [ true = true ]
41  + [ false = true ]
42  + build_dce
43  + date
44  + print Building DCE at Wed May 11 11:27:24 EDT 1994...
45  + 1>> /.../sisyphus.osf.org/fs/project/dce/build/nb1_1a_hp800/logs/.log.05-11-94 2>& 1
46  + cd src
47  + build BENCHMARK_BUILD=ON -sb nb1_1a_hp800 BUILD_OPT=-sb nb1_1a_hp800 -rc /.../sisyphus.osf.org/fs/project/dce/build/.sandboxrc -k build_all
48  + 1>> /.../sisyphus.osf.org/fs/project/dce/build/nb1_1a_hp800/logs/build.log.05-11-94 2>& 1
49  root@sofia $ tail /.../sisyphus.osf.org/fs/project/dce/build/nb1_1a_hp800/logs/build.log.05-11-94
50  sh: makepath: not found
51  *** Error code 1
52  `build_all' not remade because of errors.
53  *** Error code 1
54  exec make MAKEFILE_PASS=BASIC  _SUBMAKE_=_IS_SUBMAKE_  build_all
55  getwd: Connection timed out
56  *** Error code 2
57  rm: cannot stat /.../sisyphus.osf.org/fs/project/dce/build/nb1_1a_hp800/src/../obj/hp800/pe_cache: Connection timed out
58  *** Error code 2
59  `build_all' not remade because of errors.

[6/24/94 public]
Cancelled.  The error code is "helper catatonic", but this could
have been caused by a network partition.  There is not enough 
information in this bug report to pursue this problem further.  
We will continue to watch for it in self-host builds.



CR Number                     : 10538
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : DFS server crashes during
multi-machine smoketest
Reported Date                 : 5/4/94
Found in Baseline             : 1.1
Found Date                    : 5/4/94
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : dce_config
Sensitivity                   : public

[5/4/94 public]

In the RaT multi-machine smoketest, when running dce_config to install
and configure the DFS server (the HP budapest) for the cell, budapest
crashes, with a damaged filesystem that requires "fsck" to be run to
repair it.

Here is the output from "dce_config" just before the machine crashes:


S:****** Loading kernel extensions...
initializing Episode:  syscall return value = 0
S:****** Modifying the registry database for DFS server operation...

>>> group member added

>>> group member added

Current site is: registry server at /.../sif_cell/subsys/dce/sec/master
Domain changed to: group
Current site is: registry server at /.../sif_cell/subsys/dce/sec/master
Domain changed to: group
S:****** Starting bosserver...
Checking for a Ubik sync site in  hosts/budapest
Host /.:/hosts/budapest is now the sync site

*** /dev/rdsk/c201d2s0 ALREADY CONTAINS AN EPISODE FILE SYSTEM
*** Using default initialempty value of 1.
*** Using default number of (8192-byte) blocks: 81867
*** Defaulting to 818 log blocks (maximum of 90 concurrent transactions).
Problem with making /dev/rdsk/c201d2s0 an Episode aggregate.

S:****** Forcing /dev/dsk/c201d2s0 to be a fresh Episode aggregate...
*** /dev/rdsk/c201d2s0 ALREADY CONTAINS AN EPISODE FILE SYSTEM
*** CONTINUING
*** Using default initialempty value of 1.
*** Using default number of (8192-byte) blocks: 81867
*** Defaulting to 818 log blocks (maximum of 90 concurrent transactions).
Done.  /dev/rdsk/c201d2s0 is now an Episode aggregate.
/dev/rdsk/c201d2s0: Marked as not a BSD file system any more.
S:****** Partition /dev/dsk/c201d2s0 successfully initialized.
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
budapest.osf.org    RW       lfs_aggr1 0:00:00 hosts/budapest <nil>
Fileset 0,,1 created on aggregate lfs_aggr1 of budapest
S:****** Starting dfsbind...
S:****** Starting fxd...

[5/4/94 public]
Ron, it is unlikely this is a DFS code problem since the code
has not changed since 103a in your tree.  Can this reliably be 
reproduced?

[5/5/94 public]
Well, as luck would have it, this is not reproducible using today's
build.  It's harder to write this off to user error since the 
Rats use scripts for the smoketest that have not changed in a long 
time.  Additionally, I don't see any suspicious RPC changes going
into the build yesterday.  In any case, we've set up the HP machine 
to take a crash dump the next time it occurs.  We'll lower the 
priority on this one while we wait...

[8/10/94 public]
Haven't since this lately (or perhaps it is related to OT11070?).
In any case, there's not much value in leaving this hanging around
at this point.  Cancelling.



CR Number                     : 10379
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : other
S/W Ref Platform              : other
Component Name                : dfs
Subcomponent Name             : DCE Admin Reference
Short Description             : fts lsquota reference page missing
Reported Date                 : 4/18/94
Found in Baseline             : 1.0.3
Found Date                    : 4/18/94
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : none
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[4/18/94 public]
In the DCE Administration Reference (1.0.3 version), the fts lsquota
reference page is missing.

[4/18/94 public]
At DCE 1.0.3, the reference page for the fts lsquota command was in the
DCE User's Guide and Reference (it's a user-level command).  Per OSF
reorganization, it will be included in the new DFS reference guide for
1.1.  Hal, please cancel this one.
Filled in Interest List CC with `hal@osf.org' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `none' 
Changed Responsible Engr. from `hal' to `jeff@transarc.com' 
Changed Resp. Engr's Company from `osf' to `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `closed'

[4/18/94 public]
As of this date the reference page exists is located in the
dce_books/command_ref/man8dfs directory, as  fts_lsquota.8dfs.  While this
is not the final resting place of this ref page, it does exist, so I am
canceling the CR.



CR Number                     : 10365
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : remove redundant tracing statements
Reported Date                 : 4/15/94
Found in Baseline             : 1.1a
Found Date                    : 4/15/94
Severity                      : D
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[4/15/94 public]

We are proposing to remove the dprintfs and the dmprintfs for
which there is already a corresponding calls to icl_Trace.
Seems that we don't need debugging printf's which are turned on via a compile
switch when we have icl.  With the advent of servicability, there is 
just going to be too many ways of logging information.

Scream now if it hurts.

[06/08/94 public]

I submitted this, but somebody overwrote my changes, and
now I don't have time to fix this before I leave, therefore
I'm cancelling it.



CR Number                     : 10298
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : dfsexport core dump
Reported Date                 : 4/5/94
Found in Baseline             : 1.1a
Found Date                    : 4/5/94
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[4/5/94 public]


"dfsexport -detach -all" coredumps if dfsatab does not exist. 



Child died due to: bus error
WARNING: /usr/lib/end.o was not linked with this program (UE836)
         Shared-library debugging cannot be made available (UE837)
Procedures:      0
Files: 0
>Sig  Stop    Ignore  Report  Name
 20  No      No      Yes     virtual time alarm
>Sig  Stop    Ignore  Report  Name
 20  No      No      No      virtual time alarm
>t
 0 DMEM + 0x0d27d008 (0, 0, 0, 0)
 1 ftu_AggrSyscall + 0x00000128 (0, 0, 0, 0)
>

[9/15/94 public]
Hasn't been seen in months.  Seems to work for me, both 
platforms.  Cancel.



CR Number                     : 10225
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : porting bug in HPUX igetinode (OSI)
Reported Date                 : 3/24/94
Found in Baseline             : 1.0.3a
Found Date                    : 3/24/94
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : osi/HPUX/osi_fio.c
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96  public]

HP platform specific.  Hasn't been seen in R1.2.

[3/24/94 public]
We recently stumbled upon the following bug in igetinode()
(osi/HPUX/osi_fio.c).

For HP-UX: if iget() returns a not-in-use inode with i_mode==0, it
must be returned via idrop() instead of iput().  Calling iput() in this
situation will result in a guaranteed panic.  I'm not sure why/how we hit
this error case for the first time "ever" (since the original port, at
least) ... that might be indicative of another problem somewhere else
as well.

We've changed the code to the following and are currently in the
process of testing it.  Although it seems correct, we don't have any
easy way of forcing a running system to go down this path.

	/* For hpux: not sure if we could get away with JUST checking
	 * i_mode.  To be safe, look for both.  If i_mode==0, MUST call
	 * idrop() instead of iput(). */
	if ((ip->i_nlink == 0) || (ip->i_mode == 0)) {
	    if (ip->i_mode == 0)
		idrop(ip);
	    else
		iput(ip);
	    osi_setuerror(ENOENT);
	    return;
	}

[4/20/94 public]
Daryl,  Any test results to share on this one?  We're looking
at integrating it here. Thx

[8/26/94 public]
No ... this was found by inspection.  It's a fairly obscure error case that
I wouldn't even know how to generate.



CR Number                     : 10166
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : acls
Short Description             : cell_admin overpowerful?
Reported Date                 : 3/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 3/17/94
Severity                      : D
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[3/17/94 public]
 
The self-host cell's cell_admin has more privileges than I expected -
is this correct?
 
- directory owned by hosts/barge/self (20001) who alone has c right
- log in as cell_admin
- acl_edit and "get" access rights - only have rx
- change acl and commit successfully!
 
gmd@garlic(36) dce_login cell_admin 
Password:
gmd@garlic(3) cd ..
gmd@garlic(4) acl_edit julie
sec_acl_edit> g
Granted permissions: r-x---
sec_acl_edit> m any_other:rwxi
sec_acl_edit> commit
sec_acl_edit> exit

[3/17/94 public]
It is by design that cell_admin (well, anybody who's in the group specified
by the argument to fxd at startup) can edit any ACL on that server.
Since acl_edit doesn't know about that convention, or that special group,
it can't guess what access is to be allowed.  Looks like a feature.
Filled in Interest List CC with `cfe@transarc.com'

[4/18/94 public]
This feature is intended and documented in the admin guide (and possibly
other places).  I'd like to see the ACL reflect reality though 
there doesn't seem to be an easy way to do that.  Cancel.



CR Number                     : 9921
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : ??
Short Description             : rios server dies during CHO
Reported Date                 : 2/9/94
Found in Baseline             : 1.1b3
Found Date                    : 2/9/94
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[2/9/94 public]


This is 1.1b3 which consists of the 1.0.3a dfs which is known
to work and the 1.1 core.  During CHO testing, after 
4 hours 20 min, the RIOS DFS crashed.  There seem to be
a number of zero'd out stacks in the crash dump, but
other than that we don't know much about it.

> proc -r
SLT ST    PID   PPID   PGRP   UID  EUID  PRI   CPU   EVENT  NAME
  2 r      202     0     0     0     0   127   120          wait
        FLAGS: swapped_in no_swap fixed_pri kproc
 14 r      eff     1   eff     0     0    60     0          errdemon
        FLAGS: swapped_in wake/sig locks
 52 r     3423  3116  3116     0     0    66    12          cdsclerk
        FLAGS: swapped_in locks
 64 r     4075     1  4075     0     0    66    12          dfsbind
        FLAGS: swapped_in locks
 67 r     437f  3f77  3025     0     0    69    18          thr"
        FLAGS: swapped_in kproc
 69 r     4581  3f77  3025     0     0    79    39          thr&
        FLAGS: swapped_in kproc
 70 r     4682  3f77  3025     0     0    68    17
        FLAGS: swapped_in kproc
 71 r     4783  3f77  3025     0     0    66    13
        FLAGS: swapped_in kproc
 72 r     4884  3f77  3025     0     0    66    12
        FLAGS: swapped_in kproc
 97 r     619f  5f73  4f63     0     0    66    12          sh
        FLAGS: swapped_in


> trace
STACK TRACE:
        00000000

>> trace 64
STACK TRACE:
        .unlockl ()
        .vm_move ()
        .readi ()
        .xix_rdwr ()
        .vno_rw ()
        .rwuio ()
        .rdwr ()
        .kreadv ()

>> trace 67
STACK TRACE:
        .e_wait ()
Frame pointer not valid.
> trace 69
STACK TRACE:
        .e_wait ()
Frame pointer not valid.
> trace 72
STACK TRACE:
        00000000

> trace 71
STACK TRACE:
        00000000

> trace 70
STACK TRACE:
        00000000

>


I'm leaving the component as 'unknown' since we don't know where
the problem is yet.

[2/24/94 public]

Of course, this happens once, but never again.  Dropping the
priority to 2, and I'm going to assume this is a dfs problem,
since I haven't seen any core problems with bl3

[03/18/94 public]

Gail is now Queen of CHO, so she's going to
veryify whether this is still a problem.

[05/03/94 public]
Haven't seen this problem yet - I only have 1 RIOS to work
with so this further reduces the chances of seeing it ...
Leaving open while working on CHO test definition.

[06/06/94 public]
Cancelling - not seen again. CHO for 1.1 has been defined to
be a less i/o stressful set of tests (low/moderate removed) but
with scheduled replication testing added (dfs.repfs). The 
dfs.read_write_all.main test is being used for i/o stress CHO
testing. Neither CHO nor dfs.read_write_all.main are crashing the
RIOS.



CR Number                     : 9760
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : rios compiler message (E) and (W)
Short Description             : there are a bunch of compiler messages that were never logged
Reported Date                 : 1/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/17/94
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[11/25/96 public]
Cannot reproduce with R1.2.2.  No part of dfs testing has recognized
this problem.

[1/17/94 public]

There are a bunch of (E) and (W) that the compiler spits out
which were ignored in the log files.  These SHOULD NOT be 
ignored.  

However, it is going to be a low priority because the DCE103
was shipped with some of these (E) and (W) messages in its
log.



CR Number                     : 9756
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : rpc's build
Short Description             : we got an idl error/warning??
Reported Date                 : 1/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/17/94
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[1/17/94 public]

We got the message:

    /u2/devobj/sb/nb103a_ux/tools/hp800/bin/idl: File /project/dce/build/dce1.0.3a-snap/src/rpc/kruntime/ep.acf, line 32: [nocode] interface ept
    /u2/devobj/sb/nb103a_ux/tools/hp800/bin/idl: File /project/dce/build/dce1.0.3a-snap/src/rpc/kruntime/ep.acf, line 32: [nocode] interface ept

during the execution of rpc/kruntime .  

Is this an error or warning?

[1/20/94 public]

This is a warning, which is expected given the way we are using idl in this
instance.  Since we are telling idl to generate a client stub (for use in
libknck) but have labeled the interface with the nocode attribute, idl
warns us that the client stub won't be very usefull.  However, the routines
are implemented elsewere in the kernel runtime and this is the proper
behaviour.



CR Number                     : 9713
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ncsubik
Short Description             : beacon thread takes too long to get ubik lock
Reported Date                 : 1/11/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/11/94
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[1/11/94 public]

Sometimes the beacon thread takes a long time to get the ubik
lock.  It can take more than POLLTIME seconds from the time
that the thread actually wakes up to the time it gets the lock
and actually can send beacons.  This is part of the reason why
we loose quorum .  This problem is more evident when
ubik has a lot of write activity going on to the fldb.

I put some logging in beacon.c to see how long it takes
and this is what we have:

long time to get ubik lock 36
long time to get ubik lock 70
long time to get ubik lock 38

The numbers are the difference between the time we wake up
and the time after ubik_hold completes.

We are also seeing the Beacon thread go more than POLLTIME
seconds since LastWakeupTime. 

You don't necessarily have to have a high load average on the
machine to make this happen.

[1/17/93 public]

Move to 1.1; have got a lot of info, and it might be too risky
even if we could generate a fix.

[05/03/94 public]
Diane - are you still considering this? I'm still losing quorum in CHO ...
Let me know if there is particular data you need captured - thanks.



CR Number                     : 9683
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : client side
Short Description             : HP dfs client machine crashes
Reported Date                 : 1/4/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/4/94
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[1/4/94 public]

Using 1.0.3ab2 (latest weekly build - contains dfsbind memory leak
fix), I set up Config G - a 13 machine (10 dfs) cell. I started
dfs.lock on one of the dfs client only HP machines, with 9 client
machines specified. Sometime in the past 3 days, one of the dfs
client only HP machines involved in the test (but not hosting the
test) crashed. Unfortunately, I did not have /tmp/syscore directories
set up on the client only machines. I hope to get a chance to rerun
this exact scenario when I can again Configure "G". In the meantime,
this CR is merely serving as a placeholder/reminder/warning that
some problem exists ...

dce2 = client only machine hosting test
dce1 = client only machine that crashed
soldier = file server exporting /:/dfs_1 for test
principal = dfs_1

[01/17/94 public]
Lowering priority since the dfs.lock test contained a tight read
loop that put excessive load on all client machines. However, since
this test has not yet been run successfully in a large cell, not
closing/cancelling yet either.

[01/31/94 public]
Cancelling - new and improved dfs.lock test ran in the identical large
cell configuration WITHOUT causing any machines to crash. (Only 10% of
the iterations passed but this was sufficient to close out this CR).



CR Number                     : 9679
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bosserver
Short Description             : error message during auto-restart
Reported Date                 : 1/4/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/4/94
Severity                      : D
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[11/25/96 public]
Can't reproduce with R1.2.2.  No part of dfs testing has encountered
this problem.

[1/4/94 public]
On both the HP and RIOS, the BosLog.old resulting from the auto-restart
(Sunday 4am) contained the error message:

Sun Jan  2 04:00:00 1994: reBossvrWatchThread: no error; simple restart exit
compat_UnregisterServer: unexpected error from rpc_ep_unregister: not registered in endpoint map (dce / r
pc)
Sun Jan  2 04:00:01 1994: /opt/dcelocal/bin/bosserver: error unregistering self: not registered in endpoi
nt map (dce / rpc)
Sun Jan  2 04:00:02 1994: /opt/dcelocal/bin/bosserver: application exit

Unclear whether the error message is valid or not but I believe it is logged
regularly during these auto-restarts so it's no longer "unexpected"!

[1/6/94 public]
Consensus is that the low priority/severity appears correct so we can defer
this to 1.1.

[9/21/94 public]
Not seen during 1.1 - canceling.

[9/26/94 public]
Oops - spoke too soon - still appears to be harmless - perhaps there
are inadvertently 2 attempts to unregister self during the shutdown
so the 2nd one will always report this?

From BosLog.old:
Fri Sep 23 19:08:52 1994: /opt/dcelocal/bin/bosserver: beginning logging
Fri Sep 23 19:08:52 1994: Server directory access is okay
Sun Sep 25 04:00:00 1994: reBossvrWatchThread: no error; simple restart exit
Sun Sep 25 04:00:04 1994: /opt/dcelocal/bin/bosserver: error unregistering self:
 Not registered in endpoint map (dce / rpc)
Sun Sep 25 04:00:05 1994: /opt/dcelocal/bin/bosserver: application exit



CR Number                     : 9672
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : releasing page wrongly in vnvm_truncate
Reported Date                 : 1/3/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/3/94
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[1/3/94 public]
The criterion for calling vm_releasep from vnvm_truncate can be "off by one".
For instance, if the file size is 4096 * 10 and we are truncating to
4096 * 9 + 1, it will release one page, which could lose data.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/4/94 public]
I'm sorry, this was just a hallucination.  Thanks to Blake Lewis for pointing
out the error of my arithmetic.
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 9671
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : unknown
Short Description             : rios panic due to sched rep?
Reported Date                 : 12/31/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/31/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[12/31/93 public]
 
Rios in Config A of system test (identical to CHO config) that is
a dts server, fldb server, file server and dfs client, was found
in a panic state (blinking 888) after dfs.read_write_all had
completed 24 hours earlier. All that was going on in the cell
was scheduled replication - the replicated fileset was NOT being
written to at or within 18 hours before the panic so scheduled
replication should have been a noop (right?). Unfortunately,
the core file in /opt/dcelocal/var/dfs/adm was empty, dbx reported
that the core.bosserver file was really from the repserver and
that the framepointer was bad, and the crashdump contained no
useful information (crash device possibly too small - size increased).
Assigning to myself - hopefully I'll be able to include debug data
when this happens again. Note that the thread-aware code for aix dbx
is not yet available to system test.

[12/31/93 public]
Even in an idle cell, the repserver will update the volume status of every
replica (on which it has a token for the R/W) every 90 seconds.
 
One thing that will happen after 18 hours of inactivity is that the ZLC
manager might finally be able to obtain the open-for-delete tokens for
replicated files, the R/W fileset could have those files deleted, and
the R/O filesets could then be updated.  Not that this should fail, but
it's something that could happen after long periods of what appears to be
inactivity.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[01/20/94 public]
Dropping to a 1 and assigning to Cindy (who is finishing off the dfs.repfs
script). Craig's input is useful in clarifying possible activity in an
"idle" cell but, in this case, the replicated fileset was NOT involved in
the dfs.read_write_all test so the ZLC would not have been blocked. I suspect
the dfs.read_write_all test more than the scheduled replication however - since
I have seen it have a mysteriously destructive effect on cells (ie. test runs
fine for 44 hours, not 48 and cell does not exhibit healthy response times
after the test). If the dfs.repfs script can run successfully on rios and
hp for 48 hours with default scheduled replication settings (or if nec. with
more frequent scheduled replication updates over a shorter period of time),
then this CR can be cancelled. Ideally we could run the scripts post-read_write_all
test and be more sure there isn't a problem.

[1/31/94 public]

I did not successfully run dfs.repfs for 48 hours on the RIOS problems
on the RIOS not related to replication.  I was able to setup a fileset
on a RIOS with scheduled replication to a HP and RIOS and let run for over
48 hours without problems.  The fileset was not being updated.  Since I did
not see any problems with the replication, I am canceling this CR.



CR Number                     : 9632
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : token leak during cho
Reported Date                 : 12/20/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/20/93
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[11/25/96 public]

Cannot reproduce with 1.2.2 DFS.  This problem does NOT occur
when DFS testing is performed (CHO, Sys Test, FVTs).


Transarc Deltas                      [text]: 
Transarc Herder                      [text]: 
Transarc Status                      [text]:

[12/20/93 public]
During cho, machine 3 locks up.  This has happened
both times that I have run cho; the first time after
43 hours, and the second time it's unknown.
The difficulty of this is that for the second time,
I had machine 3 setup as a debug client during CHO,
but I was unable to get rsdbb to respond when
the server hung, so we don't have much information.

[12/20/93 public]
Please do not assign things to Transarc unless we have enough information
to make some progress on the problem.  We read the OT notices, and will
certainly take it on if we feel that we can do something.  This defect
is not detailed enough for us to make progress.
Filled in Interest List CC with `delgado@osf.org' 
Changed Responsible Engr. from `jaffe@transarc.com' to `delgado' 
Changed Resp. Engr's Company from `tarc' to `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[1/27/94 public]

Here are stack traces of processes which were running at the time:

 Command:  /opt/dcelocal/bin/fxd -mainprocs 7 -admingroup subsys/dce/df
 Kernel Stack trace:

    save+0x0000    _swtch+0x0150    sema_sleep+0x00c0
    pchansync+0x0194    _sleep+0x0154    osi_SleepW+0x00bd
    fshs_StartCall+0x00a1    fshs_RevokeToken+0x0609    tkm_HostRevokeList_DoRevoke+0x03e9
    tpq_Pardo+0x0671    tkm_HostRevokeList_SendRevocationsInParallel+0x030d    tkm_FidHash_RevokeConfli
cts+0x07dd
     tkm_FidHash_AsyncRevokeFid+0x00b1    tkm_GC_TryAsyncRevokes+0x00e5    tkm_TokenFreeList_ReserveToke
ns+0x0269
     tkm_FidHash_RevokeConflicts+0x0269    tkm_GetTokenNoVolCheck+0x03cd    tkm_GetTokenLeaveOnFid+0x026
5
     tkm_GetToken+0x0c31    ObtainSet+0x02f9    tkset_AddTokenSet+0x025d
     SAFS_RemoveFile+0x0225    op8_ssr+0x01d5    rpc__dg_execute_call+0x134d
     cthread_call_executor+0x0331    pthread_create+0x02d1    cthread_create+0x00e1
     cthread_pool_start+0x01f9    rpc__cthread_start_all+0x0105    rpc_server_listen+0x0285

----

 Command:  /opt/dcelocal/bin/fxd -mainprocs 7 -admingroup subsys/dce/df


 Kernel Stack trace:

    save+0x0000    _swtch+0x0150    sema_sleep+0x00c0
    pchansync+0x0194    _sleep+0x0154    osi_SleepW+0x00bd
    fshs_StartCall+0x00a1    fshs_RevokeToken+0x0609    tkm_HostRevokeList_DoRevoke+0x03e9
    tpq_Pardo+0x0671    tkm_HostRevokeList_SendRevocationsInParallel+0x030d    tkm_FidHash_RevokeConfli
cts+0x07dd
    tkm_FidHash_AsyncRevokeFid+0x00b1    tkm_GC_TryAsyncRevokes+0x00e5    tkm_TokenFreeList_ReserveToke
ns+0x0269
    tkm_FidHash_RevokeConflicts+0x0269    tkm_GetTokenNoVolCheck+0x03cd    tkm_GetTokenLeaveOnFid+0x026
5
    tkm_GetToken+0x0c31    ObtainSet+0x02f9    tkset_AddTokenSet+0x025d
    SAFS_Lookup+0x0215    op16_ssr+0x01d5    rpc__dg_execute_call+0x134d
    cthread_call_executor+0x0331    pthread_create+0x02d1    cthread_create+0x00e1
    cthread_pool_start+0x01f9    rpc__cthread_start_all+0x0105    rpc_server_listen+0x0285

-----------------------


 Command:  /opt/dcelocal/bin/fxd -mainprocs 7 -admingroup subsys/dce/df



    icl_AppendRecord+0x00a9    icl_Event4+0x01a1    icl_Event3+0x0045
    tkm_GC_FindCandidate+0x0181    tkm_GC_FreeOldest+0x0055    tkm_TokenFreeList_ReserveTokens+0x0139
    tkm_FidHash_RevokeConflicts+0x0269    tkm_GetTokenNoVolCheck+0x03cd    tkm_GetTokenLeaveOnFid+0x026
5
    tkm_GetToken+0x0c31    ObtainSet+0x02f9    tkset_AddTokenSet+0x025d
    SAFS_GetToken+0x0955    op17_ssr+0x01b9    rpc__dg_execute_call+0x134d
    cthread_call_executor+0x0331    pthread_create+0x02d1    cthread_create+0x00e1
    cthread_pool_start+0x01f9    rpc__cthread_start_all+0x0105    rpc_server_listen+0x0285
    afscall_exporter+0x01a5    afs_syscall+0x00a5    syscall+0x01a4


-------------------------

[2/9/94 public]


We've actually seen the token manager reach the tkm_maxTokensOutstanding
limint during CHO.  This limit is hard coded at 10000; I don't think
that CHO should need 10,000 tokens outstanding at one time, so it
seems that we have some kind of token leak here.  Tokens are supposed
to be given back when a file is closed or deleted, right?

We have also gotten TKM_ERROR_NOMEM from the RIOS server during 
the last CHO run, so the problem does not now seem to be HPUX
specific; it's just that the HP might be more sensitive to running
out of kernel memory than the RIOS.  We've also monitored the
memory consumption on the RIOS server and have seen that the
token manager is by far the largest user, with
tkm_tokenFreeList_SimpleObtain being the winner by far.

Also, we've noted that we don't see a steady growth in token
memory usage.  The tkm_maxTokensOutstanding will get bumped
up sereral times and then stay steady for hours, at which time
it might grow a little and then stay steady ... etc.

[05/03/94 public]
Diane - please point me to where/how to watch for this. During CHO, I run
with the following dfstrace settings:

> dfstrace lsset
Available sets:
cm: dormant
fx: dormant
fshost: dormant
xops: dormant
episode/anode: dormant
episode/logbuf: dormant
episode/vnopsVerbose: dormant
episode/vnopsBasic: active
tkc: active
tpq: dormant
tkm: active

Please let me know if this is sufficient/unnecessary - Thanks!

[06/02/94 public]
AHA! Much easier way to reproduce than CHO! Using dfs.read_write_all.main with
the following data settings in the CHO cell, received the following errors at the
41 hour mark :
dfs: dce errors (code 663076882) from the fx server 130.105.201.7 in cell cho_cell.qadce.osf.org
(16 times on darkman, 2 times on dce11)
Then on dce11:
dfs: set auth binding failed (code 382312711); running unauthenticated.
dfs: set auth binding failed (code 382312711); running unauthenticated.
dfs: set auth binding failed (code 382312711); running unauthenticated.

where 130.105.201.7 is cobbler and the error translation is:

663076882 (decimal), 2785c012 (hex): Token manager unable to get required memory for operation (dfs / tkm)

Data settings were:
AGGR_NAMES="m1.aggr1 m1.aggr2 m2.aggr2 m3.aggr2 "
AGGR_SERVERS="cobbler cobbler dce8 dce11 "
FILESETS_PER_AGGR="1 1 2 2 "
MACHINES="darkman dce11 dce8 cobbler"
NUMBER_OF_USERS=6

The dfs.read_write_all.main test does a lot of i/o operations but there
is no file sharing and all files are created and deleted each loop of the
test script (do.ksh) by the individual test user in their own fileset. Any
ideas where the leaks are?! I'm taking a look at what dfstrace shows on
cobbler now ...

I've placed the dfstrace dump in:
	~notuser/gmd/dfstrace.dump.9632

Fortunately or unfortunately, I had more dfstracing turned on than usual.
Note that the time of the failure was about 10:55 am on Thursday June 2nd.

Lastly, symptom in the test logs is:
... cannot stat <full path to filename in dfsexported episode> : Unknown error
ie.
rm: cannot stat /:/dfsusers/dfs_1/utils/etc/zoneinfo.new/US/Arizona: Unknown error

[06/03/94 public]
I'm even more convinced that this is the same error since after leaving
the test running, cobbler "locked up" as Diane described ie. you could
ping it but that's all - current login sessions were unresponsive and
no new login sessions could be started. Rerunning with file/osi/tom
to monitor alloc's.

[06/08/94 public]
I've now also crashed an HP server by running the dfs.read_write_all test.
Console was filled with:
dfs: dce errors (code 663076882) from fx server 130.105.202.28 in cell cho_cell.qadce.osf.org

again this error code is:
663076882 (decimal), 2785c012 (hex): Token manager unable to get required memory for operation (dfs / tkm)

[06/9/94 public]

not my problem anymore :-)



CR Number                     : 9615
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : flserver
Short Description             : flserver dumps core during cho
Reported Date                 : 12/14/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/14/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[12/14/93 public]


During CHO, after about 8 hours the flserver on one of the RIOS
core dumped.  We have a core file but dbx can't read it. 

We don't yet know about the frequency of the problem, so we
might have to adjust the priorities and fixby fields later.

[1/11/94 public]

[06/07/94 public]

Have not seen this in quite awhile.  Even if we have, we still
don't have any means of debugging it since core files are not
useable on the RIOS.


This problem occurs infrequently and it's difficult
to track due to the inability to read core files of
threaded applications on the RIOS.  Will defer to
1.1 when we hopefully can have the ability to 
read threaded core files.

[05/03/94 public]
We still do not have the ability to read threaded core files,
however, I have not seen this problem yet during 1.1 CHO runs.
I am still in the process of defining the CHO setup for the
1 RIOS - 3 HP cell I have available for it. I will watch
for flserver core files and cancel this if none are seen.



CR Number                     : 9565
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : ubik should not lose quorum because of slow beacon RPC
Reported Date                 : 12/1/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/1/93
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[12/1/93 public]
There is a problem where, under stress, a beacon RPC can make a
connection to a remote server but the manager function on that end
takes a long time to start (O(50-100) seconds).  What then happens is
that when the manager process finally runs, it will look at its local
time and the timestamp in the RPC and think that there is a time skew.
It will then return USKEWED, which causes quorum to be immediately
lost at the sync site.  Even detecting that an RPC takes a long time
to run is not enough since the amount of time it takes the RPC to
succeed may be longer than the period the sync site is guaranteed
votes from the other servers (i.e. quorum could be lost during the
wait).  Another proposal is to increase the beacon interval.  This has
the effect of limiting the number of times the problem is seen because
there simply are fewer calls.
The proposed fix is to set a call timeout on the vote binding handles
to somewhere between RPC timeout and MAXSKEW.  A rpc_s_call_timeout
error code would then be converted into a NO vote from the slow
server.  On the slow server (the non-sync site), the beacon will
either be received, in which case, it will think there is a clock skew
and reset its state; or, it will not get the RPC.  In either case, it
should be able to properly recover.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/29/93 public]
Deferring this to 1.1. The ability of a multi-flserver dfs cell to
handle "stress" must be evaluated more closely - as well as the "stress"
produced by the CHO test. We have seen quorum problems in non-stress
scenarios but we can not reproduce them reliably enough to begin to
narrow down the problem.
 
Here is the result of Mark Karuzis' analysis of rpc logs produced during
quorum failures:
 
Received: from awol.ch.apollo.hp.com by amway id <AA02403@amway> Wed, 22 Dec 93 12:22:32 EST    
Received: by awol.ch.apollo.hp.com id <AA29816@awol.ch.apollo.hp.com>; Wed, 22 Dec 93 12:22:31 -0500    
From: markar@apollo.hp.com
Date: Wed, 22 Dec 93 12:22:30 EST
Subject: flserver quorum problem
To: dfs-drb@osf.org
 
I've gone through the RPC log files that were taken during a test of the
flserver quorum problem.  The logs show that several RPCs took longer than 
45 seconds to complete.  
 
There is no evidence from the log that these long-running calls were the
result of anything other than overburdened test systems (which is not
to say that there isn't some other explanation).  The logs show that 
packets are not being dropped, and the RPC queues (call and packet) are
not being overrun.  The only conclusive evidence from the logs is that,
during the periods when we see long-running calls, there are gaps in the
timestamps spit out by the runtime.  This indicates that either the 
runtime threads, or the process as a whole, is not getting an adequate
chance to run (to meet the timing constraints of the ubik vote protocol;
the calls run successfully from RPC's perspective.)
 
The following excerpt shows the sequence of events during one of the 'long
running calls'.  I've cleaned it up to make it a little more readable;  the
actual log contains [activity UUID, seq no.] information that allows the 
messages belonging to a single call to be isolated.  The time is in 1/5
seconds.
 
On the client side, the call looked like this:
 
send request
  >> [time: 112465] (rpc__dg_xmitq_elt_xmit) request 1229.0.0 len=24 frq
2 seconds later, start pinging
  >> [time: 112475] (recv_state_timer) Starting to ping (rq->next_fragnum=0) 
2 seconds later
  >> [time: 112487] (recv_state_timer) Re-pinging (rq->next_fragnum=0) 
6 seconds later
  >> [time: 112515] (recv_state_timer) Re-pinging (rq->next_fragnum=0)
4 seconds later (receives 'working' pkt for each ping))
  >> [time: 112534] (stop_pinging) Next ping at: now = 112534, interval = 50
  >> [time: 112534] (stop_pinging) Next ping at: now = 112534, interval = 50 
  >> [time: 112534] (stop_pinging) Next ping at: now = 112534, interval = 50 
13 seconds later
  >> [time: 112598] (recv_state_timer) Starting to ping (rq->next_fragnum=0) 
2 seconds later
  >> [time: 112608] (recv_state_timer) Re-pinging (rq->next_fragnum=0) 
4 seconds later
  >> [time: 112628] (recv_state_timer) Re-pinging (rq->next_fragnum=0) 
4 seconds later (receives 'working' packet for each ping)
  >> [time: 112648] (stop_pinging) Next ping at: now = 112648, interval = 150
1 second later
  >> [time: 112656] (stop_pinging) Next ping at: now = 112656, interval = 150 
  >> [time: 112656] (stop_pinging) Next ping at: now = 112656, interval = 150 
7 seconds later (receives response)
  >> [time: 112690] (rpc__dg_call_recvq_insert) recv resp 1229.0 8
Total time for the call was 45 seconds.  As far as the RPC runtime is 
concerned, this was a successful, although slow, call.  No packets were 
dropped.  
 
On the server side, things looked like this:
 
The request packet, and the first 3 pings are all queued up on the server by
the time the server reads in the request packet.  The client sent these packets
over a 10 seconds interval;  the server/node was apparently too busy for the
server to be able to read in these packets at the time they were received.
  >>  [time: 110145] (rpc__dg_call_recvq_insert) recv requ 1229.0 24
  >>  [time: 110145] (ping_common) Working
  >>  [time: 110145] (ping_common) Working
  >>  [time: 110145] (ping_common) Working
 
Inexplicably, the DG timer thread in the server is not run for 53 seconds.
There is also no evidence that *any* threads in this process ran during this
time period. (It is possible that the process was scheduled, but then, perhaps,
did some I/O, and got itself switched out again.)
 
When the server is rescheduled, the last three pings are finally read in, and
responded to.
  >>  [time: 110410] (ping_common) Working
  >>  [time: 110410] (ping_common) Working
  >>  [time: 110410] (ping_common) Working
 
And finally, the response is sent.
 
  >>  [time: 110447] (rpc__dg_xmitq_elt_xmit) response 1229.0.0 len=8 frq
 
Again, from the server's perspective, no packets were dropped, the call didn't
need to be queued (due to lack of call threads), and the call was serviced
successfully.
 
This trace was from the longest running call in the log.  There were many
other periods of time where calls took 20, 30, and 40 seconds to complete.
However, only when the call is involved in a ubik vote is the length of time
considered significant.
 
Given that we are not seeing any failures in the runtime, the question becomes
"is it reasonable to expect that the runtime can meet the timing constraints 
of the ubik protocol under the load imposed during this test?"
 
I don't know enough about the load on these machines to give an answer. 
 
    -- Mark Karuzis
       Open Systems Software Division
       Hewlett-Packard Company
       markar@apollo.hp.com

[2/19/96]
The load under which this test ran is too high for servers which depend
on RPC.



CR Number                     : 9528
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : butc
Short Description             : butc coredumps restoring UFS fileset
Reported Date                 : 11/22/93
Found in Baseline             : 1.0.3
Found Date                    : 11/22/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[11/22/93 public]
While executing the command:
bak restoreft -server /.:/hosts/blizzard -aggregate /restore 
-fileset local.comps.dfs.dfssrc -extension .rest -tcid 1
butc coredumped with a SIGSEGV. 
The stack in dbx shows:
strlen() at 0xd00ce404
doprnt._doprnt(0x74696e2e, 0x0, 0x206f6620) at 0xd0366654
fprintf.fprintf(0x20060f28, 0x100ce203, 0x2051d860, 0x0, 0x0, 0x200a81e8, 0x200a
a0bc, 0x74696e2e) at 0xd037475c
LogTape(0x74696e2e, 0x0, 0x206f6620, 0x20060f28, 0x2014a580, 0x2051d60c, 0x69626
d2e, 0xd0368140) at 0x1000058c
Restorer(0x74696e2e) at 0x100b2734
cma__thread_base(0x74696e2e) at 0xd05d6650
I looked at the code for butc and it appears LogTape was passed the
incorrect number of parameters. Correcting this appears to fix the
problem.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[11/22/93 public]
Filled in Interest List CC with `jaffe@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `khale@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/23/93 public]
Changed Status from `open' to `cancel' 
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'



CR Number                     : 9478
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : can newaggr -overwrite mounted filesystem
Reported Date                 : 11/11/93
Found in Baseline             : 1.0.3
Found Date                    : 11/11/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[11/11/93 public]
Done accidentally on HP, reproduced on RIOS. I was glad that my
hfs filesystem had NOT been blown away by the newaggr but surprised
that having the filesystem mounted was not protection enough - and
further surprised that fts create succeeds as does cd/ls/mkdir
commands to the mounted hfs filesystem - I only caught my error
when I went to dfsexport my hfs filesystem as a native filesystem
and saw that it was already dfsexported as lfs. I've entered this
as a B2 because it was admin error but pretty easy and potentially
very painful.

[11/12/93 public]
Filled in Interest List CC with `jdp@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/12/93 public]
newaggr has protections built in against running newaggr on an
*attached* fileset.  Doing this requires an interlock that we can do
(and, in fact, do do) with Episode that we can't do with the native
file system.  This is a prime candidate for an enhancement a vendor
can make to better integrate DFS into the OS.  This is not something
we can do for a reference port, however.  I've changed this CR to an
enhancement request, but I doubt we will be able to do anything on it.
Changed Defect or Enhancement? from `def' to `enh'

[12/16/93 public]
Decided in today's meeting to defer to 1.1.

[3/21/94 public]
This was fixed under 9791.  cancelled.

[3/21/94 public]
Fixing the header above.



CR Number                     : 9365
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : cache manager
Short Description             : assertion failure in cm_setattr
Reported Date                 : 11/4/93
Found in Baseline             : 1.0.3
Found Date                    : 11/4/93
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : unknown
Sensitivity                   : public

[11/4/93 public]

Here's a test that panics the kernel:

	(1) Set up a UFS filesystem to be DFS exported, say /u3.
	(2) Create a DFS mount point for this filesystem, say /:/u3
	(3) Have a sandbox on another UFS partition (which is not
		DFS exported), say /u1/sb.
	(4) make a symbolic link /u1/sb/install -> /:/u3/install.
	(5) cd to /u1/sb/src and execute the following script:

#!/bin/ksh
typeset -i k=5

while (( (k=(k-1)) >= 0 ))
do
        echo $k
        build -k install_all >> LOG$k 2>&1 &
done

The system runs anywhere from 1 to 20 minutes before a panic.  The
panic is always in cm_setattr, line 1820:

	lock_ReleaseWrite(&scp->llock);

This is a macro which expands to:

#define lock_ReleaseWrite(lock)\
        BEGINMAC\
	    osi_assert((lock)->excl_locked == WRITE_LOCK); \
	    (lock)->excl_locked &= ~WRITE_LOCK;\
	    if ((lock)->wait_states) Lock_ReleaseR(lock);\
        ENDMAC

It is the above osi_assert which is failing.

[12/15/93 public]

Bug status is now closed, since two days of testing have not
reproduced this bug in 1.0.3a.

[12/15/93 public]

Changed status to 'cancel' since we don't really know what changed.



CR Number                     : 9319
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : config
Short Description             : epidaemon start fail message
Reported Date                 : 11/2/93
Found in Baseline             : 1.0.3
Found Date                    : 11/2/93
Severity                      : D
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : rc.dfs
Sensitivity                   : public

[11/2/93 public]
on the rios machines a start of dce/dfs always
complains that epidaemon start failed

the daemonrruning routine is trying to determine if
it started by doing a ps operation, seems the
epidaemon is never in the ps list...

this is annoying and causes the process to prompt
the user before continuing with the dce/dfs start

[11/2/93 public]
This was fixed on 10/5.  Do you have the latest release of rc.dfs?
I'll assume you didn't and cancel this bug.  If you did, and the
problem still occurs, please re-open.  Thanks.



CR Number                     : 9290
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : low test8 fails during cho
Reported Date                 : 10/29/93
Found in Baseline             : 1.0.3b8
Found Date                    : 10/29/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[10/29/93 public]


Low test8 fails rather consistently when run from a RIOS client
to an HP server on an Episode filesystem.  This failure happens
during cho.

There are 2 failures:

# fgrep ERROR low.test8.log
- ERROR 47: dup st_compare(F2.1000.777) failed failed; errno = 13
- ERROR: prog8 process 2 returned 1
- ERROR 45: st_compare(f0.111) failed failed; errno = 17
- ERROR: prog8 process 2 returned 1
#


This is a priority 0 since it prevents CHO from running sucessfully.

[10/31/93 public]

Can you run this test with debug turned on?  From looking at the test, I
believe the errno 13 and 17 don't have anything to do with anything.  With
debug turned on we will be able to see what st_compare thought was wrong.

Has this ever worked?  Did it just start failing recently?  Does it fail at
Transarc?  The only relevant recent change is one I made in
src/file/cm/cm_vnodeops.c in order to fix another low/test8 problem on HP.
My change just used code provided by Transarc as an answer to OT 7969.  At
the time I made this change I did not understand how AIX did not get the
same error as HP.  I still do not understand.  I also do not know how
Transarc found the problem since AIX did not exhibit it.  I mention this
because my non-understanding of AIX may also explain the current failure.

[11/1/93 public]

I haven't seen this particular failure before, and I tried to reproduce
it outside of CHO and was unable to .  I recompiled test8 with debug
turned on and haven't seen the problem yet since restarting CHO.

[11/5/93 public]

Cancellling -I've ben running test8 now during the last 5 days and have
not seen this again.



CR Number                     : 9263
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfsexport detach panic's machine
Reported Date                 : 10/26/93
Found in Baseline             : 1.0.3
Found Date                    : 10/26/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[10/26/93 public]

on rios machine w/aggrs m1.aggr1, m1.aggr2
performed fts move of fileset m1.lfs1 on
m1.aggr1 to m1.aggr2 apparently while low
tests were running from dfs client machine
on m1.aggr1

performed dfsexport -detach of m1.aggr1 /dev/epi1
which failed

stopped low tests  
performed dfsexport -detach /dev/epi1 again
machine panic'd

probably wasn't such a nice thing to move an
aggregate that was in use or dfsexport -detach 
one that was in use but not sure a panic should
occur as the result of these operations 

heres the crash trace:
> proc -r
SLT ST    PID   PPID   PGRP   UID  EUID  PRI   CPU   EVENT  NAME
  2 r      202     0     0     0     0   127   120          wait
        FLAGS: swapped_in no_swap fixed_pri kproc
0452-220: Cannot read process table entry  54.
0452-220: Cannot read process table entry  62.
0452-220: Cannot read process table entry  84.
0452-220: Cannot read process table entry  93.
0452-220: Cannot read process table entry  95.
100 r     64c4  62f4  64c4     0     0    60    57          dfsexport
        FLAGS: swapped_in
0452-220: Cannot read process table entry 101.
0452-220: Cannot read process table entry 103.
0452-220: Cannot read process table entry 104.
0452-220: Cannot read process table entry 105.
0452-220: Cannot read process table entry 110.
0452-220: Cannot read process table entry 111.
0452-220: Cannot read process table entry 113.
0452-220: Cannot read process table entry 114.
0452-220: Cannot read process table entry 115.
0452-220: Cannot read process table entry 116.
0452-220: Cannot read process table entry 117.
0452-220: Cannot read process table entry 118.
0452-220: Cannot read process table entry 119.
0452-220: Cannot read process table entry 120.
0452-220: Cannot read process table entry 121.
0452-220: Cannot read process table entry 122.
0452-220: Cannot read process table entry 123.
0452-220: Cannot read process table entry 124.
0452-220: Cannot read process table entry 125.
0452-220: Cannot read process table entry 126.
0452-220: Cannot read process table entry 127.
> trace 100
STACK TRACE:
        .brkpoint ()
        [dcelfs.ext:epig_CloseAggregate] ()
        [dcelfs.ext:ag_efsDetach] ()
        [dfscore.ext:ag_PutAggr] ()
        [dfscore.ext:afscall_aggr] ()


I also have dump file if you'd like

[10/26/93 public]

I've tried the low tests as well as parallel tar's all running in
a loop while executing dfsexport -detach and fts move.  The fts
move simply locks the fldb entry and blocks for some fileset
internal state, and the dfsexport -detach always correctly
fails with an error about a busy aggregate.  Since I can't
reproduce this bug I am cancelling it.



CR Number                     : 9244
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 9212
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : AFSCALL_NEWTGT
Short Description             : AFSCALL_NEWTGT with exp. time  zero should terminate PAG
Reported Date                 : 10/22/93
Found in Baseline             : 1.0.3
Found Date                    : 10/22/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : file/cm/cm_xsyscalls.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/22/93 public]
 
This is a prerequisite for fixing CR9212 in security.
 
afs_syscall(AFSCALL_NEWTGT, pag, 0) should cause DFS to stop using any
connections authenticated as `pag', so that
kdestroy/sec_login_purge_context prevent future file system operations from
continuing using the purged credentials.

[10/22/93 public]
 
Mail from CFE claims that this is already there in the code.
 
"Never Mind".
Changed H/W Ref Platform from `hppa' to `all' 
Changed S/W Ref Platform from `hpux' to `all' 
Changed Affected File from `file/cm/cm_???' to `file/cm/cm_xsyscalls.c' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'



CR Number                     : 9185
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : file/butc/butc
Short Description             : butc starts, but then exits without complaint after couple seconds
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.3
Found Date                    : 10/18/93
Severity                      : B
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/18/93 public]
When I invoke butc in a shell, I get
# butc
Tape Coordinator:  TCID 0   Debug level 0   Cell /.../mice_cell
# 
The prompt returns after a couple seconds.  My TapeConfig looks like
# cat /opt/dcelocal/var/dfs/backup/TapeConfig
2G 1M /dev/rmt/c201d3hb 0
I have succeeded at tar'ing to this device, and reading it back.  I have
done 'bak addhost'
# bak
bak> lshosts
Tape hosts:
    Host mice.ch.apollo.hp.com, port offset 0

[11/8/93 public]
Assign a responsible engineer.

[11/8/93 public]
I belive that this has already been reported and fixed.
Abhijit, please dup this if you know the ot number, otherwise
defer until the OSF gets a chance to test it with our 1.0.3 code base.
Changed Responsible Engr. from `jaffe@transarc.com' to `khale@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/10/93 public]
This bug is fixed by the delta in ot9208. Unfortunately, I cannot
mark this as a dup. Please verify this and cancel.

[12/10/93 public]
fixed as part of 9208.
Changed Status from `open' to `cancel'



CR Number                     : 9154
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : Add mmap and mmap stress to test suite
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : closed
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Will create a new subdirectory, test/file/mmap, and put both the mmap and
 mmap stress tests in it.
 bwl-Thu, 16 Sep 1993 11:33:33
**Solution Text**
 Delta bwl-o-db4336-mmap-tests.
 bwl-Tue, 28 Sep 1993 16:04:36

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/9/93 public]
Changed Status from `fix' to `cancel' 
Changed Fixed In Baseline from `1.0.3a' to `' 
Changed Transarc Status from `submit' to `closed'



CR Number                     : 9143
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : xaggr
Short Description             : not enough bits in argument to AG_DETACH
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/15/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]
HP platform specific.  Hasn't been seen in R1.2.


Transarc Deltas                      [text]: 
Transarc Herder                      [text]: jaffe@transarc.com
Transarc Status                      [text]: open

[10/15/93 public]
On all platforms, when Episode is mounted locally, we try to create a unique
file system ID for the vfs structure by putting the volume ID in the top 16
bits, then the major number in 8 bits, then the minor number in 8 bits.  It's
necessary to fit the fileset ID in there somehow, because the device major
and minor alone are not unique--there can be more than one fileset per device.
Since all these numbers are being truncated, inevitably some information is
lost.  There is at least one place where this can potentially hurt us: in
the interface to AG_DETACH.
 
Consider the epiunmount utility.  This does a statfs on the mounted filesystem,
and then tries to reconstruct the device number from the "unique file system
ID" described above.  This is then passed to ftu_DetachAggr which does the
AG_DETACH syscall, which (in the kernel) ultimately results in a call to
ag_GetAggrByDev.  This function compares the reconstructed device number
passed to it, with a real device number that was obtained when the aggregate
was first attached.  Since the reconstructed device number is likely to be
missing some bits, the comparison may fail.
 
In particular, on HP/UX, the missing bits are likely to be non-zero.  For
instance, a typical disk might be /dev/dsk/c201d1s3.  The device minor number
will be 24 bits wide:  0x201103.
 
We are currently handling this by truncating the device minor number to 8 bits
(0x3), and then in the comparison in ag_GetAggrByDev, we ignore the 16 bits
that we dropped (i.e. we mask them out of both arguments before comparing
them).  The problem with this is that we can easily end up finding the wrong
aggregate in ag_GetAggrByDev.  This could cause the detach to fail and/or
the wrong aggregate to be detached.
 
It is hard to tell at what level the interfaces have to be changed to solve
this problem.  The ultimate cause is that the information returned by statfs
does not have enough bits to uniquely identify an Episode fileset, because
fileset ID's are 64 bits wide and f_fsid[0] is only 32 bits wide.  No matter
how we jam information about fileset ID and/or device number into those 32
bits, there just isn't enough room.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'
Changed Interest List CC from `kissel@apollo.hp.com, jdp@transarc.com, 
 tu@transarc.com' to `kissel@apollo.hp.com, jdp@transarc.com, tu@transarc.com, 
 cfe@transarc.com'

[12/10/93 public]
Changed Fix By Baseline from `1.0.2a' to `1.1'



CR Number                     : 9131
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : port_gd
Short Description             : describe how to port ufs volumeops.
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.3
Found Date                    : 10/15/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[10/15/93 public]

The code in file/ufsops is one area which generally needs porting
when moving DFS to a new platform. The files in this directory
contain much OS specific code with little documentation.
It would be nice if there were a detailed description regarding
the routines in this directory, what they do, and how they
expect to interface with the native OS on which they run.
This would assist greatly those who are trying to port this
fearsome stuff.  (Currently the porting guide has one short
paragraph about this, and this isn't suffucient for anyone
who has to port this).

[10/18/93 public]

Assigned to writer Doug Weir.

[11/16/93 public]

Talked to Diane Delgado about this. There wasn't time to collect the
information for the 1.0.3 Porting Guide, so I'm deferring this to
1.0.3a (assuming that's legal), with her agreement.

[11/29/93 public]

Changed fixby-baseline to 1.1 (from 1.0.3a).

[12/10/93]

changed state from defer to open as part of post-1.0.3 undeferral of doc CRs.

[10/26/94 public]
No time to get this into the 1.1 Porting Guide. Deferred to 1.2.

[8/13/96 public]
Cancelled--Porting Guide no longer exists.



CR Number                     : 9070
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : libafs
Short Description             : can't build libafs in a sandbox without fsint.klib
Reported Date                 : 10/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/7/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[12/2/96 public]
HP platform specific.  Hasn't been seen in R1.2.


Transarc Deltas                      [text]: 
Transarc Herder                      [text]: jaffe@transarc.com
Transarc Status                      [text]: open

[10/7/93 public]
libafs/HPUX/Makefile contains various references to .o files in ../fsint.klib.
As a result, I can't build libafs in a sandbox in which I haven't already
build fsint.klib.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/14/93 public]
Is this really a problem?  If so, is there some "standard" way to solve it?
I always assumed there were lots of things you couldn't build in an empty
sandbox until you had built other things first.  Is this not true?

[10/15/93 public]
I believe that if you just put the .o files into libraries and export them,
and use the libraries in the usual way to build the kernel extensions, things
will get done in the right order, because this clever build environment builds
all libraries before building any executables or kernel extensions.  (Also
because if there is a backing sandbox, its export tree is available to the
backed sandbox, while its object tree generally is not.)
This is not exactly a show-stopping problem.  But one of the benefits of
moving to ODE, which we did with much grumbling and grousing, was that it freed
us from various dependencies between builds, and from having to build things
in a particular order ...



CR Number                     : 8998
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Test writes and truncates at indirect block tree boundaries
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/26/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : closed
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 We need to write tests to exercise boundary conditions wrt growing/shrinking a file from only direct blocks to various indirect trees.
 
 rajesh-Thu, 26 Aug 1993 11:59:39
**Solution Text**
 rajesh-db4212-episode-file-block-boundary-tests
 rajesh-Mon, 30 Aug 1993 09:00:17

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/9/93 public]
Changed Status from `fix' to `cancel' 
Changed Fixed In Baseline from `1.0.3a' to `' 
Changed Transarc Status from `submit' to `closed'



CR Number                     : 8869
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfstrace
Short Description             : dfstrace lslog does not display persistent logs
Reported Date                 : 9/29/93
Found in Baseline             : 1.0.2
Found Date                    : 9/29/93
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open
Transarc Herder               : 

[9/29/93 public]
The dfstrace lslog command should provide information on which
logs have a state of persistent.  This is useful because these logs
will not be clear using a "global" clear command (that is, dfstrace
clear without the -set or -log opitions).  Adding this enhancement
will also make dfstrace lslog consistent with dfstrace lsset which
does display persistent event sets.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[1/10/94 public]
Changed Responsible Engr. from `comer@transarc.com' to `rajesh@transarc.com' 
Added field Transarc Herder with value `'

[1/10/94 public]
Reassigning to Pervaze, for determination when this work is to be done.
Changed Responsible Engr. from `rajesh@transarc.com' to `pakhtar@transarc.com'

[1/18/94 public]
Changed Status from `open' to `cancel'



CR Number                     : 8697
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : i486
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Syntax error in file/AT386/machdep.mk
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.3
Found Date                    : 9/28/93
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : file/AT386/machdep.mk
Sensitivity                   : public

[9/28/93 public]
The fix submit for revision 1.1.4.3 of this file does not work and, in
fact, breaks the 486 build.  From file/tools/cmd:

gemini tools/cmd$ build
relative path: ./file/tools/cmd
cd ../../../../obj/at386/file/tools/cmd
"/project/dce/build/dce1.0.3/src/lbe/mk/osf.parse_ext.mk", line 330: Could
not find /tmp/addexists10354
sh: zeliff@gemini::  not found.
"/project/dce/build/dce1.0.3/src/lbe/mk/osf.parse_ext.mk", line 353:
warning: "rm -f /tmp/addexists${DO_IT}" returned non-zero
Fatal errors encountered -- cannot continue

Revision 1.1.4.1 of this makefile continues to work but presumably the
fixes applied in 1.1.4.2 and 1.1.4.3 are needed.

[9/28/93 public]
Okay, I goofed.  Sorry



CR Number                     : 8676
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8697
Project Name                  : dce
H/W Ref Platform              : i486
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : libafs
Short Description             : Poor method for find OSF/1 kernel
Reported Date                 : 9/24/93
Found in Baseline             : 1.0.3
Found Date                    : 9/24/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : file/libafs/OSF1/Makefile,
Makeconf
Sensitivity                   : public

[9/24/93 public]
In the above mentioned makefile we find the following code for linking a
DFS (and episode) kernels:

dfsvmunix: ${MAKETOP}/${OBJECTDIR}/kernel/${CONFIG}/vmunix.sys
${KERNEL_LDDEP} ${afs_libs_dep} ${rpc_libs_dep}
       ${MACHO_GCC_EXEC_PREFIX}ld ${KERNEL_LDOPTS}  -e pstart -o dfsvmunix
${LIBDIRS} \
       ${${MAKETOP}/${OBJECTDIR}/kernel/${CONFIG}/vmunix.sys:P}
${KERNEL_LDDEP} ${afs_libs} -lepidum ${afs_repeat
s} ${rpc_libs}

.ifndef NO_EPISODE
efsvmunix: ${MAKETOP}/${OBJECTDIR}/kernel/${CONFIG}/vmunix.sys
${KERNEL_LDDEP} ${afs_libs_dep} ${rpc_libs_dep} ${
efs_libs_dep}
       ${MACHO_GCC_EXEC_PREFIX}ld ${KERNEL_LDOPTS}  -e pstart -o efsvmunix
${LIBDIRS} \
       ${${MAKETOP}/${OBJECTDIR}/kernel/${CONFIG}/vmunix.sys:P}
${KERNEL_LDDEP} ${afs_libs} ${efs_libs} ${afs_repe
ats} ${rpc_libs} 
.endif

This requires that a copy of the partially-linked OSF/1 kernel be found in
the sandboxes obj tree:  obj/at386/kernel/DCE/vmunix.sys.  Since this is
only really an issue at OSF1 we could make developer lives easier by having
the makefile look in the obj tree of the osc_sbox1.2 where it really lives.

This would mean adding the following code to Makeconf:
#
#       Specify path to special OSF/1 kernel.  This is used
#       to build the DFS kernels.
   OSF1_KERNEL=/project/osc/build/osc_sbox1.2/obj/at386/kernel/DCE

and changing the lines in the libafs makefile to be:

dfsvmunix: ${OSF1_KERNEL}/vmunix.sys  ${KERNEL_LDDEP} ${afs_libs_dep}
${rpc_libs_dep}
        ${MACHO_GCC_EXEC_PREFIX}ld ${KERNEL_LDOPTS}  -e pstart -o dfsvmunix
${LIBDIRS} \
        ${OSF1_KERNEL}/vmunix.sys:P}  ${KERNEL_LDDEP} ${afs_libs} -lepidum
${afs_repeats} ${rpc_libs}

.ifndef NO_EPISODE
efsvmunix: ${OSF1_KERNEL}/vmunix.sys  ${KERNEL_LDDEP} ${afs_libs_dep}
${rpc_libs_dep} ${efs_libs_dep}
        ${MACHO_GCC_EXEC_PREFIX}ld ${KERNEL_LDOPTS}  -e pstart -o efsvmunix
${LIBDIRS} \
        ${OSF1_KERNEL}/vmunix.sys:P} ${KERNEL_LDDEP} ${afs_libs}
${efs_libs} ${afs_repeats} ${rpc_libs} 
.endif

[zeliff 9/24/93 public] 
Dropping severity & priority.  Should not have been and A0 -- must have
been a Rat reflex.

[9/28/93 public]
Work on this defect is currently gated on a problem in
file/AT386/machdep.mk which prevents me from testing this fix.  Since I
don't know if I'm going to get to this again I'm removing my name as
responsible engineer.

[9/28/93 public]
The defect this is gating on is 8697.

[10/08/93 public]
This fix is really only for internal consumption since OSF/1 
is not a supported platform.  On that basis, I'm cancelling.
If any of the DFS developers or RATS internal to OSF believes 
this change is important for internal use, please feel free
to re-open the bug and assign it to yourself.



CR Number                     : 8661
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : udebug
Short Description             : udebug -long option useless
Reported Date                 : 9/22/93
Found in Baseline             : 1.0.3
Found Date                    : 9/22/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[9/22/93 public]
The -long option is only useful if you point udebug at the sync site.
If you point it at the sync site, you get the -long info printed
anyway.  Ergo, -long is useless and should be removed.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/23/93 public]
Changed Interest List CC from 
 `kdu@transarc.com,vijay@transarc.com,dstokes%austin.ibm.com@transarc.com' to 
 `kdu@transarc.com,vijay@transarc.com,dstokes%austin.ibm.com@transarc.com,rsarbo
 @osf.org'

[10/6/93 public]
Ron -- I just wanted to make sure you saw this.  dce_config uses the -long 
option to udebug.  It looks to me like -long isn't required. -- Mike

[10/11/93 public]
Ted informs me that this option is, in fact, useful after all in diagnosing
problems with establishing quorum.  I'm going to cancel this defect and
open a new one to deal with some user interface consistency problems.  
These new changes should not have any impact on udebug usage in dce_config.
Changed Status from `open' to `cancel'



CR Number                     : 8510
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfs server goes catatonic
Reported Date                 : 8/24/93
Found in Baseline             : 1.0.3
Found Date                    : 8/24/93
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/24/93 public]
I've got an HP server that is exporting /usr/local binaries
to a single HP client. For the first few days, it worked
without incident.  However, since then I've been unable to
keep the server up for more than a few hours.  The symptom
is that the server goes completely catatonic.  The screen
refresher doesn't respond to keyboard input, the machine
doesn't respond to pings, and worse yet, the TOC button on
the back of the machine has no effect.  The kernel debugger
is of no use either.  The only way to recover is to hit
the power switch on the front of the machine and reboot.
This obviously does not produce a core dump.  I've been
unable to get pim_info either.  I suspect an infinite kernel
loop (at splhi?), but some shred of debug info is going
to be required to start tracking this one down.

[10/8/93 public]
Since I haven't seen this problem since I filed the bug, the
configuration that reproduced it has been torn down, and 
there's really no useful information to save in this report 
(other than it happened), I'm cancelling this bug.



CR Number                     : 8498
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8485
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : DFS server and client in a tight loop
Reported Date                 : 8/23/93
Found in Baseline             : 1.0.3
Found Date                    : 8/23/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : unknown
Sensitivity                   : public

[8/23/93 public]

Test config:

DFS server - dce10/HP with /u0, /u1 and /u2 exported
DFS client - toaster/HP
Test-: client doing "build install_all" with its install directory 
soft-linked to dce10:/u0.

Bug description:

Both the DFS server and the DCE client got into a tight loop after some
minutes into the installation process --- looping about five hours before 
I reboot both systems. On the server side, three dfsd kernel processes 
consumed 57% of CPU; and, on the client side, the release process and one 
dfsd spent about 30%. 

Two dfstraces, one for each side, are in "hsiao" directory:

	dfstarce.dump.server
	dfstrace.dump.client

[10/1/93 public]
Mike's looking at this one.

[10/18/93 public]
Added interdependency to CR 8498, since both are (easily) reproducible
via the same method.

[12/15/93 public]

I'm closing this bug, since I have not been able to reproduce it in
the 1.0.3a code drop.

[12/15/93 public]
Changed status to cancel since we don't really know what changed.



CR Number                     : 8485
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8498
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : communication failure with server
Reported Date                 : 8/19/93
Found in Baseline             : 1.0.3
Found Date                    : 8/19/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : unknown
Sensitivity                   : public

[8/19/93 public]

The comm-failure happened consistently(four out of four) when a client
(toaster/HP) was doing "build install_all" with the install directory 
soft-linked to a DFS server disk(dce10:/u0). 

It seems to me that the failure is too pre-matured. The install script output
the following error message immediately after the "release" line.

+++ Part of the install output +++

makepath idl/. && cd idl &&  exec make MAKEFILE_PASS=BASIC     install_all
release   -o bin -g bin -m 755 -tostage /u1/hsiao/sbox/102a_dfs/install/hp800/op
t/dce1.0 -fromfile /.../dce10_cell.osf.org/fs/dce1.0.2a/src/security/idl/rgynbas
e.idl /
share/include/dce/rgynbase.idl
dfs: communication failure with server 130.105.5.10 in cell dce10_cell.osf.org.
dfs: lost contact with the fx server 130.105.5.10 in cell dce10_cell.osf.org
release: open /u1/hsiao/sbox/102a_dfs/install/hp800/opt/dce1.0/.release.chk: mak
e: don't know how to make Connection timed out
/u1/hsiao/sbox/102a_dfs/install/hp800/opt/dce1.0/share/include/dce/rgynbase.idl:
 aborting installation
binding.idl. Stop
*** Error code 2
1 error
*** Error code 2
1 error
~

[8/19/93 public]

After the comm failure I get rpc_s_helper_wrong_user(code 382312714) on a 
"ls /:" after "cm stat":

+++ ls /:/u0 history +++

>ls /:/u0
/:/u0 not found
>cm stat
All servers are running.
>ls /:/u0
dfs: set auth binding failed (code 382312714)
dfs: Warning: created unauthenticated binding
102a        102a_dfs    disktab     foo         hsiao       lcache      lost+fou
nd
>ls /:/u0
102a        102a_dfs    disktab     foo         hsiao       lcache      lost+fou
nd

[10/14/93 public]
Haven't seen in a while, downgrading to 2.

[10/14/93 public]

This bug was quite re-producible when discovered. I haven't tried to re-produce
it recently but that doesn't mean "Haven't seen in a while".

To re-produce it, follow the following scenario:

	1. "build" the whole DCE(core+dfs)
	2. link install tree to a DFS disk
	3. "build install_all"

[10/18/93 public]

Added interdependency to CR 8498.  Both are easily reproducible via
the method described above.  Changed priority back to A1.

[12/15/93]

I'm closing this bug, since it is not reproducible in the 1.0.3a code drop.

[12/15/93 public]

Changed status to cancel, since we don't really know what changed.



CR Number                     : 8481
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8881
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : config
Short Description             : SCM doens't allow long hostname
Reported Date                 : 8/19/93
Found in Baseline             : 1.0.3
Found Date                    : 8/19/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : dfs_config
Sensitivity                   : public

[8/19/93 public]
need to allow long hostname (hostname.whatever.else) when config'g Server
Control Machine
Upclient was unable to find Upserver
Upserver was an ibm machine using long hostname....
configuration data should allow hostname to remain long
it currently strips the name to the first .
dfs_config file
config_dfsfs & config_dfsfldb routines do the following:
SCM=`echo $SCM | cut -f1 -d.`
workaround is to set hostname correctly in Bos configuration and
restart process

[10/8/93 public]
This was fixed by Tom Jordahl under 8881, but I can't dup
forward so I'll have to cancel it.



CR Number                     : 8388
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : Missing cm exportdfs and cm statservers man pages
Reported Date                 : 7/30/93
Found in Baseline             : 1.0.2
Found Date                    : 7/30/93
Severity                      : C
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : None.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[7/30/93 public]
In the Administration Reference dated 3/27/93, the man page for
'cm exportdfs' is missing.  In addition, it is not listed
in the 'cm' man page.
Also, the 'cm statservers' man page is missing.  (This one is,
however, listed in the 'cm' man page.)

[7/30/93 public]
I am canceling this defect for the following reasons:
 
1. The cm exportdfs command is not a supported DFS command; it was removed
   prior to 1.0.1.  It is not a part of the supported DFS interface, so it
   doesn't have a manpage.  If the command is visible to the user, a code
   defect needs to be opened to remove it from the interface.
 
2. The cm statservers command has a manpage in the DCE User's Guide and
   Reference.  The cm manpage in the DFS Admin Ref lists the cm statservers
   command as a level 1dfs command, directing the reader to the User's Guide
   and Ref for information about the command.  (The same is true of the other
   commands such as cm whereis.)
 
Thanks for reporting the possible problems.  It's good to know that someone
somewhere is reading this stuff.
Filled in Interest List CC with `kdu@transarc.com' 
Changed Status from `open' to `cancel' 
Filled in Affected File with `None.' 
Filled in Responsible Engr. with `jeff@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `closed'



CR Number                     : 8374
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : ?
Short Description             : fs test suite failures on hpux
Reported Date                 : 7/27/93
Found in Baseline             : 1.0.3b1
Found Date                    : 7/27/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[7/27/93 public]


The fs tests 7,8, and 10 are failing on hpux.  Here are the partial
logs:



TEST7:

test7 iteration number 1
Number of files obtained by find: 1
Filesizes obtained by wc: 16384
- diff -r failed (E3)
test7 iteration 1 done

fs/test7 returning 1 in /.../headache.qadce.osf.org/fs/dfs-test/fs
Tue Jul 27 16:56:53 EDT 1993
---------


TEST8:

This is a DFS directory
The file '.' resides in the cell 'headache.qadce.osf.org',
in fileset 'root.dfs', on host explorer.osf.org.
Fileset Name          Quota    Used  % Used   Aggregate
root.dfs             4278870971   26489     0%     0% = 26489/4278870971 (non
-LFS)
DFS is using 973 of the cache's available 10000 1K byte (disk) blocks.


Move directory failed (E13)
Copy directory failed (E15)

fs/test8 returning 2 in /.../headache.qadce.osf.org/fs/dfs-test/fs
Tue Jul 27 16:57:24 EDT 1993
#
----


TEST10:

Details of directory #2 - /.../headache.qadce.osf.org/fs/dfs-test/fs
-------------------------------
This is a DFS directory
The file '.' resides in the cell 'headache.qadce.osf.org',
in fileset 'root.dfs', on host explorer.osf.org.
Fileset Name          Quota    Used  % Used   Aggregate
root.dfs             4278870971   26716     0%     0% = 26716/4278870971 (non
-LFS)
DFS is using 1421 of the cache's available 10000 1K byte (disk) blocks.


test10 starting process number 1
   test10 starting iteration 1, process number 1
- test10 process 1: diff -r failed (E5)
- test10 process 1: diff -r failed (E8)
- test10 process 1: diff -r failed (E10)
- test10 process 1: diff -r failed (E11)
   test10 iteration 1, process number 1, done
test10 process number 1 done

fs/test10 returning 1 in /.../headache.qadce.osf.org/fs/dfs-test/fs
Tue Jul 27 16:59:18 EDT 1993

[07/28/93 public]

It appears that the diff command on HPUX has some different 
and unexpected behavior.  The following sequence of commands
illustrates the problem:

# mkdir abc abc/def abc/def/ghi
# mkdir abc1
# cp -r abc abc1
# diff -r abc abc1/abc
Common subdirectories: abc/def and abc1/abc/def
Common subdirectories: abc/def/ghi and abc1/abc/def/ghi
# echo $?
2

The same sequence of commands on an OSF system will return 0.

[07/28/93 public]


There is a bug in the HPUX diff command which is causing
all of the observed test failures.  The fix
is to obtain a new "diff".



CR Number                     : 8317
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : dfsexport hangs (sometimes) exporting episode aggregate
Reported Date                 : 7/19/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/19/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[7/19/93 public]
While testing the ability to use different block and fragment combinations
on an episode aggregate, dfsexport hung. The test script repeatedly
newaggr's the same aggregate, exports it, creates filesets and mount points,
deletes filesets and mount points, and detaches it for all possible combos.
(dfs.block_frag and block_frag.data to be submitted this week along with
other scripts that execute the "stress factors" listed in the 102 and 102a
plans).
This particular run of the script hung on the first try, although previous
runs had succeeded.
Logging was:
START DATE AND TIME IS Mon Jul 19 16:14:07 EDT 1993
dfs.block_frag: Formatting aggregate
*** /dev/rrz1a ALREADY CONTAINS AN EPISODE FILE SYSTEM
*** CONTINUING
*** Using default initialempty value of 1.
*** Using default number of (65536-byte) blocks: 1599
*** Defaulting to 15 log blocks                       (maximum of 2 concurrent transactions).
/dev/rrz1a: Already marked as non-BSD.
Done.  /dev/rrz1a is now an Episode aggregate.
dfs.block_frag: Formating w/ 65536 65536 PASSED
dfs.block_frag: Exporting w/65536 65536
I am unable to CTL-C out of the script or kill the dfsexport process. The
ps output indicates it is not gaining time/spinning in user space so a
kdb interrupt might show something useful.
dfstab file contains:
# blkdev aggname aggtype aggid 
/dev/rz1a      lfs_aggr2        lfs     2
/dev/rz1e      lfs_aggr3        lfs     3
/dev/rz1d      /u3              ufs     33      0,,28
and has NOT been editted since the last successful dfsexport of /dev/rz1a.
The command syntax was:
dfsexport /dev/rz1a

[7/19/93 public]
Using kdb, the only interesting thread stack around was:
thread_block()
mp_sleep()
sleep()
Lock_Obtain()
elbb_SyncForce()
efs_sync()
ag_efsFlagsToStates()
end()
Please let me know ASAP what other information would be useful as I would
like to be able to continue testing on this machine ... Thanks.

[7/19/93 public]
The newaggr lists the episode aggregate as having a maximum of two
concurrent transactions.  This may very well be too small.  I would attempt
to set the script up so that no matter what the block size, there were 
at least 4 concurrent transaction supported by each aggregate.
Can you please reproduce this problem on a RIOS.  We have no pmax platforms,
and thus cannot guarantee that we will be able to reproduce it here.
Can you please send us the scripts that you use to test this.  We will
run them here on a RIOS and see if we can reproduce the problem.
I've assigned this to mike comer for safe keeping.  I do not believe that 
this defect is stop ship for 1.0.2a.
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/19/93 public]
Thanks Elliot. I've mailed you the script and datafile.

[7/20/93 public]
Rebooting and retrying the dfsexport w/o any changes to the aggregate also
hangs with a stack trace of:
task ... [dfsexport] ... W_EVENT wantkmemmap
thread_block()
malloc()
osi_kalloc()
osi_Alloc()
elbb_LogInit()
epig_Init()
end() bad ref by kdb ...
This stack may be questionnable due to the "bad ref" message but just in
case, thought I'd include this clue here.

[7/20/93 public]
elbb_Alloc tries to alloc a piece of memory the size of a block.
You're starting with a block size of 64k.  Does this mean that 
the OSF/1 memory allocator has been fixed?

[7/20/93 public]
The "real" thread stack was probably more like this:
thread_block()
mp_sleep()
sleep()
Lock_Obtain()
elbb_SyncForce()
efs_sync()
ag_efsSync()
afscall_aggr()
syscall [or whatever it is called in OSF/1]
...
This represents a fileset deletion.  (The only caller of AG_SYNC is the
fileset deletion code in ftserver/ftserver_vprocs.c.)  It is trying to obtain
the global lock on the logged buffer system.  (That is, the call to Lock_Obtain
is in the body of the macro BUFFER_LOCK, invoked in elbb_SyncForce.)
.
This seems like a perfectly normal state of affairs and I cannot tell, from
this information only, why there would be a deadlock.  Presumably either
someone has forgotten to release the buffer lock, or there is some other thread
that is holding that lock and is somehow deadlocked with this thread.
Note that this is NOT the stack of a call to dfsexport.  If dfsexport is
also hung, it might be helpful to see its stack too.  (However it might just
be hung trying to obtain the same lock, which would not shed any new light
on the situation.)  My advice is, if you see this again, get stack traces
of ALL the threads that you can.  Also, if there is any way you can improve
the quality of the stack traces (i.e. show static functions such as ag_efsSync
instead of ag_efsFlagsToStates, and show the entire stack), that might be
helpful too.

[7/21/93 public]
I couldn't reproduce this on a rios so I'm assigning this back to you,
Gail, as pmax-specific.  I ran the script you sent to Elliot 14 times
in a row successfully.  I then got some failures -- probably due to my
ticket expiring -- but no hangs.  After cleaning things up (but not
rebooting), the script continues to run.
By the way, there was a bug in the version of the script I got where
the list of filesets wasn't being cleared on delete.
Filled in Interest List CC with `comer@transarc.com' 
Changed Responsible Engr. from `comer@transarc.com' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[7/30/93 public]
Since this appears to be pmax-specific BUT may prove to be a test/code/doc bug
that 4 concurrent transactions is the minimum, I'm leaving this open and moving
it to 1.0.3.

[8/9/93 public]
4 concurrent transactions should not be required for things to work
properly.  I suspect this scenario is hitting problems in the pmax
kernel -- at least on of the problems your mention above looks related
to the memory allocation bug/feature of OSF/1.  
Changed Priority from `1' to `2'

[10/8/93 public]

Aged relative to Aug 1.

[10/14/93 public]

Both Steve Moriarty and I have successfully run dfs.block_frag on the HP. I am doublechecking
that the low number of concurrent transactions was hit before cancelling.

[10/14/93 public]

Forced number of transactions to be 1 by restricting the logsize to be 10 (switch on newaggr)
and hit a whole different problem running dfs.block_frag (with all newaggr's
containing the -logsize 10):

dfs.block_frag: Creating 11 filesets on aggr w/ 32768 16384 PASSED
dfs.block_frag: Fileset operations not yet implemented
dfs.block_frag: Deleting filesets
Returned from rpc_server_listen: cthread create failed (dce / rpc)
RPC no longer listening to network: cthread create failed (dce / rpc).
AFS_SetContext(budapest.osf.org) fails: Internal corruption (dfs / fsh)
Error in delete: Internal corruption (dfs / fsh)
delete FAILED

Closing this CR and opening another ... net gain 0 except for the aging.



CR Number                     : 8282
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : utst setup script is too restrictive.
Reported Date                 : 7/15/93
Found in Baseline             : 1.0.2b7
Found Date                    : 7/15/93
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[7/15/93 public]
The setup script for the utst test requires you to be running on the 
dce server machine in order to start the dce servers, or to initailize
the namespace entries used by the test.  This requirement is correct 
for the former case, but not for the latter.  The fix is to remove the
condition that checks to see that the local host is the dce server
machine from the conditional that gates the execution of the namspace
initialization code.   The workaround is to always run the setup script
first on the dce server, then on the other machines.

[9/22/93 public]
This is a functional test defect, so i'm reassigning
it to transarc.

[9/22/93 public]
Filled in Interest List CC with `jaffe@transarc.com' 
Changed Responsible Engr. from `jaffe' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/13/93 public]
Work done under OT8593.  Sorry, ot wouldn't let me dup it.
Changed Status from `open' to `cancel'



CR Number                     : 8251
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : rep test3 fails rios to rios
Reported Date                 : 7/8/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/8/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[7/8/93 public]


Replication test3 fails when run rios to rios:


Added replication site barge m1.aggr2 for fileset Rubble.23563.3
Passed
Added replication site cobbler m3.aggr2 for fileset 5459
Passed

Checking sites...
- sites are wrong (1):
4d3
< fileset 4 on cobbler /dev/lv02, sa 1:17:00.
rtest3: Test case, E123: FAILED


----


# pwd
Checking sites...
- sites are wrong (1):
2d1
< fileset 1 on cobbler /dev/lv02, sa 0:31:00.
4d2
< fileset 2 on cobbler /dev/lv02, sa 0:10:00.
6d3
< fileset 3 on cobbler /dev/lv02, sa 0:31:00.
8d4
< fileset 4 on cobbler /dev/lv02, sa 0:30:00.
rtest3: Test case, E125: FAILED

Checking FLDB info, sixth time


We'll see if we can get more detailed information.

[07/13/93 public]

Apparently there was some cruft left over from
previous runs which failed due to setup problems.
This seems to upset the reptests (i.e., having other replicated
filesets in the cell).

Cancelling.



CR Number                     : 8227
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : ftserver
Short Description             : ftserver appears hung during cho
Reported Date                 : 7/2/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/2/93
Severity                      : B
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : open

[7/2/93 public]
This is an all-rios cho configuration.  After more than 20 hours one of the
lfs filesets exported by machine1 (barge) became unavailable:
# cd m1.lfs1
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
^Z
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
dfs: fileset (0,,4) is busy with code 691089542 on server 130.105.5.20 in cell dfs_only_works_at_transarc.
# fts statft -server /.:/hosts/barge
Error in statftserver: communications failure (dce / rpc)
Here's the end of the ftlog on barge; Note that there has not been any logging in the
last 12 hours:
 
93-Jul-01 21:53:10 vols_Restore: returning 0
93-Jul-01 21:53:10 Restored fileset 0,,5309/3: returned code 0
93-Jul-01 21:53:25 Restoring fileset 0,,5309/3
93-Jul-01 21:53:25 vols_Restore: returning 0
93-Jul-01 21:53:25 Restored fileset 0,,5309/3: returned code 0
93-Jul-01 22:16:09 ftserver_CreateVolume: created ft4.28585 as 0,,5320 on aggr 3
93-Jul-01 22:48:12 ftserver_CreateVolume: created fts.test14.28476 as 0,,5323 on aggr 3
93-Jul-01 22:58:14 ftserver_CreateVolume: created fts.test14.28476 as 0,,5323 on aggr 3
93-Jul-01 22:58:59 Restoring fileset 0,,5323/3
93-Jul-01 22:59:01 vols_Restore: returning 0
93-Jul-01 22:59:01 Restored fileset 0,,5323/3: returned code 0
93-Jul-01 22:59:20 Restoring fileset 0,,5323/3
93-Jul-01 22:59:20 vols_Restore: returning 0
93-Jul-01 22:59:20 Restored fileset 0,,5323/3: returned code 0
 
ps -deaf | fgrep ftserver
    root 15789 15018   0   Jun 30      - 23:18 /opt/dcelocal/bin/ftserver 
I tried to get an icl log but the ftserver does not appear to respond.
It does not appear to be using any cpu time either.

[7/2/93 public]
The error code is the new VOLERR_TRANS_PENDING one, saying that some process
is in the middle of a VOL_AGOPEN syscall desiring to take this fileset
off-line for some reason, and the exporter returns this failure to allow
the fileset to become available for the operation.  However, the VOL_AGOPEN
is not allowed to proceed until the activeVnops count for the fileset goes
to zero.  Clearly, here, the ftserver is the process stuck in the kernel
doing this syscall, as it's not tracing or logging anything, or responding
to RPCs: it's in the kernel, in a system call.  The question would be why
the activeVnops count isn't reaching zero.  There's a new ICL log that can
be dumped (called ``disk,'' it logs the ``xops'' ICL set among others); its
contents may be useful here.  Also useful would be to know what the current
activeVnops count is, since it looks like this value is being incremented
without being decremented.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/7/93 public]
I'm not sure that Transarc can make any further progress on this item.  I'm
happy to leave this assigned to Transarc, but I would like the next person 
to hit this problem to contact us so that we can gather as much information
as possible.
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com' 
Filled in Transarc Status with `open'

[07/16/93 public]
Defer to 103 since this does not appear to be in our
way right now.

[9/28/93 public]
This defect is getting a little long in the tooth.  Is it still a problem?
Can we cancel it?

[10/8/93 public]
There doesn't seem to be any activity on this defect so I'm giving it
back to Diane.  Please assign it back to me if there's still a
problem.
Filled in Interest List CC with `comer@transarc.com' 
Changed Responsible Engr. from `comer@transarc.com' to `delgado' 
Changed Resp. Engr's Company from `tarc' to `osf'

[10/08/93 public]

Has not been seen on the 1.0.3 code base - cancelling



CR Number                     : 8195
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : rios client hangs on close if disk quota is exceeded.
Reported Date                 : 6/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/23/93
Severity                      : C
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/23/93 public]
While running dfs.lock, the fileset became full enough that there was not
enough room for the whole data file to be written.  On a pmax, the behavior
in this case was somewhat expected; the client did a bunch of reads, and
then returned a "disc quota exceeded" error on closing the file.  On the 
rios, the client hangs on the file close in this situation.

[6/23/93 public]
Changed Responsible Engr. from `jaffey' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/28/93 public]
I have tried to run several tests to create that "disk quota exceeded" 
situation but without any success. In fact, I suspect that there was some 
other thing went wrong that caused the system hung at the time rather than 
this disk full error. Please cancel it if you have not been able to 
reproduce it any longer. thanks.
Filled in Subcomponent Name with `cm'

[10/4/93 public]
We have been unable to reproduce this problem.  You may wish 
to wait for the Transarc drop to attempt to reproduce this bug.
Changed Responsible Engr. from `tu@transarc.com' to `mhickey' 
Changed Resp. Engr's Company from `tarc' to `osf'

[10/8/93 public]

Aged relative to Aug 1.

[10/14/93 public]
Assigned to Gail to reproduce.

[11/24/93 public]
I thought I'd hung but actually I did get the following message eventually:

cp: cannot close /:/RP-final_logs: No space left on device

There was at least a five minute delay between running out of space and
having this error returned to me. This is not as troubling as the fact
that a partial file with no permission bits is left around. 

root@vodka> ls -l /:/RP*
----------   1 guest    12       88276992 Nov 24 16:43 /:/RP-final_logs

The copy was from a HP native filesystem to a RIOS lfs fileset whose
quota was larger than the available space on the logical volume for
the lfs fileset and smaller than the file being copied. Is this related
to the HP-specific cp error (CR 9552)? I will continue testing with the
filewnr program to see if the "hang" portion of the problem is specific
to the test.

[11/24/93 public]
Tested by starting a filewnr -n 10000 -f /:/<client_machine_name>_filewnr.out
on both a rios and hp. (both are flservers, file servers and clients - the
rios happens to be the file server for root.dfs). Neither test should complete,
regardless of what the other does, based on lack of physical space on the
logical volume housing root.dfs.

The rios 1/2 of the test was started ~2 minutes before the hp 1/2 and exited
correctly with a failure on close. The hp 1/2 appears to have hung - the
size of the file remained constant for 20 minutes while the filewnr 
program did not reported anything - but did eventually report that the
close failed. So ... I would say that either the hang WAS due to some
other cause OR was reported prematurely. Note that all writes returned
success even those that occured over 15 minutes after the device
was full. I believe this is the undesirable but consistent/expected behavior
of dfs.

So - are the incomplete files supposed to be left around? What happened
to the permissions on the incomplete copy? I'll cancel this and open
separate bug(s) when responses are received here - thanks!

[01/16/94 public]
Canceling - both exceeding quota AND "No space left on device" are reported.



CR Number                     : 8194
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : set auth binding fails with  sec_login_s_no_current_context
Reported Date                 : 6/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/23/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/23/93 public]
We have now seen this more than once, first with cho and then
with other stress testing:
dfs: set auth binding failed (code 387064044)
dfs: Warning: created unauthenticated binding
:#define sec_login_s_no_current_context (387064044)
I am logging this as a '1' because the use of the unauth binding 
will cause permissions problems with the cho tests.  This seems
to pop up during stress.
Also, I have not seen this before we got the dd

[6/24/93 public]
Diane, 
It could also be the problem that the caller has a bogus PAG number stored
in the CM. Would you want to set a brk pt at 
    cm_printf("dfs: set auth binding failed (code %d)\n, st)
When it stops next time, read  connp->authIdentity[0] and 
connp->authIdentity[1]. 
connp->authIdentity[1] should have the pag number and connp->authIdentity[0]
has the /use/bin/login id.
Filled in Interest List CC with `tu' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/24/93 public]
The message may be more or less harmless, depending on what the RPC handle
is to be used for.  There are some cases of move-TSR (that Tu is fixing in
related work) in which the cm_rrequest structure isn't being initialized
before it's used, but if it's used only for token grants, it really doesn't
matter.

[07/09/83 public ]

move to 103

[08/4/93 public]

We have now seen this on HPUX.  

dfs: lost contact with the fx server 130.105.5.22 in cell headache.qadce.osf.org
dfs: communication failure with server 130.105.5.22 in cell headache.qadce.osf.org.
dfs: set auth binding failed (code 387064044)
dfs: Warning: created unauthenticated binding
dfs: fx server 130.105.5.22 in cell headache.qadce.osf.org back up!

[11/5/93 public]

Cancelling - We have not seen this for at least a month.



CR Number                     : 8189
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : unknown
Short Description             : the rios panics during cho
Reported Date                 : 6/22/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/22/93
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/22/93 public]
This has happened now two nights in a row.  After about 3 hours
of cho machine 1 panics.  I have two crash dumps on alcatraz in
/u0/dump-jun-22 and /u0/dump-jun-21.  There is not much information
to go on here.  The stack of the running process is 0 in both cases.
Please do not down grade the priority or severity of this bug as it
is blocking us from meeting our exit criteria.

[6/22/93 public]
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/22/93 public]
Transarc can not make any progress on this problem until there is more
information.  Its clear that the machine is crashing, but we have no clue
why or where.  I would suggest making sure that the dump volume (/dev/hd7)
has at least as much space as the physical machine memory.  Having a debugger
on the machine will greatly improve our changes of debugging this.  Since we
havn't seen this at Transarc, we can't provide much assistance at this point.
Filled in Interest List CC with 
 `jaffe@transarc.com,cfe@transarc.com,tu@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `delgado' 
Changed Resp. Engr's Company from `tarc' to `osf'

[06/22/93 public]
Did you look at the crash dumps or are you just guessing?\
All the information in them is not zero. There's stuff
in there about what was runnable at the time, and rather
than my having to pull all of that out of the crash dumps
and put it in this ot I though it would be better for
someone at transarc to look at them.
Since we do not have a working rios debugging enviornemnt
the crash dumps will have to suffice.  They do contain
the state of the system at the time of the crash, so
there must be something in there you can use.

[6/23/93 public]
Well, This has been interesting.  I finally got through gazacha onto alcatraz.  After poking around for a while, I found the following things:
When I logged on the machine was very slow.
You are running AIX 3.2.3 (I can tell because you have a /usr/lpp/bosperf
directory, which exists only on 3.2.3)
Your machine is a 16 Meg rios.
Your paging to three paging partitions, on three different disks, and each of
these disks is operating at close to %50 of capacity.  I'de say that much of
the slowness is caused by the paging.
Your /dev/hd7 partition which is used for dumping crash dumps is only 8 meg.
This means that you never get a complete crash dump, which may be why we have
so much trouble with the crash dumps.
All I can tell from the crash dumps for 8189 is that some kernel thread caused
a Data Storage Interrupt.  With a larger /dev/hd7, or a debugger, we could
probably track this down.
I am going to reassign 8189 back to the OSF because you need to get more info
for anyone to make progress.
Changed Responsible Engr. from `jaffe@transarc.com' to `delgado' 
Changed Resp. Engr's Company from `tarc' to `osf'

[06/25/93 public]

This usually happens within 3 hours of adding the acl tests to the cho
cell. Note, we've found that it seems the system has to be loaded and
have the acl tests running to it from another client in order for this
to happen.  Yesterday, we got a hang instead of a crash, and today
we got another crash:

> trace
STACK TRACE:
        [dfscore.ext:zlc_PruneQueue] ()
Frame pointer not valid.
> pcb
        USER AREA FOR thr$ (ProcTable Address 0xe3003a00)

SAVED MACHINE STATE 
    curid:0x00003a64  m/q:0x00137683  iar:0x05989cb0  cr:0x24000082 
    msr:0x000090b0  lr:0x05989c8c  xer:0x00000018
    ctr:0x000863c0  *bus:0x00000000
    *prevmst:0x00000000  *stackfix:0x00000000  intpri:0x0000000b 
    backtrace:0x00  tid:0x00000000  fpeu:0x00  ecr:0x00000000
    Exception Struct
      0x000007ae  0x0a000000  0x00000000  0x000007ae  0x00000106
    Segment Regs
     0:0x00000000   1:0x007fffff   2:0x00001210   3:0x007fffff  
     4:0x007fffff   5:0x007fffff   6:0x007fffff   7:0x007fffff  
     8:0x007fffff   9:0x007fffff  10:0x007fffff  11:0x007fffff  
    12:0x007fffff  13:0x007fffff  14:0x00000606  15:0x007fffff  
    General Purpose Regs
     0:0x00000000   1:0x2ff7f698   2:0x059e31b0   3:0x00000001  
     4:0x00000780   5:0x00000000   6:0x00000001   7:0x00000000  
     8:0x000001f0   9:0x00000185  10:0x00137683  11:0x05d75980  
    12:0x0597f1d4  13:0xdeadbeef  14:0xdeadbeef  15:0xdeadbeef  
    16:0xdeadbeef  17:0xdeadbeef  18:0xdeadbeef  19:0xdeadbeef  
    20:0xdeadbeef  21:0xdeadbeef  22:0xdeadbeef  23:0xdeadbeef  
    24:0xdeadbeef  25:0xdeadbeef  26:0xdeadbeef  27:0xdeadbeef  
    28:0xdeadbeef  29:0xdeadbeef  30:0x0598aaa0  31:0x059899e4  
    
Floating Point Regs
        Fpscr: 0x00000000 
     0:0x00000000 0x00000000  1:0x00000000 0x00000000  2:0x00000000 0x00000000 
     3:0x00000000 0x00000000  4:0x00000000 0x00000000  5:0x00000000 0x00000000 
     6:0x00000000 0x00000000  7:0x00000000 0x00000000  8:0x00000000 0x00000000 
     9:0x00000000 0x00000000 10:0x00000000 0x00000000 11:0x00000000 0x00000000 
    12:0x00000000 0x00000000 13:0x00000000 0x00000000 14:0x00000000 0x00000000 
    15:0x00000000 0x00000000 16:0x00000000 0x00000000 17:0x00000000 0x00000000 
    18:0x00000000 0x00000000 19:0x00000000 0x00000000 20:0x00000000 0x00000000 
    21:0x00000000 0x00000000 22:0x00000000 0x00000000 23:0x00000000 0x00000000 
    24:0x00000000 0x00000000 25:0x00000000 0x00000000 26:0x00000000 0x00000000 
    27:0x00000000 0x00000000 28:0x00000000 0x00000000 29:0x00000000 0x00000000 
    30:0x00000000 0x00000000 31:0x00000000 0x00000000 
> 



-r
SLT ST    PID   PPID   PGRP   UID  EUID  PRI   CPU   EVENT  NAME
  2 r      202     0     0     0     0   127   120          wait  
        FLAGS: swapped_in no_swap fixed_pri kproc
  3 r      303     0     0     0     0    39     0          netw  
        FLAGS: swapped_in no_swap fixed_pri kproc
 13 r      d07     1   d07     0     0    60     0          errdemon  
        FLAGS: swapped_in wake/sig locks
0452-220: Cannot read process table entry  33.
 45 r     2d4b  2c4a  2c4a     0     0    68    17          flserver  
        FLAGS: swapped_in
 52 r     349a  3353  349a     0     0    93    67          crash  
        FLAGS: swapped_in
 56 r     3862  365f  1a29     0     0    61     2          thr   
        FLAGS: swapped_in kproc
 58 r     3a64  365f  1a29     0     0    61     2          thr$  
        FLAGS: swapped_in kproc
0452-220: Cannot read process table entry  78.


I haven't got through the first crash dump yet as this latter crash
happened after I restarted dfs and was trying to look at the first
crash dump.  


Crash dumps are there on alcatraz in /u0 - please have a look and
see what you think.

[07/09/93 public]

move to 103

[09/01/93 public]


This isn't a zero anymore.  Will attempt to veryify on 1.0.3
or cancel

[10/7/93 public]

We ran 48 hours of cho with the ACL tests on 1.0.3 and did
not see the problem.



CR Number                     : 8166
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : holding wrong lock in truncate
Reported Date                 : 6/16/93
Found in Baseline             : 1.0.2
Found Date                    : 6/16/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[6/16/93 public]
The solaris port is holding the wrong lock in TryToSmush and TruncateFile.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/17/93 public]
Solaris specific defects do not belong in OT.
Changed Status from `open' to `cancel'



CR Number                     : 8140
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : bakserver hangs on deletedump
Reported Date                 : 6/10/93
Found in Baseline             : 1.0.2
Found Date                    : 6/10/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open

[6/10/93 public]
The bakserver has been hanging when deletedumps are done on it. 
The sequence required to hang it is
	Do a Deletedump of a non-existent dump
	do another deletedump
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[6/11/93 public]
This is a duplicate of Sybase bug 3565
Filled in Interest List CC with `vijay' 
Changed Status from `open' to `cancel'



CR Number                     : 8135
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 8118
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : The dfs doc bugs identified in
the DCE 1.0.2 Release Notes, Section 1.10.4.5, should be fixed in the DCE
Administration Reference.
Reported Date                 : 6/10/93
Found in Baseline             : 1.0.2
Found Date                    : 6/10/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[6/10/93 public]

[6/18/93 public]
This defect pertains to fixing defects listed in Section 1.10.4.5 of the 1.0.2
DCE Release Notes.  This section documented information associated with the DFS
Admin Ref.  All DFS Admin Ref defects, however, are associated with the section
for DFS Admin Guide defects.  Therefore, I am canceling this defect; see defect
8118 for a global DFS release note defect.
Filled in Inter-dependent CRs with `8118' 
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Filled in Subcomponent Name with `admin_ref' 
Changed Status from `open' to `cancel' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `closed'



CR Number                     : 8129
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : more detach failures
Reported Date                 : 6/9/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/9/93
Severity                      : C
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : 

[6/9/93 public]
Detach appears to be successful, but it is not sucessful (code base is
nightly build as of 6/9/93):
root@explorer  # dfsexport -detach /dev/rz3d
dfsexport: Revoking tokens for filesets on aggregate 3...
root@explorer  # newaggr /dev/rz3d -block 4096 -frag 512 -log 300  
newaggr: The aggregate is in use; detach before continuing (dfs / ftu) locking device
root@explorer  # dfsexport
WARNING: The UFS aggregate /dev/rz3b has 0 filesets
dfsexport: /dev/rz3b, ufs, 2, 0,,0
dfsexport: /dev/rz3d, lfs, 3, 0,,0
root@explorer  # dfsexport -detach /dev/rz3d
WARNING: The UFS aggregate /dev/rz3b has 0 filesets
dfsexport: Revoking tokens for filesets on aggregate 3...
dfsexport: Failed to detach /dev/rz3d:exp.aggr1 (Aggregate is not attached (dfs / ftu))root@explorer  # newaggr /dev/rz3d -block 4096 -frag 512 -log 300
newaggr: The aggregate is in use; detach before continuing (dfs / ftu) locking device
The only thing I did to this aggregate was to export it; do an efts lsft and
then efts deleteft (which failed with "cross-device link).  I wanted to detach 
and run newaggr to get rid of the fileset but newaggr fails because the system 
thinks it's still attached.
I log this as a '1' because there does not appear to be any way around this
except to reboot, which is unacceptable.

[6/9/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `jdp@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[06/09/93 public]
The aggregate structure is still in the kernel's registry and  in dfsatab.
The kernel struct aggr has the following values:
    a_states = 0, a_refcount = 1, aggrId=3, aggrname=exp.aggr1
When I set a_states to 1 via kdb, I am then able to detatch successfully
this aggregate.

[6/11/93 public]
Diane,
 
Is it possible that the aggregate had one of it's filesets epimounted at
the point you tried to do the detach?

[06/11/93 public]
No, I didn't mount the aggregate; I only dfsexported it.

[6/21/93 public]
First, I note that we don't have the status returned by the first detach.  Its
possible that it failed, which would explain the later behavior.  There is
a -force flag to dfsexport -detach, which would probably have solved the 
problem.  
I'm going to downgrade this to a C3, since the only error was that detach
failed without printing a message as such.  The -force switch would have 
succeeded in detaching the fileset.
If you still have this problem (with the -force switch), and can reproduce
it, we will take another look.
Changed Severity from `B' to `C' 
Changed Priority from `1' to `3' 
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[8/25/93 public]
Diane, I am not sure the defect is  pmax specific or the problem has been
fixed recently. Regardless, I have run the same scenario mentioned above and
could not reproduct it in our new code base, 103-3.28 on RIOS. 
I assigned it to you so you can dispose it appropriately. 
Thanks,
# dfsexport
dfsexport: /dev/epi0, lfs, 2, 0,,0
dfsexport: /dev/epi2, lfs, 3, 0,,0
dfsexport: /dev/epi10, lfs, 4, 0,,0
dfsexport: /dev/lv11, ufs, 1, 0,,1
dfsexport: /dev/lv03, ufs, 100, 0,,10
                                                                            
# dfsexport -detach /dev/epi10
dfsexport: Revoking tokens for filesets on aggregate 4...
# newaggr  /dev/epi10 -block 4096 -frag 512 -log 300
*** /dev/repi10 ALREADY CONTAINS AN EPISODE FILE SYSTEM
Episode fragment size (512) must be at least 1k (1024).
*** Using default initialempty value of 1.
*** Using default number of (4096-byte) blocks: 8191
*** Reserving 300 log blocks                          (maximum of 16 concurrent
transactions).
Problem with making /dev/repi10 an Episode aggregate.
#  newaggr  /dev/epi10  8192 1024 -overwrite
*** /dev/repi10 ALREADY CONTAINS AN EPISODE FILE SYSTEM
*** CONTINUING
*** Using default initialempty value of 1.
*** Using default number of (8192-byte) blocks: 4095
*** Defaulting to 40 log blocks                       (maximum of 4 concurrent t
ransactions).
/dev/repi10: Already marked as non-BSD.
Done.  /dev/repi10 is now an Episode aggregate.
  
# dfsexport -all
dfsexport: /dev/lv11:/dfs (id 1):  Already attached: not attached
dfsexport: /dev/epi0:epi0 (id 2):  Already attached: not attached
dfsexport: /dev/epi2:epi2 (id 3):  Already attached: not attached
dfsexport: /dev/lv03:/jfs (id 100):  Already attached: not attached
Filled in Interest List CC with `jdp@transarc.com, tu@transarc.com' 
Changed Responsible Engr. from `jdp@transarc.com' to `delgado@osf.org' 
Changed Resp. Engr's Company from `tarc' to `osf' 
Filled in Transarc Herder with `jaffe@transarc.com'

[10/8/93 public]

Aged relative to Aug 1.

[10/15/93 public]

Dianne - I couldn't reproduce this either - Here's what I did:

1) Added ufs aggregate entry to dfstab w/o fsid (no fldbentry for it either)
root@dce13> cat dfstab
# blkdev aggname aggtype aggid (fsid-for-UFS)
/dev/dsk/c201d5s0       /u0     ufs     1       0,,1
/dev/dsk/c201d6s0       /       ufs     2

2) export
root@dce13> dfsexport -all
dfsexport: /dev/dsk/c201d5s0:/u0 (id 1):  Already attached: not attached
root@dce13> dfsexport
dfsexport: /dev/dsk/c201d5s0, ufs, 1, 0,,1
WARNING: The UFS aggregate /dev/dsk/c201d6s0 has 0 filesets
dfsexport: /dev/dsk/c201d6s0, ufs, 2, 0,,0

3) detach
root@dce13> dfsexport / -detach
WARNING: The UFS aggregate /dev/dsk/c201d6s0 has 0 filesets
dfsexport: Revoking tokens for filesets on aggregate 2...
root@dce13> dfsexport
dfsexport: /dev/dsk/c201d5s0, ufs, 1, 0,,1

4) export
root@dce13> dfsexport -all
dfsexport: /dev/dsk/c201d5s0:/u0 (id 1):  Already attached: not attached
root@dce13> dfsexport
dfsexport: /dev/dsk/c201d5s0, ufs, 1, 0,,1
WARNING: The UFS aggregate /dev/dsk/c201d6s0 has 0 filesets
dfsexport: /dev/dsk/c201d6s0, ufs, 2, 0,,0

On one attempt, while the server was in TSR mode, the detach failed with
a message about the server recovering tokens after a crash but the -force
option worked. If you have other ideas, let me know, otherwise we can
cancel.

[10/15/93 public]

Add me to the list of people who can't reproduce this bug.  I basically
followed gmd's steps using all LFS filesets.  The bug doesn't show up
there either.  Based on the number of people (4) who can't reproduce
it, the bug seems worthy of cancellation.



CR Number                     : 8115
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : MBZ code from epif_GetStatus during fts dump
Reported Date                 : 6/8/93
Found in Baseline             : 1.0.2
Found Date                    : 6/8/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[6/8/93 public]
From the IBM defect:
prefix        p
name          6404
reference     
abstract      panic on fts dump
duplicate     
state         open                        priority                          
severity      2                           target                            
age           1
notes:
    <Note by abansal (Anju Bansal), 93/06/06 16:07:58, action: open>
The machine was hung in an fts dump command.  Continuing through the debugger
caused a panic.
    <Note by pehkonen (Jean E. Pehkonen), 93/06/06 16:09:39, action: note>
asserted at line 474 of vol_efsScan
   468      epid_MakeIndexOnly (&fid, handlerp->index);
   469      code = epif_Open (VTOVH (volp), &fid, &ap);
   470      if (code == EPI_E_NOENT)
   471          return VOL_ERR_DELETED;
   472      MBZ (code);
   473      code = epif_GetStatus (ap, &fstat);
   474      MBZ (code);
   475      code = epif_Close (ap);
   476      MBZ (code);
   477      switch (fstat.mode & S_IFMT) {
Stack is:
panic
vol_efsScan
afscall_volser
kafs_syscall
ftserver was the running process and it had the kernel_lock at the time
of the panic.
Register state:
GPR0  00000000 2FF97938 000E7278 0000000B 2FF97988 2FF98000 0000000B 000090B0 
GPR8  00000002 6F6C6174 65640076 6F000000 00062B30 10005EF4 20184478 0000F0B0 
GPR16 201852A4 00000000 00000C06 2FF98000 00000000 014E98C8 014DE314 015B7C18 
GPR24 00000000 00000007 02130455 FFFFFFFF 200C0B58 014E3A58 00062B98 00000000 
ap is:
> d 2ff97938+44 100
2FF9797C   014E3A58 05F3A300 0584D80C 2FF97A58   |.N:X......../.zX|
2FF9798C   28002024 05CAF100 00000000 00000000   |(. $............|
2FF9799C   05CB9EA0 00000000 00000000 00000000   |................|
2FF979AC   FFFFFFFF 00000000 00000000 2FF979F8   |............/.y.|
2FF979BC   00000000 00000000 00000001 FFFFFFFF   |................|
2FF979CC   05D67BAC 00000C06 2FF98000 2FF97A18   |..{...../.../.z.|
2FF979DC   014E98C8 014DE314 015B7C0D 00000000   |.N...M...[|.....|
2FF979EC   0000000A 00000000 20184FF8 2FF97A58   |........ .O./.zX|
2FF979FC   05F3A314 014B5968 00000000 2FF97A58   |.....KYh..../.zX|
2FF97A0C   014E9E44 000610BC 00000000 2FF97A58   |.N.D......../.zX|
2FF97A1C   00000000 014A9600 00000000 2FF97AD8   |.....J....../.z.|
2FF97A2C   014AECAC 0146F804 015B7C0D 00000000   |.J...F...[|.....|
2FF97A3C   0000000A 00000000 FFFFFFFF 200C0B58   |............ ..X|
2FF97A4C   02130455 05F3A300 00000000 2FF97F38   |...U......../..8|
2FF97A5C   22002028 014B5FDC 0000007F 2FF97AD8   |". (.K_...../.z.|
From reply by ota:
It would be really nice to have the value of "code" that actually caused
the panic.  I don't see anything in the registers that looks like a code
value.  Also what you show for "ap" can't be right because the address
is on the stack.  All anode hanldes are allocated and should have a
address like 0x05F3A300.
Assuming the the "ap" returned from epif_Open is really valid then it
seems that the main source of error codes in epif_GetStatus is in
opening ACL containers.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/8/93 public]
Changed Fix By Baseline from `1.0.2' to `1.0.2a' 
Filled in Reported by Company's Ref. Number with `IBM6404'

[6/13/93 public]
IBM saw this defect once, but was unable to reproduce it or to provide ted
with any further information.  We may yet hit it again, but its not fixby
102a.
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[6/13/93 public]
Our test group hit this again overnight. We will be providing more
information tomorrow.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[11/1/93 public]
The detail here is a light so it is hard to tell which delta fixed this
particular OT.  I suggested two, and Carl suggested a third.  In any
case, the problem no longer seems to be occurring.
I discussed this with Carl and Tu and they agree this should be canceled.
Changed Status from `open' to `cancel'



CR Number                     : 8114
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : TLB miss under ParSetWorker()
Reported Date                 : 6/8/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/8/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[6/8/93 public]
Gail took a TLB miss on her mostly idle PMAX.  The stack appeared
hosed, as well as the PC.  It appears from the registers that 
a call to FlushQueuedServerTokens() was made from ParSetWorker()
immediately before the crash.  However, the PC was set to an
invalid address outside of FlushQueuedServerTokens().  Without
a valid PC, progress is difficult.  I've perused the 
FlushQueuedServerTokens() code for suspicious long jumps without
luck.

I've set the priority low until we can prove we can make this
happen more than once.  If so, we'll have to look at alternative
means for debugging this (icl logging/printfs in 
FlushQueuedServerTokens() as a brute force approach, other suggestions
welcome).

[6/21/93 public]
Cancelling until we can reproduce.



CR Number                     : 8093
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : fts, ftserver
Short Description             : Incremental "-time"
dump/restore does not work over LFS
Reported Date                 : 6/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/4/93
Severity                      : A
Priority                      : 0
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/4/93 public]
While trying out the admin_checklist system test, on a 2 pMax cell,
(/.../cell_of_death, nodes dce9 and prelude, single flserver on
dce9), dump and restore did the following:
 
root@dce9> cd /:
root@dce9> ls 
SWAPON                dce102_exp_dce_6250   prelude_lfs1
bradcj                dce102_exp_ode_6250   prelude_lfs2
dce102_dom_dce_6250   dce102_exp_proj_6250  prelude_ufs2
dce102_dom_proj_6250  dce9_lfs1
root@prelude  # cp -R /project/dce/build/dce1.0.2/src/test/directory/xds
/:/dce9_lfs1
root@prelude  # ls -l /:/dce9_lfs1
total 2
drwxrwxrwx   7 notes    daemon              800 Jun  3 14:39 xds
root@prelude  # ls -l dce9_lfs1/xds
total 1426
-r--r--r--   1 notes    daemon             1955 Jun  3 14:35 Makefile
-r--r--r--   1 notes    daemon            42407 Jun  3 14:34 dump.c
drwxrwxrwx   2 notes    daemon              704 Jun  3 14:35 examples
drwxrwxrwx   5 notes    daemon              384 Jun  3 14:40 tests.mhs
drwxrwxrwx   5 notes    daemon              384 Jun  3 14:35 tests.switch
drwxrwxrwx   7 notes    daemon              448 Jun  3 14:37 tests.xds
drwxrwxrwx   6 notes    daemon              416 Jun  3 14:39 tests.xom
drwxrwxrwx   6 notes    daemon              416 Jun  3 14:39 tests.xom
-r--r--r--   1 notes    daemon             9694 Jun  3 14:35 xt.h
-r--r--r--   1 notes    daemon            73203 Jun  3 14:35 xt_dsFuncs.c
-r--r--r--   1 notes    daemon             8387 Jun  3 14:35 xt_export.h
-r--r--r--   1 notes    daemon            32664 Jun  3 14:35 xt_file.c
-r--r--r--   1 notes    daemon            52669 Jun  3 14:35 xt_omFuncs.c
-r--r--r--   1 notes    daemon           294309 Jun  3 14:35 xt_parms.h
-r--r--r--   1 notes    daemon            90673 Jun  3 14:35 xt_rValue.c
-r--r--r--   1 notes    daemon            17755 Jun  3 14:35 xt_run.c
-r--r--r--   1 notes    daemon             1071 Jun  3 14:35 xt_test.c
-r--r--r--   1 notes    daemon            24816 Jun  3 14:35 xt_tree.c
 
# Do the full dump first . . .
 
root@dce9> fts dump -fileset dce9_lfs1 -time 0 -file /usr/users/treff/lfs1.dump
Dumped fileset dce9_lfs1 to file /usr/users/treff/lfs1.dump
root@dce9> fts delmount -dir /:/prelude_lfs2
root@dce9> fts restore -ftname prelude_lfs2 -server prelude -aggr prelude_lfs2 -file /usr/users/treff/lfs1.dump -overwrite
fts restore: The fileset prelude_lfs2 (0,,28) already exists in the FLDB.
fts restore: Overwriting the existing entry...
Restored fileset prelude_lfs2 on prelude prelude_lfs2 from /usr/users/treff/lfs1.dump
root@dce9> cd /:/dce9_lfs1/xds
root@dce9> ls  
total 1426
-r--r--r--   1 notes    daemon             1955 Jun  3 14:35 Makefile
-r--r--r--   1 notes    daemon            42407 Jun  3 14:34 dump.c
drwxrwxrwx   2 notes    daemon              704 Jun  3 14:35 examples
drwxrwxrwx   5 notes    daemon              384 Jun  3 14:40 tests.mhs
drwxrwxrwx   5 notes    daemon              384 Jun  3 14:35 tests.switch
drwxrwxrwx   7 notes    daemon              448 Jun  3 14:37 tests.xds
drwxrwxrwx   6 notes    daemon              416 Jun  3 14:39 tests.xom
drwxrwxrwx   6 notes    daemon              416 Jun  3 14:39 tests.xom
-r--r--r--   1 notes    daemon             9694 Jun  3 14:35 xt.h
-r--r--r--   1 notes    daemon            73203 Jun  3 14:35 xt_dsFuncs.c
-r--r--r--   1 notes    daemon             8387 Jun  3 14:35 xt_export.h
-r--r--r--   1 notes    daemon            32664 Jun  3 14:35 xt_file.c
-r--r--r--   1 notes    daemon            52669 Jun  3 14:35 xt_omFuncs.c
-r--r--r--   1 notes    daemon           294309 Jun  3 14:35 xt_parms.h
-r--r--r--   1 notes    daemon            90673 Jun  3 14:35 xt_rValue.c
-r--r--r--   1 notes    daemon            17755 Jun  3 14:35 xt_run.c
-r--r--r--   1 notes    daemon             1071 Jun  3 14:35 xt_test.c
-r--r--r--   1 notes    daemon            24816 Jun  3 14:35 xt_tree.c
root@dce9> cat >> woof
woof woof woof woof  
root@dce9> ls -l woof
root@dce9> cat >> woof
woof woof woof woof  
root@dce9> ls -l woof
-rw-r--r--   1 100      daemon               20 Jun  4 15:05 woof
root@dce9> date                              
Fri Jun  4 15:11:26 EDT 1993
root@dce9> fts dump -fileset dce9_lfs1 -time "06/04/93 15:10" -file /usr/users/treff/lfs1.dump
Dumped fileset dce9_lfs1 (incr from 4-Jun-1993 15:10) to file /usr/users/treff/lfs1.dump
root@dce9> fts restore -ftname prelude_lfs2 -server prelude -aggr prelude_lfs2 -file /usr/users/treff/lfs1.dump -overwrite
fts restore: The fileset prelude_lfs2 (0,,28) already exists in the FLDB.
fts restore: Overwriting the existing entry...
Restored fileset prelude_lfs2 on prelude prelude_lfs2 from /usr/users/treff/lfs1.dump
root@dce9> fts crmount -dir /:/prelude_lfs2 -fileset prelude_lfs2
root@dce9> cd /:/prelude_lfs2/xds
root@dce9> ls -l
root@dce9> ls: The file ./woof does not exist.
total 1426
-r--r--r--   1 100      daemon             1955 Jun  3 14:35 Makefile
-r--r--r--   1 100      daemon            42407 Jun  3 14:34 dump.c
drwxrwxrwx   2 100      daemon              704 Jun  3 14:35 examples
drwxrwxrwx   5 100      daemon              384 Jun  3 14:40 tests.mhs
drwxrwxrwx   5 100      daemon              384 Jun  3 14:35 tests.switch
drwxrwxrwx   7 100      daemon              448 Jun  3 14:37 tests.xds
drwxrwxrwx   6 100      daemon              416 Jun  3 14:39 tests.xom
-r--r--r--   1 100      daemon             9694 Jun  3 14:35 xt.h
-r--r--r--   1 100      daemon            73203 Jun  3 14:35 xt_dsFuncs.c
-r--r--r--   1 100      daemon             8387 Jun  3 14:35 xt_export.h
-r--r--r--   1 100      daemon            32664 Jun  3 14:35 xt_file.c
-r--r--r--   1 100      daemon            52669 Jun  3 14:35 xt_omFuncs.c
-r--r--r--   1 100      daemon           294309 Jun  3 14:35 xt_parms.h
-r--r--r--   1 100      daemon            90673 Jun  3 14:35 xt_rValue.c
-r--r--r--   1 100      daemon            17755 Jun  3 14:35 xt_run.c
-r--r--r--   1 100      daemon             1071 Jun  3 14:35 xt_test.c
-r--r--r--   1 100      daemon            24816 Jun  3 14:35 xt_tree.c
 
Oops.  Pertinent info:
 
prelude xds% fts lsaggr -server dce9
fts lsaggr -server prelude
There are 2 aggregates on the server dce9 (dce9.osf.org):
                  /u1 (/dev/rz1c): id=1     (non-LFS)
            dce9_lfs1 (/dev/rz3c): id=30    (LFS)
prelude xds% There are 3 aggregates on the server prelude (prelude.osf.org):
                  /u2 (/dev/rz1f): id=10    (non-LFS)
         prelude_lfs1 (/dev/rz1d): id=20    (LFS)
         prelude_lfs2 (/dev/rz1e): id=40    (LFS)
prelude xds% fts lsheader -server dce9 -long
Total filesets on server dce9 aggregate /u1 (id 1): 1
root.dfs 0,,1 RW non-LFS     states 0x10011085 On-line
    dce9.osf.org, aggregate /u1 (ID 1)
    Parent 0,,0       Clone 0,,2       Backup 0,,3
    llBack 0,,0       llFwd 0,,0       Version 0,,0      
     565704 K alloc limit;  299015 K alloc usage
     565704 K quota limit;  299015 K quota usage
    Creation Fri May  7 13:33:00 1993
Total filesets on-line 1; total off-line 0; total busy 0
 
Total filesets on server dce9 aggregate dce9_lfs1 (id 30): 1
dce9_lfs1 0,,40 RW LFS     states 0x10010005 On-line
    dce9.osf.org, aggregate dce9_lfs1 (ID 30)
    Parent 0,,0       Clone 0,,41      Backup 0,,42
    llBack 0,,0       llFwd 0,,0       Version 0,,3046   
    1048576 K alloc limit;    2114 K alloc usage
       5000 K quota limit;    2114 K quota usage
    Creation Wed Jun  2 00:11:40 1993
    Last Update Fri Jun  4 15:05:33 1993
Total filesets on-line 1; total off-line 0; total busy 0
Total number of filesets on server dce9: 2
 
prelude xds% fts lsheader -server prelude -long
Total filesets on server prelude aggregate /u2 (id 10): 1
prelude.ufs2 0,,4 RW non-LFS     states 0x10011085 On-line
    prelude.osf.org, aggregate /u2 (ID 10)
    Parent 0,,0       Clone 0,,5       Backup 0,,6
    llBack 0,,0       llFwd 0,,0       Version 0,,0      
     239553 K alloc limit;       1 K alloc usage
     239553 K quota limit;       1 K quota usage
    Creation Sun May 23 22:26:42 1993
Total filesets on-line 1; total off-line 0; total busy 0
 
Total filesets on server prelude aggregate prelude_lfs1 (id 20): 1
prelude_lfs1 0,,37 RW LFS     states 0x10010005 On-line
    prelude.osf.org, aggregate prelude_lfs1 (ID 20)
    Parent 0,,0       Clone 0,,38      Backup 0,,39
    llBack 0,,0       llFwd 0,,0       Version 0,,5671   
    1048576 K alloc limit;     137 K alloc usage
       5000 K quota limit;     137 K quota usage
    Creation Tue Jun  1 15:37:46 1993
    Last Update Tue Jun  1 17:23:30 1993
Total filesets on-line 1; total off-line 0; total busy 0
Total filesets on server prelude aggregate prelude_lfs2 (id 40): 1
 
prelude_lfs2 0,,28 RW LFS     states 0x10010005 On-line
    prelude.osf.org, aggregate prelude_lfs2 (ID 40)
    Parent 0,,0       Clone 0,,29      Backup 0,,30
    llBack 0,,0       llFwd 0,,0       Version 0,,3046   
    1048576 K alloc limit;    2114 K alloc usage
       5000 K quota limit;    2114 K quota usage
    Creation Mon May 24 23:48:48 1993
    Last Update Fri Jun  4 15:31:02 1993
Total filesets on-line 1; total off-line 0; total busy 0
Total number of filesets on server prelude: 3
 
Note that a dump -version using the exact same testcase data, filesets,
etc, worked.  Go figure.
 
I'll put the 2 original dump files, plus a dump of the, uhhhh,
incomplete target fileset,  down into the cell at 
/afs/dce/project/dce/transarc/OTxxxx, with xxxx being whatever OT
number this becomes.  Any other data I can dig up for you, let me know.

[6/04/93 public]
Since the cell won't let me do any writing into dce/project/dce/transarc,
the three dump files are in /afs/dce/project/dce/osf/OT8093.
This is an A0, since this rather simple dump/restore is, nonetheless,
part of system test . . .

[6/7/93 public]
Changed Responsible Engr. from `Elliot_Jaffe@transarc.com' to 
 `khale@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/7/93 public]
This test is mis-conceived and I'd recommend that this bug be cancelled.
The time given to the incremental dump should have been a time before the
initial complete dump was taken; that is, if the initial complete dump
were made at 15:00, then it won't make sense to create an incremental dump
(to be used with only that complete dump) based on a timestamp any later than
15:00.
 
In particular, the file ``woof'' was created after the complete dump was
made, at 15:05, and the incremental dump includes only those changes made
between 15:10 and the time of the dump (>= 15:11); this pair of dumps
(one complete and one incremental) was disjoint, and didn't span the whole
creation of the ``woof'' file.
 
The test is also flawed in that the first restoration (of the complete dump)
should have been made to create a new fileset, not overwrite an existing
fileset.  Alternatively, the prelude_lfs2 fileset should have been deleted
before the first restoration.  Ideally, you'd also inform the cache manager
that you were about to re-use a fileset name (prelude_lfs2) so that it should
discard any old information from prior incarnations of that fileset.
Filled in Interest List CC with `cfe@transarc.com'



CR Number                     : 8086
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : episode holds vol lock too long
Reported Date                 : 6/2/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/2/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[6/2/93 public]
Episode can deadlock doing a vol detach.  Here's the stack trace of interest:
(from Jean P. at IBM)
I was running connectathons from 2 different clients in the cell when I 
attempted to do a dfsexport -detach on the aggregate which contains root.dfs 
(The connectathons were running against directories in root.dfs, also).  One 
of the clients was on test4; the other on test5 when I attempted the dfsexport.
dfsexport hung with the following stack:
vms_delete			;waits for vnode I/O done
vnvm_recycle
vnm_Inuse			;iterates over lots of vnodes
vol_efsDetach			;holds volume lock
vol_Detach
ag_UnRegisterVolume
afscall_aggr
kafs_syscall
The stacks for the Episode daemon processes were:
e_wait
e_sleep
osi_aix_Sleep
Lock_Obtain
vol_efsHold		; locks volume lock to bump ref count
volreg_Lookup
efs_getvolume
efs_BioDaemon		; finishes I/O 
efs_vmm_proc
procentry
e_wait
e_sleep
e_sleepl
sleepxTc
get_bioreq
efs_BioDaemon
efs_vmm_proc
procentry
e_wait
e_sleep
osi_aix_Sleep
osi_SleepWI
epia_Async
efs_vmm_proc
procentry
The proper fix is to modify vol_efsDetach to release the volp->lock
over the vnm_InUse call, and perhaps over the epig_CloseVolume call.
Perhaps some revalidation would also need to be done when re-obtaining
the lock to verify that nothing too serious happened to the volume
structure, but I'm not sure what the rules are here when detaching a volume.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/3/93 public]
Please reset your default to 1.0.3 mike.
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[6/4/93 public]
Changed Subcomponent Name from `episode' to `lfs'

[7/6/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
--Other explanation below--
Associated information:
Tested on TA build:  
Tested with backing build:  
Changed Responsible Engr. from `kazar@transarc.com' to `prem@transarc.com'
Changed Responsible Engr. from `prem@transarc.com' to `jdp@transarc.com'

[9/24/93 public]
I am cancelling this defect for the following reasons:
                                                                               
1) Kazar's "ot export" operation did not leave any info about the associated 
  delta and I could not find any deltas that is named after this ot number
  in all the recent 103 builds.                                                
2) The problem (holding a lock to long) mentioned above did not hold true
   any more. The vol_efsDetach DOES  release the volp->lock before a call
   to vnm_InUse. This fix is in all the available source trees.
   I suspect someone may have fixed it in another delta by accident.
Changed Status from `open' to `cancel'



CR Number                     : 8075
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts restore insists on getting tokens even on damaged filesets
Reported Date                 : 5/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/28/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[5/28/93 public]
Currently, ``fts restore'' insists on getting tokens for filesets that are
about to be restored onto, even if the command is restoring a complete dump
and is thus about to make the fileset consistent.  The problem is that if
a restoration is interrupted, the target is left inconsistent and you can't
get tokens on such a fileset.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/1/93 public]
Craig, if this needs to be in 1.0.2a, then please update the Fix by field.
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[8/5/93 public]
I can no longer reproduce this in the current code base.
I suspect that it was repaired, or worked around, in the fixes for 6180.
I was able to create an inconsistent fileset with an interrupted restore
and was then able to do a complete restoration without problems.
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 8061
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : dfsd can't free cacheblocks
Reported Date                 : 5/26/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/26/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/26/93 public]
This ot is to track a problem we have been seeing since the
advent of dfs.carl.   I currently have in the cho cell a 
client only pmax which has been getting for over 24 hours
continuous messages about "dfs: can't free cacheblocks".
There is a process which is stuck with the following stack trace:
cm_rdwr
cm_GetDLock
cm_GetDOnLine
cm_ReserveBlocks
osi_Wait
also here are the values for some cm variables:
cm_blocksUsed = 9984
cm_CacheBlocks = 10000
cm_cacheFiles = 1250
Note that cm_blocksUsed and cm_cacheFiles values have remained
the same for about the last 18 hours.  I can see where cm_blocsUsed
gets incremented, but where does this value get decremented?
I also set a break point in AFS_StoreData to see if we
were even writing back anything and we never seem to hit
this break point (maybe were not supposed to - I'm still
reading the code to try to figure out what we should be
doing under normal circumstances).  
Here's a partial dfstrace log 
time 597.520760, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 597.520760, pid 874: in cm_GetDownD, need space 1 
time 597.567632, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 597.567632, pid 2412: in cm_GetDownD, need space 1 
time 598.524666, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 598.524666, pid 874: in cm_GetDownD, need space 1 
time 598.579350, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 598.579350, pid 2412: in cm_GetDownD, need space 1 
time 599.528572, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 599.528572, pid 874: in cm_GetDownD, need space 1 
time 599.591068, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 599.591068, pid 2412: in cm_GetDownD, need space 1 
time 600.532478, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 600.532478, pid 874: in cm_GetDownD, need space 1 
time 600.598880, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 600.598880, pid 2412: in cm_GetDownD, need space 1 
time 601.141814, pid 0: running write through dslots 
time 601.145720, pid 0: servertokenmgt running 5 subops 
time 601.145720, pid 0: running major renewlazyreps 
time 601.145720, pid 0: Renewlazyreps starts 0 subjobs 
time 601.149626, pid 0: in cm_FlushQueuedServerTokens for server c2772300 
time 601.149626, pid 0: in cm_FlushQueuedServerTokens for server c28e6700 
time 601.149626, pid 0: in cm_FlushQueuedServerTokens for server c2776200 
time 601.149626, pid 0: in cm_FlushQueuedServerTokens for server c2776700 
time 601.149626, pid 0: in cm_FlushQueuedServerTokens for server c28e6800 
time 601.540290, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 601.540290, pid 874: in cm_GetDownD, need space 1 
time 601.606692, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 601.606692, pid 2412: in cm_GetDownD, need space 1 
time 602.544196, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 602.544196, pid 874: in cm_GetDownD, need space 1 
time 602.614504, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 602.614504, pid 2412: in cm_GetDownD, need space 1 
time 603.552008, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 603.552008, pid 874: in cm_GetDownD, need space 1 
time 603.622316, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 603.622316, pid 2412: in cm_GetDownD, need space 1 
time 604.555914, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 604.555914, pid 874: in cm_GetDownD, need space 1 
time 604.630128, pid 2412: reserveblocks emergency, scp c390af80, truncating dcp c28cf3a0! 
time 604.630128, pid 2412: in cm_GetDownD, need space 1 
time 605.153532, pid 0: flushactivescaches starting 3 concurrent storebacks 
time 605.157438, pid 0: running FlushActiveSCaches 
time 605.157438, pid 0: running FlushActiveSCaches 
time 605.157438, pid 0: running FlushActiveSCaches 
time 605.559820, pid 874: reserveblocks emergency, scp c3910e00, truncating dcp c28d1a24! 
time 605.559820, pid 874: in cm_GetDownD, need space 1 
Elliott, I put you on the cc list; maybe you can give us some pointers
about where to look next.  In the meantime we'll try to collect more
data.

[05/26/93 public]
More data.
freeDCCount = 1091
cm_indexTable => first 500 entries are all zero
The dfstrace log still looks the same.  It says it's truncating cache
files but we don't see any further log info which verifies that
that is actually happening (like cm_FlushDCache).

[05/26/93 public]
Even more stuff:
The cm_indexFlags array looks like:
first 714 entries = 2 => index entry in freeDCList
after this there are some 4's and 3 41's
It looks like in cm_GetDownD we are not finding anything which
is flushable (we never make the call to cm_FlushDCache).

[5/27/93 public]
Changed Interest List CC from `jaffe@transarc.com' to `jaffe@transarc.com, 
 tu@transarc.com, kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/28/93 public]
The problem seems to be very odd. The cm did not write any data back to 
the server, (if I read your report correctly), and therefore it could not
free any dcache blocks. I will read it first and phone you later this 
afternoon.

[5/28/93 public]
Re-assigned to tu who's been investigating this.
[06/18/83 public]
We've only seen this in conjuction with 8052 (no quorum).  Now that
the quorum problems have been fixed this problem is no longer blocking
us - move to 103

[9/14/93 public]
Diane, 
As mentioned above, this anamoly was only seen when flservers could not reach
quorum. If this is not reproduciable (it seems to be the case) after ot 8052 
was fixed, would you want to cancel this one. 
Thanks, 
Tu

[11/4/93 public]
Diane, 
This bug report has been very old and I assume that you have not seen this
happen again. If this is the case, do you think I (or you ) can cancel it. 
Thank you. 
Changed Interest List CC from `jaffe@transarc.com, tu@transarc.com, 
 kazar@transarc.com' to `jaffe@transarc.com, tu@transarc.com, 
 kazar@transarc.com, comer@transarc.com'

[12/10/93 public]
Assign to the OSF so that they can test and/or cancel it.
Changed Responsible Engr. from `tu@transarc.com' to `delgado' 
Changed Resp. Engr's Company from `tarc' to `osf'

[1/11/94 public]
This is cancelled.  I shot the flserver while copying a large
amount of data into DFS.  I successfully copied about 120MB of 
data before stopping because I was about to run out of space.  
When I returned ~2 hours later, I was unable to access DFS 
because the cm went to the flserver to refresh it's fileset 
name to location mapping, and there was no flserver (expected
behavior).



CR Number                     : 8048
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8055
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : Rep functional tests fail
Reported Date                 : 5/24/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/24/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/24/93 public]
I ran the rep tests over the weekend between a PMAX and a RIOS
and encountered a spin in rep test 1.
 
The spin occurs while reading a directory in a RO fileset.  I
eventually ran out of tokens on the PMAX housing the RO fileset.
I believe this may be a secondary effect (cascading errors) which
was already reported by Carl Burnett.
 
Also, the repserver was unable to start at 4AM and 9AM
on Sunday because it couldn't determine the DFS junction for
the local cell name: Timeout occurred, operation not performed (dce/cds)
Registry server unavailable.  I believe this may have been caused
by the fact that I have many megabytes worth of the following messages in
my messages file which filled up /usr:
 
May 21 21:19:05 shotz vmunix: 738033545: tkm_TokenFreeList_ObtainToken: out of \
free tokens, about to force garbage collection
 
So, I have lots of secondary symptoms, but little about the root
of the problem.  I did notice the following message sequence in
the Ftserver log file on the pmax (I started the rep tests at
about this time):
 
93-May-21 18:06:33 SFTSERVER_ListVolumes: can't open 0,,8: 691089536
93-May-21 18:06:33 Restoring fileset 0,,8/11
93-May-21 18:06:36 vols_Restore: returning 0
93-May-21 18:06:36 Restored fileset 0,,8/11: returned code 0
93-May-21 18:07:28 Restoring fileset 0,,10/11
93-May-21 18:07:29 vols_Restore: returning 0
93-May-21 18:07:29 Restored fileset 0,,10/11: returned code 0
93-May-21 18:12:35 Restoring fileset 0,,12/11
93-May-21 18:12:37 vols_Restore: returning 0
93-May-21 18:12:37 Restored fileset 0,,12/11: returned code 0
93-May-21 18:19:25 Restoring fileset 0,,10/11
93-May-21 18:19:26 vols_Restore: returning 0
93-May-21 18:19:26 Restored fileset 0,,10/11: returned code 0
93-May-21 18:20:13 Restoring fileset 0,,12/11
93-May-21 18:20:16 vols_Restore: returning 0
93-May-21 18:20:16 Restored fileset 0,,12/11: returned code 0
93-May-21 18:22:21 Restoring fileset 0,,10/11
93-May-21 18:22:23 vols_Restore: returning 0
93-May-21 18:22:23 Restored fileset 0,,10/11: returned code 0
I've seen the above error message in the past as a result of 
fts release.  The message translates to "repserver forward fileset".
 
I know this isn't alot of information to go on, but I've tried
the rep functional tests ~8 times on various builds over the
past few months and have not been able to get a clean run yet (with 
no errors).  This fact needs to be addressed.
 
The fldb looks like:
 
# fts lsfldb
 
Lemieux.23686
        readWrite   ID 0,,11  valid
        readOnly    ID 0,,12  valid
        backup      ID 0,,13  invalid
number of sites: 2
  Sched repl: maxAge=0:03:00; failAge=0:05:00; reclaimWait=1:30:00; minRepDelay0
   server           flags     aggr   siteAge principal      owner
barge.osf.org       RW,RO    lfs_aggr 0:02:00 hosts/barge    <nil>
shotz.osf.org       RO       lfs_aggr 0:02:00 hosts/shotz    <nil>
 
lfs_set
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
shotz.osf.org       RW       lfs_aggr 0:00:00 hosts/shotz    <nil>
 
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
barge.osf.org       RW       lfs_aggr 0:00:00 hosts/barge    <nil>
----------------------
Total FLDB entries that were successfully enumerated: 3 (0 failed)
 
After clearing space and restarting the repserver, lsreplicas reveals:
 
# fts lsrep -fileset 0,,11 -all
On barge.osf.org:
Lemieux.23686, cell 134240781,,1166164071: src 0,,11 (lfs_aggr) (on barge.osf.org) => barge.osf.org 0,,12 (lfs_aggr)
   flags 0x20001, volstates 0x10423206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,214; curVV: 0,,214; WVT ID = 738018798,,927
   Lost token 7895074 ago; token expires 10544 hence; new version published 738263113 ago
   vvCurr 730368039.310929 (7895074 ago); vvPingCurr 738259257.038818 (3856 ago)
   Last update attempt 0.000000 (738263113 ago); next scheduled attempt 0.000000 (-738263113 hence)
   Status msg: GetToken returned 0x404 token, ID 738018798,,927.
 
On shotz.osf.org:
Lemieux.23686, cell 134240781,,1166164071: src 0,,11 (lfs_aggr) (on barge.osf.org) => shotz.osf.org 0,,12 (lfs_aggr)
   flags 0x20001, volstates 0x10423206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,214; curVV: 0,,214; WVT ID = 738018798,,932
   Lost token 7776144 ago; token expires 14264 hence; new version published 738263523 ago
   vvCurr 730487379.716331 (7776144 ago); vvPingCurr 738263387.107030 (136 ago)
   Last update attempt 0.000000 (738263523 ago); next scheduled attempt 0.000000 (-738263523 hence)
   Status msg: GetToken returned 0x404 token, ID 738018798,,932.

[5/24/93 public]
I'm sorry, but this isn't just ``little'' information about a primary problem,
it's no information.  The only thing that could be construed as a primary
problem is the token leak problem that Carl found and for which he's built
a fix.  The message in the FtLog is completely normal, and simply says that
one process is trying to do an ``fts lsheader'' while the repserver has
a fileset in use.  The rep functional tests by themselves could cause this.
 
I expect that your diagnosis of the filled /usr partition is correct for the
repserver restart problems.  Otherwise, I'm not sure what you want the
repserver to do about it.
 
What constitutes a ``run'' of the rep functional tests?  I would think that
it would be a single execution of rtest1, rtest2, and rtest3 (or just the
first two if you have only one server in your cell).  Is this what you meant
when you say you ran them ``over the weekend''?  Or were you running them
in a loop somewhere?  (Running them in a loop will guarantee that you'll
exercise the token manager's R/O token leak, and eventually run it out of
tokens.)  Which problem have you run into 8 times in various builds--not
being able to run them through once (which takes maybe half an hour), or
not being able to leave them running in a loop all weekend?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/24/93 public]
Given the lack of useful diagnostic information, is it really appropriate to
leave this as an A0?
 
Changed Interest List CC from `cfe@transarc.com' to `pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com'

[5/24/93 public]
I've reproduced the problem.  I'll put the dfstrace dump and the 
test output in ~notuser/rep_spin.

[5/25/93 public]
The second time this problem showed, the spin eventually corrected 
itself, but it took ~6 hours to run rtest1 and I ran out of tokens
again.

[5/25/93 public]
I looked at the rep_spin data and the code in question, and when I went to
coordinate an approach with Mike Kazar, he was already working on a solution
that would accomplish everything I was planning and more.  He's doing that
work under OT 8055.
 
As usual, I'd have preferred to dup this OT report to 8055, but the numbers
are in the wrong order, so I can't.  Cancelling this and filling in its
related-OT number with 8055, and upping the priority of 8055.
Filled in Inter-dependent CRs with `8055' 
Filled in Priority with `1' 
Changed Status from `open' to `cancel'



CR Number                     : 7953
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fx: protection failure in readdir (perms = 1, have 0)
Reported Date                 : 5/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/7/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/7/93 public]
CONFIG: 	2 pmax cell, 2 flservers, root.dfs in lfs
BUILD:		nb available 4/26/93
TEST:		unclear as yet
Logging that the symptom of repeated error messages of the
form:
fx: protection failure in readdir (perms = 1, have 0) 
were seen. Low severity and priority since it is not clear yet how these
are related to other problems seen on this server (the pmax exporting root.dfs
and epi.1 lfs filesets). These other problems do actually involve unexpected
access to files and directories. Narrowing down which problem produces this
message.

[5/7/93 public]
Upping the priority/severity and reassigning to Rajesh. The test case is the
following:
- cd to a directory in lfs, this directory should contain files that are
NOT accessible by unauthenticated users
- login as a Unix user and dce principal that DOES have access to the directory
and the files that directory contains
- read some or all of the directory's files
- kdestroy and exit the dce_login shell
- read some or all of the directory's files
NOTE: you still have access to any you read before the kdestroy!

[5/7/93 public]
kdestroy does not destroy your credentials in the kernel and this could 
allow you access to files even after the kdestroy.
Exiting the dce_login shell should really revert to the unauth user and
access to files and dirs that do not allow unauth access should 
not be allowed.
 
Many problems have been found in the past week w.r.t. unauthenticated
access and ACLs. These problems are being fixed and hence downgrading
the priority of this bug down to 2. If the problem still exists
once those fixes are in place, we should upgrade the priority
on this one again.
Filled in Interest List CC with `comer' 
Changed Priority from `1' to `2'

[5/21/93 public]
Now that a lot of the ACL bugs have been fixed, I retried with a system
with the ACL fixes and was unable to reproduce the problem.
Gail, if the sequence of operations I tried is not representative, let
me know and I can retry. Also, once you have a build with the changes
for the following ACL related OT's could you retry and let me know if
the problem still exists. OTs = 7961, 7966, 8002, 8003, 8005, 8023.
The deltas for the above OTs have been exported. Thanks.
The sequence of operations I tried

[rajesh] cd /:
dfs: set auth binding failed (code 387064044)
ldfs: Warning: created unauthenticated binding

[:] klist
No DCE identity available: No currently established network identity for which c
ontext exists (dce / sec)
Kerberos Ticket Information:
klist: No credentials cache file found (dce / krb) while setting cache flags (ti
cket cache /tmp/krb5cc_4970)

[:]  ls -l
total 14
-rw-r--r--   1 rajesh   system         0 May 21 11:21 a
-rw-r--r--   1 100      12            22 May 21 13:38 afile
lrwxrwxrwx   1 rajesh   transarc       1 May 21 10:03 b@ -> a
drwxr-xr-x   2 100      12           256 May 19 11:46 bwl/
drwxrwxr-x   2 100      12           288 May 19 10:27 dir1/
drwxrwxrwx   2 ota      episode      256 May 19 11:31 ota/
drwxrwxrwx   2 rajesh   system       256 May 19 11:30 rajesh/
drwxr-xr-x   2 100      12           256 May 19 10:40 ravi/
drwxrwxr-x   4 100      12           512 May 20 15:34 unauth-nonroot/
drwxrwxr-x   2 100      12           256 May 18 13:20 unauth-root/

[:] acl_edit afile -l
Warning - binding to registry is unauthenticated
dfs: set auth binding failed (code 387064044)
dfs: Warning: created unauthenticated binding
# SEC_ACL for afile:
# Default cell = /.../power7.dce.transarc.com
user_obj:rw-c--
group_obj:r-----
other_obj:r-----

[:] cat afile
cat: cannot open afile

[:] dce_login rajesh rajesh
% klist
DCE Identity Information:
        Warning: Identity information is not certified
        Global Principal: /.../power7.dce.transarc.com/rajesh
        Cell:      0051b0f8-11d2-1bf9-bbe4-02608c2ef49f /.../power7.dce.transarc
.com
        Principal: 0000136a-467d-2bfa-9b00-02608c2ef49f rajesh
        Group:     00000000-11d6-2bf9-bb01-02608c2ef49f system
        Local Groups:
                00000000-11d6-2bf9-bb01-02608c2ef49f system
Identity Info Expires: 93/05/21:23:49:00
Account Expires:       never
Passwd Expires:        never
Kerberos Ticket Information:
Ticket cache: /opt/dcelocal/var/security/creds/dcecred_41000010
Default principal: rajesh@power7.dce.transarc.com
Server: krbtgt/power7.dce.transarc.com@power7.dce.transarc.com
        valid 93/05/21:13:49:00 to 93/05/21:23:49:00
Server: dce-rgy@power7.dce.transarc.com
        valid 93/05/21:13:49:01 to 93/05/21:23:49:00
Server: dce-ptgt@power7.dce.transarc.com
        valid 93/05/21:13:49:03 to 93/05/21:15:49:03
Client: dce-ptgt@power7.dce.transarc.com        Server: krbtgt/power7.dce.transa
rc.com@power7.dce.transarc.com
        valid 93/05/21:13:49:03 to 93/05/21:15:49:03
Client: dce-ptgt@power7.dce.transarc.com        Server: dce-rgy@power7.dce.trans
arc.com
        valid 93/05/21:13:49:04 to 93/05/21:15:49:03
Client: dce-ptgt@power7.dce.transarc.com        Server: hosts/power7/dfs-server@
power7.dce.transarc.com
        valid 93/05/21:13:49:04 to 93/05/21:15:49:03
% ls -l
total 14
-rw-r--r--   1 rajesh   system         0 May 21 11:21 a
-rw-r--r--   1 100      12            22 May 21 13:38 afile
lrwxrwxrwx   1 rajesh   transarc       1 May 21 10:03 b -> a
drwxr-xr-x   2 100      12           256 May 19 11:46 bwl
drwxrwxr-x   2 100      12           288 May 19 10:27 dir1
drwxrwxrwx   2 ota      episode      256 May 19 11:31 ota
drwxrwxrwx   2 rajesh   system       256 May 19 11:30 rajesh
drwxr-xr-x   2 100      12           256 May 19 10:40 ravi
drwxrwxr-x   4 100      12           544 May 21 13:48 unauth-nonroot
drwxrwxr-x   2 100      12           256 May 18 13:20 unauth-root
% touch c
% cat afile
Created by cell_admin
% kdestroy
% exit

[:] klist
No DCE identity available: No currently established network identity for which context exists (dce / sec)
Kerberos Ticket Information:
klist: No credentials cache file found (dce / krb) while setting cache flags (ticket cache /tmp/krb5cc_4970)
y% [:] cat afile
cat: cannot open afile

[:] more afile
afile: Permission denied

[:] echo $status
1
Changed Interest List CC from `comer' to `comer, jaffe'

[5/21/93 public]
Gail, 
one more ACL OT - 7959 (though this should not affect this bug at all).

[9/24/93 public]
Changed Responsible Engr. from `rajesh@transarc.com' to `tu@transarc.com'

[9/24/93 public]
Gail, 
I have just tired the test scenario mentioned above on the HP platform and
could not reproduce it at all.  I don't think the bug is there.
In fact, I suspect the defected you saw could be the result of a very
confused environment at that time. The CM always chooses/creates an 
unauthenticated rpc binding for an user who does do dce_login. Accordingly,
the fx server always perform the authorization based on the  PAC derived
from the rpc binding.  This code path have been very stable for a long time.
                                                                              
I was wondering that you want to cancel it if you too cannot reproduce it.
Thanks,

[10/4/93 public]
Assigned to gail so that she can cancel it.
Changed Responsible Engr. from `tu@transarc.com' to `gmd'

[10/4/93 public]
Changed Resp. Engr's Company from `tarc' to `osf'

[10/8/93 public]

Aged relative to Aug 1.

[12/16/93 public]
Now that the Transarc drop is here, able to try this out - acl changes (and others?)
appear to have removed the ability to cd to a directory you don't have permission to
as an unauthenticated user (this was fixed before the end of 1.0.2a) nor can I read
files I had read previously as an authenticated user. Note that
root's authenticated identity as self has also clarified things in this area as
well.

> su gmd /* no dce account or credentials */
% su cell_adm             /* unix user corresponding to dce user cell_admin */
> dce_login cell_admin -dce- /* create shell with cell_admin credentials */
Password must be changed!
> cd /:/epi_1	/* cd to an episode fileset */
> ls 
dce.clean         dce_com_utils     dce_config_utils  rc.dce.std
dce.unconfig      dce_config        rc.dce
dce_com_env       dce_config_env    rc.dce.orig
root@cobbler> cat rc.*
(text deleted)
> kdestroy /* destroy credentials files */
> ^D	/* exit dce_login shell */
> id
uid=100(cell_adm) gid=1(staff)
root@cobbler> klist
No DCE identity available: No currently established network identity for which context exists (dce / sec)

Kerberos Ticket Information:
klist: No credentials cache file found (dce / krb) while setting cache flags (ticket cache /tmp/krb5cc_100)
> cd /:/epi_1
ksh: /:/epi_1: Permission denied.
> cat /:/epi_1/*
cat: 0652-050 Cannot open /:/epi_1/*.



CR Number                     : 7952
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : Dummy acl for dir owned by foreign user has wrong default realm
Reported Date                 : 5/7/93
Found in Baseline             : 1.0.2
Found Date                    : 5/7/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : sshi!ibm.com
Transarc Status               : open

[5/7/93 public]
An authenticated user is not able to update the ic or io ACL of a 
directory to add permission bits for local cell users. The
following log illustrate the problem. The default cell for the ic 
and io ACL is still the local cell, not the dummy cell created 
for unauth users. We suspect that this is reason why acl_edit
reject the update request...
> klist | grep principal
Default principal: hosts/pal411.austin.ibm.com/self@icdfs2.austin.ibm.com

[root@pal411] /.:/fs/shi1
> mkdir foodir

[root@pal411] /.:/fs/shi1
> acl_edit foodir -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (
dce
 / sec)
INFO: Local cell will be used for operations requiring default cell info.
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for foodir:
# Default cell = fffffffd0000.00.00.00.00.00.00.00.00
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
foreign_other:/.../icdfs2.austin.ibm.com:r-x---
> acl_edit foodir -ic -l
# Initial SEC_ACL for directories created under: foodir:
# Default cell = /.../icdfs2.austin.ibm.com
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
> acl_edit foodir -io -l
# Initial SEC_ACL for objects created under: foodir:
# Default cell = /.../icdfs2.austin.ibm.com
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
> acl_edit foodir -io
sec_acl_edit> m foreign_other:/.../icdfs2.austin.ibm.com:rwxcid
sec_acl_edit> exit
ERROR: not a valid DFS acl (dce / sec)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `sshi!ibm.com' 
Added field Transarc Status with value `open'

[5/17/93 public]
As Shepherd notes, the real problem is that the default realm of the
dummy initial ACL is not the owner's cell but the storing cell. 
This problem has been fixed under OT 8005. Hence this one should be
cancelled.
The following log of operations show that problem is solved.
# whoami
root
# klist
DCE Identity Information:
        Global Principal: /.../bugacl/hosts/power7/self
        Cell:      006df556-1151-1bf0-b468-02608c2ef49f /.../bugacl
        Principal: 00000066-1154-2bf0-b400-02608c2ef49f hosts/power7/self
        Group:     0000000c-1154-2bf0-b401-02608c2ef49f none
        Local Groups:
                0000000c-1154-2bf0-b401-02608c2ef49f none
Identity Info Expires: 93/05/17:22:05:05
Account Expires:       never
Passwd Expires:        never
Kerberos Ticket Information:
Ticket cache: /opt/dcelocal/var/security/creds/dcecred_ffffffff
Default principal: hosts/power7/self@bugacl
Server: krbtgt/bugacl@bugacl
        valid 93/05/17:12:05:05 to 93/05/17:22:05:05
Server: dce-rgy@bugacl
        valid 93/05/17:12:05:05 to 93/05/17:22:05:05
Server: dce-ptgt@bugacl
        valid 93/05/17:16:35:32 to 93/05/17:18:35:32
Client: dce-ptgt@bugacl Server: krbtgt/bugacl@bugacl
        valid 93/05/17:16:35:32 to 93/05/17:18:35:32
Client: dce-ptgt@bugacl Server: hosts/power7/cds-server@bugacl
        valid 93/05/17:16:35:35 to 93/05/17:18:35:32
Client: dce-ptgt@bugacl Server: dce-rgy@bugacl
        valid 93/05/17:18:25:51 to 93/05/17:18:35:32
# cd /:
# ls
acltest         dir1            rajesh          unauth-nonroot
cell_admin      e               test            unauth-root
d               foo             testdir         who
# ls
acltest         dir1            rajesh          unauth-nonroot
cell_admin      e               test            unauth-root
d               foo             testdir         who
# cd unauth-root
# acl_edit . -l
# SEC_ACL for .:
# Default cell = /.../bugacl
mask_obj:rwx-id
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id
any_other:rwx-id
# mkdir foreign
# acl_edit foreign -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for foreign:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
# acl_edit foreign -ic -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# Initial SEC_ACL for directories created under: foreign:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
# acl_edit foreign -io -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# Initial SEC_ACL for objects created under: foreign:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
user_obj:rwxc--
group_obj:r-x---
other_obj:r-x---
# acl_edit foreign -io
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
sec_acl_edit> l
# Initial SEC_ACL for objects created under: foreign:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
user_obj:rwxc--
group_obj:r-x---
other_obj:r-x---
sec_acl_edit> m foreign_other:/.../bugacl:rwx
sec_acl_edit> l
# Initial SEC_ACL for objects created under: foreign:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
mask_obj:rwx---
user_obj:rwxc--
group_obj:r-x---
other_obj:r-x---
foreign_other:/.../bugacl:rwx---
sec_acl_edit> exit
# acl_edit foreign -io -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# Initial SEC_ACL for objects created under: foreign:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
mask_obj:rwx---
user_obj:rwxc--
group_obj:r-x---
other_obj:r-x---
foreign_other:/.../bugacl:rwx---
#
Changed Short Description from `Can not set permission bits in ic and io ACL as 
 aunauth user' to `Dummy acl for dir owned by foreign user has wrong default 
 realm'

[5/18/93 public]
This problem was fixed by OT 8005.  Since I can't dup into the future, I'm 
cancelling this defect.
Changed Status from `open' to `cancel'



CR Number                     : 7941
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7896
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : unable to recover agg after rep
Reported Date                 : 5/6/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/6/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/6/93 public]
Three machine cell, 2 PMAX 1RIOS.  After running through the 
system test test/systest/file/dfs.repfs_checklist, I rebooted
the machines because the cdsd had core dumped on the RIOS.
On reboot, the Episode aggregate on the RIOS did not recover 
cleanly:
Verifying /dev/lv02
Will run recovery on /dev/lv02
recovery statistics:
        Elapsed time was 515 ms
        7 log pages recovered consisting of 476 records
        Modified 4 data blocks
        199 redo-data records, 0 redo-fill records
        0 undo-data records, 0 undo-fill records
Ran recovery on dev 1/1
Verify Zero Link Count List: Volume epi.1 file counts didn't match: claims 0 but should be 2.
Verify: Zero Link Count list in volume epi.1 needs updating.
Verify Zero Link Count List: Volume epi.1.readonly file counts didn't match: claims 0 but should be 2.
Verify: Zero Link Count list in volume epi.1.readonly needs updating.
zeroLinkCnt <no name was available>: volume index: 7 anode index 10
zeroLinkCnt <no name was available>: volume index: 7 anode index 9
zeroLinkCnt <no name was available>: volume index: 6 anode index 10
zeroLinkCnt <no name was available>: volume index: 6 anode index 9
Processed 4 vols 47 anodes 4 dirs 43 files 0 acls
Done.  Some inconsistencies found verifying /dev/rlv02
        WARNING: PROBLEM RECOVERING /dev/lv02
        Press <ENTER> to continue or CTRL-C to exit
I'm trying to find room to squirrel away the aggregate.  What additional 
information would be useful?

[5/6/93 public]
I've squirrelled the recovered aggregate if any one's interested.  The 
salvager was apparently able to fix the problem:
root@singsing> salvage /dev/lv02
Salvaging /dev/lv02
Will run recovery on /dev/lv02
Verify Zero Link Count List: Volume epi.1 file counts didn't match: claims 0 but should be 2.
Verify: Zero Link Count list in volume epi.1 needs updating.
Salvage: replacing Zero Link Count list
Verify Zero Link Count List: Volume epi.1.readonly file counts didn't match: claims 0 but should be 2.
Verify: Zero Link Count list in volume epi.1.readonly needs updating.
Salvage: replacing Zero Link Count list
zeroLinkCnt <no name was available>: volume index: 7 anode index 10
zeroLinkCnt <no name was available>: volume index: 7 anode index 9
zeroLinkCnt <no name was available>: volume index: 6 anode index 10
zeroLinkCnt <no name was available>: volume index: 6 anode index 9
Processed 4 vols 47 anodes 4 dirs 43 files 0 acls
Done.  Some inconsistencies repaired salvaging /dev/rlv02
root@singsing> salvage /dev/lv02
Salvaging /dev/lv02
Will run recovery on /dev/lv02
Processed 4 vols 47 anodes 4 dirs 43 files 0 acls
Done.  /dev/rlv02 checks out as Episode aggregate.
root@singsing>

[5/6/93 public]
Turns out both the PMAX and RIOS aggregates had trouble recovering, 
here's the log from the PMAX:
 recovering aggregate /dev/rz1f... 
Verifying /dev/rz1f
Will run recovery on /dev/rz1f
recovery statistics:
        Elapsed time was 9648 ms
        10 log pages recovered consisting of 502 records
        Modified 10 data blocks
        241 redo-data records, 0 redo-fill records
        0 undo-data records, 0 undo-fill records
Ran recovery on dev 1/1
Verify Zero Link Count List: Volume epi.1.readonly file counts didn't match: claims 1 but should be 2.
Verify: Zero Link Count list in volume epi.1.readonly needs updating.
zeroLinkCnt <no name was available>: volume index: 7 anode index 10
zeroLinkCnt <no name was available>: volume index: 7 anode index 9
Processed 3 vols 30 anodes 3 dirs 27 files 0 acls
Done.  Some inconsistencies found verifying /dev/rrz1f
        WARNING: PROBLEM RECOVERING /dev/rz1f
        Press <ENTER> to continue or CTRL-C to exit
I don't have room to save this PMAX aggregate.  I assume you'd prefer 
the RIOS version anyway.

[5/6/93 public]
Changed Responsible Engr. from `pakhtar@transarc.com' to `ota@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/7/93 public]
It is interesting that there are no other link count errors.
The repserver messes with the ZLC list when it does clones and restores.
There is some possibility that the fileset header management code here
is not doing this correctly.
Filled in Interest List CC with `cfe, jdp'

[5/11/93 public]
In particular, the fixes for OT 7896 may have fixed this problem.
Could you re-test once those are all up and running?  Thanks.
Changed Interest List CC from `cfe, jdp' to `cfe, jdp, ota' 
Changed Responsible Engr. from `ota@transarc.com' to `rsarbo' 
Changed Resp. Engr's Company from `tarc' to `osf'

[6/7/93 public]
I haven't been able to reproduce this on dfs.carl.  Cancelling and
filling in interdependent cr with 7896.



CR Number                     : 7932
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 6959
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : fxd? krpc?
Short Description             : comm failure/set context lost contact with fx server
Reported Date                 : 5/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/4/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/4/93 public]
BUILD:	102a available 4/26/93
CONFIG:	2 pmax cell, 2 flservers, root.dfs in lfs on one, ufs exported on other
TEST/ACTIVITY:	had run dfs.lock and dfs.glue overnight (unsuccessfully due
	to a dfsbind core dump and connection timed out errors)
This problem differs from the intermittent time out errors in that you do
not ever recover. All clients trying to access dfs receive either:
% cd /:
dfs: comm failure/set context (code 382312470)
dfs: lost contact with the fx server 130.105.5.27 in cell p102a_cell.qadce.osf.org
/:: Connection timed out
or simply:
root@dce12> cd /:
/:: Connection timed out
Assigning to myself since I believe this will be reproducable and NEXT time
I'll have krpc debugging on. Whatever this problem is, I believe it's been
in the code awhile (OT 6959).

[5/5/93 public]
This is most certainly a dfs problem.  The dfstrace dumps should
clearly identify the culprit if we can get them soon after the 
failure.  I expect that a locking problem occured on 130.105.5.27.
This is another case where a debugger should clearly deliniate the 
problem.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/6/93 public]
Seen again - this time with a different code though - is that significant?
dfs: comm failure/set context (code 382312556)
dfs: lost contact with the fx server 130.105.5.27 in cell p102a_cell.qadce.osf.org
./bin/stcode 382312556 
382312556 (decimal), 16c9a06c (hex): call timed out (dce / rpc)
Logs taken within an hour of the original problem - cell fairly
idle so hopefully they're good. See ~notuser/gmd/7932_logs.

[5/7/93 public]
Good test run.  There were at least three failures that I can detect.
on valentine, you were running some type of ACL tests, and an AFS_GetAcl
call to the local server timed out.  It looks like valentine in
SAFS_FetchACL, attempted to collect the correct tokens, and while getting
a token, it got hung.  This caused the AFS_GetAcl RPC to timeout.  (marking
the host as down).
Meanwhile, someone on dce12 tried to make an AFS_SetContext call to
valentine, and it timed out.  (I looked for the px side on valentine, but
could not positively identify it.)  
What is interesting about this whole mess is that the dfsbind on dce12
got the auth_ticket_expired failure at about this same time.  Its hard to
tell what the order of failures was.  Some of our new debugging will help 
with this.
The logs still don't deliver a smoking gun.  Can you get the systems set up 
for debugging or a crash dump so that we can see what was happening at the
time.  You mentioned in the README that krpc debug was turned on.  I thought
that this would put more debug information in the dfstrace dump, but I saw
nothing.  Its not helpfull to turn on krpc debugging once the failure occurs,
we really need to see the failure happen with the tracing already in place.
Synopsis: Still nothing to fix.  Maybe some more info.  Need to debug the
system in place.
to see that in the log.
caused the

[5/7/93 public]
Thanks for looking at the logs. The krpc debugging was on the whole time -
but I didn't set it at the right level. You are correct that I happened
to have been running ACL tests at the time but I don't believe that's
significant. The auth ticket expired incident may be significant - I'll
watch for that - I believe Mike Comer is researching the auth ticket
expired problems in general, correct? When will the new and improved debugging
code arrive at OSF?

[5/7/93 public]
To be more clear, I have seen this problem MANY times - usually by running
dfs.glue. This is the first time I've seen it while running ACL tests.

[5/17/93 public]
Problem still being seen with dfs.carl - is anyone looking at this?!

[5/18/93 public]
Can you provide some more information.  In particular, this defect should
address the problem that you see a timeout, and then the server never comes
back.  If the server comes back, then your seeing a different problem
(ot 7785).  If the server is really dead, then you need to get a debugger 
on the machine, and get the stack traces.  We are not seeing this type
of problem here at Transarc, so we cannot debug it alone.

[5/18/93 public]
You're correct - I didn't realize there's a separate bug for the "recovers"
case because it's only a B1 - I'll update that one, thanks. To be clear, so
far all we've seen with dfs.carl are timeouts from which we eventually recover.

[6/1/93 public]
This hasn't shown up yet with dfs.carl - unclear whether this has been fixed
or is just masked by other problems. Dropping the priority to a 1.

[6/3/93 public]
I'm assigning this to dave so that he can try and move this forward.  Diane's
last note suggests that this problem may have been fixed with the initial
dfs-carl drop.  Transarc has not seen this type of problem for a long time.
Changed Responsible Engr. from `jaffe' to `davel'

[06/11/93 public]

We have not seen the unrecoverable comm failure/set context in quite
a while; I have seen comm failures and set context during cho but
the system always recovers. - cancelled.



CR Number                     : 7927
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Hang during attempted write
Reported Date                 : 5/3/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/3/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/3/93 public]
Elliot - assigned to you since you've probably got the most recent
system restart experience - please reassign if appropriate. 
BUILD:	nb available 4/27/93
CONFIG:	single machine rios cell, only fileset = root.dfs in lfs
(no replicas, clones, etc.)
TEST:	nada - rebooted and restarted dce and dfs
without dce_log'ing in, immediately after reboot and restart,
as Unix user root in the same window rc.dce and rc.dfs were run:
root@cobbler> cd /:
root@cobbler> ls
cobbler_date  dce12_date    val_date
root@cobbler> cat *date
Wed Apr 28 11:47:11 EDT 1993
Wed Apr 28 11:47:19 EDT 1993
Wed Apr 28 11:47:17 EDT 1993
root@cobbler> echo `date` > cobbler_date
(hung for 30 minutes so far)
This should have failed with "Permission denied" since the existing
/:/cobbler_date was owned by cell_adm/cell_admin (Unix/dce).
In a remote cell, I am able to overwrite existing files dce12_date and val_date,but these were owned by "nobody".
In a 2nd window, I am able to run a similar command successfully.
root@cobbler> echo `date` > cobbler_nobody_date
root@cobbler> ls -l
total 0
-rw-r--r--   1 cell_adm none          29 Apr 28 11:47 cobbler_date
-rw-r--r--   1 nobody   nobody        28 May 03 19:24 cobbler_nobody_date
-rw-r--r--   1 nobody   nobody        28 May 03 19:19 dce12_date
-rw-r--r--   1 nobody   nobody        28 May 03 19:23 val_date
(note that this success did NOT require overwriting an existing file)
Other strange behavior seen is this short hour of uptime:
comm failure/lost contact
the file . does not exist
root@cobbler> rm cobbler_nobody_date
root@cobbler> kdestroy
root@cobbler> env |grep KRB
dfs: communication failure with server 130.105.201.7 in cell r102a_cell.qadce.osf.org.
dfs: lost contact with the fx server 130.105.201.7 in cell r102a_cell.qadce.osf.org
KRB5CCNAME=FILE:/opt/dcelocal/var/security/creds/dcecred_4100000c
root@cobbler> ls
ls: 0653-341 The file . does not exist.
root@cobbler> pwd
/:
root@cobbler> ls
ls: 0653-341 The file . does not exist.
root@cobbler> echo `date` > cobbler
/bin/ksh: cobbler: 0403-005 Cannot create the specified file.
root@cobbler> cm check
All backup filesets checked.
root@cobbler> cm stat
All servers are running.
root@cobbler> ls
dfs: set auth binding failed (code -1)
dfs: Warning: created unauthenticated binding
cobbler_date  dce12_date    val_date
I have dfstrace dumps and an icl.bind.
I've left the hung command hanging - please contact me
asap. Thanks. The /opt/dcelocal/var/dfs/adm logs show nothing.

[5/4/93 public]
I've looked at hung.dump, and I can see the shell attempting to access the
file.  The cache manager correctly failed the cm_access call with 13 (EACCES).
The issue here is where is the process hung.  The trace clearly shows that 
the kernel returned from the access system call.  Unfortunatly, there is 
no further activity in the log for the hung shell.
Can you hook up a dump terminal to the rios and find out where this process
is.  If there is no hope, then at least get a core dump from the machine.
You can do this by making sure that there is at least 20Meg in /dev/hd7, 
turning the switch to "service mode", and pushing the yellow button.  After
some time, the led's will go from C02 to C00 or C04.  C00 is success, and
you can now extract the crash from /dev/hd7 when the machine comes up.
C04 means that you did not have enough space in /dev/hd7, and the dump is
worthless.
How about calling Mike K. (at the OSF today) and have him look at this.
Changed Fix By Baseline from `1.0.3' to `1.0.2a' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/6/93 public]

Crashdump didn't tell us anything - taken too late. Couldn't reproduce
after reboot - dropping to a 2.

[05/28/93 public]
Canceling - no one's seen this again.



CR Number                     : 7845
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bakserver
Short Description             : dumpinfo only displays last dump
Reported Date                 : 4/29/93
Found in Baseline             : 1.0.2
Found Date                    : 4/29/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : open

[4/29/93 public]
I did several dumps of the filesets in my cell using bak dump. After the dumps,
I tried to do a bak dumpinfo but it only displays the entry for the last dump 
that was done in the cell. 
I also noticed when doing the dump that I got the following messages. It appears
the bakserver has lost the information for the previous dumps.
No parent dump Flintstone.incr, looking for a higher-level dump
No parent dump Flintstone.full, looking for a higher-level dump
Doing level 0 dump due to missing higher-level dumps
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[5/3/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/3/93 public]
Are you sure you are using different tapes for each of those dumps you did.
When a tape is reused for a dump, the dump that is already on tape is
removed from the database before the new dump is put on it. The dumpID is
stored in the tape label and this is how butc knows which dump to delete from
the database. When doing the incremental dump, try doing it on a different
tape. If you really want to do more than one dump onto the same tape without
removing the dump entry from the database, do a tar on the tape in between
the full and incremental dumps. This tar overwrites the tape label, and the
incremental dump that follows does not recognize the tape label and proceeds
without deleting any dump entry.

[5/3/93 public]
Well, I have been using the same tape to do multiple dumps. I have been doing
a "bak labeltape" with a Null label between dumps - doesn't this delete the
dump ID? It seems like it should.  I tried what you suggested (doing a tar 
in between dumps) and I was able to get more than one level of dump so I 
guess this defect may be cancelled.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[5/4/93 public]
Yes, a labeltape also removes the dump from the database if the tape holds one.
Once a tape holding a valid dump gets overwritten (labeltape, dump), the dump
is first removed and then the overwriting process proceeds. I'll go ahead and
cancel this defect.
Changed Status from `open' to `cancel'



CR Number                     : 7831
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7720, 8003
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : ACL for files created by unauth users is wrong
Reported Date                 : 4/29/93
Found in Baseline             : 1.0.2
Found Date                    : 4/29/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : sshi!ibm.com
Transarc Status               : open

[4/29/93 public]
The following problems exist for files created by unauthenticated 
users:
1.A foreign_other entry was added in the ACL.       
2.Defaul cell name is bogus.
The log attached illustrates the problem:

[sshi@kazuko]>klist | grep principal
Default principal: hosts/kazuko.austin.ibm.com/self@shepherd.cell.austin.ibm.c
om

[sshi@kazuko]>touch foounauth

[sshi@kazuko]>acl_edit foounauth -l
/*********** ************ *****/
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce
 / sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for foounauth:
/********************** *****/
# Default cell = fffffffd0000.00.00.00.00.00.00.00.00
user_obj:rw-c--
group_obj:rw----
other_obj:rw----
/***************************/
foreign_other:/.../shepherd.cell.austin.ibm.com:rw----
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `sshi!ibm.com' 
Added field Transarc Status with value `open'

[5/4/93 public]
Add myself to CC list.
Filled in Interest List CC with `rajesh@transarc.com'

[5/10/93 public]
Rajesh,
Are you still planning to use the same default cell id (-3) or will
invent something different? You have addressed the reason why there 
is an foreign_other entry last week. So it is not a real problem now. 
Thanks...
Shepherd
--------
Changed Reported by from `lhughes@transarc.com' to `sshi@ausvm1.vnet.ibm.com' 
Changed Responsible Engr. from `lhughes@transarc.com' to `rajesh@transarc.com'

[5/12/93 public]
During last week's conference call, we decided to go with -2 if my memory
serves right. Please correct if this is not so. 
Delta cburnett-ot7720-change-anon-to-minus-3 changes the -2 to -3. I
have sent mail to Carl requesting that we "unexport" that delta. Once
that has been done, the issues raised in this OT will have been
resolved and we can close/cancel this one. Will update the OT appropriately
when I get Carl's response.
Filled in Inter-dependent CRs with `7720'

[rajesh@transarc.com 5/17/93 public] 
Carl created an anti delta for ot 7720 under ot 8003. Hence this OT
should really be marked as dup of 8003. But since that cannot be done,
I am cancelling this one.
Changed Inter-dependent CRs from `7720' to `7720, 8003' 
Changed Status from `open' to `cancel'



CR Number                     : 7830
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : ACL id permission bits were not inherited correctly
Reported Date                 : 4/29/93
Found in Baseline             : 1.0.2
Found Date                    : 4/29/93
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : sshi!ibm.com
Transarc Status               : closed

[4/29/93 public]
Permissions bits id were not inherited from the parent directroy for a
file object. According to the latest admin guide:
> The control, insert and delete permissions for the file's user_obj 
> entry are copied directly from the user_obj entry of the parent's
> initial Object Creation ACL. 
However, Rajesh mentioned that id permissions do not apply to a file
object, which I aggreed. In this case, the user should not be allowed
to specify id permission bits for io entry. The admin guide should
reflect this change too. 
The following log illustrates the problem
---------------------------------------------------------------------

 [sshi@kazuko]>klist | grep principal
Default principal: cell_admin@shepherd.cell.austin.ibm.com

 [sshi@kazuko]>pwd
/.:/fs

 [sshi@kazuko]>acl_edit /.:/fs -l
# SEC_ACL for /.:/fs:
# Default cell = /.../shepherd.cell.austin.ibm.com
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id

 [sshi@kazuko]>acl_edit /.:/fs -ic -l
# Initial SEC_ACL for directories created under: /.:/fs:
# Default cell = /.../shepherd.cell.austin.ibm.com
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id

 [sshi@kazuko]>acl_edit /.:/fs -io -l
# Initial SEC_ACL for objects created under: /.:/fs:
# Default cell = /.../shepherd.cell.austin.ibm.com
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id

 [sshi@kazuko]>touch foo

 [sshi@kazuko]>acl_edit foo -l
# SEC_ACL for foo:
# Default cell = /.../shepherd.cell.austin.ibm.com
user_obj:rw-c--
group_obj:rw----
other_obj:rw----
Shepherd Shi
------------
be allowed to specify 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `sshi!ibm.com' 
Added field Transarc Status with value `open'

[5/4/93 public]
I talked with senior developers here and the train of thought is that
even though the id bits are meaningless for files now, they could be
used for other user/system defined semantics.
Hence, we should advise users that they should not set id bits on
files or initial object ACLS. That way the id bits would never appear
in any inherited ACL and hence never in the system. But if the users
do explicitly set the id bits for a file/initial object ACL, then neither
should we generate a hard error nor do anything special in the
code to strip off the id bits.
Comments are appreciated.
Filled in Interest List CC with `rajesh, ota, cfe'

[5/6/93 public]

[5/10/93 public]
>Hence, we should advise users that they should not set id bits on
>files or initial object ACLS. That way the id bits would never appear
>in any inherited ACL and hence never in the system. But if the users
>do explicitly set the id bits for a file/initial object ACL, then neither
>should we generate a hard error nor do anything special in the
>code to strip off the id bits.
I agree with your suggestions. Please correct the code which strip off
id bits...
Shepherd
---------
 
Changed Reported by from `lhughes@transarc.com' to `sshi@ausvm1.vnet.ibm.com' 
Changed Reported by Company from `tarc' to `ibm' 
Changed Responsible Engr. from `lhughes@transarc.com' to `rajesh@transarc.com'

[5/11/93 public]
Actually, the code right now does not do anything special to strip off
id bits for files. I regression tested this feature in dfs-102-2.10 +
deltas rajesh-db3330-correct-permission-listed-in-dummy-io-acls and
rajesh-db3333-umask-and-acls. The log of operations performed is
attached below.
As a result I have downgraded the Priority/ Severity. Since this has a
potential doc impact I have added Jeff K. to the CC list.
Shepherd, let me know if you find the id bits being explicitly stripped
of for files. If you find it to be working correctly, we should cancel this
one.
Log: 
# pwd
/.../bugacl/fs/rajesh
# ls -l
total 0
-rw-r--r--   1 rajesh   system         0 May 11 17:34 a
# acl_edit a -n
sec_acl_edit> l
# SEC_ACL for a:
# Default cell = /.../bugacl
user_obj:rw-c--
group_obj:r-----
other_obj:r-----
sec_acl_edit> m user_obj:rwcid
sec_acl_edit> l
# SEC_ACL for a:
# Default cell = /.../bugacl
user_obj:rw-cid
group_obj:r-----
other_obj:r-----
sec_acl_edit> m other_obj:rwxid
sec_acl_edit> l
# SEC_ACL for a:
# Default cell = /.../bugacl
user_obj:rw-cid
group_obj:r-----
other_obj:rwx-id
sec_acl_edit> m group_obj:rid
sec_acl_edit> l
# SEC_ACL for a:
# Default cell = /.../bugacl
user_obj:rw-cid
group_obj:r---id
other_obj:rwx-id
sec_acl_edit> exit
# acl_edit a -l
# SEC_ACL for a:
# Default cell = /.../bugacl
user_obj:rw-cid
group_obj:r---id
other_obj:rwx-id
# mkdir d
# acl_edit d -io -l
# Initial SEC_ACL for objects created under: d:
# Default cell = /.../bugacl
user_obj:rwxc--
group_obj:r-x---
other_obj:r-x---
# acl_edit d -io -n
sec_acl_edit> l
# Initial SEC_ACL for objects created under: d:
# Default cell = /.../bugacl
user_obj:rwxc--
group_obj:r-x---
other_obj:r-x---
sec_acl_edit> m user_obj:rwxcid
sec_acl_edit> m group_obj:rwxcid
sec_acl_edit> m other_obj:rwxcid
sec_acl_edit> co
sec_acl_edit> exit
# acl_edit d -io -l
# Initial SEC_ACL for objects created under: d:
# Default cell = /.../bugacl
user_obj:rwxcid
group_obj:rwxcid
other_obj:rwxcid
# pwd
/.../bugacl/fs/rajesh
# ls -l
total 2
-rw-r--rwx   1 rajesh   system         0 May 11 17:34 a
drwxr-xr-x   2 rajesh   system       256 May 11 17:38 d
# touch d/f		<<<<< creates file with 0666 mode
# acl_edit d/f -l
# SEC_ACL for d/f:
# Default cell = /.../bugacl
user_obj:rw-cid
group_obj:rw-cid
other_obj:rw-cid
# acl_edit d/f			<<<< modify acl again to test robustness
sec_acl_edit> l
# SEC_ACL for d/f:
# Default cell = /.../bugacl
user_obj:rw-cid
group_obj:rw-cid
other_obj:rw-cid
sec_acl_edit> m user:ravi:rid
sec_acl_edit> l
# SEC_ACL for d/f:
# Default cell = /.../bugacl
mask_obj:rw-cid
user_obj:rw-cid
user:ravi:r---id
group_obj:rw-cid
other_obj:rw-cid
sec_acl_edit> co
sec_acl_edit> exit
# acl_edit d/f -l
# SEC_ACL for d/f:
# Default cell = /.../bugacl
mask_obj:rw-cid
user_obj:rw-cid
user:ravi:r---id
group_obj:rw-cid
other_obj:rw-cid
#
Changed Interest List CC from `rajesh, ota, cfe' to `rajesh, ota, cfe, 
 jeff@transarc.com' 
Changed Severity from `B' to `C' 
Changed Priority from `1' to `3'

[5/12/93 public]
Some more interesting data that shows things are working as they should.
Try acl inheritance for files when no "w" but with "id" and when
the file mode do not specify w.

 [rajesh] dce_login rajesh rajesh



CR Number                     : 7803
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : acl entries unthreaded incorrectly
Reported Date                 : 4/26/93
Found in Baseline             : 1.0.2
Found Date                    : 4/26/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[4/26/93 public]
The invalidate acl user function leaves the acl entries in the vnode's
queue, which it shouldn't.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/1/93 public]
This bug turned out to be a porting bug to SUNOS, which mike originally
thought was an acl bug.  Its not, so its canceled.
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 7786
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ftserver
Short Description             : comm failure on fts rename
Reported Date                 : 4/22/93
Found in Baseline             : 1.0.2
Found Date                    : 4/22/93
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[4/22/93 public]
This ot is a carryforward from 7475.  I'm including the appropriate info from 
that defect:
 gmd 4/21/93 public]
  root@dce12> fts rename -oldname epi.3 -newname epi.4
  [Failed to revert one or more names on filesets: communications failure (dce / rpc)]
  Error in rename: communications failure (dce / rpc)
  
  This is a 102a build (available 4/20/93). 3 pmax cell, 3 flservers, root.dfs in
  lfs (replicated root.dfs).
  There is nothing in the /usr/adm/messages or var/dfs/adm/*Log files near the
  time of this comm failure. I have dfstrace dumps taken immediately after
  this on all 3 machines. They are in ~notuser/gmd and named:
  7475.valentine.dump (root.dfs server)
  7475.dce5.dump (epi.3 server)
  7475.dce12.dump (dce12.u3 ufs server + core server + machine where fts rename
  command issued)
 
 tu@transarc.com 4/21/93 public]
  I read your all three traces mentioned above and there is not that much 
  info in them. By looking at error messages, it seems that fts could not talk
  to the ftserver.  I also confirmed this with Craig. 
                                                                       
  Have you checked whether your ftserver is still up ? If the ftserver is 
  still up, then it would be a rpc comm failure under a different situation.
  That is, it is not between fx server and CM.
 
 gmd 4/21/93 public]
  The ftservers' last restarts were all ~20 minutes before the comm failure. Your
  comment implies that the "comm failure" is therefore a different problem -
  Should I log the new CR with you, Tu, as responsible engineer?
 
I've set this a priority 2, because we do not have enough information to 
understand what happened.  Please collect the following info should this
occur again:
 
  icl dump (kill -30) of the ftserver where is fileset resides.
  icl dump (kill -30) of all flservers in the cell.
  dfstrace dump of the server where the fileset resides.
 
Hopefully, this will give us enough info to deduce what is happening.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/22/93 public]
The output from this fts command is consistent with the ftserver on the
machine housing epi.3 simply not being up at all.  As to why it wasn't
responding to RPC requests, I'd have no idea--if indeed it was still up
when you tried the command.  You might use some command that talks only
to an ftserver, such as ``fts statft /.:/hosts/valentine'', to tell.

[05/20/93]

We'll be investigating this.

[10/8/93]

Aged relative to Aug 1.

[1/11/94 public]
Cancelling.  I ran a script that changed the name of a fileset 
back and forth between two names.  It changed the name ~50 times 
successfully until I shot the ftserver.  I got the error shown 
above (expected behavior).  I restarted the ftserver and the script 
picked up again where it left off.  After another ~50 renames,
I killed the script, ran cm checkfilesets and was able to access
the fileset through the mount point.

This gets tested in the fts tests anyway.



CR Number                     : 7785
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : krpc
Short Description             : call timeout and communication failure in cmfx.
Reported Date                 : 4/22/93
Found in Baseline             : 1.0.2
Found Date                    : 4/22/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[4/22/93 public]
This ot report is a continuation of 7475 with clearer direction.
The symptoms are:
  A) The cache manager returns ETIMEDOUT on file system access.
  B) Either:		"cm checkservers" shows all servers up and access is
			now successful.
     Or:		After some amount of time (we've seen up to 80 minutes,
			The cm shows the server as coming back up, at which
			point access is now successful.
  C) The dfstrace output shows either 
	   382312556 call timed out (dce / rpc)
	or 
	   382312470 communications failure (dce / rpc)
	as the result of an RPC call (usually from AFS_GetTime).
Potential culprits in this failure are:
  1) The RPC subsystem has failed to correctly handle the RPC.
  2) The DFS protocol is such that occasional long pauses occur.
	Its possible that a potential token management issue might cause 
	this problem, but we need more information in order to track it down.
When you suspect this problem, we need the following information:
  "dfstrace dump" logs from the client where the failure occured, and all
	servers that it was accessing.
  icl dumps from the user level dfsbind on the client.
  icl dumps from the flservers and potentially the ftservers in the cell.
I realize that this is a lot of information, but since we have no clear
culprit, I'de rather err on the side of too much information than too little. 
Please open a new ot for user level comm failures in dfs processes.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/18/93 public]
Timeout problems are still being seen with dfs.carl. Correcting severity and
priority.

[5/19/93 public]
Elliot - reread your instructions and I have one question: can you do a
"kill -30" of the ftserver pid for an icl dump of an ftserver or is there
some other method?

[5/19/93 public]
Yes, you can kill -30 the ftserver process and it will do the dump

[05/20/93 public]

We are going to look at this here.

[6/1/93 public]
Dropping this to a priority 1 - unclear whether this is gone or just blocked
by other problems.

[06/11/93 public]

just noting that this is still a problem even with the latest dfs.

# dfs: Warning: the rpc call to server 130.105.5.78 in cell leprosy timed out.
dfs: comm failure/set context (code 382312556)
dfs: lost contact with the fx server 130.105.5.78 in cell leprosy
dfs: comm failure/set context (code 382312556)
dfs: lost contact with the fx server 130.105.5.78 in cell leprosy
dfs: comm failure/set context (code 382312556)
dfs: lost contact with the fx server 130.105.5.78 in cell leprosy
dfs: Warning: the rpc call to server 130.105.5.3 in cell leprosy timed out.
dfs: Warning: the rpc call to server 130.105.5.3 in cell leprosy timed out.
dfs: Warning: the rpc call to server 130.105.5.3 in cell leprosy timed out.
dfs: Warning: the rpc call to server 130.105.5.3 in cell leprosy timed out.

[06/18/93 public]

more data:

We have a dfsbind trace log from around the time where we hit a set context
failure and it shows that infrequently do_auth_request can take more than
30 seconds to complete:

time 804.901110, pid 8: do_auth_request: entry 
time 844.411285, pid 8: do_auth_request: exit code:0  


time 382.820110, pid 11: do_auth_request: entry 
time 641.690084, pid 11: do_auth_request: exit code:0  


time 961.831674, pid 7: do_auth_request: entry 
time 12.790498, pid 7: do_auth_request: exit code:0  


time 661.979857, pid 8: do_auth_request: entry 
time 747.667597, pid 8: do_auth_request: exit code:0

[07/07/93 public]

Many of the problems we were seeing were due to insufficient physical
memory in our dfs servers.  The recommended memory size is 32 meg and
we were running with 16 meg rios servers.  Upgrading to 32 meg servers
greatly lessened the severity of this problem.



CR Number                     : 7775
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : xvolume
Short Description             : afscall_volser is too big
Reported Date                 : 4/21/93
Found in Baseline             : 1.0.2
Found Date                    : 4/21/93
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : vol_init.c
Sensitivity                   : public
Transarc Status               : 
Transarc Deltas               : 
Transarc Herder               : 

[4/21/93 public]
I observed that afscall_volser() generates a lot of code.
I tried to just invoke the VOLCHECK() macro in fewer places,
in the obvious way, by just invoking once before the switch 
statement instead of in most of the cases of the switch statement.
This cut the binary size on HPUX from 16+KBytes to less than
12KBytes, for a savings of 4+KBytes.  When you guys get to it...

[6/21/93 public]
This is an enhancment, since the code works just fine as it stands.
We'll try and get this one in 1.0.3
Changed Defect or Enhancement? from `def' to `enh' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/23/93 public]
Changed Responsible Engr. from `jaffe' to `kazar@transarc.com'

[1/20/94 public]
I did this as part of some other work, but neglected to tag it in this
OT report.  The OSF should have the code that does this.

[1/20/94 public]
Changed Status from `open' to `cancel'



CR Number                     : 7753
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : assertion failure block_alloc.c line 3125
Reported Date                 : 4/19/93
Found in Baseline             : 1.0.2b23
Found Date                    : 4/19/93
Severity                      : A
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[4/19/93 public]
I was trying to clean up some files left over from a low.moderate
run (via dfs) on a fileset which was previously healthy (at mount
time) and I hit the following assertion failure:
 
asstion failed line 3125 anode/block_alloc.c
 
siteDescOur
SAFS_RemoveDir+7d8
vrele+13c
xglue_inactive+4c
nosf_inactive+4c
efs_inactive+1e4
epif_ChangeLink+47c
epia_Truncate+564
epib_FreeFrags+460
 
  3118      BEGIN_LCRIT (struct elbb_buffer *buf,
  3119                   code = epix_GetBuffer (h->bitmap, p, &buf),
  3120                   code = elbb_Release (buf, elbb_lazy)) {
  3121          struct bitmapPage *page = (struct bitmapPage *) elbb_BufferData (buf);
  3122          u_long firstFrag;
  3123          u_long nFrags;
  3124
  3125          assert (BITMAP_PAGE_OK (page, h));
  3126          assert ((frags->block >= page->baseOffset) &&
  3127                  (frags->block < page->baseOffset + page->blocksPerPage));
  3128
  3129          firstFrag = (frags->block - page->baseOffset)*page->fragmentsPerBlock
+
  3130              frags->first + keep;
  3131          nFrags = frags->length-keep;
  3132
  3133          if (keep) {                     /* can't be empty if keeping any */
  3134              if (frags->length == h->fragmentsPerBlock)
  3135                  empty = -2;             /* signal to update logged map */
  3136              else empty = 0;
  3137          }
 
This is the fldb info for the fileset which caused the problem:
 
# fts lsfldb -server /.:/hosts/fire
m1.lfs2  
        readWrite   ID 0,,487  valid
        readOnly    ID 0,,1265  invalid
        backup      ID 0,,489  valid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
Error: communications failure (dce / rpc)
fire.osf.org        RW,BK    3       0:00:00 hosts/fire     <nil>

[4/19/93 public]
Changed Responsible Engr. from `jsk@transarc.com' to `ota@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[04/20/93 public]
I don't know what's going on here, but I got tired of this and newaggr'd
the aggregate, created a new fileset, and started copying a file tree there
and got yet another panic in block_alloc.c.  There are no disk errors
logged to the console to indicate that this might be a disk problem.
 
Here's what is probably another useless stack trace:
 
assertion failed line 2746  block_alloc.c
 
epia_Read + 1dd0 
epib_Allocate_5b4
 
  2743          BEGIN_CRIT (code = epix_GetBuffer (h->bitmap, p, &buf),
  2744                      code = elbb_Release (buf, elbb_lazy)) {
  2745              page = (struct bitmapPage *)elbb_BufferData(buf);
  2746              assert (BITMAP_PAGE_OK (page, h));
  2747              if (start == EPIX_ANODE_BLOCKEMPTY)
  2748                  start = page->baseOffset, wait = 0;
  2749
  2750              /* as long as everything is okay, keep looking for blocks */
  2751              while ((code == 0) && (b < nBlocks)) {
  2752

[4/20/93 public]
I assume that you newaggr'ed the aggregate before starting up DFS or
exporting the aggregate in a freshly-rebooted system.  Right?
Also, I expect that Ted will need some compressed version of the aggregate
data in order to debug this, but I'll defer to him to pick the right way
to get the bits packed up.

[4/20/93 public]
It would be important to have the aggregate if I am to find out exactly
what is going on.  However, it is clear that the disk is damaged.  Did
you run the salvager when this happened?  What did it say?  That might
go a long way to explaining the problem.
It also looks like the same error both times.  Are you sure the newaggr
really took effect on the aggregate in question?
Filled in Interest List CC with `cfe'

[5/18/93 public]
Assigning to diane.  We need the aggregate to make any more progress.
Changed Interest List CC from `cfe' to `cfe@transarc.com, ota@transarc.com' 
Changed Responsible Engr. from `ota@transarc.com' to `delgado'

[06/02/93 public]

We no longer have the aggregate which had this feature - cancelled.



CR Number                     : 7737
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : not all rep tests get installed.
Reported Date                 : 4/16/93
Found in Baseline             : 1.0.2
Found Date                    : 4/16/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/16/93 public]
There are 4 rep tests, but only 3 get installed into dcetest.

[4/16/93 public]
Only rep tests rtest1, rtest2, rtest3 are actually run.  rtest4 is a relic
and should perhaps be deleted rather than installed.  In itself, this isn't
a high-priority bug.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/16/93 public]
Sounds reasonable given the runtests script doesn't try to execute
rtest4.  Downgrading and changing purpose of bug to remove 
references to rtest4 in README and delete rtest4 itself (unless 
of course, it is of some value to the end user in which case it
should be documented in the README).  

Once the updates are made, the README file could be installed 
under dcetest too.



CR Number                     : 7718
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fs tests use unsupported cmd
Reported Date                 : 4/14/93
Found in Baseline             : 1.0.2
Found Date                    : 4/14/93
Severity                      : D
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[4/14/93 public]
The fs test script runtests uses the command pstat which is not 
available on OSF/1.

[10/8/93 public]
Cancelling.  OSF/1 is no longer a reference platform and this
has already been fixed on it replacement, HP-UX.



CR Number                     : 7667
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfs test build problems
Reported Date                 : 4/7/93
Found in Baseline             : 1.0.2b21
Found Date                    : 4/7/93
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[4/7/93 public]

The itl/utils/* files #include lines should have sys/file instead of
just file.

[10/1/93 public]

There do not seem to be any build problems currently with the dfs
functional test itl directory - cancelling.



CR Number                     : 7576
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : pmax crash - Read address error
Reported Date                 : 3/23/93
Found in Baseline             : 1.0.2
Found Date                    : 3/23/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[3/23/93 public]
Assigning to myself until I see it again. Highly possible that 1st pmax
client's bizarre/unknown state recovering from CR 7568 crash caused
2nd pmax client's demise. Note that the 3rd pmax, the core and root.dfs
server and sync site is acting as it should - reporting lost contact/loss
of quortum.
BUILD:	nb available 3/19/93
CONFIG	3 pmax, 3 flserver cell - config'd today (3/23/93) - healthy ~3 hours
DETAILS:	1st pmax client went down and out with CR 7568
		(suspected kernel overflow)
		brought 1st pmax client back up (rc.dce, rc.dfs)
		there was an error about "entry not found" running
		rc.dts but APPEARED benign
		on 2nd pmax client, cd /:/gmd_dfs -> CRASH
stack trace from kdb:
		malloc
		rpc__mem_malloc
		rpc_string_binding_parse
		rpc_binding_from_string_binding
		krpc_BindingFromSockaddr
		cm_ConnByHost
		cm_ConnByMHosts
		cm_GetVolumeByName
		cm_HoldVolume
		end
		on 1st pmax client, appears to
		be messed up more seriously than I thought:
root@valentine> rpccp show group /.:/fs
>>> rpc runtime error: entry not found (dce / rpc)
root@valentine> ps laxww|grep rpc
    0   202     1   9 -13   0 9.27M 1.32 event  S    ??         1:03.20
/opt/dcelocal/bin/rpcd
    0  5174  4972   4 -13   0 7.72M 444K spipe  S  + pts/2      0:00.41
grep rpc
root@valentine> cdscp show dir /.:
                        SHOW
                   DIRECTORY   /.../dce5_cell
                          AT   1993-03-23-12:37:52
Error on entity: /.../dce5_cell
Requested entry does not exist (dce / cds)
Function: dnsEnumAttr
dnsEnumAttr: partial results = %x000000000000000000000000000000000000

[3/24/93 public]
NOTE: I had turned krpc debugging on on BOTH pmax clients but NOT on
the pmax server that remained healthy.

[03/25/93 public]
I've seen this crash now twice in the OSF CHO cell, both times on
machine 4 (the PMAX client-only machine in the cell).  Whether
coincidentally or not, on both occasions this crash happened immediately
after m3 in the CHO cell had hung as described in OT 7568.
The stack trace I got was the same as the one Gail shows above,
from the cm_ConnByMHosts entry point downward (different call
stack above that, but I think that's irrelevant).  In my crashes,
the stack looks like this (with offsets added):

		malloc+640
		rpc__mem_malloc+200
		rpc_string_binding_parse+41c
		rpc_binding_from_string_binding+ac
		krpc_BindingFromSockaddr+1cc
		cm_ConnByHost+528
		cm_ConnByMHosts+49c

In poking around with kdb, I see that something has corrupted one of
the cache "buckets" used by the OSF/1 kernel malloc() routine.  The
first word of this bucket is supposed to be a pointer to the first
region of memory associated with the bucket.  At the point of the
crash, this word contains the value "5".  (This was true in both
crashes I've looked at.)

Naturally, it's very hard to determine after the fact what was
responsible for this corruption.  I believe the failing call to
malloc() is the one at line 1306 of rpc/runtime/combind.c, though I
believe this call is not at fault.  (All the information it passes
downward looks valid.)

I have two theories about what might have happened.  I'm hoping someone
with RPC experience (Hi, Bill!) can evaluate these and provide some a
priori direction to the search for what's going wrong, rather than
having me or another OSF'er do a brute-force hunt for problems.  Both
times I've seen this, I, like Gail, was running with the new KRPC
debugging/logging turned on.  I'd like to concentrate on the
possibility that turning this on is responsible for the corruption.
There are at least three possibilities.  If the problem is linked to
KRPC logging, perhaps we can evaluate these.  If not, the possibilities
are too general to provide any guidance.

	First, OSF/1 malloc() could be misbehaving.  Seems unlikely
	(given the rarity of this symptom), but I guess it's possible.

	Second, some caller of malloc() could be writing to mallocked
	memory after it's been freed.  The regions of memory in a
	malloc() bucket are linked using the first word of each region,
	and exactly these regions are returned to its callers.  So a
	late write into the first word of mallocked memory could
	corrupt the bucket list as I have seen.

	Third, there could be a mismatch in RPC memory "type"
	(RPC_MEM_IS_FL class) between some rpc__mem_alloc()/
	rpc__mem_free() pair.  I see that, when the type is not "fixed
	length", the RPC allocation code stores an entry length in the
	first word of the region returned by the underlying malloc()
	package, and modifies the address returned to its caller
	appropriately.  This kind of mismatch could also corrupt
	malloc() internal data, though it's harder to see exactly how
	this particular corruption could result.

Anyway, I very much hope that this bug is linked to enabling KRPC
logging, since at least that gives us some orientation in starting to
look for it.  I will start up the CHO cell again, without KRPC logging
turned on, and see if I can make the failure happen in that case.
Raising priority to a 1.
Bill, if you want your usual debug info I can provide it, but I'm
not sure it will do you any good -- I've been over it already.

[3/25/93 public]
Hmm.  dfs_icl_printf doesn't allocate memory itself so it seems unlikely
that logging is responsible (but you never know).
Option 2 or 3 seems fairly unlikely to me, unless some CM code is calling
the wrong version of free() with strings returned from RPC..
With respect to option 3: it could be possible to verify if this is the
case by building an instrumented version of rpcmem which includes the "type
tag" in the allocated memory itself.

[3/25/93 public]
see also my note to #7568 regarding the stack usage in dfs_icl_printf..

[3/26/93 public]
Filled in Responsible Engr. with `bolinger' 
Filled in Resp. Engr's Company with `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[04/09/93 public]
No real progress in tracking this one down.  I haven't been able to
extract any interesting data post-mortem that seem relevant to this
failure.  The call chain down to the panic seems to be working with
valid data -- unsurprising, I guess, since the corruption probably
resulted from some _earlier_ memory allocation or free.  My only
suggestion in debugging this further is to enable OSI-level malloc()
debugging via the AFSDEBMEM preprocessor symbol and/or RPCMEM tracing
and perform more testing.

[04/16/93 public]
Downgraded since this doesn't prevent meeting exit criteria for 
1.0.2a.  Will upgrade if we see it without krpc debug on.

[08/3/93 public]
Cancelling since PMAX'es are not supported in 1.0.3.



CR Number                     : 7539
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm setsetuid doesn't report failures
Reported Date                 : 3/18/93
Found in Baseline             : 1.0.2
Found Date                    : 3/18/93
Severity                      : D
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : 

[3/18/93 public]
The command cm setsetuid returns 0 and reports no error even if it
failed (ie. when you do not have enough privilege to successfully
execute this command).
(admin_checklist)

[3/23/93 public]
Changed Responsible Engr. from `pakhtar' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/9/93 public]
It succeeds and fails appropriately for me.  Could you please re-check?

[7/20/93 public]
Gail, I have tried it on AIX and could not reproduce it. I suspect this 
could have been fixed or it is a very PMAX specific. 
Please cancel it if it is really not reproducd any more or let me know 
any new development. 
Thanks, 
Tu
Changed Responsible Engr. from `tu@transarc.com' to `gmd@osf.org' 
Filled in Transarc Herder with `jaffe@transarc.com'

[7/22/93 public]
Hmm - I can no longer reproduce this problem either. I may have been confusing
local Unix identity with DCE principal identity ie. logged in as local Unix 
user root but as DCE prinipal gmd, you can successfully setsetuid and the
reverse is NOT the case (Unix user gmd, DCE principal root). In any case,
my current testing appears to be consistent with documentation - canceling.



CR Number                     : 7419
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : kernel TLB miss in streams driver
Reported Date                 : 3/4/93
Found in Baseline             : 1.0.2
Found Date                    : 3/4/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open

[3/4/93 public]
Kernel TLB miss while running lots of things on a pmax in a CHO cell.
the machine paniced after ~12 hours of heavy use:
Ksegv pc = 80158354, va = 0, sp = c391a628
Kdb kernel trap [test7a]: TLB miss (read) trap, code=0
stopped at      putq_owned+230: sw      s0,0(v0) <0>    <s0=c27d6800,v0=0>
kdb> $k
thread 0xc13f8000, pcb 0xc391ac08, W_EVENT 0
<Exception frame> Cause=0x30000008 Badvaddr=0x0 SR=0xff14 EPC=0x80158354
0xc391a6cc putq_owned() from putq+54
0xc391a6fc putq() from strtty_wput+1d4
0xc391a71c strtty_wput() from csq_lateral+c8
0xc391a74c csq_lateral() from puthere+68
0xc391a77c puthere() from ldtty_start+e8
0xc391a79c ldtty_start() from ldtty_write+2bc
0xc391a7c4 ldtty_write() from ldtty_wput+e8
0xc391a7fc ldtty_wput() from csq_lateral+c8
0xc391a82c csq_lateral() from puthere+68
0xc391a85c puthere() from osr_write+694
0xc391a87c osr_write() from osr_run+2d8
0xc391a8d4 osr_run() from pse_write+bc
0xc391a914 pse_write() from spec_write+12c
0xc391a94c spec_write() from ufsspec_write+70
0xc391a9bc ufsspec_write() from vn_write+238
0xc391a9f4 vn_write() from rwuio+16c
0xc391aa54 rwuio() from write+50
0xc391aa94 write() from syscall+27c
0xc391aadc syscall() from VEC_syscall+44
0xc391ab64 VEC_syscall()
         <Exception frame> Cause=0x20 Badvaddr=0x100000f4 SR=0xff3c EPC=0x405020
kdb> $r
at    1
v0    0
v1    0
a0    c253c88c
a1    c27d6800
a2    801dfe30  strtty_wput
a3    c253c8e8
t0    c2503f00
t1    0
t2    0
t3    0
t4    0
t5    0
t6    0
t7    0
s0    c27d6800
s1    c27d6800
s2    c253c88c
s3    0
s4    0
s5    c253c68c
s6    c270fa00
s7    400
t8    804f9e14  partab+20
t9    0
k0    800d2d50  simple_unlock+6c
k1    80158354  putq_owned+230
gp    ff3d
sp    c391a6cc
s8    c391a77c
ra    80157d04  putq+5c
sr    ff14
lo    c
hi    0
badv  0
cs    30000008
pc    80158354  putq_owned+230
putq_owned+230: sw      s0,0(v0) <0>    <s0=c27d6800,v0=0>
kdb> putq_owned+190,50/ia
putq_owned+190: nop
putq_owned+194: lw      v0,8(s1)
putq_owned+198: nop
putq_owned+19c: bne     v0,zero,putq_owned+20c
putq_owned+1a0: nop
putq_owned+1a4: lw      v0,10(s1)
putq_owned+1a8: lw      a0,c(s1)
putq_owned+1ac: nop
putq_owned+1b0: subu    s3,v0,a0
putq_owned+1b4: slt     at,zero,s3
putq_owned+1b8: beq     at,zero,putq_owned+20c
putq_owned+1bc: nop
putq_owned+1c0: lw      v0,8(a1)
putq_owned+1c4: nop
putq_owned+1c8: subu    v0,v0,v1
putq_owned+1cc: slt     at,v0,s3
putq_owned+1d0: bne     at,zero,putq_owned+20c
putq_owned+1d4: nop
putq_owned+1d8: move    a1,v1
putq_owned+1dc: move    a2,s3
putq_owned+1e0: jal     bcopy
putq_owned+1e4: nop
putq_owned+1e8: lw      v0,10(s0)
putq_owned+1ec: nop
putq_owned+1f0: addu    v0,s3,v0
putq_owned+1f4: sw      v0,10(s0)
putq_owned+1f8: move    a0,s1
putq_owned+1fc: jal     freeb
putq_owned+200: nop
putq_owned+204: j       putq_owned+248
putq_owned+208: nop
putq_owned+20c: move    s3,zero
putq_owned+210: move    s0,s1
putq_owned+214: lw      v0,8(s2)
putq_owned+218: nop
putq_owned+21c: sw      v0,4(s0)
putq_owned+220: beq     v0,zero,putq_owned+23c
putq_owned+224: nop
putq_owned+228: lw      v0,8(s2)
putq_owned+22c: nop
putq_owned+230: sw      s0,0(v0)
putq_owned+234: j       putq_owned+240
putq_owned+238: nop
putq_owned+23c: sw      s0,4(s2)
putq_owned+240: sw      s0,8(s2)
putq_owned+244: sw      zero,0(s0)
putq_owned+248: beq     s3,zero,putq_owned+268
putq_owned+24c: nop
putq_owned+250: lw      v0,18(s2)
putq_owned+254: nop
putq_owned+258: addu    v0,s3,v0
putq_owned+25c: sw      v0,18(s2)
putq_owned+260: j       putq_owned+298
putq_owned+264: nop
putq_owned+268: lw      v0,10(s0)
putq_owned+26c: lw      v1,c(s0)
putq_owned+270: nop
putq_owned+274: subu    v0,v0,v1
putq_owned+278: lw      v1,18(s2)
putq_owned+27c: nop
putq_owned+280: addu    v0,v0,v1
putq_owned+284: sw      v0,18(s2)
putq_owned+288: lw      s0,8(s0)
putq_owned+28c: nop
putq_owned+290: bne     s0,zero,putq_owned+268
putq_owned+294: nop
putq_owned+298: jal     splimp
putq_owned+29c: nop
putq_owned+2a0: move    s0,v0
putq_owned+2a4: addiu   a0,s2,7c
putq_owned+2a8: jal     simple_lock
putq_owned+2ac: nop
putq_owned+2b0: lw      v0,18(s2)
putq_owned+2b4: lw      v1,28(s2)
putq_owned+2b8: nop
putq_owned+2bc: sltu    at,v0,v1
putq_owned+2c0: bne     at,zero,putq_owned+2d8
putq_owned+2c4: nop
putq_owned+2c8: lw      v0,1c(s2)
putq_owned+2cc: nop
putq_owned+2d0:
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[03/04/93 public]
Comments from Tom Talpey (tmt@osf.org):

I need to calibrate my instruments. This is DCE 1.0.2, meaning it's
running on a Pmax with OSF/1 1.1.1? Also, it appears this was a write
to a hardware TTY, is this correct?

If both these answers are yes, I believe it's a known bug which, due to
the reference platform change, has been deferred indefinitely. However,
I know of a fairly straightforward fix (the driver fails to protect
itself with spltty()). Let me know and I'll investigate further.

Thanks for the report.

Tom Talpey.

[03/11/93 public]
I have made changes to the console driver in OSF/1 1.1.1 that are meant
to address this bug.  The new driver works at OSF in our usage patterns
(very limited local console access + use of X on console display) as
well as the old driver did.  However, we've never seen the Streams
failure described here, because we don't use the PMAX console like
Transarc does.  I do not know whether the new driver would make
Transarc's usage pattern (heavy remote console access) work better.

I asked Elliot Jaffe yesterday if he'd be willing to test the new
driver to see if the Streams problem had gotten better -- the short
answer is that he didn't think that would be worthwhile.  Transarc has
worked around the occasional failures they've seen, and will not be
using OSF/1 on their PMAX's after DCE 1.0.2 (real soon now).

So I believe that it's not worthwhile to submit the new driver before
1.0.2.  There's a good chance that it will never be submitted at all,
in which case this defect should eventually be cancelled.

For now, reduce the priority and defer the defect.  If Elliot or
anyone else at Transarc changes their minds, we can revisit this.

[04/27/93 public]
Oops...  Missed this one in scanning OT for defects assigned to me.

[09/28/93 public]
PMAX OSF/1 is no longer a reference platform.  Cancelled.



CR Number                     : 7407
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : assert in vnm_Rouse after hitting disk quota
Reported Date                 : 3/3/93
Found in Baseline             : 1.0.2
Found Date                    : 3/3/93
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[3/3/93 public]
2 machine hetero cell
core components on pmax
ufs root.dfs on pmax, RIOS jfs and PMAX Episode filesets 
mounted underneath.
As a DFS client on the PMAX, while copying test binaries into the 
PMAX Episode fileset, got multiple "server disk quota exceeded 
messages" followed by an assertion failure on the PMAX at line 1373 
of efs_misc.c.  Looking at source, I believe cause of assert is 
non-zero error return from call to epif_Open() in vnm_Rouse().
kdb says stack is:
gimmeabreak
Debugger
vnm_Rouse
vnm_Unrouse
but I think it's confused and actual stack trace is (based on manual
examination of stack contents):
gimmeabreak
Debugger
vnm_Rouse
Recycle
efs_reclaim
nosf_reclaim
The machine is still in kdb if I can gather any further info, but I
can't guarantee how long it will stay that way.  I'm not sure the return 
value from epif_Open() is accessible at this point since there's been 
an intervening function call to blow away the register.

[3/3/93 public]
At Bruces's request, here's the raw stack (or at least part of it):
c2abd200		c2abd200	c2953200		6
vnm_Rouse+68		c2a3fe8c	0			c2a4ac8c
copyright+9844 		55d		copyright+97a9		2
c2abd200		vnm_Unrouse+104	processory_array	VEC_tlbmiss+28
a010			94c38		0			fff05558
c2abd200		c2abd200	c3ad98a4		efs_reclaim+78
c2abd258		c2abd200	simple_unlock+6c	(blank)
VEC_tlbmiss+28		c1551e88	U_ADDRESS+4		0
ff34			vnode_stats+c	c3ad9894		c2abd200
vinvalbuf+24c		c2abd200	nosf_reclaim+54		8
c2d96204		8		3b8			c2abd278
c2abd200		c2abd200	xglue_reclaim+54	(blank)
0			dead_vnodeops	5a32			icache_stats+c
c3ad98a4		vclean+ec	20004			vclean+1d4
simple_unlock+6c	(blank)		5a32			c2abd200
dead_vnodeops		2		1			ufs_vnodeops
c274e000		vgone+174	vgone+150		simple_unlock+6c
iget+134		c2953794	c3ad97f4		simple_unlock+6c
getnewvnode+1c4		c2abd200	vnode_stats+c		c3ad9894
getnewvnode+340		3000000c	getnewvnode+24c
Changed Interest List CC from `ota@transarc.com, bwl@transarc.com' to 
 `ota@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/12/93 public]
We don't have enough information to go any further on this.  Assigned to
Ron to attempt to reproduce, and cancel if we don't see it under 2.8.  I've 
dropped the priority since it hasn't been reproduced and there is nothing
we can do with the current information.  Please call us when/if this 
occurs again.
Changed Priority from `1' to `2' 
Changed Responsible Engr. from `bwl@transarc.com' to `rsarbo'

[3/30/93 public]
We've seen something which appears to be the same thing as this with
b20.  I had a 1 cell machine (pmax) with an episode fileset which
was mounted locally, but not accessed through dfs.  I was copying a
file tree to this episode fileset when the paritition on which this
fileset resided ran out of space.  Shortly afterward the machine 
crashed because it attempted to reference a bad address of 0x06.
The stack was
  vnm_Unrouse +fc
  vnm_Rouse +2c
  epif_Open +ac
epif_Open +ac corresponds to lw a1,4(s0); s0 at this point is 2
This is the instruction right before jal epid_IndexFromFid.
Here's some information I collected about the vnode which this
code was operating on:
v_usecount=0
v_holdcount=0
v_flag=0x20004 - VXLOCK|V_CONVERTED
v_op -> this has the xglue*, efs*, nosf* routines
vfreef, vfreeb = 0
vmountf, vmountb = 0
vtype = VNON
v_mount = dead_mount
efs specific data:
vd_flags = 4
vdu_volp = 2

[4/7/93 public]
Changed Transarc Status from `open' to `import'

[4/19/93 public]
Not sure why this was changed to import.  If there is a delta, can
someone please ot export this and add it here.
Changed Transarc Status from `import' to `open'

[6/7/93 public]
We've not seen this in >2 months. Cancelling.



CR Number                     : 7401
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : TLB miss (read) - cm_CheckSize
Reported Date                 : 3/2/93
Found in Baseline             : 1.0.2b17
Found Date                    : 3/2/93
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[3/2/93 public]
BUILD:	b17 (aka dfs 2.6)
CONFIG:	4 pmax cell
	PMAX1	- sec, cds, dts, fl server and dfs client
	PMAX2	- dts, fl server and dfs client
	PMAX3	- dts, fl server and dfs client
	PMAX4	- file server and dfs client
root.dfs in ufs on PMAX1
all dfs clients with small (100 block) caches
registry altered such that default cert lifetime = forever but
server principal cert lifetimes = 10h and gmd cert lifetime = 4d
Had run dfs.glue successfully for 5 hours to a ufs partition on PMAX2,
then the dfsbind's on PMAX1 and PMAX4 core dumped (core files unusable -
Diane tried to look at them) and shortly (but definitely not immediately)
after that the test on PMAX2 hung the rest of the night.
Next day -> Connection timed out/lost contact errors appear and then disappear
with a new dce_login - although lifetime of certificate had at least 3 days.
dfstrace dump, kill -30 (dfsbind_pid) and drop into kdb to look around.
Come back up - correct time - test still hanging - no activity in cell and PMAX2crashes with:
TLB miss (read)
cm_CheckSize() from end+###
That's all kdb would tell me - couldn't continue.
Long description included because I doubt this will be easily reproducable
(note no activity at time of crash). I have the dump, iclbind and notes
on some of the threads. High priority on bugs found by system test a policy.

[3/3/93 public]
Clearly, this is going to be tough to reproduce or verify.  I did learn
something, though, which is that the only call to cm_CheckSize() in the
whole system is from a procedure that services the pioctl() that sets
a new cache size.  One micro bug is that cm_CheckSize is called to reduce
cache usage to the requested amount before sanity-checking the input
value.  So it becomes very interesting to know what you were trying to set
the cache size to (e.g. with ``cm setcachesize'' or an equivalent).
And there must have been such a process running, or else this stack trace
is totally useless.
.
The unusable dfsbind core dumps will also contribute to the amorphous
and fundamentally insoluble nature of this report, unless it's a
reproducible case.
Filled in Interest List CC with `cfe' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[3/4/93 public]
Thanks for the info Craig - cancelling since the kdb stack trace must
be garbage - no cm commands were being issued at the time - the hung
test may have come back to life but it's only a basic file write/read
test - not something that directly manipulates cache parameters.

[4/23/93 public]
BUILD:	102a available 4/26/93
CONFIG: 3 pmax cell, 3 flservers
TEST:	client to both a glue and a lock run - definitely no cm set/get
	cache size commands running

Same questionable kdb stack trace seen again - unfortunately I rebooted
before realizing this. Re-opening, dropping to a "2" and assigning to
myself. NEXT time ...

[05/28/93 public]
Haven't seen this one - cancelling.



CR Number                     : 7312
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Recreating an exported UFS filesystem makes mount point "impassible"
Reported Date                 : 2/22/93
Found in Baseline             : 1.0.2b16
Found Date                    : 2/22/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[2/22/93 public]
Note:  this defect was observed in a system installed from DFS 2.4.  I
have not upgraded the test system to 2.6 due to the continuing serious
problems Diane et al. have had in using the NB since 2.6 was dropped.
I have not been able to try reproducing this under 2.6 due to a lack
of crash machines with spare disk partitions.
Given a single-machine cell in which a UFS filesystem has been
exported, it seems that an operation that "re-creates" the filesystem
(i.e., that overwrites the existing root inode with one containing a
new generation number) can change the behavior of the filesystem when
it's remounted.  The change is that traversal of the mount point for
the filesystem fails in the "upward" direction.  For instance, running
pwd in the root of the filesystem will print "/".  Sometimes, trying to
cd out of the filesystem via a relative pathname ("../newfileset" or
whatever) also fails.
This problem seems always to occur on a filesystem that has been
newly-restored from a dump via the "fts restore -overwrite" command.
The problem sometimes occurs on a previously-exported filesystem that
is detached/unmounted, newfs'd, then remounted/re-exported.
Here is a log that illustrates the problem.  "/u1/ppd" is a program
that prints the device/i-number of successive parent directories
of "." until it reaches what seems to be the root of the directory
tree.
In this first output (from Saturday), the problematic fs is exported at
/.:/fs/fire.fs2.  fire.fs1 is another UFS fs that hadn't been touched
since the machine was booted.
        root@fire  # cd /.:/fs
        root@fire  # pwd
        /.../fire_cell.osf.org/fs
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = ffff0066
        path ./../: current dev = 1, current ino = ffff0065
        path ./../../: current dev = 1, current ino = 2
        path ./../../../: current dev = 800, current ino = 2
        current == root
        root@fire  # cd fire.fs1
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = 10009
        path ./../: current dev = 1, current ino = ffff0066
        path ./../../: current dev = 1, current ino = ffff0065
        path ./../../../: current dev = 1, current ino = 2
        path ./../../../../: current dev = 800, current ino = 2
        current == root
Note that we can't traverse the mount point of fire.fs2 at all:
        root@fire  # cd ../fire.fs2
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = 70002
        current == previous
        root@fire  # pwd
        /
        root@fire  # cd ..
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = 70002
        current == previous
        root@fire  # pwd
        /
        root@fire  # cd ../fire.fs1
        ../fire.fs1: No such file or directory
fire.fs1 is still available, however:
        root@fire  # cd /.:/fs/fire.fs1
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = 10009
        path ./../: current dev = 1, current ino = ffff0066
        path ./../../: current dev = 1, current ino = ffff0065
        path ./../../../: current dev = 1, current ino = 2
        path ./../../../../: current dev = 800, current ino = 2
        current == root
        root@fire  # pwd
        /.../fire_cell.osf.org/fs/fire.fs1
Today, things look a bit different.  I've just restored a new
dump into fire.fs2:
        root@fire  # cd /.:/fs/fire.fs2
        root@fire  # pwd
        /
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = 70002
        current == previous
So pwd (i.e., getcwd()) still seems unable to traverse the mount
point of fire.fs2, but I can cd out of fire.fs2 via a
relative pathname:
        root@fire  # cd ../fire.fs1
        root@fire  # pwd
        /.../fire_cell.osf.org/fs/fire.fs1
        root@fire  # /u1/ppd
        root dev = 800, root ino = 2
        path ./: current dev = 1, current ino = 10009
        path ./../: current dev = 1, current ino = ffff0066
        path ./../../: current dev = 1, current ino = ffff0065
        path ./../../../: current dev = 1, current ino = 2
        path ./../../../../: current dev = 800, current ino = 2
        current == root
Craig Everhart speculates that a cache entry within the CM is
not being invalidated when the UFS filesystem is detached.  That
sounds about right...

[2/22/93 public]
Filled in Interest List CC with `kazar@transarc.com' 
Filled in Responsible Engr. with `jdp@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[02/23/93 public]
I've had a chance to merge up the OSF/1 volops code to DFS 2.6, and to
test the failure scenario described here on a single-machine PMAX cell
installed from the current OSF NB, which is based on 2.6.  I have not
been able to reproduce the failure so far.  Details:
	On a filesystem that's overwritten with fts restore, there
	seems no longer to be a problem.  Both pwd and the parent
	directory printer function normally.
	If I try the other way of reproducing the failure (detach,
	newfs, re-attach a DFS-exported UFS filesystem), I find that
	I can no longer do the detach.  If I try, I see this pair
	of messages:
		root@fire  # ./do-u 3d
		+ dfsexport -detach /dev/rz3d 
		dfsexport: Revoking tokens for filesets on aggregate 2231311...
		dfsexport: Failed to detach /dev/rz3d:/fs1 (Device busy)
Carl mentioned yesterday that you could no longer detach locally-mounted
"filesets" (i.e., UFS filesystems).  If this is known and intended
behavior, then I'm willing to say the defect described by this CR no longer
exists.
If someone will please confirm that the dfsexport behavior noted above
is correct, I will cancel this defect.
Thanks.

[2/24/93 public]
adding me to CC list.
I don't believe that it was our intention to prevent the detaching of
non-LFS filesets in the dfsexport stuff, even though they're mounted
locally.  We have several restrictions on what you can do to locally-mounted
filesets (you can't destroy them, you can't move them to another server),
but I think that you should be able to change your mind and stop exporting
a fileset.
There's a bug (OT 7225, fix included in DFS 2.7 as
jdp-ot7225-fix-detach-refcount-management) that corrects at least some of
the problems we had in detaching filesets with dfsexport that were new in
DFS 2.6.
Changed Interest List CC from `kazar@transarc.com' to `kazar@transarc.com, 
 cfe@transarc.com'

[02/24/93 public]
Per phone conversation with Pervaze:  Though the symptoms noted
here have "disappeared" in DFS 2.6, the people at Transarc who have
looked at the relevant CM code are convinced that a problem still
exists.  Since the latent bug(s) are no longer holding up my
testing of the new OSF/1 volops code, we agreed to reduce the
priority of this defect -- but with the understanding that Transarc
will continue analyzing it with a view to fixing it in 1.0.2.
There's apparently a good deal of surprise that the scenario
described here (restoring atop an exported UFS filesystem) doesn't
cause more problems than it seems to.  If, for whatever reason,
the bug does again become more visible, we may have to raise the
priority again.  When OSF receives DFS 2.7, I will again try
to detach exported UFS partitions.  Assuming that then succeeds,
I'll try to reproduce this mount point weirdness using that
method, as well.

[2/24/93 public]
The actual work to resolve this problem is in Mike Kazar's domain.
Changed Interest List CC from `kazar@transarc.com, cfe@transarc.com' to 
 `jdp@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `jdp@transarc.com' to `kazar@transarc.com'

[7/7/93 public]
Just as a reminder.  The cause of the problem of traversing .. up from the
root of a UFS fileset seemed to be this.  The uniquifier of the root directory
of a filesystem can change when the filesystem is restored.  As it happens,
both JFS and Episode use a uniquifier of 1 for the root directory of all
filesets, but OSF/1 UFS (and probably other UFSes also) start the uniquifier
at an arbitrary value.  Now, when the CM traverses a mount point into a UFS
fileset, it looks up the afsFid for the root directory and remembers it in
the volp->rootFid field for the volume, as well as in the scp->mountRoot
information.
 
Now, if that UFS fileset has a complete dump restored to it, in general the
uniquifier of the root directory will change.  The vnode number of the root
directory will not generally change, though I suppose it might.  It's my
guess that the CM isn't handling this change in uniquifiers well, and that
it is perhaps reporting obsolete information for the value of '..' in the
fileset's root directory.
 
Just for clarity, the situation is that there are two UFS filesets, U1 and U2,
created/newfs'ed at different times, with different uniquifiers on their
root directories.  There are DFS mount points for both of these filesets--
in particular, for fileset U2.  The CM has traversed the mount point into U2,
and has thus cached information about it.  Fileset U1 is then dumped, and
is restored onto fileset U2, thus changing the uniquifier for its root
directory.
 
FYI, the only signal that is generated at present to mark this restoration
is that read&write data&status tokens are revoked for the target fileset.
 
I believe that when Don Bolinger was working on this problem, he attempted
to change the OSF/1 UFS dump and restore operations to force the root
directory's uniquifier to 1, in order to achieve interoperability with JFS
and Episode, but I can't find evidence of that now.

[7/19/93 public]
I took a look at the CM code, and as far as I can tell, things should
work in 1.0.3, *as long as you do a "cm checkft"* command after doing
the restore.  The issue is this: there no object that gets a callback
when the root file ID changes for a file system.  Frankly, I can't
quite see why the file ID *is* changing in this case, rather than
being restored, but that's a different issue.
In any event, when we have data not connected with a token, we
typically revalidate it on a periodic basis, and provide the "cm
checkft" command to trigger these checks on a demand basis.
I've looked through the code, and it looks to me like a checkft
command will force the revalidation of all of the mount points in the
cache.  When we cruise through the mount point again (you'll have to
re-CD, since you're CD'd to a stale vnode right now), the
cm_EvalMountPoint function should mark the new root as a fileset root,
and setup the ".." pointers properly.
I'd like to cancel this bug, unless someone can replicate it in the
1.0.3 code base after doing the "cm checkft" command.

[8/17/93 public]
                                                                              
Don, I have run the same test scenario on RIOS/103 (dfs-103-3.25) with the 
test program you provided to try to reproduce this problem. I could not 
reproduce it. The cm restore works just fine. 
                                                                              
In fact, you mentioned in this report dated [bolinger 02/23/93 public] that 
you even could not reproduce it. I believe, as Mike indicated above also,  
that the problem no longer exists. Unless you have further suggestions, I 
would like to ask you to cancel it. 
Thanks, 
Tu
Changed Interest List CC from `jdp@transarc.com, cfe@transarc.com' to 
 `jdp@transarc.com, cfe@transarc.com, tu@transarc.com'

[8/18/93 public]
I've assigned this to Ron since Don no longer works on the DFS.
Changed Responsible Engr. from `kazar@transarc.com' to `rsarbo'

[8/18/93 public]
Cancelling after speaking with Don.



CR Number                     : 7187
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : utst_server core dumps on pmax sync site
Reported Date                 : 2/9/93
Found in Baseline             : 1.0.2b15
Found Date                    : 2/9/93
Severity                      : A
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : 

[2/9/93 public]
Unfortunately, I can't supply anymore details at this point. Hoping this
will jog someone's memory of an as-yet-unsubmitted fix. I came across
this because I am hoping to use the utst suite within the repfldb_checklist.

[9/29/93 public]
Gail, 
I took the liberty to cancel this one. I don't think we still support
the pmax platform any more. Please, let me know if you have any comment.
Thanks, 
tu
Changed Status from `open' to `cancel' 
Filled in Responsible Engr. with `tu@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `'



CR Number                     : 7180
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : quota lies about space available
Reported Date                 : 2/9/93
Found in Baseline             : 1.0.2
Found Date                    : 2/9/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[2/9/93 public]
In the self-host cell I created a big fileset to hold many-megabyte files
generated from dumping big filesets, in the course of debugging other
problems.  The fileset is ``user.cfe.dumptemp'', and it had a 200 MB quota.
I had created three files, each about 44.5 MB.  I was about to create a
fourth, and it might have gotten even bigger, so I deleted two of these big
files and started a process that created what woud have been the fourth file.
After writing about 31 MB, though, my writing process started to get EIO
back from write() system calls.  Here's some pertinent poking around:
saratoga ~/dumptemp % fts lsq .
Fileset Name          Quota    Used  % Used   Aggregate
user.cfe.dumptemp    200000   74536    37%    28% = 229772/819196 (LFS)
saratoga ~/dumptemp % fts aggrinfo saratoga
LFS aggregate epi0 (/dev/epi0): 469332 K free out of total 511992
LFS aggregate epi1 (/dev/epi1): 589424 K free out of total 819196
saratoga ~/dumptemp % fts lsft . -verbose
_____________________________________________
user.cfe.dumptemp 0,,609 RW LFS     states 0x14005 On-line
    saratoga.transarc.com, aggregate epi1 (ID 12)
    Parent 0,,0       Clone 0,,610     Backup 0,,611
    llBack 0,,0       llFwd 0,,0       Version 0,,66575
    1048576 K alloc limit;   74536 K alloc usage
     200000 K quota limit;   74536 K quota usage
    Reclaim wait: 0:00:00
    Maximum *node index: 3
    Creation Wed Feb  3 15:50:04 1993
    Last Update Tue Feb  9 19:17:47 1993
    Last Access Tue Feb  9 19:17:47 1993
user.cfe.dumptemp
        readWrite   ID 0,,609  valid
        readOnly    ID 0,,610  invalid
        backup      ID 0,,611  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
saratoga.transarc.c RW       epi1    0:00:00 hosts/saratoga <nil>
_____________________________________________
saratoga ~/dumptemp % ls -l
total 149056
-rw-r--r--   1 cfe      transarc 44540000 Feb 03 20:08 mason.dump
-rw-r--r--   1 cfe      transarc 31756288 Feb 09 18:00 tryagain.3
saratoga ~/dumptemp % cat tryagain.3 tryagain.3 > ta.5
cat: 0652-054 Cannot write to output.
There is not enough space in the file system.
cat: 0652-054 Cannot write to output.
There is not enough space in the file system.
saratoga ~/dumptemp % ls -l
total 149064
-rw-r--r--   1 cfe      transarc 44540000 Feb 03 20:08 mason.dump
-rw-r--r--   1 cfe      transarc   36864 Feb 09 19:19 ta.5
-rw-r--r--   1 cfe      transarc 31756288 Feb 09 18:00 tryagain.3
saratoga ~/dumptemp % fts lsq .
Fileset Name          Quota    Used  % Used   Aggregate
user.cfe.dumptemp    200000   74540    37%    28% = 229776/819196 (LFS)
saratoga ~/dumptemp %
Any clues?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/10/93 public]
I don't see any obvious problem.  It kinda seems like a quota
reservation problem.  One way to test this is to run "sync" a few times
to flush out VM and see if that helps.  Actually you may have to detach
the aggregate to really Unreserve all the vnodes.
It is interesting that this is just after deleting much space.  The
files aren't still open or else the aggreaget wouldn't show the space as
free.  Could it be that the freedblock code is tying down this many
blocks?  It should be able to hold on to more than 10*10 blocks/aggr.
Whoops, I just looked at the code.  It seems as it I was unable to
continue forcing the log to maintain this limit.  Note that when you get
those "increasing elbt_MaxTrans" messages it also means that the freed
block code will have to allocate records for each block freed in each of
those transactions.  I just ran (for instance) "test_anode
big_files.test" and it got to (elbt_MaxTrans == 1100) it also allocated
2225 freed block records (presumably about 90% for blocks and 10% for
transactions.  Thus there were around 2000 blocks tied up here.  I don't
know what the stats were for your system but this is a pretty-smoking
gun, I'd say.
Perhaps Bruce or Tony have some good ideas?
Filled in Interest List CC with `bwl, mason'

[6/7/93 public]
haven't seen any good ideas lately . . . and it _is_ unfriendly.
Elevated to B.

[9/14/93 public]
I have been looking for a reproducible case of this recently.  We are
about to cancel 3899 as fixed-by-accident.
Filled in Reported by Company's Ref. Number with `DB 3899'

[9/30/93 public]
This isn't reproducible.  Any further problems of this type will be
tracked under DB 3899 (which is still open at low priority).
Changed Status from `open' to `cancel'



CR Number                     : 7158
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfsexport -all -detach/-attach not clean
Reported Date                 : 2/8/93
Found in Baseline             : 1.0.2
Found Date                    : 2/8/93
Severity                      : C
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[2/8/93 public]
BUILD:	b14
CONFIG:	2 pmax cell
	1 = sec,cds,fl server and dfs client (dce5)
	1 = file server, sec, cds, dfs client (valentine)
DETAILS:
Able to dfsexport -all -detach, or so it seems (no feedback and 
subsequent dfsexport returns nothing either).
Remove dfsatab, doublecheck dfstab, dfsexport -all to reattach
and I appear to fail to attach /u2 - but I am able to create a
mount point and cd successfully to this empty fileset/aggregate.
Basically, it looks like dfsexport's output is faulty but it is
doing the job correctly.
root@valentine(120) dfsexport -all -detach -verbose
root@valentine(121) dfsexport
root@valentine(122) ls -l dfsatab
-rw-------   1 root     bin                   0 Feb  8 13:59 dfsatab
root@valentine(123) rm dfsatab
root@valentine(124) cat dfstab
# blkdev aggname aggtype aggid (fsid-for-UFS)
/dev/rz1b      /u2        ufs     3  0,,100
/dev/rz1e      /u4        ufs     4  0,,7
root@valentine(126) dfsexport -all
dfsexport: Failed to attach /dev/rz1b:/u2 (File exists).  Ignoring it.
root@valentine(127) dfsexport
dfsexport: /dev/rz1e, ufs, 4, 0,,7
root@valentine(113) fts crmount -dir /:/val_u2 -fileset val.u2
root@valentine(114) cd /:/val_u2
root@valentine(115) ls
root@valentine(116) fts lsfldb -server valentine
val.u2
        readWrite   ID 0,,100  valid
        readOnly    ID 0,,101  invalid
        backup      ID 0,,102  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
valentine.osf.org   RW       /u2     0:00:00 hosts/valentine<nil>

[2/9/93 public]
Changed Responsible Engr. from `pakhtar@transarc.com' to `jdp@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[2/19/93 public]
More minor problems with dfsexport
BUILD:	nb available 2/17/93
CONFIG: 3 pmax cell, 3 fl servers
TEST:	test/systest/admin/file/tests/admin_checklist (to be submitted
	next week)
Doublechecking instructions in the checklist, I found that dfsexport
"loses" the fileset id of a ufs aggregate if you:
fts crfldbentry ...
update /opt/dcelocal/var/dfs/dfstab 
rm /opt/dcelocal/var/dfs/dfsatab
dfsexport -aggregate /u2
dfsexport -aggregate /u2 -detach
fts delfldbentry ...
fts crfldbentry ...
update /opt/dcelocal/var/dfs/dfstab (with NEW fsid)
rm /opt/dcelocal/var/dfs/dfsatab
dfsexport -aggregate /u2
dfsexport now reports the fsid for /u2 as 0,,0 and NOT 0,,13.
Hmm - now a less than minor problem, operator error of removing the
dfsatab file and then trying :
root@valentine> dfsexport -all -detach -force
dfsexport: Revoking tokens for filesets on aggregate
system dropped into kdb with
kdb> $k
gimmeabreak()
Debugger()
simple_lock()
vrele()
dounmount()
unmount()
syscall()
VEC_syscall()

[7/14/93 public]
Gail, I would like to assign this to you also for now. Here are reasons:
1) We cannot reproduce it here and we have not seen this for a long time,
2) Jeff has also made several fixes in this area and may have already fixed it,
and 3) it is really a pmax platform specific ..
If you cannot reproduce it, would you like to cancel it ?
  
Thanks,
Filled in Interest List CC with `jdp@transarc.com' 
Changed Responsible Engr. from `jdp@transarc.com' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[7/30/93 public]
I am unable to reproduce this because I am blocked by ANOTHER dfsexport
problem, both on the rios and the pmax. I am reporting the new problem
and leaving this one to be reproduced/cancelled in 1.0.3.

[10/8/93 public]

Aged relative to Aug 8.

[12/17/93 public]

There are really two bugs reported in this CR, both involving cells
with only pmaxes.  The first [gmd 2/8/93] was a two pmax cell.  I have
been unable to reproduce this bug in a two HPUX cell.

The second bug [gmd 2/19/93] was reported for a three pmax cell with
3 flservers.  I have been unable to reproduce this in a 2 HPUX cell.

Since pmax'es are not a reference platform anyway, I am canceling this CR.



CR Number                     : 6957
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7879
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts lsquota against a JFS prints a non-pretty value
Reported Date                 : 1/21/93
Found in Baseline             : 1.0.2
Found Date                    : 1/21/93
Severity                      : D
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : open
Transarc Deltas               : 
Transarc Herder               : jaffe

[1/21/93 public]
If I do fts lsquota /.:/fs where root.dfs is a JFS, fts returns the following
error:
Error in lsquota: Cross-device link
Isn't it supposed to return the same information as a df against the local
mount point?
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `mason@transarc.com' 
Added field #Transarc Status with value `open'

[1/21/93 public]
Changed Responsible Engr. from `mason@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[1/22/93 public]
Well, it's supposed to return something comparable to the mount-point `df',
but not exactly the same: the aggregate information in ``fts lsquota'' is
supposed to be the same as the `df' information.
I haven't reconfigured my test cell to try to reproduce this, but I have an
exported JFS fileset named varda.tmp, and the command
	fts lsquota -fileset varda.tmp
is reporting what I'd expect.  Your ``Cross-device link'' error suggests that
you've re-configured this cell without flushing all caches.  Would you mind
trying:
	cm checkfilesets
	fts lsq /.:/fs
My expectation is that this will work, since my guess is that you've renamed
some JFS fileset to be root.dfs without telling the CM.  If it doesn't fix
the problem, please help diagnose it by getting the output of:
	fts lsfldb root.dfs
	cm whereis /.:/fs
	fts lsquota -fileset root.dfs
	fts lsft -fileset root.dfs
	fts lsft /.:/fs
Thanks!

[1/22/93 public]
I figured out what the problem was. In my dfstab file, the aggrid for root.dfs
was 2 but the FLDB entry had 3, thus the "Cross-device" error.  By syncing
those up, fts lsquota /.:/fs now works with a JFS.
This does bring up a couple questions, though.  How was I able to get to root.dfs
in the first place?  Does the server just go by the fileset number in the dfstab 
in the case of JFS and not the aggrid.
Also, is this proper output for lsquota:
# fts lsquota /.:/fs
Fileset Name          Quota    Used  % Used   Aggregate
root.dfs             4194303       0     0%     4% = 548/12288 (non-LFS)
I would think the value for Quota should be 0 since Used and % Used
are both displayed as 0.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[1/22/93 public]
Yup, the CM doesn't need the aggrid; all it wants is the fileset ID and the
server identity (address + principal).  Glad you were able to determine
what the test-case problem was.
The 4194303 value for the quota is artificial; it's set in ufsops/ufs_volops.c
to a fixed value.  This is a temp work-around for problems that Carl was
having with dumping JFS filesets and restoring them into Episode: UFS/JFS
was reporting zero values for ``fileset quota,'' but Episode was having trouble
restoring data into filesets for which the quota values had been set to zero.
Not that I know what the real solution is to that one, other than perhaps
implementing an ``unbounded'' quota limit whose representation is 0.
However, given that that's the only remaining problem, I'm changing this
OT report to an enhancement.
Changed Defect or Enhancement? from `def' to `enh' 
Filled in Interest List CC with `demail1!burnett, jdp@transarc.com' 
Changed Severity from `B' to `D' 
Changed Priority from `1' to `3'

[1/25/93 public]
Filled in Transarc Herder with `jaffe' 
Filled in Transarc Status with `open'
Changed Short Description from `fts lsquota against a JFS is broken' to `fts 
 lsquota against a JFS prints a non-pretty value'

[3/30/94 public]
This was fixed by Jeff Prem in connection with OT 3298 in March of 1993.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `3298'

[3/30/94 public]
Oops--OT 7879, not OT 3298.
I'd dup this to OT 7879, but I can't, so I'm cancelling this one.
Filled in Inter-dependent CRs with `7879' 
Changed Status from `dup' to `cancel' 
Changed Duplicate Of from `3298' to `'



CR Number                     : 6903
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Salvage Authorization Check Test does not check salvage success correctly.
Reported Date                 : 1/19/93
Found in Baseline             : 1.0.2b6
Found Date                    : 1/19/93
Severity                      : D
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[1/19/93 public]
The salvage authorization check test needs to consider that a
successful salvage command can have a non-zero return code 3.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/1/93 public]
Canceled because it is no longer true in the 1.0.2 code base.
Changed Status from `open' to `cancel'



CR Number                     : 6801
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : system call write failed for a large file.
Reported Date                 : 1/12/93
Found in Baseline             : 1.0.2
Found Date                    : 1/12/93
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[1/12/93 public]
Have two processes runing on same machine and let them writting 64k data
a time into a file interleavely and repeatly via DFS then check the contents 
and the file size. Both the content and the size will be wrong. This problem 
was found from dfs-102-2.2 by running my ITL scripts"read-write-dist.scr" that
can be found form "~jess/dfs/sb/src/test/file/os_calls/stress". It has no
problem to pass the test form glue layer.
To run the scripts: (telnet to moe)
  1. Have three windows (one HUB, two spokes).
  2. cd /:/test1/stress (for all windows).
  3. issur the command "./file_test -l /.:/dfs_test/a" from one of windows.
  4. issur the command "./file_test -c /.:/dfs_test/a" from the rest two.
  5. type "inc read-write-dist.scr" form the window for step 3.
  6. check the part "Test write consistency among spokes".
You will see the problem.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/8/93 public]
These semantics are beyond what POSIX requires for a local file system.  While
we might wish to enhance this functionality at some point, it is not a defect
in DFS.
Changed Status from `open' to `cancel'

[2/8/93 public]
After more discussion, this is being reopened.  POSIX requires strong length
setting semantics.  Not a P1, however.
Changed Priority from `1' to `2' 
Changed Status from `cancel' to `open'

[8/31/93 public]
Could not reproduce it myself in 103 code base. Jess mentioned that 
he could not reproduce it either. In fact, he just ran that specific itl script
yesterday in 103 code base on Solaris and passed the test.
This defect could be fixed as a result of other works in CM's dcache area. 
                                                                               
Mark it Cancelled
Changed Status from `open' to `cancel'



CR Number                     : 6772
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : incorrect aggregate size calculation in salvage
Reported Date                 : 1/8/93
Found in Baseline             : 1.0.2
Found Date                    : 1/8/93
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[1/8/93 public]
The edsk_CheckAsEpisode function, used by the salvage program to compute
and verify superblock parameters, incorrectly calculates the aggregate
size when it is a multiple of 64KB, or is up to 1KB smaller than a
multiple of 64KB.  This causes salvage to complain "Device too small
for aggregate" when the aggregate is salvaged.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/30/93 public]
A reexamination of this code with Blake shows no bug.  Even at the time
this defect was reported there was no common case where the salvager
complained about "Device too small".  Either the bug was fixed
"accidently" or the original report was in error.
Filled in Interest List CC with `ota' 
Changed Status from `open' to `cancel'



CR Number                     : 6494
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : clean up obsolete ifdefs
Reported Date                 : 12/16/92
Found in Baseline             : 1.0.2
Found Date                    : 12/16/92
Severity                      : E
Priority                      : 4
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : closed
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com

[12/16/92 public]
Clean up obsolete ifdefs in the DFS code base.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[5/20/94 public]
This is a reminder that has outlived its usefulness.
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 6375
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 6248
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : butc -adminlist option not documented
Reported Date                 : 12/8/92
Found in Baseline             : 1.0.2
Found Date                    : 12/8/92
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[12/7/92 public]
   The -adminlist option has recently been added to the butc code.  This
needs to be documented.  The SYNOPSIS section on the butc.8dfs manpage
should read:
	butc [-tcid <tc_number>] [-adminlist <filename>]
	     [-debuglevel <trace_level>] [-bakgroup <server_group>]
	     [-cell <cellname>] [-help]
   Note also that a reference to this admin.butc file
should be included in the bakserver.8dfs manpage.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/23/93 public]
I am lowering the priority of this defect because, while the interface change
made its way into the 1.0.2 code, the functionality provided by the change
did not.  It means nothing for 1.0.2 to be included in or excluded from the
admin.butc administrative list.  Knowing the priority of this defect was lower
than that of the others that remained, I chose to postpone its resolution
beyond 1.0.2.  It sounded like the right choice.
Changed Priority from `1' to `2'

[3/9/93 public]
Filled in Inter-dependent CRs with `6248'

[6/18/93 public]
The admin.butc admin list will never be functional.  No documentation for this
defect will ever be required.  This defect is being canceled.
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 6302
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 5329,6278
Project Name                  : dce
H/W Ref Platform              : other
S/W Ref Platform              : other
Component Name                : dfs
Subcomponent Name             : 
Short Description             : apparent violation of POSIX semantics
Reported Date                 : 12/2/92
Found in Baseline             : 1.0.2
Found Date                    : 12/2/92
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[12/2/92 public]

Using the 1.55 bits on HPUX, the following sequence run on a single-machine
DFS cell produces an error:

cd /:
cp 4MB 4MBcopy &
rm 4MB

The intention is that the 'rm' take place while 'cp' is still running.
If DFS is POSIX compliant, we should get a complete copy in '4MBcopy' 
and when 'cp' closes the '4MB' file, it will be actually deleted.
But on HPUX, we get a read error in mid-copy.

This OT may be related to 6278 and/or 5329, which is still listed as open.
I thought I'd log it separately in case it isn't.
If people cannot reproduce the error on a reference platform, we'll 
chase the bug here.

[12/14/92 public]
I can't duplicate this problem, and we've fixed plenty of problems in this area,
so I'm cancelling this bug.  Reopen it if it appears again.

Here's the log of two of these runs, using a 3 MB cache.  I did the operations twice,
once with a flushed file and once without the flush.  In both cases it worked:

# cat /unix /unix /unix >bigfile
ls -l bigfile
# -rw-r--r--   1 root     system   4402329 Dec 14 17:04 bigfile
# cm flush bigfile
# cp bigfile bigfile2&

 [1] 25623
# rm bigfile
# ls -l bigfile2
-rw-r--r--   1 root     system    794624 Dec 14 17:05 bigfile2
# !!
ls -l bigfile2
-rw-r--r--   1 root     system   1499136 Dec 14 17:05 bigfile2
# !!
ls -l bigfile2
-rw-r--r--   1 root     system   2555904 Dec 14 17:05 bigfile2
# !!
ls -l bigfile2
-rw-r--r--   1 root     system   3596288 Dec 14 17:06 bigfile2
# !!
ls -l bigfile2
-rw-r--r--   1 root     system   4382720 Dec 14 17:06 bigfile2
# !!
ls -l bigfile2
-rw-r--r--   1 root     system   4402329 Dec 14 17:06 bigfile2

 [1]  + Done                 cp bigfile bigfile2
#
#
# \

#
#
#
# mv bigfile2 bigfile
# ls -l bigfile*
-rw-r--r--   1 root     system   4402329 Dec 14 17:06 bigfile
# sync
cp bigfile big# file2&^R
cp bigfile bigfile2&

 [1] 25634
# rm bigfile
#
#
#
# ls -l bigfile*
-rw-r--r--   1 root     system    712704 Dec 14 17:07 bigfile2
#

 [1]    Done                 cp bigfile bigfile2
# ls -l bigfile*
-rw-r--r--   1 root     system   4402329 Dec 14 17:07 bigfile2
#
Changed Status from `open' to `cancel' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'



CR Number                     : 6277
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : os_calls
Short Description             : Add set of syscall scripst.
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : closed
Transarc Deltas               : jess-ot6277-add-syscall-tests-1
Transarc Herder               : jaffe@transarc.com

[12/1/92 public]
This delta is open for adding the syscall scripts written in ITL.
There will be 20 files to be added.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/1/92 public]
Filled in Interest List CC with `pakhtar@transarc.com'

[12/9/92 public]
moved to export to match the delta.  jess, please insert a defect closure form.
Filled in Transarc Deltas with `jess-ot6277-add-syscall-tests-1' 
Changed Transarc Status from `open' to `export'

[12/9/92 public]
The files checked in by this delta have heen tested on RIOS machine.
They have not fully tested on PMAX platform, but the delta ot6344
solves most of script problems concerning about the different semantics,
different limitations, and so on. So better take 6344 with this delta.

[12/14/92 public]
This delta was imported into TA dfs-102 2.3, but will be backed out by
hand.

[3/15/93 public]
This delta adds twenty ITL test scripts which have been tested on RIOS 
and PMAX platforms. 
As I know, script "dirops.scr" failed on PMAX/LFS (we have a bug opend in 
sybase). Script "truncat.scr" may fail on PMAX/LFS also (not very often).
It may cause some other problems when running them crossing machines or
in some stress way. The purpose to export the delta is to let some body
else to run the tests and to find problems (test or code).
The latest dfs release on which the ITL scripts have been running is 
dfs-102-2.7 backed by dce1.0b17.

[3/15/93 public]

[3/15/93 public]

[12/9/93 public]
This code was not submitted to the OSF.
Changed Status from `fix' to `cancel' 
Changed Fixed In Baseline from `1.0.3a' to `' 
Changed Transarc Status from `submit' to `closed'



CR Number                     : 6258
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : "addftentry" won't accept DCE pathname for -server option
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[12/1/92 public]
   "addftentry" only accepts short names instead of full DCE pathnames for
the -server option.  e.g. "foo" for "/.../<cellname>/hosts/foo
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/20/93 public]
ot6724 actually fixed, among other fixes, this particular problem reported
in this defect. The delata,  vijay-ot6724-bak-resolve-host-name-issues, has
been already integrated into the 1.0.2 tree around Jan, 93. 
Mark this cancelled. 
Changed Status from `open' to `cancel'



CR Number                     : 6249
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : "restoreft" doesn't recognize DCE pathname for server
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[12/1/92 public]
   "restoreft" recognizes 'short' names for the -server option, not DCE
pathnames.  e.g. It recognizes dewitt instead of
/.../dewitt.transarc.com/hosts/dewitt.  The procedure
bak/commands.c:bc_VolRestoreCMD calls bak/dsvs.c:bc_ParseHost which, in
turn, calls ???:gethostbyname and this procedure takes only short names.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/20/93 public]
As with ot6258, this defect was already fixed by ot6724, as part of other
fixes. The delta, vijay-ot6724-bak-resolve-host-name-issues, has been
already integrated into the 1.0.2 tree around Jan of 93. 
Mark it cancelled
Changed Status from `open' to `cancel'



CR Number                     : 5973
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : "df(1)" filesystem name for episode breaks whitespace usage norm.
Reported Date                 : 11/6/92
Found in Baseline             : 1.0.1b24
Found Date                    : 11/6/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[11/6/92 public]
The system is OSF1 and code base is dfs-102-1.53 and the kernel image
efsvmunix is used.  
The output of 'df' for mounted episode filesets is
not consistent with other filesystems in terms of its usage of
whitespace. This will break shell scripts that rely on
whitespace being the field delimiter for df output.
An example output of df is
root@galileo.transarc.com  # df
Filesystem    512-blks    used   avail capacity  Mounted on
/dev/rz0a        39486   35696    -160   100%    /
/dev/rz0g       242922  202162   16466    92%    /usr
/dev/rz1a        39026      16   35106     0%    /tmp
/dev/rz0b       118646     578  106202     1%    /paging
sybil:/afs     1393664       0 1393664     0%    /afs
DCE/LFS: rcx.1  524288     160  524128     0%    /tmp/rcx/epi0/rcx.1
        ^
        ^ breaks accepted norm of whitespace usage.
Also may be the aggregate name should also be reflected under the
filesystem column as is done on the AIX systems. ?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[11/9/92 public]
Changed short description.
Changed Short Description from `Output of 'df' for mounted episode filesets not 
 consistent with other mounted filesystems in terms of whitespace usage.' to 
 `"df(1)" filesystem name for episode breaks whitespace usage norm.'

[2/8/93 public]
Since this is only used in testing, I'm making this a test defect.
Changed CR in Code, Doc, or Test? from `code' to `test'

[8/25/93 public]
This is obviously a pmax-specific defect. I did repeat the same 
scenario on RIOS/103.2.28 by epimounting a local fileset to make sure 
that the output from the df comamnd does not have the same defect as mentioned
before. 
Cancelled. 
Filled in Interest List CC with `tu' 
Changed Status from `open' to `cancel' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com'



CR Number                     : 5816
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 5815
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : stub code
Short Description             : DFS stub code is bloated
Reported Date                 : 10/26/92
Found in Baseline             : 1.0.2
Found Date                    : 10/26/92
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : Various Makefile's and .acf files.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/26/92 public]

Please refer to OT5815 for a description of an IDL compiler
enhancement that can be used to help reduce DFS kernel stub sizes.

Once IDL is so enhanced, the DFS .acf files should be modified to
make more aggressive use of the IDL [out_of_line] attribute, and
the Makefiles should be modified to eliminate the use of 'caux'
files.  Also, the couple of 'xxx_s2c.h' header files can be defuncted.

This change cannot be implemented until the fix of OT5815 is submitted.

[2/9/93 public]
And it should stay marked as an enhancement at least until OT 5815 is
addressed.
Changed Defect or Enhancement? from `def' to `enh' 
Changed Fixed In Baseline from `1.0.2' to `' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/26/92 public]
I believe this enhancement was included in the 1.0.3 IDL compiler.
Assigned to Mike to verify since it's relevant to small resource
effort.

[4/29/94 public]
I'm canceling this bug since the problem has indeed been
fixed in the new IDL compiler.  (Note that the prerequesite
OT 5815 has also been cancelled.)



CR Number                     : 5762
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : should allow delete even if can't get tokens?
Reported Date                 : 10/22/92
Found in Baseline             : 1.0.2
Found Date                    : 10/22/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[10/22/92 public]
At the moment, when a fileset is marked ``inconsistent,'' you can't delete
it with ``fts delete'' since it will be unable to obtain a whole-fileset
token that will freeze out other users (data&status read&write).  You can
simply combine ``fts zap'' with ``fts delfldbentry'' to achieve a comparable
effect, but maybe it would be a fine idea to let ``fts delete'' work.
Possible methods would include letting the TKM grant the token in some
way, or letting ``fts delete'' ignore some class of failures in getting the
token.
There's a danger in allowing the TKM to grant the token anyway, in that it will
seek to revoke write-style tokens, and that will prompt CMs to store back
changes.  But CMs will be unable to do that, since the fileset is really
inconsistent.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/5/93 public]
I can no longer reproduce this problem
I believe that it was repaired, or worked around, in the fixes for OT 6180.
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 5722
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : px
Short Description             : scout (or fshost) doesn't update Ws field
Reported Date                 : 10/20/92
Found in Baseline             : 1.0.2
Found Date                    : 10/20/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[10/20/92 public]
The Ws field in the scout display is always 0.  This doesn't seem right...
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[10/4/93 public]
The original defect described above was fixed. That is, the scout does
display some valid number in Ws field. However, the real problem was
that Ws (the number of clients accessing this server) sometimes did not go
down if one client crashed and stoped accessing the server. 
This defect was actually coming from file server's fs host package.  The host
manager did not recycle the host data structure representing the down server 
even after a long period of time. The reason is that the reference count
with the host remains high all the time. Somehow, the host manager 
did not decrement the host in a rare race condition. This would cause
a small core leak in the server.                                           
Unfortunatelly, it is hard to reproduce the process to reproduce it is very
time consuming. I was only able to reproduce it twice and recently I could
not reproduce it anymore. 
I am cancelling it since the defect is not reproducible and insignificant. 
Changed Subcomponent Name from `scout' to `px' 
Changed Status from `open' to `cancel' 
Changed Responsible Engr. from `vijay@transarc.com' to `tu@transarc.com'



CR Number                     : 5630
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : panic: doing a dfsexport -all -detach
Reported Date                 : 10/14/92
Found in Baseline             : 1.0.2
Found Date                    : 10/14/92
Severity                      : B
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[10/14/92 public]
As I was trying to shut down my pmax secondary file server, I
successfully dce.clean'd all, and then I tried running dfsexport -all
-detach.  I got an error message:
dfsexport: Failed to detach /dev/epi0: luthien.epi0 (Device busy)
^C'd the dfsexport, tried again, got the same message, and then got
the kernel panic:
    $k
    thread 0x0c15121a4,  pcb 0x80496b10, W_EVENT 0
    <Exception frame> Cause= 0x30000008 Badvaddr= 0x0 SR= 0x0ff14
    epif_Open() from vnm_VInit+34
    vnm_VInit() from vnm_FindVnode+7b0
    vnm_FindVnode() from VFtoEV+154
    VFtoEV() from vol_efsVget+28
    vol_efsVget() from volreg_LookupExtended+770
    volreg_LookupExtended() from SAFS_StoreStatus+248
    SAFS_StoreStatus() from op7_ssr+508
    op7_ssr() from rpc__dg_execute_call+175c
    rpc__dg_execute_call() from cthread_call_executor+3ac
    cthread_call_executor() from base_routine+104
    base_routine() from find_thread_args+0b0
    $l
    task 0x0c1329d 0x0c1339eac  R    pri = 31, 0u 1674s 0c
    thread 0x0c1338fd4   W N vm_page_free_wanted
    thread 0x0c1338e58   W N reaper_queue
    thread 0x0c1338cdc   W N swapin_queue
    thread 0x0c1338b60   W N thread_swap_tick
    thread 0x0c13389e4   W N 0
    thread 0x0c1338868   W N psig_queue
    thread 0x0c13386ec   W N acctwatch
    thread 0x0c1337b0c   W N nfs_timer
    thread 0x0c1396bb4   W N 0c1354c60
    thread 0x0c13968bc   W N 0c1354c40
    thread 0x0c13965c4   W   8047b5d0
    thread 0x0c13962cc   W N delay_tbl+4
    thread 0x0c1512d84  R    pri = 6, 0u 811s 0c
    thread 0x0c1512c08   W N 0c14e820c
    thread 0x0c1512a8c   W N 0c14e8234
    thread 0x0c1512910   W N 0c14e825c
    thread 0x0c1512794   W N 0c14e8284
    thread 0x0c1512618   W N 0c14e82ac
    thread 0x0c151249c   W N 0c14e82d4
    thread 0x0c1512320   W N 0c14e82fc
    thread 0x0c15121a4  R    pri = 6, 0u 722s 0c
    thread 0x0c1512028   W N 0c14e33b4
    thread 0x0c1511eac   W   8047b5d0
    thread 0x0c15112cc   W N 0c150684c
    thread 0x0c1554910   W N 0c1354860
    thread 0x0c1554794   W   8047b5d0
    thread 0x0c1554618   W N 0c1354800
    thread 0x0c155449c   W N 0c13547e0
    thread 0x0c1554320   W N 0c13547c0
    thread 0x0c15541a4   W   8047b5d0
    thread 0x0c1511448   W N 0c1354700
    thread 0x0c1575618   W N 0c1354600
    thread 0x0c1575794   W N 0c15c94c0
    task 0x0c1329b60 [init                ] proc 80647768 pid 1 map 0x0c003c900: thread 0x0c1339d30   W   U_ADDRESS pause
    task 0x0c1329a54 [vnode_pager         ] proc 80647850 pid 2 map 0x0c003c7e0: 6 threads:
    thread 0x0c1339bb4   W   0
    thread 0x0c1339740   W   0
    thread 0x0c13395c4   W   0
    thread 0x0c1339448   W   0
    thread 0x0c13392cc   W   0
    thread 0x0c1339150   W N swbuf pgout_done
    task 0x0c1329948 [device pager        ] proc 80647938 pid 3 map 0x0c003c6c0: thread 0x0c1339a38   W   0
    task 0x0c132983c [exception hdlr      ] proc 80647a20 pid 4 map 0x0c003c630: thread 0x0c13398bc   W  (swapped) 0
    task 0x0c1329730 [mach_init           ] proc 80647b08 pid 5 map 0x0c003c5a0: thread 0x0c133ad84   W  (swapped) 0
    task 0x0c1329518 [kloadsrv            ] proc 80647cd8 pid 15 map 0x0c003c480: thread 0x0c13383f4   W  (swapped) 0c1358e40 network
    task 0x0c1329624 [update              ] proc 80647bf0 pid 16 map 0x0c003c510: thread 0x0c1338570   W   U_ADDRESS pause
    task 0x0c13290e8 [syslogd             ] proc 80648078 pid 55 map 0x0c003c1b0: thread 0x0c1337e04   W   0c1337ea4 event
    task 0x0c1328fdc [named               ] proc 80648160 pid 59 map 0x0c003c120: thread 0x0c1337c88   W  (swapped) 0c137ad40 network
    task 0x0c1328ed0 [portmap             ] proc 80648248 pid 81 map 0x0c003c000: thread 0x0c1337990   W  (swapped) 0c1337a30 event
    task 0x0c1328dc4 [mountd              ] proc 80648330 pid 83 map 0x0c1385f30: thread 0x0c1337814   W  (swapped) 0c13378b4 event
    task 0x0c1328cb8 [nfsd                ] proc 80648418 pid 85 map 0x0c1385e10: thread 0x0c1337698   W   0c1369640 network
    task 0x0c1328bac [nfsd                ] proc 80648500 pid 87 map 0x0c1385cf0: thread 0x0c133751c   W   0c1369640 network
    task 0x0c1328aa0 [nfsd                ] proc 806485e8 pid 88 map 0x0c1385c60: thread 0x0c13373a0   W   0c1369640 network
task 0x0c1328994 [nfsd                ] proc 806486d0 pid 89 map 0x0c1385bd0: thread 0x0c1337224   W   0c1369640 network
    task 0x0c1328888 [nfsiod              ] proc 806487b8 pid 90 map 0x0c1385b40: thread 0x0c13370a8   W   asyncdaemon_requests
    task 0x0c1329300 [nfsiod              ] proc 806488a0 pid 91 map 0x0c003c240: thread 0x0c13380fc   W   asyncdaemon_requests
    task 0x0c132877c [nfsiod              ] proc 80648988 pid 92 map 0x0c1385a20: thread 0x0c1397d84   W   asyncdaemon_requests
    task 0x0c1328670 [nfsiod              ] proc 80647ea8 pid 93 map 0x0c1385990: thread 0x0c1397c08   W   asyncdaemon_requests
    task 0x0c1328458 [ntpd                ] proc 80648b58 pid 97 map 0x0c1385870: thread 0x0c1397910   W   0c13979b0 event
    task 0x0c132834c [inetd               ] proc 80648c40 pid 103 map 0x0c13857e0: thread 0x0c1397794   W  (swapped) 0c1397834 event
    task 0x0c1328240 [cron                ] proc 80648d28 pid 108 map 0x0c13856c0: thread 0x0c1397618   W   0c13a3f40 network
    task 0x0c1328134 [lpd                 ] proc 80648a70 pid 120 map 0x0c13855a0: thread 0x0c139749c   W  (swapped) 0c139753c event
    task 0x0c132940c [getty               ] proc 80647dc0 pid 124 map 0x0c003c2d0: thread 0x0c1338278   W  (swapped) cons ttyin
    task 0x0c1328028 [getty               ] proc 80648ef8 pid 125 map 0x0c1385480: thread 0x0c1397320   W  (swapped) cons+140 ttyin
    task 0x0c13291f4 [telnetd             ] proc 80647f90 pid 126 map 0x0c003c360: thread 0x0c1337f80   W   0c1338020 event
    task 0x0c1328564 [csh                 ] proc 80648e10 pid 127 map 0x0c1385900: thread 0x0c1397a8c   W   U_ADDRESS pause
    task 0x0c13a9a54 [dfsbind             ] proc 80649550 pid 183 map 0x0c13851b0: thread 0x0c1396448   W  (swapped) 8047d7ec intr_wait
    task 0x0c13a983c [fxd                 ] proc 806491b0 pid 186 map 0x0c13acc60: thread 0x0c1396150   W N shutdown_cond
    task 0x0c13a9624 [fxd                 ] proc 80649720 pid 187 map 0x0c13aca20: thread 0x0c1511d30   W   8047b5d0
    task 0x0c13a9518 [bosserver           ] proc 80649808 pid 190 map 0x0c13ac990: thread 0x0c1511bb4  R    pri = 15, 334786u 6018s 441598c
    task 0x0c13a940c [bosserver           ] proc 80649ba8 pid 198 map 0x0c13ac870: thread 0x0c1511a38   W   0c1511ad8 event
    task 0x0c154ec6c [dfsd                ] proc 8064a118 pid 209 map 0x0c13ac480: thread 0x0c1511150   W N cm_shuttingDown
    task 0x0c154ed78 [dfsd                ] proc 8064a200 pid 210 map 0x0c13ac360: thread 0x0c1554d84   W N cm_bkgDaemons
    task 0x0c154eb60 [dfsd                ] proc 8064a2e8 pid 211 map 0x0c13ac2d0: thread 0x0c1554c08   W N cm_bkgDaemons
    task 0x0c13a9d78 [dfsexport           ] proc 80649f48 pid 2980 map 0x0c13acf30:
    thread 0x0c1397028  R    pri = 23, 0u 9s 874248c
    task 0x0c13a90e8 [who                 ] proc 80649c90 pid 2981 map 0x0c13ac7e0:
    thread 0x0c15115c4  R  N pri = 12, 0u 0s 0c
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mcinerny@transarc.com' 
Added field Transarc Status with value `open'

[10/14/92 public]
Changed Subcomponent Name from `episode' to `lfs'

[7/21/93 public]
No longer reproduciable. 
I believe this defect has been fixed, along with OT#7762 and OT#7158, 
by Jeff's recent (April/May) work in dfsexport area.
Mark it cancelled.
Changed Status from `open' to `cancel' 
Changed Transarc Herder from `mcinerny@transarc.com' to `jaffe@transarc.com'



CR Number                     : 5604
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : CM gives EACCES incorrectly.
Reported Date                 : 10/9/92
Found in Baseline             : 1.0.2
Found Date                    : 10/9/92
Severity                      : C
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[10/9/92 public]
Scenario: two machine cell; machine "A" is the primary DCE & DFS server,
machine "B" is a DCE client and secondary DFS server (no flserver).  The
primary crashed (for an unrelated reason, I've not yet opened up that OT).
The reboot of the primary DCE server required that I restart the security
daemon on the secondary (DCE client).
An attempt to look at a particular directory returned EPERM:
% pwd
pwd: The file access permissions do not allow the specified action.
% cd ..
% ls
bar  bin  dfs
% pwd
/.../mandos.dce.transarc.com/fs/usr/mason
I have full rights on the directory:
% acl_edit dfs
sec_acl_edit> l
# SEC_ACL for dfs:
# Default cell = /.../mandos.dce.transarc.com
user_obj:rwxc--
group_obj:rwx---
other_obj:rwx---
sec_acl_edit> ^D
Interestingly, though, this operation and/or "cm flush" did the trick:
% cm flush
% cd dfs
% ls
export    link      obj       rc_files  src       tools
%
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/12/92 public]
The icldump is in:
/afs/transarc.com/usr/mason/debug-info/icldump/ot5604-olorin-09oct92-eperm.Z

[10/14/92 public]
Changed Interest List CC from 
 `kazar,bb+transarc.afs.dce.self-host@transarc.com' to 
 `kazar,bb+transarc.afs.dce.self-host@transarc.com,demail1!carl'

[3/11/93 public]
All this time I thought the problem was an errno of 1 (EPERM), which would be
quite weird.  Instead, it's just a vanilla EACCES problem that needs to be
re-tested on more recent software.
Changed Short Description from `CM gives EPERM incorrectly.' to `CM gives 
 EACCES incorrectly.'

[9/14/93 public]
I have tried several times before and could not reproduce it any more. 
I suspect this could be fixed as part of acl_edit work. 
Cancel it.
Changed Status from `open' to `cancel'



CR Number                     : 5269
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 5259,5260,5261,5263,5264,5266,5267
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need branch coverage for Episode tools pkg.
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
After higher-level tests have completed their branch-coverage work,
additional work will be required to complete branch coverage work for
the tools directory in Episode (as opposed to the utilities).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/7/94 public]
Canceled.  It was considered inappropriate that this work-list item was
registered as a defect.
Changed Status from `open' to `cancel'



CR Number                     : 5266
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 5259,5260,5261,5263,5264
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need branch coverage numbers for logbuf
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
After higher-level branch coverage testing is complete, we must devise
tests to cover additional branches.  In this area, I expect the
coverage to principally be in recovery, so it is possible the recovery
testing will cover this already.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
Changed CR in Code, Doc, or Test? from `code' to `test'

[10/18/93 public]
This is just a work item that should never have been entered into OT
Changed Defect or Enhancement? from `def' to `enh' 
Changed Status from `open' to `cancel'



CR Number                     : 5264
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 5259,5260,5261,5263
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need branch coverage results for anode layer
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
Need to obtain branch coverage number unobtainable from previous work
(see the inter-dependent CRs field).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
Changed CR in Code, Doc, or Test? from `code' to `test'

[3/7/94 public]
Canceled.  It was considered inappropriate that this work-list item was
registered as a defect.
Changed Status from `open' to `cancel'



CR Number                     : 5261
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 5259,5260
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need branch coverage for dir package
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
After previous branch coverage efforts have exhausted reasonable
coverage, we will need to build additional tests for branch coverage
of the dir package.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
Changed CR in Code, Doc, or Test? from `code' to `test'

[3/7/94 public]
Canceled.  It was considered inappropriate that this work-list item was
registered as a defect.
Changed Status from `open' to `cancel'



CR Number                     : 5260
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 5259
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need branch coverage results for eacl package.
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
Need branch coverage #'s for eacl package.  If sufficient results
cannot be obtained via OT 5259 testing (vnode layer) we need to build
additional tests (possibly using test_acl?)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
Changed CR in Code, Doc, or Test? from `code' to `test'

[3/7/94 public]
Canceled.  It was considered inappropriate that this work-list item was
registered as a defect.
Changed Status from `open' to `cancel'



CR Number                     : 5259
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need branch coverage for vnode layer operations.
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
Need tests to achieve branch coverage goals beginning with the vnode
layer (should measure branch coverage of all lower layers including
vnode layer).  Tests at this layer are user space tests on a kernel,
and script language tests such as test_vnodeops.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
Log this against test, not code.
Changed CR in Code, Doc, or Test? from `code' to `test'

[3/7/94 public]
Canceled.  It was considered inappropriate that this work-list item was
registered as a defect.
Changed Status from `open' to `cancel'



CR Number                     : 5257
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need better salvager tests
Reported Date                 : 8/31/92
Found in Baseline             : 1.0.2
Found Date                    : 8/31/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/31/92 public]
We need better test facilities for the salvager, as described in
Rajesh's Recovery/Salvager testing proposal.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
This is an enhancement, not a defect.
Changed Defect or Enhancement? from `def' to `enh'

[11/11/93 public]
This defect is overly broad.
Changed Status from `open' to `cancel'



CR Number                     : 5198
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : scavenger errors require investigation and repair
Reported Date                 : 8/25/92
Found in Baseline             : 1.0.2
Found Date                    : 8/25/92
Severity                      : C
Priority                      : 4
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/25/92 public]
The scavenger tests need to be re-run and the failure conditions analyzed and
repaired.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/27/92 public]
Reassigned to Blake.
Changed Interest List CC from `jdp@transarc.com' to `jdp,ota' 
Changed Responsible Engr. from `ota@transarc.com' to `blake@transarc.com'

[7/21/93 public]
After talking to Blake and realized that this test program is no longer
useful. There are some other test programs that also cover this area. 
So cancelled. 
Changed Status from `open' to `cancel'



CR Number                     : 5114
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts lsquota makes a fileset busy
Reported Date                 : 8/18/92
Found in Baseline             : 1.0.1
Found Date                    : 8/18/92
Severity                      : E
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jafe@transarc.com
Transarc Status               : open

[8/18/92 public]

Currently the fts lsquota command will cause a fileset to be considered
busy if accessed concurrently by other processes.  In discussing this with
Craig he said this does not need to be.  The current behaviour is only
annoying, and is not causing any failures.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[11/11/92 public]
In creating OT 6025, Elliot correctly wondered why the CM's error message
looks like
	dfs: fileset (id = XX) is busy with error NNNNNNNNNN
That is, why the CM's console message is confused about the state of
the fileset--in one part of the message it's just busy, and in the other
part of the message it has an ``error'' code.

[11/11/92 public]

The CM output has an error in it. From Elliot's message in OT 6025

	dfs: fileset (id = 1) is busy with error 691089516
	dfs: waiting (691089516) for busy fileset 0,,0

the busy fileset's id should be 0,,1 and not 0,,0.

[11/18/92 public]
Filled in Interest List CC with `dawn%liz.austin.ibm.com@transarc.com'

[1/20/93 public]
I fixed the ``0,,0'' and problem a while ago.
Mike Kazar has already fixed the fundamental problem in 6180, so I'll let him
cancel this when he's submitted 6180.
Changed Interest List CC from `dawn%liz.austin.ibm.com@transarc.com' to 
 `dawn%liz.austin.ibm.com@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `kazar@transarc.com' 
Changed Transarc Herder from `mason@transarc.com' to `jafe@transarc.com'

[3/21/94 public]
Assigned to Diane to verify.

[3/22/94 public]

This no longer appears to be a problem.



CR Number                     : 4623
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : AFS_VFSCACHE feature ifdef has platform dependencies
Reported Date                 : 7/7/92
Found in Baseline             : 1.0.1
Found Date                    : 7/7/92
Severity                      : B
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[7/7/92 public]
Recently while adding support for the AFS_VFSCACHE feature macro
on the AIX platform, I found several places where the code was
platform specific.  While adding the support for AIX I tried to
implement it using newly added support in the osi layer to make
it more portable at the higher layers of the system.  Since I
do not have the resources to make and test these changes on the
OSF1 platform I wrapped the AIX code in platform ifdefs and did
not attempt to change the OSF1 code.  I would like for the OSF
to make these changes for OSF1 by adding the neccessary support
in the osi layer and then using the code that is currently within
the AIX platform ifdefs, and eliminate the else portions of these
ifdefs which are the OSF1 platform specific code.
I added the following support to the osi layer:
osi_vptofh() 
    an abstraction for VFS_VPTOFH which does not exist on all platforms
osi_getattr()
    replacement for VOP_GETATTR which assumes a vnode op macro convention
osi_getfh()
    getfh() interface for AIX.  For OSF this whould just be a define to getfh()
osi_fhandle_t
    fhandle structure which is not the same on all platforms.  I only used
    it where I had to in the code, but it should probably be changed
    in all the occurrances of fhandle_t.
osi_vnodecovered()
    replacement for platform specific code which looks at vfs_mntdover
    field of the vnodes vfsp (mount) memter.
I changed the following DFS files for the VFSCACHE support for AIX:
    Editing afsd/afsd.c 4.16 
    Editing cm/cm_init.c 4.27 
    Editing cm/cm_vnodeops.c 4.175 
    Editing config/RIOS/param.h 4.2 
    Editing libafs/RIOS/dfscore.exp 4.18 
    Editing osi/RIOS/Makefile 4.3 
    Editing osi/RIOS/osi_fio.c 1.3 
    Editing osi/RIOS/osi_port_aix.c 4.2 
    Editing osi/RIOS/osi_port_aix.h 4.5 
    Editing osi/osi_printf.c 4.8 
    Editing osi/osi_ufs.c 4.3 
    Adding osi/RIOS/osi_syscall.exp 
I have dropped these changes into the Transarc tree.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[7/7/92 public]
Changed H/W Ref Platform from `rs6000' to `all' 
Changed S/W Ref Platform from `aix' to `all'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[7/29/92 public]

[7/29/92 public]
According to Carl's note this is a work item for OSF.  I'm assigning it to
treff so he can decide who best to assign it to at OSF.  If it shouldn't be
assigned to OSF, we should decide to whom it SHOULD be assigned.
Changed Responsible Engr. from `mason@transarc.com' to `treff' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com'

[07/29/92 public]
Alright, already.  We'll tackle this during 1.0.2 Deluge Week.

[7/20/93 public]
Dave, 
This defect somehow is assigned to Transarc even the resp. eng is you. 
Changed to OSF. Note that I am not sure that this defect is still applicable 
any more. 
Filled in Subcomponent Name with `dfs' 
Changed Interest List CC from `kazar@transarc.com,treff@osf.org,toml@osf.org' 
 to `kazar@transarc.com,treff@osf.org, tu' 
Changed Resp. Engr's Company from `tarc' to `osf'

[10/8/93 public]

Aged relative to Aug 1.

[10/14/93 public]
This has been deemed a code clean up item for 1.1 (if it still applies
after the OSI changes expected from xarc.)

[7/7/92 public]
Looks like this has been cleaned up already.  Cancelled.



CR Number                     : 4283
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : newaggr links against xaggr code
Reported Date                 : 6/16/92
Found in Baseline             : 1.0.1b18
Found Date                    : 6/16/92
Severity                      : E
Priority                      : 4
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : open
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com

[6/16/92 public]
The newaggr program calls as_init in the xaggr layer, seemingly without any
purpose.  This bug is a reminder we need to look at this and figure out if
there is still good reason for this.  If not, we shouldn't do it.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/16/92 public]
The purpose of calling as_init is to initialize some data structures used in
the dfstab library.  The dfstab library is used to look up aggregate names in
the dfstab file.  The capability of looking up aggregate names in the dfstab
file was added recently (October 1991).  I recommend that this bug be
cancelled.

[6/16/92 public]
When I talked with Ted he wanted this code to either not link against
xaggr code or be moved out of anode.   I'll leave it to him to decide now
if it should still be moved out of anode.

[7/8/92 public]
Changed Defect or Enhancement? from `def' to `enh'

[1/24/94 public]
Changed Subcomponent Name from `lfs' to `EPISODE'

[1/25/94 public]
Canceled because newaggr can depend on xaggr which is in the dfscore.
Also we expect newaggr to eventually use the ftu_ API to lookup
aggregate names so it will do the dfstab lookup on our behalf [TR Defect
3128].
Changed Status from `open' to `cancel'



CR Number                     : 4193
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : can't NFS mount DFS
Reported Date                 : 6/10/92
Found in Baseline             : 1.0.1b18
Found Date                    : 6/10/92
Severity                      : C
Priority                      : 4
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : open
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com

[6/10/92 public]
Tried to NFS mount /... from a DFS client (actually it was a single 
machine client/server).  mount returned EACCES.  We did trip
over a breakpoint that was set on the mount() system call on the 
client which leads me to believe the mount daemon responded to
our mount request with a file handle.

I see that AFS file ids are 6 longs, while the OSF1 notion
of a file id is at most 5 longs.  I need to look at this more,
but one suspicion I have is that the last long of the file id 
may be getting dropped somewhere.

[6/15/92 public]
More info on the same bug.  It appears the command to enable the 
translator capability is failing on the DFS client.  

"cm exp nfs on" returns the message "The nfs-exporter type is currently 
not supported on this DFS client."  If this feature is not supported
for 1.0.1, it should be documented in the release notes.

[6/16/92 public]
I will look at this, but I need some background.  Can anyone at Transarc,
or IBM fill in the blanks?  Has this ever worked before on PMAX, RIOS?  
For read-only, for read-write? Is there a commitment to support this for 
1.0.1?  Am I doing all the right things to enable this capability?

Is there any documentation on this topic beyond the cm exportdfs man page?  
I haven't found any.  Also, any architectural notes would be useful.  I 
assume the NFS server goes through the glue code on the DFS client.  It gets 
uid and gid information from the NFS client at the RPC layer.  How does 
this translate into DCE security credentials?  What about foreign users
etc.  Any and all information appreciated.

[6/16/92 public]
Filled in Responsible Engr. with `pakhtar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/18/92 public]
I do not believe that this (NFS/DFS translator) functionality is supported, 
or planned to be supported in the near future. I doubt that anyone has
tried to exercise it. It is not expected to work in its present form.

[7/16/92 public]
This isn't planned for any time in the forseeable future. This OT can be
re-opened when or if such an enhancement is ever scheduled.

Changed Defect or Enhancement? from `def' to `enh' 
Changed Priority from `3' to `4' 
Changed Status from `open' to `defer'

[3/21/94 public]
Last time I checked, this worked on both ref platforms.  Cancelled.



CR Number                     : 4185
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : do_GetBind returns reply code " ept_s_not_registered"
Reported Date                 : 6/10/92
Found in Baseline             : 1.0.1b18
Found Date                    : 6/10/92
Severity                      : C
Priority                      : 1
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[6/10/92 public]

This is on a 1 rios machine [server cum client] cell configured from
the local dfs-101-1.21 build backed by osf build 18. I rebooted the
machine, logged in as rootl and started DCE and DFS by the rc scripts.

 [ dfsbind was started with the debug -d flag]. Then used a local klog
program to authenticate to DCE as cell_admin. Then ran fts tests on
exported UFS fileset. The tests seemed to hang. The messages on the
screen at this point was

do_GetBind (/.../dewitt.com): reply code 382312662
dfsbind: failed to send response back to kernel: (code = -1, errno = 2).
do_GetBind (/.../dewitt.com): reply code 382312662
ls: 0653-341 The file /:/BR.18765.7 does not exist.
do_GetBind (/.../dewitt.com): reply code 382312662

The reply code maps to ept_s_not_registered in usr/include/dce/rpcsts.h.

Vijay used cdscp to find the entry for /.../dewitt.com. The output from cds
is shown below:

 [(rootl)fts] /opt/dcelocal/bin/cdscp
cdscp> show dir /.../dewitt.com

                        SHOW
                   DIRECTORY   /.../dewitt.com
                          AT   1992-06-10-16:03:14
Error on entity: /.../dewitt.com
Error with socket (dce / cds)
Function: dnsEnumAttr

The screen output of the fts tests upto hang point were:

 [The interesting info is right at the end]

fts/runtests running on dewitt at Wed Jun 10 08:49:19 EDT 1992
   in cell dewitt.
Servers are dewitt and dewitt.
1 processes; 1 times; parent process number 18225.
>>> Skipping test12 (replication) (need 2 servers) <<<

Starting iteration number 1

Iteration 1 starting process number 1 at Wed Jun 10 08:49:19 EDT 1992.

Listing /:/...
ls: 0653-341 The file /:/BB.18880.1 does not exist.
ls: 0653-341 The file /:/BB.18880.2 does not exist.
ls: 0653-341 The file /:/BB.18880.3 does not exist.
ls: 0653-341 The file /:/BB.18880.4 does not exist.
ls: 0653-341 The file /:/BB.18880.5 does not exist.

ls: 0653-341 The file /:/BB.18880.6 does not exist.
ls: 0653-341 The file /:/BB.18880.7 does not exist.
ls: 0653-341 The file /:/BB.18880.8 does not exist.
ls: 0653-341 The file /:/BB.18880.9 does not exist.
ls: 0653-341 The file /:/BB.18880.10 does not exist.
ls: 0653-341 The file /:/BB.18880.11 does not exist.
ls: 0653-341 The file /:/BB.18880.12 does not exist.
ls: 0653-341 The file /:/BB.18880.13 does not exist.
ls: 0653-341 The file /:/BB.18880.14 does not exist.
ls: 0653-341 The file /:/BR.18765.7 does not exist.
ls: 0653-341 The file /:/BR.18765.8 does not exist.
ls: 0653-341 The file /:/BR.18765.A does not exist.
total 102
Frw-------   1 100      12             0 Jun 09 16:08 a
Drwxrwxr-x   6 root     adm          512 Feb 19 11:40 adm
drwxrwxrwx   7 root     system      2048 Mar 12 11:19 afsws
Drwxr-xr-x   2 bin      bin         4608 Jun 09 11:41 bin
Frwxr-xr-x   1 root     transarc   57501 May 12 16:33 dce_klog
Drwxr-xr-x   2 bin      bin          512 Jan 21 17:31 dict
Drwxr-xr-x   3 root     system      1024 Jan 21 18:12 etc
Dr--r--r--   2 root     transarc     512 Jun 09 13:17 fred
Frw-------   1 100      12             0 Jun 09 13:17 g
Drwxr-xr-x   2 100      usr          512 Jan 21 16:44 guest
Frw-------   1 4294967294 4294967294       0 Jun 09 13:56 h
Drwxr-xr-x  17 bin      bin         2560 Jan 21 18:17 include
Drwxr-xr-x  15 bin      bin         2048 Jun 09 11:28 lib
Lrwxrwxrwx   1 root     transarc      32 Jun 03 10:41 local -> /afs/transarc.com/@sys/usr/local
Drwx------   2 root     system       512 Jan 21 16:34 lost+found
Drwxrwxr-x   5 root     printq       512 Jan 29 08:30 lpd
Drwxr-xr-x  28 bin      bin          512 Jan 21 18:25 lpp
drwxrwxr-x   2 bin      mail         512 Mar 04 03:34 mail
Drwxr-xr-x  22 bin      bin          512 Jan 21 16:45 man
Drwxrwxrwt   2 bin      bin          512 Jan 21 16:45 msgs
Drwxrwxrwt   2 bin      bin          512 Jan 21 16:45 news
Drwxrwxrwt   2 bin      bin          512 Jan 21 16:45 preserve
Drwxr-xr-x   2 bin      bin          512 Jan 21 16:39 pub
Drwxrwxr-x  10 bin      bin          512 Jan 21 17:36 spool
Drwxr-xr-x   2 root     system       512 Jan 21 18:27 sys
Drwxrwxrwt   2 bin      bin          512 Jun 10 08:37 tmp
Frw-r--r--   1 root     transarc    5219 Apr 23 16:08 typescript
Drwxrwxr-x   2 bin      bin         1536 Jan 21 18:13 ucb
Drwxrwxr-x   2 bin      bin          512 Jan 21 17:49 usg
Drwxr-xr-x   6 100      12           512 Jun 10 08:47 usr
Drwxr-xr-x   4 root     system       512 Jan 21 17:21 vice

Iteration 1 process 1 running test2.
No active transactions on /.:/hosts/dewitt
There is 1 aggregate on the server /.:/hosts/dewitt (dewitt.transarc.com):
                 /usr (/dev/hd2): id=3     (non-LFS)
Non-LFS aggregate /usr (/dev/hd2): 16088 K free out of total 65536
Total filesets on server /.:/hosts/dewitt aggregate /usr (id 3): 1
/dev/hd2 0,,1 RW non-LFS      0 K  states 0x15085 accStatus 0x4  On-line
On-line
    dewitt.transarc.com, aggregate /usr (ID 3)
    Parent 0,,0       Clone 0,,0       Backup 0,,0
    llBack 0,,0       llFwd 0,,0       Version 0,,0      
    MaxQuota       0 K
    Creation Tue Jun  9 18:36:07 1992

Total filesets on-line 1; total off-line 0; total busy 0

Total number of filesets on server /.:/hosts/dewitt: 1 

root.dfs  
	readWrite   ID 0,,1  valid
	readOnly    ID 0,,2  invalid
	backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt   <nil>          <nil>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)

Done at Wed Jun 10 09:02:41 EDT 1992.

test2 PASSED

Iteration 1 process 1 test2 returned 0.

Listing /:/...
ls: 0653-341 The file /:/BB.18880.1 does not exist.
ls: 0653-341 The file /:/BB.18880.2 does not exist.
ls: 0653-341 The file /:/BB.18880.3 does not exist.
ls: 0653-341 The file /:/BB.18880.4 does not exist.
ls: 0653-341 The file /:/BB.18880.5 does not exist.
ls: 0653-341 The file /:/BB.18880.6 does not exist.
ls: 0653-341 The file /:/BB.18880.7 does not exist.
ls: 0653-341 The file /:/BB.18880.8 does not exist.
ls: 0653-341 The file /:/BB.18880.9 does not exist.
ls: 0653-341 The file /:/BB.18880.10 does not exist.
ls: 0653-341 The file /:/BB.18880.11 does not exist.
ls: 0653-341 The file /:/BB.18880.12 does not exist.
ls: 0653-341 The file /:/BB.18880.13 does not exist.
ls: 0653-341 The file /:/BB.18880.14 does not exist.
ls: 0653-341 The file /:/BR.18765.7 does not exist.
ls: 0653-341 The file /:/BR.18765.8 does not exist.
ls: 0653-341 The file /:/BR.18765.A does not exist.
total 102
Frw-------   1 100      12             0 Jun 09 16:08 a
Drwxrwxr-x   6 root     adm          512 Feb 19 11:40 adm
drwxrwxrwx   7 root     system      2048 Mar 12 11:19 afsws
Drwxr-xr-x   2 bin      bin         4608 Jun 09 11:41 bin
Frwxr-xr-x   1 root     transarc   57501 May 12 16:33 dce_klog
Drwxr-xr-x   2 bin      bin          512 Jan 21 17:31 dict
Drwxr-xr-x   3 root     system      1024 Jan 21 18:12 etc
Dr--r--r--   2 root     transarc     512 Jun 09 13:17 fred
Frw-------   1 100      12             0 Jun 09 13:17 g
Drwxr-xr-x   2 100      usr          512 Jan 21 16:44 guest
Frw-------   1 4294967294 4294967294       0 Jun 09 13:56 h
Drwxr-xr-x  17 bin      bin         2560 Jan 21 18:17 include
Drwxr-xr-x  15 bin      bin         2048 Jun 09 11:28 lib
Lrwxrwxrwx   1 root     transarc      32 Jun 03 10:41 local -> /afs/transarc.com/@sys/usr/local
Drwx------   2 root     system       512 Jan 21 16:34 lost+found
Drwxrwxr-x   5 root     printq       512 Jan 29 08:30 lpd
Drwxr-xr-x  28 bin      bin          512 Jan 21 18:25 lpp
drwxrwxr-x   2 bin      mail         512 Mar 04 03:34 mail
Drwxr-xr-x  22 bin      bin          512 Jan 21 16:45 man
Drwxrwxrwt   2 bin      bin          512 Jan 21 16:45 msgs
Drwxrwxrwt   2 bin      bin          512 Jan 21 16:45 news
Drwxrwxrwt   2 bin      bin          512 Jan 21 16:45 preserve
Drwxr-xr-x   2 bin      bin          512 Jan 21 16:39 pub
Drwxrwxr-x  10 bin      bin          512 Jan 21 17:36 spool
Drwxr-xr-x   2 root     system       512 Jan 21 18:27 sys
Drwxrwxrwt   2 bin      bin          512 Jun 10 08:37 tmp
Frw-r--r--   1 root     transarc    5219 Apr 23 16:08 typescript
Drwxrwxr-x   2 bin      bin         1536 Jan 21 18:13 ucb
Drwxrwxr-x   2 bin      bin          512 Jan 21 17:49 usg
Drwxr-xr-x   6 100      12           512 Jun 10 08:47 usr
Drwxr-xr-x   4 root     system       512 Jan 21 17:21 vice

Iteration 1 process 1 running test1.

Wed Jun 10 09:16:59 EDT 1992
RUNNING BASIC fts COMMANDS on SERVER dewitt in CELL dewitt.
========================================================================

Running fts
fts: Type 'fts help' or 'fts help <topic>' for help

Running fts statftserver
fts: Missing required parameter '-server'
GetServer: host 'fubar_argument' could not be translated: not found (dce / rpc).
No active transactions on /.:/hosts/dewitt
No active transactions on /.:/hosts/dewitt
fts: DFS junction for cell '/.:/hosts/dewitt' not found: entry not found (dce / rpc).
The parameter, -cell, requires a non-null argument value.
fts: Too many values after switch -cell
No active transactions on /.:/hosts/dewitt
fts: DFS junction for cell 'fubar_cell' not found: incomplete name (dce / rpc).
GetServer: host 'fubar_server' could not be translated: not found (dce / rpc).

Running fts apropos
fts: Missing required parameter '-topic'
Sorry, no commands found
aggrinfo: list aggregate information
lsaggr: list aggregates on a server
lock: lock FLDB entry for fileset
unlock: release lock on FLDB entry for fileset
unlockfldb: unlock all locked entries in FLDB
clone: create backup clone of fileset
clonesys: create backup clone of group of filesets
create: create a new fileset
crfldbentry: create FLDB entry
crmount: create mount point
crserverentry: create a new server entry in FLDB
lsquota: list fileset quota
setquota: set fileset quota

Running fts help and fts <subcommand> -help
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Unknown topic 'fubar_argument'
fts: Unrecognized switch '-fubar_argument'; type 'fts help help' for detailed help
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edit server attributes in FLDB
help            get help on commands
lock            lock FLDB entry for fileset
lsaggr          list aggregates on a server
lsfldb          list filesets in FLDB
lsft            show information about fileset
lsheader        list filesets on server (bypass FLDB)
lsmount         list mount point
lsquota         list fileset quota
lsreplicas      get status of current replicas
lsserverentry   show attributes of a particular server in FLDB
move            move fileset
release         release fileset
rename          rename fileset
restore         restore fileset
rmsite          remove replication site
setquota        set fileset quota
setrepinfo      set replication parameters
statftserver    report fileset server status
statrepserver   get status of replication server
syncfldb        synchronize FLDB with fileset server
syncserv        synchronize fileset server with FLDB
unlock          release lock on FLDB entry for fileset
unlockfldb      unlock all locked entries in FLDB
update          update fileset replica
zap             delete fileset (bypass FLDB)
fts: Commands are:
addsite         add replication site
aggrinfo        list aggregate information
apropos         search by help text
clone           create backup clone of fileset
clonesys        create backup clone of group of filesets
create          create a new fileset
crfldbentry     create FLDB entry
crmount         create mount point
crserverentry   create a new server entry in FLDB
delete          delete fileset
delfldbentry    delete FLDB entry for fileset
delmount        delete mount point
delserverentry  delete a server entry in FLDB
dump            dump fileset
edserverentry   edi[pakhtar@transarc.com 6/18/92 public]
On the face of it, this looks like a configuration, setup, dfsbind or 
CDS problem.

Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com'

[11/10/92 public]
This was ignored for a long time since its Component was dce rather than
dfs.

In any case, it's a very old defect that should be re-tested and/or
cancelled.  I've reassigned it to Vijay here, but it might need to go to
Diane since she's working on dfsbind these days.

Vijay has been running these fts tests, but I don't think that he has seen
this kind of behavior.  If we can't reproduce this, it should be cancelled.
This is the kind of odd behavior that would at least have changed, if not
gotten fixed, in the intervening months.

[01/18/93 public]
Cancelled



CR Number                     : 4040
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7044
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm may panic if cache is full
Reported Date                 : 6/3/92
Found in Baseline             : 1.0.1b24
Found Date                    : 6/3/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : closed

[6/3/92 public]
The configuration is a rios server with a pmax client.  The pmax client panic'd while running the test10 script
of the low tests.  The stack trace is:
read
vn_read
xglue_read
nosf_read
cm_rdwr
cm_read
cm_getDCache
cm_GetDSlot+1c4
panic: getdslot: !freeDSList

[6/5/92 public]
Filled in Responsible Engr. with `pakhtar@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/5/92 public]
Filled in Interest List CC with `kazar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `tu@transarc.com'

[6/11/92 public]
I ran the low/test10 and it did not panic. I believe the fact the system 
panics has nothing to do with low/test10. 
But, I do believe there could, indeed, be a bug regarding dcaches. That is, 
The CM might run out (it should not) of dcaches after a long series of tests 
and the system panics. You may want to consider to lower the priority since 
it is hard to reproduce ? 
Tu 
Changed Interest List CC from `kazar@transarc.com' to `kazar@transarc.com, 
 pakhtar@transarc.com'

[06/11/92 public]
Priority lowerd.  I also hear that this problem was supposed to have been
fixed quite some time ago by Carl?  If this is the case, can someone
knowledgeable take a look at the official source treee to make sure this
fix is in fact in there.

[7/20/92 public]
Changed "Fix by" to 1.0.2

[7/28/92 public]
This problem is still present in b24.  I've just seen it on a 2 
machine PMAX cell.  Tu is correct, it has nothing to do with test 
10.  I was doing a DCE build over DFS when it occured.  It looks 
like the panic Diane saw was in a read while mine is in a write.  
However, the panic message is identical.
panic("!freeDSList")
cm_GetDSlot
cm_GetDCache
cm_write
cm_rdwr
nosf_write
xglue_write
vn_write
rwuio
write
Could this be related to the fact that I had just restarted the client
and I did not clean out the old cache files?

[7/28/92 public]
I have a 1.0.1 configuration that has reproduced this bug 3 times in a row.
Tu believes this problem has been fixed in 1.0.2 since major portions of
the dcache have been re-written and Transarc has not seen this problem
in "a long time".  Therefore, I will document my configuration and
suggest that it might be useful for Transarc to give this a try on
their 1.0.2 source base to see if the problem can be reproduced.
I'm using a 2 machine cell, both PMAX, one client and one server.  I'm
building DCE through DFS.  The panic occurs when building in the gds
directory (cd src/directory/gds;build).  More specifically, the panic
occurs when creating the libaplv2c.a library.  RPC_SUPPORTED_PROTSEQS
was wide open. On all occasions, the client had been restarted after
already being configured on the server.  On 1 of the 3 occasions, all
cache files were cleaned out.  I was dce_login'ed as cell_admin from
my regular user id rsarbo.  I was running sh, and the build command
line was: 
% build 2>&1 | tee build.log

[11/3/92 public]
Both Mike and I believe that GetDownD problem is fixed and works in 1.50 and
beyond (ie., 102).  We have not been experienced this problem for a long
time. 
Ron, 
Please verify it and if the problem does not surface in the next three
weeks, we should consider to cancel it. 
Thanks,

[11/4/92 public]
DFS is currently not sufficiently stable to even think about attempting
to verify this.  Until DFS is stable enough to run a build of dce to
completion, this bug should remain open.

[11/17/92 public]
This defect needs to be resolved for 1.0.2 in order to meet the defect
age limit.
Changed Priority from `2' to `1'

[1/6/93 public]
As agreed, this is being reassigned to the OSF DFS team. If this problem
can be reproduced, we'll be happy to investigate further. Otherwise, we
recommend cancellation.
Changed Interest List CC from `kazar@transarc.com, pakhtar@transarc.com' to 
 `kazar@transarc.com, pakhtar@transarc.com, tu@transarc.com' 
Changed Responsible Engr. from `tu@transarc.com' to `rsarbo@osf.org' 
Changed Resp. Engr's Company from `tarc' to `osf'

[1/15/93 public]
BUILD:	b11 aka dfs 2.2
CONFIG:	2 pmax cell, 1 pmax = sec, cds server, dfs client
			1 pmax = fl server, sec, cds and dfs client
TEST:	dfs.glue with data file containing:
	MACHINES="dce5 dce12"
	DFS_PATH="/:/gmd_dfs"
	UFS_PATH="/usr/gmd_dfs"
	NUMDIRENTRIES=10
	NUMPROCPERMACH=5
	PRINC=gmd 
REGISTRY: unchanged except for addition of test principal gmd
DETAILS:
	Logged in as Unix user gmd and dce principal gmd, attempted
	to run test for 8 hours. Unfortunately, I had left log and
	core files from CR 6846 in the /usr/opt area so I ran out
	of space on the filesystem in ~ 1/2 hour. (NOTE: this filesystem
	is also the one being exported AND the location of the on
	disk cache)
	
	At some point after the /usr: file system full error message had been 
	repeated ~20,000 times - I got the following:
	panic: getdslot: !freeDSList
	kdb kernel trap [dfsd]: Interrupt trap, code=0
	stopped at gimmeabreak: break 1
	kdb> $k
		gimmeabreak
		Debugger
		panic
		cm_GetDSlot
		cm_GetDCache
		cm_bkgQueue
		cm_bkgDaemon
		afscall_cm
	Since I was clearly absolutely out of disk space, is this behavior
	acceptable?

[1/19/93 public]
Pervaze, I'm re-assigning this to you since the symptom has been 
reproduced, though I understand that the underlying cause might well 
be the full filesystem in this latest case.  Is it intended behavior 
for DFS to panic when the cache filesystem fills?  It seems a very
unfriendly failure mode.  Sorry to bounce this around, but as you
know, we've got to get it resolved before 1.0.2 freeze.  Thanks.

[1/19/93 public]
>Is it intended behavior for DFS to panic when the cache filesystem fills?  
In short, no. 
>It seems a very unfriendly failure mode.  
I agree. We should address this problem. The questions are:
- should this be attempted for 1.0.2?
- how destabilizing will the changes be?
- what level of confidence will we have that the system won't fail when
the cache partition fills? i.e. how well will we be able to test the fix, and
are there likely to be other problems in this area?
A conversation with Mike Kazar about this a few days ago, led me to believe
that it would probably be prudent to defer this until after 1.0.2. It
would be reasonable to evaluate this further before making a final decision.
I'll send out mail shortly, to the people concerned.

[1/28/93 public]
It has been agreed that the code changes will be deferred until after 1.0.2.
The documentation will reflect the configuration limitation. Doc defect for
1.0.2 is 7044.
Filled in Inter-dependent CRs with `7044' 
Changed Interest List CC from `kazar@transarc.com, tu@transarc.com' to 
 `pakhtar@transarc.com, tu@transarc.com' 
Changed Severity from `A' to `C' 
Changed Priority from `1' to `2' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `kazar@transarc.com'

[1/28/93 public]
Changed Defect or Enhancement? from `def' to `enh'

[9/10/93 public]
The original panic in getdslot was caused by a race condition that
has been fixed in the Transarc 1.0.3 code base.
This enhancment should be left open since there is still a
possibility that a cache manager could panic if the dfs/cache 
directory runs out of space.  So far, this panic has not been
observed even when the cache paritition was full.
Changed Short Description from `pmax client panics in getdslot' to `cm may panic if cache is full'

[8/12/94 public]
You just can't get this panic any more.  (This panic string is no longer in
our repertoire.)  The definitive fix was in the change that fixed OT defect
8161.  Since that fix is in 1.0.3, I am cancelling this defect.
Changed Interest List CC from `pakhtar@transarc.com, tu@transarc.com' to 
 `bwl@transarc.com' 
Changed Status from `open' to `cancel' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 3649
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 6814
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Change syntax of salvage command
Reported Date                 : 5/19/92
Found in Baseline             : 1.0.1
Found Date                    : 5/19/92
Severity                      : D
Priority                      : 3
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[5/19/92 public]
The interface of the salvage command needs to be modified for consistency
and clarity.  The syntax of the command is currently
   salvage -aggregate <name> [-verify] [-recover] [-norecover]
      [-verbose] [-nosalvage] [-help]
The syntax of the command should be changed to
   salvage -aggregate <name> [{-verify | -nosalvage}] [-norecover]
      [-verbose] [-help]
The interaction of the command's various options needs to change as well. 
The following table lists the functionality the command needs to be able
to achieve and the options used to achieve it with both the current and
new syntax:
FN'ALITY	CURRENT STX.    	NEW STX.
==================================================
verify		-verify			-verify
					-norecover
recover,	-verify			-verify
verify		-recover
recover		-nosalvage		-nosalvage
recover,	<default>		<default>
salvage
salvage		-norecover		-norecover
==================================================
The primary differences between the command's current and proposed
interface and implementation are
1. For consistency with other commands, the -verbose option is listed
   last.
2. The -recover option is unnecessary because, unless -norecover is used
   with the command, the command *always* runs recovery.
3. The -verify option still turns off salvaging by default; thus, the two
   are syntactically mutually exclusive.
If/when this change is incorporated, the documentation will need to be
changed accordingly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[5/19/92 public]
Ted, Jeff, and I discussed the command further and have decided on a
different set of modifications.  The proposed changes would result in the
following syntax:
   salvage -aggregate <name> [{-readonly | -verify | -nosalvage | -norecover}]
      [-verbose] [-help]
The options have the following effect:
FN'ALITY	CURRENT STX.
============================
verify		-readonly
recover,	-verify
verify
recover		-nosalvage
recover,	<default>
salvage
salvage		-norecover
============================
Usability being the main thrust of the proposed changes to the interface,
the above modifications seem very reasonable.  One additional change to be
considered is a different name for the -readonly option.

[9/22/92 public]
What's the status of this?  I thought the corresponding doc changed had
already been made so that the coding changes to the salvager should be
made soon.  No?
Filled in Subcomponent Name with `lfs' 
Changed Interest List CC from `jeff@transarc.com, ota@transarc.com' to 
 `jeff@transarc.com, mason@transarc.com, jdp@transarc.com' 
Changed Responsible Engr. from `jdp@transarc.com' to `ota@transarc.com'

[9/22/92 public]
The manpage for salvage remains as it did at release of 1.0.1.  Nothing has
been done yet to update the documentation.  The above changes are still the
most reasonable alternative, but a number of people have expressed disgust
with the -nosalvage option.  We should probably revisit this, but I would
personally rather wait a couple of weeks until I have the time.  Does that
sound acceptable to all for now?

[9/22/92 public]
How about replacing "-nosalvage" in the above proposal with "-recoveronly"?

[1/25/94 public]
These suggestions have been superceded by reorganization of the command
options outlined in OT6814.  These changes have been made and so this
defect is no longer applicable.
Filled in Inter-dependent CRs with `6814' 
Changed Status from `open' to `cancel' 
Filled in Reported by Company's Ref. Number with `4977'



CR Number                     : 3579
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 9022
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Link count errors develop: root can link/unlink dirs
Reported Date                 : 5/15/92
Found in Baseline             : 1.0.1
Found Date                    : 5/15/92
Severity                      : C
Priority                      : 2
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[5/15/92 public]
The salvager reports link count errors on an aggregate that has been in
use for a while.  It isn't possible to say much about the circumstances.
This is what the errors look like:
    In volume fileset1 (avl #5)
      in anode (#4567)
	File's link count is 1 but should be 0
      in anode (#4572)
	File's link count is 1 but should be 0
      in anode (#4886)
	File's link count is 1 but should be 0
The message is saying that these files are not in any directory but the
anode has a link count of one.  I have tried pretty hard to think of a
scenario where the bug fixed by delta "bwl-ot3198-unlink-inactive" might
have caused this type of error but have been unsuccessful.  Bruce and I
have also done a some code reading this morning looking for error paths
that could lead to trouble like this.  These efforts have been
unsuccessful as well.  There is a good chance that the stat bug which
resulted in closing the device out from under Episode could cause
problems like this because Episode isn't very robust to I/O errors.
However, I am loath to write this off so cavalierly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/12/92 public]
In examining the aggregate more carefully, the orphan files are newly
created directories.  This implies that there is an error path out of
mkdir that doesn't clean things up correctly.
The offending aggregate is in ~ota/de/salvage-data/ot3579.bad_linkcount.Z.

[6/23/92 public]
Further examiniation of the code paths out of mkdir didn't not reveal
any problems.  However, Bruce noticed that superuser does have access to
orphan directories and produce result that look like this.
A similar looking problem has been found associated with rename test17
in the pcts POSIX conformance suite.  Investigation revealed that that
test tries to clean up by unlinking "foo/.."  and "foo" (but not
"foo/.")  on exit from test17.  Thus each round of the pcts test was
leaving an orphaned directory behind.
Apparently it is traditional to allow superuser to link and unlink
directories.  This can easily and accidently lead to path inconsistency
problems that will cause fsck errors.  SunOS and BSD have this feature.
Interestingly JFS also supports the travesty only at the system call
level (the "ln" and "rm" commands will not allow directory targets under
any cirsumstances).  With only a little trouble we were able to panic
the AIX machine during a rename.  It seems that IBM must have found some
fairly good reason to preserve this functionality, but it doesn't get
used, or tested, very often.  From this I infer the existence of some
legacy program(s) of considerable importance to some customer(s).  I
have no idea of the identity of these programs.
Since I am rasher than IBM (indeed probably even Transarc is rasher than
IBM) I am inclined to disable this "feature" at the vnode level.  The
most flexible approach is to control this with a global variable that is
off by default.  If epistat is taught about this flag we can enable
superuser directory mangling on demand if necessary (or a desparate user
could enable it with adb in an emergency).  I have no intention of
documenting this, however, which is why epistat is a reasonable place to
put this escape hatch.  Let these legacy programs come to us!
Comments?
Changed Interest List CC from `wang@sni-usa.com' to `wang@sni-usa.com, 
 bwl@transarc.com, burnett@liz.austin.ibm.com' 
Changed Fix By Baseline from `1.0.1' to `1.0.2'

[6/23/92 public]
Just as you guessed Ted IBM supports this functionality because we found
customers that used it and wanted it.  Like you IBM wanted to take it
out, but did not because of the above.  However thanks for pointing
out the panic.  I will relay this on to our FS group so they can get
it fixed if it is not already fixed in a more recent release. As for 
Episode support of this I think at a minimum you should make it an optional
feature like you described earlier that can be turned on or off.
As for wether it should be the default functionality I can't say.  
What do the folks at OSF say?.  Since
we already know Sun, and BSD support it, what does OSF1 do.  And while
we are asking does any one know what the Veritas File System does?  The
bottom line is we should give the customers what they want.

[8/26/92 public]
As per Ted's request, this is reassigned to Bruce.
Changed Interest List CC from `wang@sni-usa.com, bwl@transarc.com, 
 burnett@liz.austin.ibm.com' to `wang@sni-usa.com, ota@transarc.com, 
 burnett@liz.austin.ibm.com' 
Changed Responsible Engr. from `ota@transarc.com' to `bwl@transarc.com'

[9/3/92 public]
Changed Short Description from `Link count errors develop' to `Link count 
 errors develop: root can unlink dirs'

[10/27/92 public]
 This problem deals with supporting non-POSIX semantics of
questionable value, especially in DFS where there is no concept of
"root" privs for unlinking directories.  As such, I'm marking this as
an enhancement request, not a defect.
Changed Defect or Enhancement? from `def' to `enh' 
Changed Interest List CC from `wang@sni-usa.com, ota@transarc.com, 
 burnett@liz.austin.ibm.com' to `wang@sni-usa.com, ota, 
 burnett@liz.austin.ibm.com'

[9/13/93 public]
I've opened this same defect in Transarc's defect database as DB 4305.
Changed Short Description from `Link count errors develop: root can unlink dirs' to `Link count errors develop: root can 
 link/unlink dirs' 
Filled in Reported by Company's Ref. Number with `4305'

[3/7/94 public]
Through an oversight a new defect, 9022, was created for the same bug, and
the fix was submitted under 9022.  OT does not allow us to dup 3579 to 9022,
so I am canceling 3579.
Filled in Inter-dependent CRs with `9022' 
Changed Status from `open' to `cancel'



CR Number                     : 2549
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : osi dir shouldn't contain lock.[ch] files
Reported Date                 : 4/6/92
Found in Baseline             : 1.0.1
Found Date                    : 4/6/92
Severity                      : E
Priority                      : 4
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : open
Transarc Deltas               : mason-ot2549-deal-with-lock-package
Transarc Herder               : jaffe@transarc.com
Transarc Delta                : 

[4/6/92 public]
Locking stuff is handled by code in top-level osi layer only thread support is
in episode tools dir.

[7/20/92 public]
Changed "Fix by" to 1.0.2

[8/31/92 public]
Reassigned.

[9/18/92 public]
Since this currently works, it is not a defect but and enhancment.
The problem is that the Episode version of lock.c includes "thread.h", which
redefines osi_Thread calls in terms of LWP calls when out of the kernel.
I would suggest having all of episode use the CMA version of LWP, and then
remove thread.h, which defines OSI primatives as LWP, and let the DFS osi 
code use the correct normal CMA definitions.  This would then let episode 
use the correct version of lock.c (and lock.h)
The problem with this approach is that there are a number of user level
programs in episode which link with lwpdummy in order to avoid using CMA.
Since those programs link with code that uses lock.c, this code will now
need to be linked with libdce.a.
Changed Defect or Enhancement? from `def' to `enh' 
Filled in Interest List CC with `mason@transarc.com'

[9/21/92 public]
Changed Interest List CC from `mason@transarc.com' to `mason@transarc.com, 
 bwl@transarc.com'

[11/12/92 public]
Canceled by Tony's request.  It just does not seem like a necessary thing to 
do.
Changed Status from `open' to `cancel' 
Filled in Resp. Engr's Company with `tarc'

[11/13/92 public]
Actually, as it turns out it looks like this is only defuncting the file.
I'll handle it.
Changed Interest List CC from `mason@transarc.com, bwl@transarc.com' to `bwl' 
Changed Status from `cancel' to `open' 
Changed Responsible Engr. from `jaffe@transarc.com' to `mason@transarc.com'

[11/16/92 public]
 It appears that nobody uses lock.c except the Episode code.  This is
a straight-forward fix.
Filled in Transarc Deltas with `mason-ot2549-deal-with-lock-package'

[12/11/92 public]
Changed Transarc Status from `open' to `import'

[12/15/92 public]
Tony's change needs a lot more work.  This ain't ready for prime time yet.
Changed Transarc Status from `import' to `export'

[12/16/92 public]
Actually, this is a lot harder than it looks.
Furthermore, osi.h includes lock.h, which is, by measured expert
opinion, wrong.  Unfortunately, excising this little nested include is
non-trivial.
Changed Short Description from `tools dir shouldn't contain lock.[ch] files' to 
 `osi dir shouldn't contain lock.[ch] files' 
Changed Transarc Status from `export' to `open'

[12/17/92 public]
It may be a mistake to get involved with this but let me make a few
quick points.
1. Locking is an appropriate function of an osi layer to provide.
2. All of DFS uses the same conceptual model for locking, namely single
   writer, multiple readers.
3. All user space DFS code can now rely on a pthreads substrate for
   implementing osi_Sleep and osi_Wakeup.
Point 3 above means that lock can be implemented in terms of other osi
primitives in a OS and user/kernel independent fashion (i.e. no ifdefs).
The only problem is that standalone Episode must link against a LWP
implementation of pthreads which supports osi_Sleep/Wakeup.  There is
already support for doing this.  Other user space code already uses
whatever pthreads code it can find (native or CMA).
Thus I suggest that we leave this code here and reorganize things so we
can punt ncs_compat locking and move towards a real osi lock module.
This is not to disagree with MikeM that this is "non-trival", it is.

[11/15/93 public]
Changed Status from `open' to `cancel' 
Added field Transarc Delta with value `'



CR Number                     : 1043
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : other
S/W Ref Platform              : other
Component Name                : dfs
Subcomponent Name             : cmd processor
Short Description             : bos addadmin not displaying correct syntax
Reported Date                 : 08/26/91
Found in Baseline             : .56e
Found Date                    : 
Severity                      : E
Priority                      : 4
Status                        : cancel
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : open

[01/29/92 public]
Orbit reference number 2726
reference     
product
phaseFound    development
History:
lastUpdate    91/11/12 15:32:15
endDate                                
assignDate    91/08/26 14:09:57   
    addDate              action          userLogin (userName)
    -------------------- --------------- ----------------------------------
    91/08/26 14:09:58    create          pehkonen (J. Pehkonen)
    91/09/23 13:31:52    note            hirsch (Phil Hirsch)
    91/10/07 16:08:03    note            pehkonen (J. Pehkonen)
    91/10/08 07:01:41    note            hirsch (Phil Hirsch)
    91/11/05 16:06:31    modify          drb (Defect Review Board)
    91/11/06 10:29:10    note            dstokes (D. Stokes)
    91/11/06 11:22:21    modify          drb (Defect Review Board)
    91/11/06 11:43:32    note            liz (Liz Hughes)
    91/11/12 15:32:15    modify          drb (Defect Review Board)

[08/26/91 public]
The syntax being shown for bos addadmin is incorrect. Executing "bos addadmin -help" shows the following:
bos addadmin -server <machine> -adminlist <filename> [-user <user_name>...] 
 [-group <group_name>...] [-createlist] [-noauth] [-localauth] [-help]
but if you execute "bos addadmin -server <machine> -adminlist <filename>" an
error is returned "bos: Error: no users or groups specified"
It appears either the -user or -group is a required parameter.
This is the AFS5.6 drop (dce.56e).

[09/23/91 public]
This is a limitation of the DFS command parser. The truth of the matter is
that you must specify at least one of -user, -group, or -createlist, but
you're welcome to specify more than one of the three as well. That is, bos
addadmin junior admin.bos -createlist is legal, as is bos addadmin junior
admin.bos -user pdh -createlist and bos addadmin junior admin.bos -user pdh,
but ``bos addadmin junior admin.bos'' by itself is illegal (what would it
do?).

[10/07/91 public]
I understand that one of -user, -group, or -createlist is needed but doing -help
does not show that. It shows it as an unrequired option.  bos addadmin -help 
should be something like this:
bos addadmin -server <machine> -adminlist <filename> { -user <name> | -group
<name> | -createlist} [-noauth] [-localauth] [-help]

[10/08/91 public]
The syntax you suggest would limit the user to choosing exactly one of
the three switches; we'd like to allow more than one to be specified on
the same command line. Unfortunately, our command parser can't do that -
there's no way to say "the user must choose one or more of these options".
The current help message is a compromise, I admit, but it's the best we
can do given our current command parser.

[11/06/91 public]
Transarc would like
to see this defect changed to deferred:
From: Julie_Kownacki@transarc.com:
  The bos addadmin requires that one of three arguments be present.
  Our help messages mark arguments as either being required, or optional.
  (Optional ones are enclosed by brackets.)  Since the help syntax has
  no notion of 'one of several arguments is required', the three arguments
  in question are marked as optional.  The full documentation explains
  that you need to specify one of them.
  The amount of work involved in defining additional syntax for the
  help message and making major changes to the command parser is not
  justified at this time.  The benefit is just too small to warrant that
  amount of work.  We therefore propose that this bug be deferred past
  the DCE 1.0 delivery.

[11/06/91 public]
The IBM dfs team agrees that this defect can be deferred.

[11/12/91 public]
This defect has been marked 'deferred' (do not fix in 1.0) by the DRBoard.  Reason:
As long as the man page is clear, this is too much work for limited gain.

[03/19/92 public]
The complaint is a small detail about the automatically-generated syntax
summary, in a case in which the abstractions of the command-line parser
cannot handle the real constraints of the command.  There are plenty of
such situations in DFS's user-level commands for all kinds of arcane
constraints (e.g. can't use parameter A unless parameter B or parameter C
is given) that [1] don't have a conventional, widely-understood notation in
command-line strings, and [2] don't have an explicit representation in the
abstractions of our command-line parser.  I would recommend deferring the
problem indefinitely.

[4/9/92 public]
Updated transarc status.

[4/15/92 public]
Trying to reflect ``indefinite deferral'' in the severity ranking.
Changed Severity from `D' to `E'

[7/16/92 public]
Mark as an enhancement request
Changed Defect or Enhancement? from `def' to `enh' 
Filled in Subcomponent Name with `cmd processor'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[9/2/92 public]
Could we cancel this, please?
We'll never express all the odd inter-dependencies in the command processor
to make the usage line look just the way it does in the man page.

[8/5/93 public]
Changed Status from `open' to `cancel'



