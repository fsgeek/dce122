CR Number                     : 13605
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : security enhancements
Reported Date                 : 8/19/96
Found in Baseline             : 1.2.1
Found Date                    : 8/19/96
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 1.2.2
Affected File(s)              : 
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/cm_getprotectlevels.8dfs
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/cm_setprotectlevels.8dfs
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/fts_setprotectlevels.8dfs
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/dfsd.8dfs
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/fts_lsfldb.8dfs
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/fts_lsft.8dfs
/src/dcebooks/dfs_admin_gdref/ref/man8dfs/fxd.8dfs
/src/dcebooks/dfs_admin_gdref/gd/ftmgmt.gpsml
/src/dcebooks/dfs_admin_gdref/gd/ftavail.gpsml
/src/dcebooks/dfs_admin_gdref/gd/overview.gpsml
/src/dcebooks/dfs_admin_gdref/gd/cachemgr.gpsml
/src/dcebooks/dfs_admin_gdref/gd/issues.gpsml
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[8/19/96 public]

Added information concerning DFS RPC authentication levels for the Cache
Manager and File Server, as well as a discussion of the advisory
authentication levels that can be set for individual filesets.



CR Number                     : 13566
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Document support for
multihomed servers
Reported Date                 : 7/16/96
Found in Baseline             : 1.2.1
Found Date                    : 7/16/96
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.2
Fixed In Baseline             : 1.2.2
Affected File(s)              : 
/src/dce_books/dfs_admin_gdref/gd/overview.gpsml
/src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml
/src/dce_books/dfs_admin_gdref/gd/issues.gpsml
/src/dce_books/dfs_admin/gdref/gd/cachemgr.gpsml
/src/dce_books/dfs_admin/gdref/ref/man8dfs/cm_setpreferences.8dfs
/src/dce_books/dfs_admin/gdref/ref/man8dfs/cm_getpreferences.8dfs
/src/dce_books/dfs_admin/gdref/cm_statservers.8dfs
/src/dce_books/dfs_admin/gdref/ref/man8dfs/cm_lscellinfo.8dfs
/src/dce_books/dfs_admin/gdref/ref/man8dfs/cm_whereis.8dfs
/src/dce_books/dfs_admin/gdref/gd/figures/pref1.ps
src/dce_books/dfs_admin/gdref/gd/figures/pref2.ps
src/dce_books/dfs_admin/gdref/gd/figures/pref3.ps
src/dce_books/dfs_admin/gdref/gd/figures/override1.ps
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[7/16/96 public]

Added information about the operation, administration, and use of
multihomed server support.  The FLDB now supports up to four entries per
server and the Cache Manager preference list now supports four preference
entries per server.



CR Number                     : 13553
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : DFS Client faults when configuring
Reported Date                 : 6/27/96
Found in Baseline             : 1.2.1
Found Date                    : 6/27/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 1.2.2
Affected File(s)              : file/afsd/afsd.c.
Diff supplied by              : 
Diff release                  : 
Sensitivity                   : public

[10/15/96 public]

Wally, I am unable to update ot 13553.
It tells me:
couldn't admin new CR file (c013553)

Could you please close it for me?
The fix has been submitted in bl04 by transarc. The affected
file is file/afsd/afsd.c.

Thanks,
Vikram.

[6/27/96 public]
hile trying to configure a DFS client on either the same machine as
the DFS fldb server or as a remote client the configuration fails with the 
response:
 
dfs: TKN server starts listening...
dfsd: init cache failed (code 22)
 
and no further references succeed to /:/ or /:.
 
Further testing and conversations with Jason Gait of Transarc have resulted in
several tries to obtain more info. However, there is still the error preventing
system testing.
 
Further Information follows for the record:
=========================================
> I thought of a few more things to check out.
>
>       1. Verify that /opt/dcelocal/etc/CacheInfo is present and is
>          world readable and reachable. This file contains
>          the configuration info for the cache. Should be
>          something like:
>
>               /...:/opt/dcelocal/var/adm/dfs/cache:10000
 
(root@litl_blu) -->ls /opt/dcelocal/etc/
CacheInfo         cdscp.bpt         dce_config_env    dfs.unconfig      rlogind
audit             dce.rm            dce_config_utils  dfs_config        rshd
 
(root@litl_blu) -->more /opt/dcelocal/etc/CacheInfo
/...:/opt/dcelocal/var/adm/dfs/cache:10000
 
>
>          (Hmmm, could you verify that /... is there? With
>          protections dr-xr-xr-x.)
 
(root@litl_blu) -->ls -l /
drwxrwxr-x   2 root     system       512 May  2 06:47 ...
lrwxrwxrwx   1 root     system        20 Jun 26 10:40 .: -> /.../dce122_dfstest3
....
-rw-------   1 root     system      3648 Jun 26 10:43 .sh_history
lrwxrwxrwx   1 root     system        23 Jun 26 10:40 : -> /.../dce122_dfstest3/
fs
drwx--x--x   2 root     system       512 Apr 23 13:28 Mail
......
 
>
>       2. After the failure, try starting dfsd by hand (dfsd&) and
>          see what happens. Also try as "dfsd -chunksize 16" to
>          explicitly set the chunksize to 64 kbytes. (This will
>          only work if no dfsd is running after the failed config.)
 
(root@litl_blu) -->ps -e |grep dfs
 17179      -  0:00 dfsbind
 20605  pts/1  0:00 dfsd
 20864  pts/1  0:00 dfsd
 21119  pts/1  0:00 dfsd
 
As you can see, three dfsd are running plus dfsbind.
Now try
 
(root@litl_blu) -->dfsd -chunksize 16
dfsd: init cache failed (code 22):dce122_dfstest3 /opt/dcelocal/var/adm/dfs/cach
e 300 1000 10000 100 50 16
dfsd: init cache failed:0 0 256
 
followed by:
(root@litl_blu) -->ps -e |grep dfs
 17179      -  0:00 dfsbind
 20605  pts/1  0:00 dfsd
 20864  pts/1  0:00 dfsd
 21119  pts/1  0:00 dfsd
 21400  pts/1  0:00 dfsd
 21657  pts/1  0:00 dfsd
 
Two more dfsd appear.
 
>
>       3. Might also try a reboot and restart DCE from scratch
>          (/etc/rc.dce). Might try changing the line that starts
>          dfsd in rc.dce to explicitly set the chunksize. (If the
>          failed config leaves a dfsd running we will have to try
>          it this way.)
>
This is a normal restart/startup for DFS.  As you can see from below, this was the
original method of start-up on a rebooted, cleaned out cell.  (Method of
clean-out:
cd /opt/dcelocal
rm -r *
cd /<install area>
dce_config -i
then follow with a 99 to exit the install.
Then proceed to do a real dce_config locally (shown below).
 
>       4. Do you execute dfs.clean and dfs.rm before installing a
>          new cell? I do and this might be a difference to explore.
 
If I've been running DFS, I reboot the system before starting the test
so that nothing should be left over from the earlier run.  I also
remove all elements of the cell and 'start from scratch' which also
triggers dfs.clean and dfs.rm (albeit on both HP-UX and AIX it is still
necessary to reboot the system to prevent the kernel pieces from
causing strange results).  For this run:
-----------------------------------------------
1.  Rebooted the system.
2.  (root@litl_blu) -->dfs.clean
    (root@litl_blu) -->dfs.rm
 
3.  Moved the dfsd and dfsdsd.cat into their respective locations.
4.  Configured the node:
0k19Droot on litl_blu: /
         DCE Main Menu ( on litl_blu )
 
         1. INSTALL     -install dce software
         2. CONFIGURE   -configure and start DCE daemons
         3. START       -re-start DCE daemons
         4. STOP        -stop DCE daemons
         5. UNCONFIGURE -remove a host from CDS and SEC databases
         6. REMOVE      -stop DCE daemons and remove data files
                         created by DCE daemons
        99. EXIT
        selection:  2
 
         DCE Configuration Menu ( on litl_blu )
         1. Initial Cell Configuration
         2. Additional Server Configuration
         3. DCE Client
         4. DFS Client
 
        98. Return to previous menu
        99. Exit
 
        selection:  1
S:****** Configuring initial cell...
 
         Initial Cell Configuration ( on litl_blu )
 
         1. Initial Security Server
         2. Initial CDS Server
         3. Initial DTS Server
 
        98. Return to previous menu
        99. Exit
 
        selection:  1
S:****** Configuring initial Security Server...
        Do you wish to first remove all remnants of
        previous DCE configurations for all components (y/n)?
        You should do so only if you plan on re-configuring
        as a client of a different cell: (n) y
S:****** Attempting to stop all running DCE daemons...
S:****** Successfully stopped all running DCE daemons.
S:****** Attempting to remove all remnants of previous DCE configurations...
S:****** Successfully removed all remnants of previous DCE configurations for all components.
        Enter the name of your cell (without /.../): dce122_dfstest3
S:****** Starting dced...
S:****** Initializing dced...
        Enter keyseed for initial database master key:
        Enter desired principal name for the Cell Administrator: (cell_admin)
        Enter desired password for the Cell Administrator:
        Re-enter desired password:
 
S:****** The current highest UNIX ID for persons on this node is 203.
        Enter the starting point to be used for UNIX ID's that
        are automatically generated by the Security Service
        when a principal is added using "rgy_edit": (303)
S:****** The current highest UNIX ID for groups is 1001.
        Enter the starting point to be used for UNIX ID's that
        are automatically generated by the Security Service
        when a group is added using "rgy_edit": (1101)
S:****** Starting secd...
S:****** Checking for active sec_client service...
S:****** Starting sec_client service...
S:****** Checking for active krb5 client...
S:****** Waiting for registry propagation...
S:****** Initializing the registry database...
 
         Initial Cell Configuration ( on litl_blu )
 
         1. Initial Security Server
         2. Initial CDS Server
         3. Initial DTS Server
 
        98. Return to previous menu
        99. Exit
 
        selection:  2
S:****** Configuring initial CDS Server...
S:****** Please wait for user authentication and authorization...
S:****** Checking for active sec_client service...
        Create LAN profile so clients and servers can be divided
        into profile groups for higher performance in a multi-lan cell ? (n)
S:****** Waiting for registry propagation...
S:****** Starting cdsadv...
S:****** Starting cdsd...
S:****** Setting ACLs for all new namespace entries...
 
         Initial Cell Configuration ( on litl_blu )
 
         1. Initial Security Server
         2. Initial CDS Server
         3. Initial DTS Server
 
        98. Return to previous menu
        99. Exit
 
        selection:  98
 
         DCE Configuration Menu ( on litl_blu )
 
         1. Initial Cell Configuration
         2. Additional Server Configuration
         3. DCE Client
         4. DFS Client
 
        98. Return to previous menu
        99. Exit
 
        selection:  2
S:****** Configuring additional server...
S:****** Please wait for user authentication and authorization...
 
         Additional Server Configuration ( on litl_blu )
 
         1. Additional CDS Server(s)
         2. DTS
         3. DFS System Control Machine
         4. DFS Private File Server
         5. DFS File Server
         6. DFS Fileset Location Database Server
         7. GDA Server
         8. Replica Security Server
         9. Auditing
         10. Password Management Server
         11. Unconfigure Password Management Server
 
        98. Return to previous menu
        99. Exit
 
        selection:  6
S:****** Configuring DFS Fileset Location Database Server...
0k19Droot on litl_blu: /
        Enter Cell Administrator's principal name: (cell_admin)
        Enter password:
 
S:****** Modifying the registry database for DFS operation...
S:****** Loading kernel extensions...
        Should the LFS Kernel Extension be loaded (n)?
S:****** Modifying the registry database for DFS server operation...
 
>>> group member added
>>> group member added
 
Current site is: registry server at /.../dce122_dfstest3/subsys/dce/sec/litl_blu
 
Domain changed to: group
Current site is: registry server at /.../dce122_dfstest3/subsys/dce/sec/litl_blu
 
Domain changed to: group
S:****** Starting bosserver...
Checking for a Ubik sync site in  hosts/litl_blu
Host /.:/hosts/litl_blu is now the sync site
        Enter the name of the system control machine:  litl_blu
        Enter the fileset name (root.dfs):
        Enter the filesystem type for root.dfs:
        1. Native File System (e.g. UFS, JFS)
        2. Episode File System (LFS)
selection:  1
        Enter the device name (e.g. /dev/lvXX)/dev/hd9var
        Ensure this is a valid, mounted filesystem partition.
        Press <RETURN> to continue, CTRL-C to exit:
        Enter the aggregate name (/export):  /var
        Enter the aggregate ID (1):
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
litl_blu.ch.apollo. RW       1       0:00:00 hosts/litl_blu <nil>
FLDB entry created for fileset root.dfs (0,,1) on aggregate 1 of litl_blu
Aggregate Id 1 is not exported from the server
S:****** Starting dfsbind...
S:****** Starting fxd...
fx: FX server starts listening...
 
         Additional Server Configuration ( on litl_blu )
 
         1. Additional CDS Server(s)
         2. DTS
         3. DFS System Control Machine
         4. DFS Private File Server
         5. DFS File Server
         6. DFS Fileset Location Database Server
         7. GDA Server
         8. Replica Security Server
         9. Auditing
         10. Password Management Server
         11. Unconfigure Password Management Server
 
        98. Return to previous menu
        99. Exit
 
***     selection:  98
 
         DCE Configuration Menu ( on litl_blu )
 
         1. Initial Cell Configuration
         2. Additional Server Configuration
         3. DCE Client
         4. DFS Client
 
        98. Return to previous menu
        99. Exit
 
        selection:  4
S:****** Configuring DFS client...
 
0k19Droot on litl_blu: /
        Enter Cell Administrator's principal name: (cell_admin)
        Enter password:
 
S:****** Loading kernel extensions...
/opt/dcelocal/ext/cfgdfs: dfscore_kmid already loaded
/opt/dcelocal/ext/cfgdfs: dfscmfx_kmid already loaded
        Is the cache :
        1. in memory
        2. on the local disk
selection:  2
        Enter the size of the cache (10000):
        Enter the name of the cache directory (/opt/dcelocal/var/adm/dfs/cache):
 
S:****** Starting dfsd...
dfs: TKN server starts listening...
dfsd: init cache failed (code 22):dce122_dfstest3 /opt/dcelocal/var/adm/dfs/cach
e 300 1000 10000 100 50 0
dfsd: init cache failed:0 0 256
 
         DCE Configuration Menu ( on litl_blu )
 
         1. Initial Cell Configuration
         2. Additional Server Configuration
         3. DCE Client
         4. DFS Client
 
        98. Return to previous menu
        99. Exit
 
        selection:  99
S:****** Exiting from dce_config.
WARNING: The password for the "cell_admin none none" user is a well-known defaul
t value.
Since this is a security hole, it is recommended that the password be changed im
mediately
after exiting this script by using "dce_login", then the "rgy_edit change" comma
nd.
 
5.   At this point, I exited config and proceeded to get you info for the other
    items above. (1-3)
---------------------------
 
>
>       5. THE CACHE MANAGER IS FULL OF CONSOLE PRINTF'S. Can we
>          be truly sure that one of them isn't slipping by? I
>          realize that you are not coterminous (pardon me) with
>          the physical console, but I wonder if it wouldn't be
>          possible to have someone watch it while the attempt to
>          config the cache manager is made?
>
I went out to the REAL console on litl_blu to insure no messages had
'snuck out there on me'.  This procedure has been true since you sent me the
dfsd version.  Further, to send console data to my window, I exported
the litl_blu display to my workstation to make it coresident.
 
> The next step will be to put traces into cm_init() and drop you
> a new cache manager. If nothing comes of the above suggestions, I'll
> work on that tomorrow.
Can you bring the OSF version down there and test it?  It may be faster than
this remote control.  (Just a suggestion).
 
I'll append the LOgs again, although I don't know if it will help.
Russ
 
======================================================
Bos Config file first:
/opt/dcelocal/var/dfs
(root@litl_blu) -->more BosConfig
restarttime 16 0 0 0 0
checkbintime 3 0 5 0 0
bnode simple flserver 1
parm /opt/dcelocal/bin/flserver
end
bnode simple bakserver 1
parm /opt/dcelocal/bin/bakserver
end
bnode simple ftserver 1
parm /opt/dcelocal/bin/ftserver
end
bnode simple repserver 1
parm /opt/dcelocal/bin/repserver
end
=============  Logs follow  ============================
/opt/dcelocal/var/dfs/adm
(root@litl_blu) -->ls
BakLog  BosLog  FlLog   FtLog   RepLog
 
/opt/dcelocal/var/dfs/adm
(root@litl_blu) -->more *Log
::::::::::::::
BakLog
::::::::::::::
Wed Jun 26 10:35:16 1996
: /opt/dcelocal/bin/bakserver: log initialized to /opt/dcelocal/var/dfs/adm/BakL
og
Wed Jun 26 10:35:25 1996
: Ready to process requests at Wed Jun 26 10:35:25 1996
 
Wed Jun 26 10:35:32 1996
: dbread: ubik_Read pos 0, buff 537727656, len 920
Wed Jun 26 10:35:32 1996
: DFS:ubk: status 0x27d3200f (dfs / ubk)
CheckInit: No data base, code 668147727
Wed Jun 26 10:35:32 1996
: dbread: ubik_Read pos 0, buff 537727656, len 920
Wed Jun 26 10:35:32 1996
: DFS:ubk: status 0x27d3200f (dfs / ubk)
CheckInit: No data base, code 668147727
Wed Jun 26 10:35:32 1996
: Error discovered in header, rebuilding.
::::::::::::::
BosLog
::::::::::::::
Wed Jun 26 10:33:51 1996: /opt/dcelocal/bin/bosserver: beginning logging
Wed Jun 26 10:33:51 1996: Server directory access is not okay
Wed Jun 26 10:34:52 1996: Server directory access is not okay
Wed Jun 26 10:35:10 1996: Server directory access is not okay
Wed Jun 26 10:35:28 1996: Server directory access is not okay
Wed Jun 26 10:35:34 1996: Server directory access is not okay
repserver: no aggregates on this server
::::::::::::::
FlLog
::::::::::::::
96-Jun-26 10:34:57 flserver: log initialized to /opt/dcelocal/var/dfs/adm/FlLog
96-Jun-26 10:34:57 flserver: using default group of /.../dce122_dfstest3/fs
96-Jun-26 10:35:07 flserver: cell dce122_dfstest3, trial ID 134240839,,848351079
, 1 servers.
96-Jun-26 10:35:07 flserver: ready to service requests.
96-Jun-26 10:35:53 flserver: Initializing the FLDB header.
96-Jun-26 10:35:53 flserver: FLDB header initialized; space for 64 sites, cell i
d 134240839,,848351079.
96-Jun-26 10:35:55 flserver: created site 15.21.249.80(0xf15f950), principal 'ho
sts/litl_blu', quota 0
96-Jun-26 10:35:57 flserver: filling in the principal names for the 1 DB servers
.
96-Jun-26 10:35:57 flserver: site 0 (f15f950) has principal 'hosts/litl_blu'
::::::::::::::
FtLog
::::::::::::::
1996-Jun-26 10:35:35 Log file initialized as /opt/dcelocal/var/dfs/adm/FtLog
1996-Jun-26 10:35:37 Ftserver starting
1996-Jun-26 10:37:36 Unknown message number 0x22243018
::::::::::::::
RepLog
::::::::::::::
96-Jun-26 10:35:42 repserver: log initialized to /opt/dcelocal/var/dfs/adm/RepLo
g
96-Jun-26 10:35:44 Replication server started.  Mainprocs=4; tokenprocs=4.
96-Jun-26 10:35:58 repserver: no aggregates on this server

[8/14/96 public]
This bug was fixed by Mike Burati over a month ago.  Can we close this
bug?  The ultimate problem was that dfsd was looking at errno without
a system call actually having failed.

[8/14/96 public]
The fix is in the DFS code base being tested at Transarc.  It will
be submitted to the OSF with our drop, currently scheduled for
mid-Sept 1996.  My understanding is that this bug is to be closed
after the code base containing the fix is dropped to the OSF.



CR Number                     : 13382
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: defunct rep_data.acf
Reported Date                 : 3/6/96
Found in Baseline             : 1.2.1
Found Date                    : 3/6/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.2
Affected File(s)              : src/file/fsint/rep_data.acf
Sensitivity                   : public

[3/6/96 public]
OT 13380 was incomplete.  The file src/file/fsint/rep_data.acf
must be defunct from the OSF source tree.  This file had been
defuncted from Transarc's tree.



CR Number                     : 13380
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: afs4int_s2c.h not needed
Reported Date                 : 3/5/96
Found in Baseline             : 1.2.1
Found Date                    : 3/5/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/file/fsint/Makefile, src/file/fsint/afs4int_s2c.h
Sensitivity                   : public
+0HISTORY Tue Mar  5 15:48:14 1996 mcg	Created

[3/5/96 public]
ENDGAME: afs4int_s2c.h not needed
Fix applied to src/file/fsint/Makefile in OT 13372 
should be backed out.  Correct fix is to defunct 
src/file/fsint/afs4int_s2c.h

[3/5/96 public]
9,13d8
< # Revision 1.1.101.3  1996/03/05  21:02:47  root
< #     Backout change made in 1.1.101.2.
< #     Better fix is to defunct afs4int_s2c.h
< #     [1996/03/05  21:01:59  root]
< #
17c12
< # 
---
> #
138c133,134
<                         afs4int.h tkn4int.h rep_proc.h scx_errs.h
---
>                         afs4int.h tkn4int.h rep_proc.h scx_errs.h \
>                         afs4int_s2c.h


Also defuncted src/file/fsint/afs4int_s2c.h from dce 1.2.1



CR Number                     : 13373
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: defunct src/file/ftutil/RIOS/*
Reported Date                 : 2/26/96
Found in Baseline             : 1.2.1
Found Date                    : 2/26/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/file/ftutil/RIOS
					     src/file/ftutil/RIOS/Makefile
					     src/file/ftutil/RIOS/ftutil_aixmount.c
Sensitivity                   : public

[2/26/96 public]
src/file/ftutil/RIOS/* was defuncted in Transarc's source tree, but
not in the OSF version of the dce sources.  It's existance generates
build errors.



CR Number                     : 13359
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: Remove reference to defunct file
Reported Date                 : 2/23/96
Found in Baseline             : 1.2.1
Found Date                    : 2/23/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/test/file/ubik/utst_server.c
Sensitivity                   : public

[2/23/96 public]
Remove reference to defunct file

[3/6/96 public]
Delete unused include file as in this diff:

96d95
< #include <netdb.h>



CR Number                     : 13358
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: Eliminate unnecessary seccond pass
Reported Date                 : 2/23/96
Found in Baseline             : 1.2.1
Found Date                    : 2/23/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/test/file/mmap/Makefile
Sensitivity                   : public

[2/23/96 public]
Eliminate unnecessary seccond pass

[3/6/96 public]
Change src/test/file/mmap/Makefile according to the following diff:

9,10c9,10
< .if exists(${MAKETOP}/file/${TARGET_MACHINE}/machdep.mk)
< .include "${MAKETOP}/file/${TARGET_MACHINE}/machdep.mk"
---
> .if exists(${TARGET_MACHINE}/machdep.mk)
> .include "${TARGET_MACHINE}/machdep.mk"
13,14c13,14
< .if exists (${MAKETOP}/file/${TARGET_OS}/machdep.mk)
< .include "${MAKETOP}/file/${TARGET_OS}/machdep.mk"
---
> .if exists (${TARGET_OS}/machdep.mk)
> .include "${TARGET_OS}/machdep.mk"



CR Number                     : 13353
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: Add NO_LICENSE_MANAGEMENT
Reported Date                 : 2/22/96
Found in Baseline             : 1.2.1
Found Date                    : 2/22/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : ./Makeconf
Sensitivity                   : public

[2/22/96 public]
Add NO_LICENSE_MANAGEMENT

[2/22/96 public]

DIFF: OLD,NEW : ./Makeconf===========================
*** ./Makeconf.orig	Thu Feb 22 09:25:18 1996
--- /project/dce/build/dce1.2/src/./Makeconf	Thu Feb 22 10:51:52 1996
***************
*** 10,22 ****
  #
  # HISTORY
  # $Log:	c013353,v $
# Revision 1.2  96/02/22  12:16:20  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
  # Revision 1.2.55.1  1995/12/07  21:42:23  root
  # 	Submit OSF/DCE 1.2.1
! #
  # 	HP revision /main/jrr_1.2_mothra/1  1995/11/22  22:53 UTC  psn
  # 	Merge second XIDL drop for DCE 1.2.1
  # 	[1995/11/17  17:00 UTC  dat  /main/dat_xidl2/1]
! #
  # Revision 1.2.37.11  1994/07/26  20:45:27  annie
  # 	added VISTA - 11163
  # 	[1994/07/26  20:45:15  annie]
--- 10,26 ----
  #
  # HISTORY
  # $Log:	c013353,v $
# Revision 1.2  96/02/22  12:16:20  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
+ # Revision 1.2.55.2  1996/02/22  16:51:50  root
+ # 	ENDGAME Submission
+ # 	[1996/02/22  16:10:45  root]
+ #
  # Revision 1.2.55.1  1995/12/07  21:42:23  root
  # 	Submit OSF/DCE 1.2.1
! # 
  # 	HP revision /main/jrr_1.2_mothra/1  1995/11/22  22:53 UTC  psn
  # 	Merge second XIDL drop for DCE 1.2.1
  # 	[1995/11/17  17:00 UTC  dat  /main/dat_xidl2/1]
! # 
  # Revision 1.2.37.11  1994/07/26  20:45:27  annie
  # 	added VISTA - 11163
  # 	[1994/07/26  20:45:15  annie]
***************
*** 82,88 ****
  #
  #  OSF DCE Version 1.0
  #
- 
  #
  # This file contains a stanza for each known platform where variables to be
  # used in makfiles are set; eg: TARGET_OS.  If your machine is not here, you
--- 86,91 ----
***************
*** 125,130 ****
--- 128,136 ----
  #
  # DES encryption code switch
  USE_DES=1
+ 
+ # Don't use DFS License Management Code
+ NO_LICENSE_MANAGEMENT=1
  
  #
  # Machine specific conditions



CR Number                     : 13352
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: dfsload.ext not installed
Reported Date                 : 2/22/96
Found in Baseline             : 1.2.1
Found Date                    : 2/22/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/file/sys/Makefile
Sensitivity                   : public

[2/22/96 public]
dfsload.ext not installed

[2/22/96 public]
Fixed by gait, diffs:

DIFF: OLD,NEW : ./file/sys/Makefile===========================
*** ./file/sys/Makefile.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/sys/Makefile	Thu Feb 22 10:44:53 1996
***************
*** 8,17 ****
  #
  # HISTORY
  # $Log:	c013352,v $
# Revision 1.2  96/02/22  12:14:50  root
# changed fields: Short Description  Status  Fixed In Baseline   new/changed/deleted note(s) [mcg 2/22/96 public] [psn 2/22/96 public] [mcg 2/22/96 public]
# 
  # Revision 1.1.115.2  1996/01/31  17:11:35  andi
  # 	Transarc DFS 1.2 Submission
  # 	[1996/01/30  20:18:45  andi]
! #
  # Revision 1.1.113.5  1994/06/09  14:20:04  annie
  # 	fixed copyright in src/file
  # 	[1994/06/09  13:31:45  annie]
--- 8,21 ----
  #
  # HISTORY
  # $Log:	c013352,v $
# Revision 1.2  96/02/22  12:14:50  root
# changed fields: Short Description  Status  Fixed In Baseline   new/changed/deleted note(s) [mcg 2/22/96 public] [psn 2/22/96 public] [mcg 2/22/96 public]
# 
+ # Revision 1.1.115.3  1996/02/22  16:44:53  root
+ # 	ENDGAME Submission
+ # 	[1996/02/22  16:10:50  root]
+ #
  # Revision 1.1.115.2  1996/01/31  17:11:35  andi
  # 	Transarc DFS 1.2 Submission
  # 	[1996/01/30  20:18:45  andi]
! # 
  # Revision 1.1.113.5  1994/06/09  14:20:04  annie
  # 	fixed copyright in src/file
  # 	[1994/06/09  13:31:45  annie]
***************
*** 90,101 ****
  
  .else
  
! .if exists (${MAKETOP}/file/sys/${TARGET_MACHINE}/Makefile)
! .include "${MAKETOP}/file/sys/${TARGET_MACHINE}/Makefile"
  .endif
  
! .if exists (${MAKETOP}/file/sys/${TARGET_OS}/Makefile)
! .include "${MAKETOP}/file/sys/${TARGET_OS}/Makefile"
  .endif
  
  libafssys_OFILES = lclcalls.o ${OFILES}
--- 94,105 ----
  
  .else
  
! .if exists (${TARGET_MACHINE}/Makefile)
! .include "${TARGET_MACHINE}/Makefile"
  .endif
  
! .if exists (${TARGET_OS}/Makefile)
! .include "${TARGET_OS}/Makefile"
  .endif
  
  libafssys_OFILES = lclcalls.o ${OFILES}



CR Number                     : 13351
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: Make not finding include files
Reported Date                 : 2/22/96
Found in Baseline             : 1.2.1
Found Date                    : 2/22/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/file/cm/test/Makefile
Sensitivity                   : public

[2/22/96 public]
ENDGAME: Make not finding include files

[2/22/96 public]
Fixed by gait, diffs:

DIFF: OLD,NEW : ./file/cm/test/Makefile===========================
*** ./file/cm/test/Makefile.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/cm/test/Makefile	Thu Feb 22 10:43:13 1996
***************
*** 1,5 ****
  #
- #
  # @OSF_COPYRIGHT@
  # COPYRIGHT NOTICE
  # Copyright (c) 1990, 1991, 1992, 1993, 1994 Open Software Foundation, Inc.
--- 1,5 ----
+ 
  #
  # @OSF_COPYRIGHT@
  # COPYRIGHT NOTICE
  # Copyright (c) 1990, 1991, 1992, 1993, 1994 Open Software Foundation, Inc.
***************
*** 9,18 ****
  #
  # HISTORY
  # $Log:	c013351,v $
# Revision 1.3  96/02/22  12:11:35  root
# changed fields: Status  Fixed In Baseline   new/changed/deleted note(s) [mcg 2/22/96 public] [psn 2/22/96 public] [mcg 2/22/96 public]
# 
  # Revision 1.1.4.1  1996/02/09  20:02:21  qlong
  # 	Transarc DCE 1.2 drop.
  # 	[1996/02/09  19:31:58  qlong]
! #
  # $EndLog$
  #
  # Copyright (C) 1995, 1994 Transarc Corporation
--- 9,22 ----
  #
  # HISTORY
  # $Log:	c013351,v $
# Revision 1.3  96/02/22  12:11:35  root
# changed fields: Status  Fixed In Baseline   new/changed/deleted note(s) [mcg 2/22/96 public] [psn 2/22/96 public] [mcg 2/22/96 public]
# 
+ # Revision 1.1.4.2  1996/02/22  16:43:12  root
+ # 	ENDGAME Submission
+ # 	[1996/02/22  16:10:47  root]
+ #
  # Revision 1.1.4.1  1996/02/09  20:02:21  qlong
  # 	Transarc DCE 1.2 drop.
  # 	[1996/02/09  19:31:58  qlong]
! # 
  # $EndLog$
  #
  # Copyright (C) 1995, 1994 Transarc Corporation
***************
*** 19,31 ****
  # All rights reserved.
  #
  
! VPATH			= ../
  
  PROGRAMS		= cm_preftest
  
  SCRIPTS			= test_preflock generate_prefs
  
! CFLAGS			= ${USERCFLAGS} -DCM_USERTEST -I${MAKETOP}../../src/file/cm
  
  cm_preftest_OFILES	= cm_preftest.o cm_serverpref.o cm_server.o \
  			  cm_cell.o cm_rrequest.o cm_volume.o
--- 23,35 ----
  # All rights reserved.
  #
  
! VPATH			= ..
  
  PROGRAMS		= cm_preftest
  
  SCRIPTS			= test_preflock generate_prefs
  
! CFLAGS			= ${USERCFLAGS} -DCM_USERTEST -I${MAKETOP}../../src/file/cm -I${SOURCEDIR}/file/cm
  
  cm_preftest_OFILES	= cm_preftest.o cm_serverpref.o cm_server.o \
  			  cm_cell.o cm_rrequest.o cm_volume.o



CR Number                     : 13347
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: eliminate build of src/file/libdcedfs
Reported Date                 : 2/21/96
Found in Baseline             : 1.2.1
Found Date                    : 2/21/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/Makefile
Sensitivity                   : public

[2/21/96 public]

ENDGAME: eliminate build of src/file/libdcedfs

[2/22/96 public]
Fixed as shown in diffs:

DIFF: OLD,NEW : ./Makefile===========================
*** ./Makefile.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./Makefile	Thu Feb 22 10:39:27 1996
***************
*** 11,20 ****
  #
  # HISTORY
  # $Log:	c013347,v $
# Revision 1.2  96/02/22  12:04:21  root
# changed fields: Status  Fixed In Baseline   new/changed/deleted note(s) [domian 2/21/96 public] [psn 2/22/96 public] [domian 2/21/96 public]
# 
  # Revision 1.2.46.1  1995/12/07  21:42:15  root
  # 	Submit OSF/DCE 1.2.1
  # 	[1995/12/07  20:57:55  root]
! #
  # Revision 1.2.32.27  1994/09/07  17:36:54  annie
  # 	fixed for cr10833
  # 	[1994/09/07  16:18:10  annie]
--- 11,24 ----
  #
  # HISTORY
  # $Log:	c013347,v $
# Revision 1.2  96/02/22  12:04:21  root
# changed fields: Status  Fixed In Baseline   new/changed/deleted note(s) [domian 2/21/96 public] [psn 2/22/96 public] [domian 2/21/96 public]
# 
+ # Revision 1.2.46.2  1996/02/22  16:39:25  root
+ # 	ENDGAME Submission
+ # 	[1996/02/22  16:10:46  root]
+ #
  # Revision 1.2.46.1  1995/12/07  21:42:15  root
  # 	Submit OSF/DCE 1.2.1
  # 	[1995/12/07  20:57:55  root]
! # 
  # Revision 1.2.32.27  1994/09/07  17:36:54  annie
  # 	fixed for cr10833
  # 	[1994/09/07  16:18:10  annie]
***************
*** 156,164 ****
  .if ${TARGET_OS} == "HPUX"
  BUILD_DFS += noship
  .endif
! .if ${TARGET_OS} == "AIX"
! BUILD_LIBDCEDFS = libdcedfs
! .endif
  .endif
  
  .if ${USE_SHARED_LIBRARIES} == 0
--- 160,168 ----
  .if ${TARGET_OS} == "HPUX"
  BUILD_DFS += noship
  .endif
! #.if ${TARGET_OS} == "AIX"
! #BUILD_LIBDCEDFS = libdcedfs
! #.endif
  .endif
  
  .if ${USE_SHARED_LIBRARIES} == 0



CR Number                     : 13346
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : ENDGAME: resetpag link error
Reported Date                 : 2/21/96
Found in Baseline             : 1.2.1
Found Date                    : 2/21/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/file/sys/lclcalls.c,./file/kutils/syscall.c,./file/kutils/syscall.h,./file/libafs/RIOS/dfscorecfg.c,./file/osi/osi_pag.c
Sensitivity                   : public

[2/21/96 public]
It is unknown if a missing componenet, "resetpag", is actaully required in DFS.
libdce does not build due to missing symbol "resetpag".

[2/22/96 public]
Code submitted by Jason Gait @ Transarc. He tested it. The diffs are:

DIFF: OLD,NEW : ./file/sys/lclcalls.c===========================
*** ./file/sys/lclcalls.c.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/sys/lclcalls.c	Thu Feb 22 10:41:56 1996
***************
*** 1,7 ****
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
--- 1,8 ----
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, In
! c.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
***************
*** 8,20 ****
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.126.3  1996/02/18  18:43:09  marty
!  * 	Update OSF copyright years
!  * 	[1996/02/18  17:58:51  marty]
   *
   * Revision 1.1.126.2  1996/01/31  17:11:43  andi
!  * 	Transarc DFS 1.2 Submission
!  * 	[1996/01/30  20:19:17  andi]
   * 
   * $EndLog$
   */
--- 9,25 ----
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.126.4  1996/02/22  16:41:55  root
!  * 	ENDGAME Submissions
!  * 	[1996/02/22  16:24:48  root]
   *
+  * Revision 1.1.126.3  1996/02/18  18:43:09  marty
+  * 	     Update OSF copyright years
+  * 	     [1996/02/18  17:58:51  marty]
+  * 
   * Revision 1.1.126.2  1996/01/31  17:11:43  andi
!  * 	     Transarc DFS 1.2 Submission
!  * 	     [1996/01/30  20:19:17  andi]
   * 
   * $EndLog$
   */
***************
*** 25,32 ****
   */
  
  #include <dcedfs/param.h>
! #include <signal.h>
! #include <errno.h>
  #include <dcedfs/syscall.h>
  
  #ifdef	AFS_HPUX_ENV
--- 30,36 ----
   */
  
  #include <dcedfs/param.h>
! #include <dcedfs/osi.h>
  #include <dcedfs/syscall.h>
  
  #ifdef	AFS_HPUX_ENV
***************
*** 33,72 ****
  extern int dfs_is_present();
  #endif
  
  /* setpag() -- do the syscall locally */
! setpag()
  {
!     int errcode;
  
  #ifdef	AFS_HPUX_ENV
      if (!dfs_is_present())
  	return -1;			/* emulate stub in sec_login_pag.c */
  #endif	/* AFS_HPUX_ENV */
!     errcode = afs_syscall(AFSCALL_SETPAG, 0, 0, 0, 0, 0);
!     return (errcode);
  }
  
  /* newtgt() -- do the syscall locally */
! newtgt(pag, time)
!     u_long pag;
!     u_long time;
  {
!     int code = 0;
  #ifdef	AFS_HPUX_ENV
      if (!dfs_is_present())
  	return 0;			/* emulate stub in sec_login_pag.c */
  #endif	/* AFS_HPUX_ENV */
!     if ((code = afs_syscall(AFSCALL_NEWTGT, pag, time, 0, 0, 0)) < 0)
!         return errno;
!     else 
!         return code;
  }
  
- 
  /* pioctl() -- do the syscall locally */
! pioctl(path, cmd, cmarg, follow)
!     char *path, *cmarg;
!     int cmd, follow;
  {
      return afs_syscall(AFSCALL_PIOCTL, (int)path, cmd, (int)cmarg, follow, 0);
  }
--- 37,81 ----
  extern int dfs_is_present();
  #endif
  
+ RCSID("$Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $")
+ 
  /* setpag() -- do the syscall locally */
! int
! setpag(void)
  {
! #ifdef	AFS_HPUX_ENV
!     if (!dfs_is_present())
! 	return -1;			/* emulate stub in sec_login_pag.c */
! #endif	/* AFS_HPUX_ENV */
!     return afs_syscall(AFSCALL_SETPAG, 0, 0, 0, 0, 0);
! }
  
+ int
+ resetpag(void)
+ {
  #ifdef	AFS_HPUX_ENV
      if (!dfs_is_present())
  	return -1;			/* emulate stub in sec_login_pag.c */
  #endif	/* AFS_HPUX_ENV */
!     return afs_syscall(AFSCALL_RESETPAG, 0, 0, 0, 0, 0);
  }
  
  /* newtgt() -- do the syscall locally */
! int
! newtgt(u_long pag, u_long time)
  {
!     int code;
  #ifdef	AFS_HPUX_ENV
      if (!dfs_is_present())
  	return 0;			/* emulate stub in sec_login_pag.c */
  #endif	/* AFS_HPUX_ENV */
!     code = afs_syscall(AFSCALL_NEWTGT, pag, time, 0, 0, 0);
!     return (code < 0 ? errno : code);
  }
  
  /* pioctl() -- do the syscall locally */
! int
! pioctl(char *path, int cmd, char *cmarg, int follow)
  {
      return afs_syscall(AFSCALL_PIOCTL, (int)path, cmd, (int)cmarg, follow, 0);
  }

DIFF: OLD,NEW : ./file/kutils/syscall.c===========================
*** ./file/kutils/syscall.c.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/kutils/syscall.c	Thu Feb 22 10:41:50 1996
***************
*** 1,7 ****
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
--- 1,8 ----
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, In
! c.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
***************
*** 8,30 ****
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.20.2  1996/02/18  23:15:42  marty
!  * 	Update OSF copyright years
!  * 	[1996/02/18  23:01:53  marty]
   *
   * Revision 1.1.20.1  1996/01/30  16:51:48  andi
!  * 	Transarc DFS 1.2 Submission
!  * 	[1996/01/30  16:27:09  andi]
   * 
   * $EndLog$
  */
- /*
-  * @OSF_COPYRIGHT@
-  * COPYRIGHT NOTICE
-  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
-  * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
-  * the full copyright text.
-  */
  /* Copyright (C) 1995, 1989 Transarc Corporation - All rights reserved */
  
  #include <dcedfs/param.h>		/* Should be always first */
--- 9,28 ----
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.20.3  1996/02/22  16:41:48  root
!  * 	ENDGAME Submissions
!  * 	[1996/02/22  16:24:49  root]
   *
+  * Revision 1.1.20.2  1996/02/18  23:15:42  marty
+  * 	     Update OSF copyright years
+  * 	     [1996/02/18  23:01:53  marty]
+  * 
   * Revision 1.1.20.1  1996/01/30  16:51:48  andi
!  * 	     Transarc DFS 1.2 Submission
!  * 	     [1996/01/30  16:27:09  andi]
   * 
   * $EndLog$
  */
  /* Copyright (C) 1995, 1989 Transarc Corporation - All rights reserved */
  
  #include <dcedfs/param.h>		/* Should be always first */
***************
*** 33,39 ****
  #include <dcedfs/osi_sysconfig.h>
  #include <dcedfs/syscall.h>
  
! RCSID("$Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $")
  
  /* for possible CM/PX communication. */
  unsigned long afscall_timeSynchDistance = 0x7ffffff1;
--- 31,37 ----
  #include <dcedfs/osi_sysconfig.h>
  #include <dcedfs/syscall.h>
  
! RCSID("$Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $")
  
  /* for possible CM/PX communication. */
  unsigned long afscall_timeSynchDistance = 0x7ffffff1;
***************
*** 98,103 ****
--- 96,102 ----
      afs_nosys,		/*  ALL  AFSCALL_BOMB       : afscall_bomb */
      afs_nosys,		/*  ALL	 AFSCALL_KLOAD	    : afscall_kload (HP/UX) */
      afs_nosys,		/*  AT   AFSCALL_AT         : afscall_at */
+     afs_nosys,		/*  CM   AFSCALL_RESETPAG   : afscall_resetpag */
  #ifdef	AFS_HPUX_ENV
  /* Add enough dummies to make the table at least 31 entries long. */
      afs_nosys,
***************
*** 113,119 ****
      afs_nosys,
      afs_nosys,
      afs_nosys,
-     afs_nosys,
      afs_nosys
  #endif	/* AFS_HPUX_ENV */
  };
--- 112,117 ----
***************
*** 156,169 ****
  #endif
  
  #ifndef	AFS_DYNAMIC
- 
- #define NIL 0
- 
  /*
   * Configure routines for each system call.
   * N.B. This table is modified at runtime.
   */
! CFG_RTN config_rtn[AFSCALL_LAST+1] = {
  	    cm_configure,	/* AFSCALL_CM       */
  	    cm_configure,	/* AFSCALL_PIOCTL   */
  	    cm_configure,	/* AFSCALL_SETPAG   */
--- 154,164 ----
  #endif
  
  #ifndef	AFS_DYNAMIC
  /*
   * Configure routines for each system call.
   * N.B. This table is modified at runtime.
   */
! CFG_RTN config_rtn[AFSCALL_LAST + 1] = {
  	    cm_configure,	/* AFSCALL_CM       */
  	    cm_configure,	/* AFSCALL_PIOCTL   */
  	    cm_configure,	/* AFSCALL_SETPAG   */
***************
*** 171,179 ****
  	    px_configure,	/* AFSCALL_VOLSER   */
  	    px_configure,	/* AFSCALL_AGGR     */
  	    epi_configure,	/* AFSCALL_EPISODE  */
! 	    NIL,
! 	    NIL,
! 	    NIL,
  	    all_configure,	/* AFSCALL_VNODE_OPS */
  	    cm_configure,	/* AFSCALL_GETPAG    */
  	    all_configure,	/* AFSCALL_PLUMBER   */
--- 166,174 ----
  	    px_configure,	/* AFSCALL_VOLSER   */
  	    px_configure,	/* AFSCALL_AGGR     */
  	    epi_configure,	/* AFSCALL_EPISODE  */
! 	    0,
! 	    0,
! 	    0,
  	    all_configure,	/* AFSCALL_VNODE_OPS */
  	    cm_configure,	/* AFSCALL_GETPAG    */
  	    all_configure,	/* AFSCALL_PLUMBER   */
***************
*** 183,192 ****
  	    cm_configure,       /* AFSCALL_NEWTGT    */
  	    all_configure,	/* AFSCALL_BOMB	     */
  	    all_configure,	/* AFSCALL_KLOAD     */
! 	    at_configure	/* AFSCALL_AT        */
! 	    };
  
! short config_done[AFSCALL_LAST+1];
  
  #endif	/* !AFS_DYNAMIC */
  
--- 178,188 ----
  	    cm_configure,       /* AFSCALL_NEWTGT    */
  	    all_configure,	/* AFSCALL_BOMB	     */
  	    all_configure,	/* AFSCALL_KLOAD     */
! 	    at_configure,	/* AFSCALL_AT        */
! 	    cm_configure        /* AFSCALL_RESETPAG  */
! };
  
! short config_done[AFSCALL_LAST + 1];
  
  #endif	/* !AFS_DYNAMIC */
  

DIFF: OLD,NEW : ./file/kutils/syscall.h===========================
*** ./file/kutils/syscall.h.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/kutils/syscall.h	Thu Feb 22 10:41:52 1996
***************
*** 1,7 ****
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
--- 1,8 ----
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, In
! c.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
***************
*** 8,45 ****
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.19.3  1996/02/18  23:42:04  marty
!  * 	Update OSF copyright years
!  * 	[1996/02/18  22:39:52  marty]
   *
   * Revision 1.1.19.2  1996/01/30  16:51:50  andi
!  * 	Transarc DFS 1.2 Submission
!  * 	[1996/01/30  16:27:16  andi]
   * 
!  * 	Transarc DFS 1.2 Submission
!  * 	[1996/01/29  23:07:37  andi]
   * 
   * Revision 1.1.16.3  1994/08/08  18:54:44  mckeen
!  * 	Added AT function to DFS syscall to support gateway
!  * 	[1994/08/08  17:52:27  mckeen]
   * 
   * Revision 1.1.16.2  1994/06/09  14:12:23  annie
!  * 	fixed copyright in src/file
!  * 	[1994/06/09  13:25:49  annie]
   * 
   * Revision 1.1.16.1  1994/02/04  20:21:54  devsrc
!  * 	Merged from 1.0.3a to 1.1
!  * 	[1994/02/04  15:14:34  devsrc]
   * 
   * Revision 1.1.14.1  1993/12/07  17:27:31  jaffe
!  * 	1.0.3a update from Transarc
!  * 	[1993/12/06  13:41:53  jaffe]
   * 
   * $EndLog$
   */
! /* Copyright (C) 1989, 1994 Transarc Corporation - All rights reserved */
  
! /* $Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $ */
  
  #ifndef	TRANSARC_KUTILS_SYSCALL_H_
  #define	TRANSARC_KUTILS_SYSCALL_H_
--- 9,50 ----
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.19.4  1996/02/22  16:41:51  root
!  * 	ENDGAME Submissions
!  * 	[1996/02/22  16:24:51  root]
   *
+  * Revision 1.1.19.3  1996/02/18  23:42:04  marty
+  * 	     Update OSF copyright years
+  * 	     [1996/02/18  22:39:52  marty]
+  * 
   * Revision 1.1.19.2  1996/01/30  16:51:50  andi
!  * 	     Transarc DFS 1.2 Submission
!  * 	     [1996/01/30  16:27:16  andi]
   * 
!  * 	     Transarc DFS 1.2 Submission
!  * 	     [1996/01/29  23:07:37  andi]
   * 
   * Revision 1.1.16.3  1994/08/08  18:54:44  mckeen
!  * 	     Added AT function to DFS syscall to support gateway
!  * 	     [1994/08/08  17:52:27  mckeen]
   * 
   * Revision 1.1.16.2  1994/06/09  14:12:23  annie
!  * 	     fixed copyright in src/file
!  * 	     [1994/06/09  13:25:49  annie]
   * 
   * Revision 1.1.16.1  1994/02/04  20:21:54  devsrc
!  * 	     Merged from 1.0.3a to 1.1
!  * 	     [1994/02/04  15:14:34  devsrc]
   * 
   * Revision 1.1.14.1  1993/12/07  17:27:31  jaffe
!  * 	     1.0.3a update from Transarc
!  * 	     [1993/12/06  13:41:53  jaffe]
   * 
   * $EndLog$
   */
! /* Copyright (C) 1995, 1989 Transarc Corporation - All rights reserved */
  
! /* $Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $ */
  
  #ifndef	TRANSARC_KUTILS_SYSCALL_H_
  #define	TRANSARC_KUTILS_SYSCALL_H_
***************
*** 46,51 ****
--- 51,59 ----
  
  #include <dcedfs/stds.h>
  
+ /*
+  * System call number for afscalls.
+  */
  #ifndef AFS_SYSCALL
  #ifdef __OSF1__
  #define AFS_SYSCALL     258
***************
*** 75,81 ****
  
  #define	AFSCALL_CM	0	/* Main syscall entry to the Cache Manager */
  #define	AFSCALL_PIOCTL	1	/* XXX pioctl(2) cm call XXX */
! #define	AFSCALL_SETPAG	2	/* XXX setpag(2) cm call XXX */
  #define	AFSCALL_PX	3	/* syscall entry to the Decorum exporter */
  #define	AFSCALL_VOL	4	/* syscall entry to the volume module */
  #define	AFSCALL_AGGR	5	/* syscall entry to the aggregate module */
--- 83,89 ----
  
  #define	AFSCALL_CM	0	/* Main syscall entry to the Cache Manager */
  #define	AFSCALL_PIOCTL	1	/* XXX pioctl(2) cm call XXX */
! #define	AFSCALL_SETPAG	2	/* setpag(2) syscall */
  #define	AFSCALL_PX	3	/* syscall entry to the Decorum exporter */
  #define	AFSCALL_VOL	4	/* syscall entry to the volume module */
  #define	AFSCALL_AGGR	5	/* syscall entry to the aggregate module */
***************
*** 93,99 ****
  #define AFSCALL_BOMB	17	/* bomb point interface */
  #define AFSCALL_KLOAD	18	/* HP/UX kernel extension load */
  #define AFSCALL_AT      19      /* gateway authentication table */
! #define	AFSCALL_LAST	19	/* Adjust as new entries are added */
  
  /*
   * XXX Although seemingly irrelevant to this include file, the following struct
--- 101,108 ----
  #define AFSCALL_BOMB	17	/* bomb point interface */
  #define AFSCALL_KLOAD	18	/* HP/UX kernel extension load */
  #define AFSCALL_AT      19      /* gateway authentication table */
! #define	AFSCALL_RESETPAG 20	/* reset PAG syscall */
! #define	AFSCALL_LAST	20	/* Adjust as new entries are added */
  
  /*
   * XXX Although seemingly irrelevant to this include file, the following struct

DIFF: OLD,NEW : ./file/libafs/RIOS/dfscorecfg.c===========================
*** ./file/libafs/RIOS/dfscorecfg.c.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/libafs/RIOS/dfscorecfg.c	Thu Feb 22 10:41:53 1996
***************
*** 1,7 ****
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
--- 1,8 ----
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, In
! c.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
***************
*** 8,30 ****
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.60.2  1996/02/18  23:15:47  marty
!  * 	Update OSF copyright years
!  * 	[1996/02/18  23:01:58  marty]
   *
   * Revision 1.1.60.1  1996/01/30  16:52:16  andi
!  * 	Transarc DFS 1.2 Submission
!  * 	[1996/01/30  16:28:40  andi]
   * 
   * $EndLog$
  */
- /*
-  * @OSF_COPYRIGHT@
-  * COPYRIGHT NOTICE
-  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
-  * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
-  * the full copyright text.
-  */
  #include <dcedfs/param.h>
  #include <dcedfs/osi.h>
  #include <dcedfs/osi_uio.h>
--- 9,28 ----
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.60.3  1996/02/22  16:41:52  root
!  * 	ENDGAME Submissions
!  * 	[1996/02/22  16:24:52  root]
   *
+  * Revision 1.1.60.2  1996/02/18  23:15:47  marty
+  * 	     Update OSF copyright years
+  * 	     [1996/02/18  23:01:58  marty]
+  * 
   * Revision 1.1.60.1  1996/01/30  16:52:16  andi
!  * 	     Transarc DFS 1.2 Submission
!  * 	     [1996/01/30  16:28:40  andi]
   * 
   * $EndLog$
  */
  #include <dcedfs/param.h>
  #include <dcedfs/osi.h>
  #include <dcedfs/osi_uio.h>
***************
*** 49,54 ****
--- 47,53 ----
  extern int afscall_bomb();
  extern int afscall_setpag();
  extern int afscall_getpag();
+ extern int afscall_resetpag();
  extern int zlc_Init();
  extern int agfs_Init();
  
***************
*** 131,136 ****
--- 130,136 ----
  		afs_set_syscall(AFSCALL_BOMB, afscall_bomb);
  		afs_set_syscall(AFSCALL_SETPAG, afscall_setpag);
  		afs_set_syscall(AFSCALL_GETPAG, afscall_getpag);
+ 		afs_set_syscall(AFSCALL_RESETPAG, afscall_resetpag);
  
  		/*
  		 * make sure that we pin everything
***************
*** 296,299 ****
  {
      return (*kluge_dc_purge)(dev);
  }
- 
--- 296,298 ----

DIFF: OLD,NEW : ./file/osi/osi_pag.c===========================
*** ./file/osi/osi_pag.c.orig	Thu Feb 22 09:25:17 1996
--- /project/dce/build/dce1.2/src/./file/osi/osi_pag.c	Thu Feb 22 10:41:55 1996
***************
*** 1,7 ****
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
--- 1,8 ----
  /*
   * @OSF_COPYRIGHT@
   * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, In
! c.
   * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
   * the full copyright text.
   */
***************
*** 8,32 ****
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.111.2  1996/02/18  23:16:10  marty
!  * 	Update OSF copyright years
!  * 	[1996/02/18  23:02:22  marty]
   *
   * Revision 1.1.111.1  1996/01/31  17:07:44  andi
!  * 	Transarc DFS 1.2 Submission
!  * 	[1996/01/30  20:08:30  andi]
   * 
   * $EndLog$
  */
- /* Copyright (C) 1989, 1994 Transarc Corporation - All rights reserved */
  
! /*
!  * @OSF_COPYRIGHT@
!  * COPYRIGHT NOTICE
!  * Copyright (c) 1990, 1991, 1992, 1993, 1994, 1996 Open Software Foundation, Inc.
!  * ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
!  * the full copyright text.
!  */
  #include <dcedfs/param.h>
  #include <dcedfs/osi.h>
  #include <dcedfs/osi_cred.h>
--- 9,30 ----
  /*
   * HISTORY
   * $Log:	c013346,v $
# Revision 1.2  96/02/22  12:01:44  root
# changed fields: Status  Fixed In Baseline  Affected File(s)   new/changed/deleted note(s) [psn 2/22/96 public]
# 
!  * Revision 1.1.111.3  1996/02/22  16:41:54  root
!  * 	ENDGAME Submissions
!  * 	[1996/02/22  16:24:53  root]
   *
+  * Revision 1.1.111.2  1996/02/18  23:16:10  marty
+  * 	     Update OSF copyright years
+  * 	     [1996/02/18  23:02:22  marty]
+  * 
   * Revision 1.1.111.1  1996/01/31  17:07:44  andi
!  * 	     Transarc DFS 1.2 Submission
!  * 	     [1996/01/30  20:08:30  andi]
   * 
   * $EndLog$
  */
  
! /* Copyright (C) 1989, 1994 Transarc Corporation - All rights reserved */
  #include <dcedfs/param.h>
  #include <dcedfs/osi.h>
  #include <dcedfs/osi_cred.h>
***************
*** 36,42 ****
  #include <pthread.h>
  #endif /* !KERNEL */
  
! RCSID("$Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $")
  
  /*
   * Pags are implemented as follows: the set of groups whose long representation
--- 34,40 ----
  #include <pthread.h>
  #endif /* !KERNEL */
  
! RCSID("$Header: /project/ot/dce/d01/d33/RCS/c013346,v 1.2 96/02/22 12:01:44 root Exp $")
  
  /*
   * Pags are implemented as follows: the set of groups whose long representation
***************
*** 127,133 ****
  }
  
  #if OSI_HAS_CR_PAG
- 
  /*
   * Get the pag value (if any) from the credential struct cred.
   */
--- 125,130 ----
***************
*** 149,154 ****
--- 146,156 ----
      return (0);
  }
  
+ static void
+ osi_DelPagFromCred(osi_cred_t *credp)
+ {
+     credp->cr_pag = 0;
+ }
  #endif /* OSI_HAS_CR_PAG */
  
  /*
***************
*** 201,206 ****
--- 203,220 ----
      credp->cr_groups[ngroups - 1] = pagvalue;
      return (0);
  }
+ 
+ static void
+ osi_DelPagFromCred(osi_cred_t *credp)
+ {
+     int ngroups = osi_GetNGroups(credp);
+ 
+     if (ngroups != 0 && OSI_IS_PAG(credp->cr_groups[ngroups - 1])) {
+ 	ngroups--;
+ 	credp->cr_groups[ngroups] = 0;
+ 	osi_SetNGroups(credp, ngroups);
+     }
+ }
  #endif /* !OSI_HAS_CR_PAG */
  
  /*
***************
*** 221,227 ****
  	return OSI_NOPAG;
  
      g0 = credp->cr_groups[0];
!     g1 = credp->cr_groups[1]; 
  
      if (g0 < AFS_MINGROUP || g0 > AFS_MAXGROUP ||
  	g1 < AFS_MINGROUP || g1 > AFS_MAXGROUP)
--- 235,241 ----
  	return OSI_NOPAG;
  
      g0 = credp->cr_groups[0];
!     g1 = credp->cr_groups[1];
  
      if (g0 < AFS_MINGROUP || g0 > AFS_MAXGROUP ||
  	g1 < AFS_MINGROUP || g1 > AFS_MAXGROUP)
***************
*** 307,313 ****
  	osi_Wait_r(1000 * (soonest - now), (struct osi_WaitHandle *)0, 0);
  	osi_mutex_enter(&osi_pagLock);
  	now = osi_Time();
!  	soonest = osi_firstPagTime +
  			osi_setpagInterval * (OSI_MAXPAG - osi_pagCounter);
  
      }
--- 321,327 ----
  	osi_Wait_r(1000 * (soonest - now), (struct osi_WaitHandle *)0, 0);
  	osi_mutex_enter(&osi_pagLock);
  	now = osi_Time();
! 	soonest = osi_firstPagTime +
  			osi_setpagInterval * (OSI_MAXPAG - osi_pagCounter);
  
      }
***************
*** 328,338 ****
  /* ARGSUSED */
  long
  afscall_getpag(
!   long parm,
!   long parm2,
!   long parm3,
!   long parm4,
!   long parm5,
    int *rval)
  {
      long pag =  osi_GetPagFromCred(osi_getucred());
--- 342,352 ----
  /* ARGSUSED */
  long
  afscall_getpag(
!   long unused,
!   long unused2,
!   long unused3,
!   long unused4,
!   long unused5,
    int *rval)
  {
      long pag =  osi_GetPagFromCred(osi_getucred());
***************
*** 341,347 ****
  	return (ENOENT);
      } else {
  	*rval = pag;
! 	return(0);
      }
  }
  #endif /* KERNEL */
--- 355,384 ----
  	return (ENOENT);
      } else {
  	*rval = pag;
! 	return (0);
      }
+ }
+ 
+ long
+ afscall_resetpag(
+   long unused,
+   long unused2,
+   long unused3,
+   long unused4,
+   long unused5,
+   int *rval)
+ {
+     osi_cred_t *credp;
+     struct proc *p = osi_curproc();
+ 
+     osi_MakePreemptionRight();
+     osi_pcred_lock(p);
+     credp = crcopy(osi_getucred());
+     osi_DelPagFromCred(credp);
+     osi_setucred(credp);
+     osi_pcred_unlock(p);
+     osi_set_thread_creds(p, credp);
+     osi_UnmakePreemptionRight();
+     return (0);
  }
  #endif /* KERNEL */



CR Number                     : 13281
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : rpc/kruntime
Short Description             : new file for macro use
Reported Date                 : 1/17/96
Found in Baseline             : 1.2
Found Date                    : 1/17/96
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : kruntime/HP800 and kruntime/sec_authn_krpc.c
Sensitivity                   : public

[1/17/96 public]
/dce/src/rpc/kruntime/sec_authn_krpc.c include ifdef'd HPUX
krcp_osi_mach.h

/dce/src/rpc/kruntime/HP800/comsoc_sys.c, krpc_helper_mach.c,
pthread_sys.c, uuidsys.c include krpc_osi_mach.h

the HPUX product release for our 10.x product introduced
several changes that would require ifdef'g these files src
code

it was determined better to introduce a 'new' file containg
macros with appropriate ifdefs for specific product release
changes

this new file is needed to build the HPUX rpc/kruntime files

[01/22/96 public]

Fixed in DCE 1.2.1 release
Closed



CR Number                     : 13273
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : DFS Submit Request
Reported Date                 : 1/4/96
Found in Baseline             : 1.2.1
Found Date                    : 1/4/96
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/file/*
Sensitivity                   : public

[1/4/96 public]




1.      Name of deliverable(s) to be submitted:
	dfs

2.      Was source code submitted via ODE? (Y or N):
        Y

3.      Name of file(s) changed/added/obsoleted (NOTE: a single filename
        is appropriate if the entire component is being submitted.): 
        src/file/*

4.      Location of build logs from the pre-submission build of this code,
        using the 1.2.1 tree on the AIX RP via ODE: 
        (Comments):

5.      Location of objects from the pre-submission build of this code:
        (Comments):

6.      Location of tests and test logs used to verify this code on the AIX RP:
        /afs/dce.osf.org/project/dce/transarc/LOGS

7.      Types of test(s) used to verify this code on the AIX RP (FVT,
        regression, System, CHO):
        All of the above, plus interoperability tests against DCE/DFS 1.1.

8.      ID number, severity and priority of each OT associated with the build
        and test of this submission: 29 defects
        
CRnum Compo Status Resp_Eng ReptDate S P Short_Description
13300 dfs   open   gait@tra 02/06/96 D 3 upgrade DFS service msgs
13282 dfs   open   ota      01/17/96 C 2 not zero-filling unused file blocks
13202 dfs   open   cfe      11/15/95 D 3 Thinkos in provider_version comments
13149 dfs   open            10/10/95 C 2 fshs_InqContext() has old redundant cruft hanging around
13145 dfs   open            10/05/95 C 2 Improper parameter passing in px_invokeserver()
12839 dfs   open            04/11/95 B 2 inconsistant acl include files
12764 dfs   open            12/07/94 E 3 Exceeding Maximum Aggregate ID in fts crfldbentry command is ungracefu
12763 dfs   open            12/06/94 E 2 fts aggrinfo output format bad, and incorrect info
12761 dfs   open            12/06/94 B 2 DFS does not return ENOSPC
12745 dfs   open            11/10/94 C 1 BosConfig file is invalid
12724 dfs   defer  rsarbo   10/25/94 B 2 Using osi_err[En|De]code()
12686 dfs   defer  rsarbo   10/20/94 C 2 Remove superfluous dacl i/f's from libdce
12621 dfs   defer  rsarbo   10/13/94 B 2 functional tests fail
12620 dfs   defer  rsarbo   10/13/94 B 2 bad dfs_logout exit value
12358 dfs   defer  gmd      09/27/94 A 1 dfs.read_write_all test fails w/ Conn timeouts
12130 dfs   defer  rsarbo   09/12/94 B 2 dataCacheBytesReadFromCache always zero
11490 dfs   defer  ruby     07/29/94 B 2 segment fault if filename w/o fullpn
11353 dfs   defer  other    07/18/94 C 3 DFS_ONLY has BUILD_DCE depended on security
10952 dfs   defer  kissel@a 06/15/94 A 3 possible runtime error
10907 dfs   defer  mckeen   06/08/94 B 2 dfs.unconfig should do a few more things ...
10258 dfs   defer  rsarbo   03/31/94 B 3 Need TRY/CATCH blocks for non-fault/comm_status exceptions
10225 dfs   defer  mckeen   03/24/94 A 2 porting bug in HPUX igetinode (OSI)
 9760 dfs   defer  other    01/17/94 C 2 there are a bunch of compiler messages that were never logged
 9679 dfs   defer  gmd      01/04/94 D 3 error message during auto-restart
 9632 dfs   defer  mckeen   12/20/93 A 2 token leak during cho
 9303 dfs   defer  mckeen   11/01/93 C 3 dfs_config is missing log messages.
 9143 dfs   defer  bwl@tran 10/15/93 C 2 not enough bits in argument to AG_DETACH
 9134 dfs   defer  rsarbo   10/15/93 C 2 problems with /.:
 9070 dfs   defer  kissel@a 10/07/93 C 2 can't build libafs in a sandbox without fsint.klib

9.      Location of Test Coverage analysis:
        Not Applicable

10.     Location of Defect Density ratio (for code only):



CR Number                     : 13140
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Fixed quotes in change markers
Reported Date                 : 10/4/95
Found in Baseline             : 1.1
Found Date                    : 10/4/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : See list in detailed comments
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/4/95 public]
Fixed quotes in change markers and made minor edits.
Affected file list:
src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
src/dce_books/dfs_admin_gdref/gd/backrest.gpsml
src/dce_books/dfs_admin_gdref/gd/backup.gpsml
src/dce_books/dfs_admin_gdref/gd/cachemgr.gpsml
src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml
src/dce_books/dfs_admin_gdref/gd/issues.gpsml
src/dce_books/dfs_admin_gdref/gd/overview.gpsml
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_dump.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoredisk.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoreft.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_restorefamily.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_status.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/cm.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/cm_getpreferences.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/cm_setpreferences.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/fts_release.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/fts_update.8dfs
src/dce_books/dfs_admin_gdref/ref/man4dfs/conf_tape_device.4dfs
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/4/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[10/12/95 public]

Marked closed. I visually inspected the relevant macro strings
(though the spots before my eyes as I came to the end of this
task may have interfered with the efficiency of my optics).



CR Number                     : 13128
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : more on fts release -wait
Reported Date                 : 9/19/95
Found in Baseline             : 1.1
Found Date                    : 9/19/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : /src/dce_books/dfs_admin_gdref/ref/man8dfs/fts_release.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/19/95 public]
Further clarification of -wait option for fts release command.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/11/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[10/12/95 public]

Marked closed. But note that this work was apparently done
silently under CR 13109 (the original "-wait" CR), and is
not separately marked in the file.



CR Number                     : 13112
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Add bak restoreftfamily command
Reported Date                 : 9/9/95
Found in Baseline             : 1.1
Found Date                    : 9/9/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : See list in detailed description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/9/95 public]
The DFS Backup System now includes an additional bak restoreftfamily
command.  The command allows an administrator to restore data by
fileset family.
Affected files are:
src/dce_books/dfs_admin_gdref/gd/backrest.gpsml
src/dce_books/dfs_admin_gdref/gd/overview.gpsml
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_dump.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoredisk.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoreft.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoreftfamily.8dfs
src/dce_books/dfs_admin_gdref/ref/man8dfs/bak_status.8dfs 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/20/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[10/4/95 public]

[10/12/95 public]

Marked closed.



CR Number                     : 13111
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Document added support for "sparse files"
Reported Date                 : 9/9/95
Found in Baseline             : 1.1
Found Date                    : 9/9/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : /src/dce_books/dfs_admin_gdref/gd/overview.gpsml /src/dce_books/dfs_admin_gdref/gd/backup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/9/95 public]
DFS now supports file dumps of sparse files.  Sparse files are
database files which remain compact by not storing "empty records."
In previous versions of DFS, a file dump of a sparse file would expand
by adding actual empty records. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/16/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[10/4/95 public]
Fixed list of affected files
Changed Affected File from `/src/dce_books/dfs_admin_gdref/gd/overview.gpsml' 
 to `/src/dce_books/dfs_admin_gdref/gd/overview.gpsml 
 /src/dce_books/dfs_admin_gdref/gd/backup.gpsml'

[10/12/95 public]

Marked closed.



CR Number                     : 13109
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Document -wait option for fts_release command
Reported Date                 : 9/8/95
Found in Baseline             : 1.1
Found Date                    : 9/8/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : /src/dce_books/dfs_admin_gdref/ref/man8dfs/fts_release.8dfs src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/8/95 public]
The -wait option was added to ensure that a read/write server isn't
shut down before all read-only replicas are updated.  
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/18/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[10/4/95 public]

[10/12/95 public]

Marked closed.



CR Number                     : 13108
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Clean up graphics in DFS manual
Reported Date                 : 9/8/95
Found in Baseline             : 1.1
Found Date                    : 9/8/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : /src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml /src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/8/95 public]
Replace existing graphics with "cleaner" figures.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/16/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[9/16/95 public]

[10/3/95 public]

[10/12/95 public]

Marked closed.



CR Number                     : 13107
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : clarify how DFS honors shared-file locks
Reported Date                 : 9/8/95
Found in Baseline             : 1.1
Found Date                    : 9/8/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/dce_books/dfs_admin_gdref/ftavail.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/8/95 public]
Clarify how shared-file locks are handled in DFS (DFS ignores
read-only file locks when updating read-only data).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/18/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[10/4/95 public]

[10/12/95 public]

Marked closed.



CR Number                     : 13096
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Add description of concurrent replication of
Reported Date                 : 9/5/95
Found in Baseline             : 1.1
Found Date                    : 9/5/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/5/95 public]
Add information detailing that read-only fileset replication is now
parallel (not serial).  This is a minor documentation update.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/20/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1' 
Filled in Affected File with `src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml'

[10/3/95 public]
Fixed list of affected files
Changed Interest List CC from `jeff@transarc.com' to `gray@transarc.com'

[10/12/95 public]

Marked closed.



CR Number                     : 13093
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Add description of server preferences
Reported Date                 : 9/1/95
Found in Baseline             : 1.1
Found Date                    : 9/1/95
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2.1
Fixed In Baseline             : 1.2.1
Affected File(s)              : src/dce_books/dfs_admin_gdref/gd/cachemgr.gpsml
preferences.8dfs src/dce_books/dfs_admin_gdref/ref/mand8dfs/cm_getpreferences.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[9/1/95 public]
Add the ability to set a Cache Manager's preferences for the File
Server machines from which to access read-only fileset replicas.  The
Bulk of the change comes in the form of two new commands in the cm
command suite:  getpreferences and setpreferences.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/16/95 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.2.1'

[9/16/95 public]
Changed Status from `verified' to `fix'

[10/3/95 public]
Modifed list of affected files
Changed Affected File from `src/dce_books/dfs_admin_gdref/gd/cachemgr.gpsml 
 src/dce_books/dfs_admin_gdref/gd/issues.gpsml 
 src/dce_books/dfs_admin_gdref/ref/mand8dfs/cm.8dfs 
 src/dce_books/dfs_admin_gdref/ref/mand8dfs/cm_set' to 
 `src/dce_books/dfs_admin_gdref/gd/cachemgr.gpsml'

[10/11/95 public]
Changed Defect or Enhancement? from `def' to `enh' 
Changed Interest List CC from `jeff@transarc.com' to 
 `anne_jane_gray@transarc.com' 
Changed Status from `fix' to `verified'

[10/12/95 public]

Marked closed.



CR Number                     : 12992
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Deadlock due to race in CM
Reported Date                 : 8/2/95
Found in Baseline             : 1.0.3
Found Date                    : 8/2/95
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 1.2.1
Affected File(s)              : file/cm/SUNOS5/cm_sun5vm.c
Sensitivity                   : public

[8/2/95 public]

After a few hours or running the vnode recycling tests on two machines
the following deadlock scenario occured:

1. A call to nfsrdwr() page faulted in uiomove and called cm_getpage
which went to sleep in cm_bkgWait()

2. The cm_bkgDeamon was blocked waiting for the cm_dcachelock

3. The process holding the cm_dcachelock was blocked in a call to
ufs_write that pagefaulted while doing uimove waiting for the as->a_lock
(to complete the as_fault() call) which was held by 1.

The relative thread traces follow:

1. holds the as->a_lock for as = 0xf010fe38. is waiting for 2 to finish

cv_wait(0xfc63d324,0xfc63d324,0xe53520ff,0xfc63d7ec,0xfca1eea0,0xfc53abe4)
osi_SleepW(0xfc63d324,0xfc53abe0,0xfc53abec,0x4,0xfc53abec,0x698) + dc
cm_bkgWait(0xfc53ae8c,0xfca96814,0x3,0xfc53ae8c,0xfc53abe0,0xfc53aeab) + 2c
cm_getpage(0xfca96814,0x1000,0x0,0xfc53ae8c,0xfcaa768c,0x2) + 37c
ns5_getpage(0xfca96814,0x0,0x10000,0x1000,0xf059537c,0xf0595370) + 50
xglue_getpage(0xfca96814,0x0,0x10000,0x1000,0xf059537c,0xf0595370) + b4
segmap_fault(0xfc06f000,0xfc10f348,0xf18aa000,0x1000,0x0,0x2) + f8
as_fault(0xfc06f000,0xf010fe38,0xfc10f348,0x1000,0x0,0x2) + 1cc
pagefault(0xf18aa000,0x0,0x2,0x1,0xf18aa000,0xf010fe38) + 34
trap(0x9,0xf059554c,0xf010fe04,0x3a6,0x2,0x5) + 718
fault(0x37322037,0x33203734,0x20373520,0x37360a09,0x1,0x0) + 84
bcopy(0xef703000,0xf18aa000,0x2000,0x100,0x0,0xb988) + 268
uiomove(0xf18aa000,0x2000,0x2000,0xf059582c,0xf0595800,0xfc10f350) + 98
cm_nfsrdwr(0xf18ac000,0xf0595800,0x1,0xf18aa000,0x2000,0x0) + 528
ns5_write(0xfca96814,0xf0595800,0x0,0xfcab9d00,0xfc5f7bd4,0xfc1bc480) + 20
xglue_write(0xfca96814,0xf0595800,0x0,0xfcab9d00,0x1,0x0) + c8
rw(0x1b988,0xf0595918,0x2,0x0,0x0,0xfca96814) + 28c
syscall(0xf00df354) + 534

2. is waiting for the cm_dcachelock

mutex_enter(0xfc53902c,0xfc539031,0xfc539032,0x82d3f6ff,0xf05a7ec0,0xf00fa828)
lock_ObtainWrite(0xfc539020,0xfc628778,0xfc53902c,0xfc63dfb8,0x0,0x1) + c4
cm_ReserveBlocks(0x11,0xfcaa768c,0x0,0xfc53902c,0x0,0xfc539020) + 94
cm_GetDOnLine(0xfca96814,0xfcaa768c,0xfcaa769c,0xfca968c0,0xfca968b4,0xfcaa76b0)
+ 310
cm_GetDLock(0xfca96814,0xfcaa768c,0x1,0x2,0xf04fb610,0x2) + 228
bkg_Prefetch(0xfc53ae8c,0x29c9c007,0xfca968b4,0x1,0xfca96814,0xfcaa768c) + 48
cm_bkgDaemon(0x1,0x0,0xfffffff6,0xfc53abec,0xfc53abe0,0xfc53ae8c) + 200
afscall_cm(0x2,0x0,0x0,0xfc53810c,0xfc538100,0xfc527048) + 4bc
afs_syscall(0xf04fbe90,0x0,0xf04fb9d8,0xf04fb918,0xfc621f60,0xfc4a0800) + f4
syscall(0xf00df354) + 534

3. has the cm_dcachelock. Is waiting for the as->a_lock for as = 0xf010fe38

rw_enter(0xf010fe5a,0xfca1ee20,0xf00f69f0,0xf010fe58,0xf010fe55,0xf010fe56)
as_fault(0xfc06f000,0xf010fe38,0xf161d001,0x1,0x0,0x2) + 118
pagefault(0xf161d000,0x0,0x2,0x1,0xf161d000,0xf010fe38) + 34
trap(0x9,0xf05a7974,0xf010fe04,0x3a7,0x2,0xafb8) + 718
fault(0x3,0x1,0x7fc0,0x1b4754e0,0x200,0xf0013404) + 84
bcopy(0xb48957c,0xf161d000,0x0,0x10,0x0,0x58) + 2bc
uiomove(0xf161cfb8,0x58,0x58,0xf05a7c08,0xf05a7be0,0xf161cfb8) + d0
rwip(0xfc1af000,0xf05a7be0,0x1,0x0,0x58,0x1) + 498
ufs_write(0xfc1af008,0xf05a7be0,0x0,0xfc117400,0x0,0xfc1af000) + 114
vn_rdwr(0x1,0xfc1af008,0xfcaa6534,0x1,0xafb8,0x1) + cc
osi_rdwr(0x1,0xfc1af008,0xfcaa6534,0x58,0xafb8,0x1) + 40
osi_Write(0xfca8ac0c,0xfcaa6534,0x58,0xf00d07c0,0x0,0xafb8) + 40
cm_WriteDCache(0xfcaa650c,0x1,0x7fc0,0xfc63dfb8,0x0,0x1) + 64
WriteThroughDSlots(0x4e2,0xfc539020,0x1,0xfc2c780c,0x7fc,0x1ff) + a8
tpq_HelperThread(0xfc2c780c,0x0,0xfca1b60c,0x0,0x0,0xfca19f8c) + 350

[8/3/95 public]

Delta: kazar-db4552-two-stage-copy-for-rdwr
Change: file/cm/SUNOS5/cm_sun5vm.c from 1.64 to 1.68
*** file/cm/SUNOS5/cm_sun5vm.c
--- 1.68	1993/10/28 21:09:21
***************
*** 769,774 ****
--- 769,775 ----
      long fileBase, size;
      long pageBase;
      long extraResid;
+     char *pinnedBufferp;
      long psize;
      char *paddr;
      register long tsize;
***************
*** 861,866 ****
--- 862,893 ----
  	return code;
      }
  
+     /* here's a hack done for a truly disgusting reason.  It turns
+      * out that we can either call segmap_fault below with either
+      * F_SOFTLOCK or F_INVAL.  If we call it with F_INVAL, then
+      * the pages aren't pinned, and we could lose a page before
+      * doing the uiomove call, and pagefault.  If we pagefault
+      * here, we end up calling cm_getpage while holding the kas.a_lock,
+      * and when we call UFS from cm_getpage, it may pagefault again,
+      * and relock kas.a_lock.
+      *
+      * On the other hand, if we call segmap_fault with F_SOFTLOCK,
+      * then if we're writing from a mapped DFS file, the uiomove
+      * can fault, not because the target pages are paged out (they're
+      * pinned), but because the *source* pages in the user space process
+      * are paged out.  Then we end up calling cm_getpage while some other
+      * DFS pages for (perhaps) a different vnode are locked.  This causes
+      * obscure deadlocks.
+      * 
+      * Our hack is to copy to/from user space w/o holding the CM locks,
+      * and then we copy to/from pinned cm pages (F_SOFTLOCK) from our
+      * pinned kernel buffer.  This represents an extra copy, which hopefully
+      * we can get rid of by Solaris 2.4 or so.
+      */
+     pinnedBufferp = (char *) osi_AllocBufferSpace();
+     /* compile-time check for buffer space */
+     if (osi_BUFFERSIZE < MAXBSIZE) panic("cm_rdwr: buffer too small");
+ 
      while (1) {
  	/* compute the amount of data to move into this block,
  	 * based on auiop->osi_uio_resid.  Note that we copy data in
***************
*** 919,975 ****
  	/* drop lock, since uiomove call could fault and call getpage */
  	lock_ReleaseWrite(&scp->llock);
  	osi_RestorePreemption(0);
  	data = segmap_getmap(segkmap, (struct vnode *) scp, pageBase);
  
  	/* find covering set of pages, to fault them in */
  	paddr = (char *) (((long)data+pageOffset) & PAGEMASK);
  	psize = (((long)data+pageOffset+tsize+PAGEOFFSET) & PAGEMASK)
  	    - (long)paddr;
! 	code = segmap_fault(kas.a_hat, segkmap, paddr, psize, F_INVAL, mode);
  	if (code == 0) {
  	    if (arw == UIO_READ) {
! 		code = uiomove(data+pageOffset, tsize, arw, auiop);
  	    } else {
- #ifdef notdef
- 		caddr_t ap = data+pageOffset;
- 		u_int cnt = tsize, nbytes;
- 		int pc, toff;
- 		do {
- 		    toff = auiop->uio_offset;
- 		    nbytes = MIN((PAGESIZE - ((u_int)ap & PAGEOFFSET)), cnt);
- 		    pc = 0;
- 		    /* For performance reasons if we're going to write
- 		     * to a new page create it here so that uiomove won't
- 		     * call our cm_getpage set of routines and save some
- 		     * cycles.
- 		     */
- 		    if (DO_PAGECREATE && 	/* XXX */
- 			(!((u_int)ap & PAGEOFFSET) &&
- 			 (nbytes == PAGESIZE)
- 			 || ((toff+nbytes) >= origLength(?)))) {
- 			segmap_pagecreate(segkmap, ap, nbytes, 0);
- 			pc = 1;
- 		    }
- 		    code = uiomove(ap, nbytes, UIO_WRITE, auiop);
- 		    nbytes = auiop->uio_offset - toff;
- 		    ap += nbytes;
- 		    cnt -= nbytes;
- 		    if (pc && ((auiop->uio_offset & PAGEOFFSET) || !nbytes)) {
- 			kzero(ap, PAGESIZE-nbytes);
- 		    }
- 		} while (cnt > 0 && !code);
- #else /* really do this */
  		/* no need to create one at a time if segmap_fault
  		 * already created the pages.
  		 */
! 		code = uiomove(data+pageOffset, tsize, arw, auiop);
! #endif /* not notdef */
  	    }
! 	    /* now release the pages */
! #ifdef notdef
! 	    /* don't do this when using F_INVAL in above segmap_fault */
  	    segmap_fault(kas.a_hat, segkmap, paddr, psize, F_SOFTUNLOCK, mode);
- #endif
  	}
  
  	if (code == 0) {
--- 946,977 ----
  	/* drop lock, since uiomove call could fault and call getpage */
  	lock_ReleaseWrite(&scp->llock);
  	osi_RestorePreemption(0);
+ 	if (arw == UIO_WRITE)
+ 	    code = uiomove(pinnedBufferp, tsize, arw, auiop);
+ 	else code = 0;
  	data = segmap_getmap(segkmap, (struct vnode *) scp, pageBase);
  
  	/* find covering set of pages, to fault them in */
  	paddr = (char *) (((long)data+pageOffset) & PAGEMASK);
  	psize = (((long)data+pageOffset+tsize+PAGEOFFSET) & PAGEMASK)
  	    - (long)paddr;
! 	if (code == 0)
! 	    code = segmap_fault(kas.a_hat, segkmap, paddr, psize,
! 				F_SOFTLOCK, mode);
  	if (code == 0) {
  	    if (arw == UIO_READ) {
! 		bcopy(data+pageOffset, pinnedBufferp, tsize);
  	    } else {
  		/* no need to create one at a time if segmap_fault
  		 * already created the pages.
  		 */
! 		bcopy(pinnedBufferp, data+pageOffset, tsize);
  	    }
! 
! 	    /* now release the pages; don't do this when using
! 	     * F_INVAL in above segmap_fault
! 	     */
  	    segmap_fault(kas.a_hat, segkmap, paddr, psize, F_SOFTUNLOCK, mode);
  	}
  
  	if (code == 0) {
***************
*** 978,988 ****
--- 980,994 ----
  	else {
  	    segmap_release(segkmap, data, 0);
  	}
+ 	if (arw == UIO_READ)
+ 	    code = uiomove(pinnedBufferp, tsize, arw, auiop);
  	osi_PreemptionOff();
  	lock_ObtainWrite(&scp->llock);
  	if (code) break;
      }	/* while (1) */
      lock_ReleaseWrite(&scp->llock);
+ 
+     osi_FreeBufferSpace((struct osi_buffer *) pinnedBufferp);
  
      /* if things worked, add in as remaining in request any bytes
       * we didn't write due to file size ulimit.

[biyani 12/21/95 public] 

fixed in DCE 1.2.1



CR Number                     : 12618
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : UHYP:implement S12Y in remaining DFS servers
Reported Date                 : 10/13/94
Found in Baseline             : 1.1
Found Date                    : 10/13/94
Severity                      : B
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.2
Fixed In Baseline             : 1.2
Affected File(s)              : bss.sams,lsv.sams
Sensitivity                   : public

[2/23/96 public]
I've changed this defect to enhancement upon request from Transarc.

[10/13/94 public]
A placeholder to track the remaining work on DFS S12Y which will
go into the unintegrated tree and include bosserver, flserver, and
ubik.  I'll leave it to Susan to insert the UHYP designation.

[10/13/94 public]

Designated hypercritical.

[10/20/94 public]

The following files have been submitted to UNINTEGRATED for the dfs bosserver
subcomponent:
  Date: 10/20/94; Time: 15:29
  Number of files: 14; Defect number: 12618.
  Set name: sasala_svc2; Sandbox: svc2
  List of files and revisions:
./file/Makefile 1.1.32.1        1.1.27.10
./file/bosserver/Makefile       1.1.102.1       1.1.100.3
./file/bosserver/bbos_bnode.c   1.1.83.1        1.1.81.3
./file/bosserver/bbos_util.c    1.1.66.1        1.1.64.3
./file/bosserver/bossvr_afsFiles.c      1.1.47.1        1.1.45.3
./file/bosserver/bossvr_afsconf.c       1.1.11.1        1.1.9.2
./file/bosserver/bossvr_bnode_cronops.c 1.1.69.1        1.1.67.3
./file/bosserver/bossvr_main.c  1.1.23.1        1.1.18.3
./file/bosserver/bossvr_ncs_procs.c     1.1.23.1        1.1.18.3
./file/bosserver/bossvr_thread_bnodeTimeout.c   1.1.92.1        1.1.90.3
./file/bosserver/bossvr_thread_childWatch.c     1.1.103.1       1.1.101.3
./file/bosserver/bossvr_thread_reBossvrWatch.c  1.1.74.1        1.1.72.3
./file/bosserver/bss.sams       1.1.2.1 1.1
./file/bosserver/bss_svc.c      1.1.2.1 1.1

[10/26/94 public]
The following files have been added to UNINTEGRATED for the flserver sub-
component:
  Submitted by Ronald Sasala; User name: sasala
  Date: 10/26/94; Time: 11:45
  Number of files: 9; Defect number: 12618.
  Set name: sasala_svc2; Sandbox: svc2
  List of files and revisions:
./file/Makefile	1.1.32.2	1.1.32.1
./file/flserver/Makefile	1.1.24.1	1.1.22.4
./file/flserver/flc.c	1.1.71.1	1.1.69.3
./file/flserver/flclient.c	1.1.73.1	1.1.70.3
./file/flserver/flprocs.c	1.1.94.1	1.1.92.3
./file/flserver/flserver.c	1.1.87.1	1.1.84.3
./file/flserver/flutils.c	1.1.67.1	1.1.65.3
./file/flserver/lsv.sams	1.1.2.1	1.1
./file/flserver/lsv_svc.c	1.1.2.1	1.1

[4/24/96 public]
This work was incorporated in DFS during the 1.1 timeframe by Craig
Everhart at Transarc. About time to close the ot. The code is
included in DFS 1.2.1.



CR Number                     : 12575
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : unit tests
Short Description             : HYP: Enhancements to make test_acl.epi use delegation
Reported Date                 : 10/11/94
Found in Baseline             : 1.1b20
Found Date                    : 10/11/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b22
Affected File(s)              : file/security/dacl_lfs/testacl_drv.ksh
Sensitivity                   : public

[10/11/94 public]

There have been some enhancements to test_acl.epi to allow it to test
delegation.  Although this is not part of the product, it should be
submitted so that it does not get lost.



CR Number                     : 12569
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Correct description of scout -server option
Reported Date                 : 10/9/94
Found in Baseline             : 1.0.3
Found Date                    : 10/9/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/9/94 public]
The scout documentation currently states that only a full DCE pathname of a
File Server machine can be specified with the -server option of the command.
However, the option also accepts the hostname (which can be abbreviated) or
the IP address of the machine.  The specification you employ also impacts
your use of the command's -basename option.  The following files are affected:
 
./src/dce_books/dfs_admin_gdref/gd/scout.gpsml
./src/dce_books/dfs_admin_gdref/ref/man8dfs/scout.8dfs
 
This is a relatively simple fix that I can incorporate well before the freeze
for 1.1.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/11/94 public]
I corrected the desriptions of the -server and -basename options in the scout
documentation.  The changes were verified by Craig Everhart.  This one can be
closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1doc'

[10/27/94 public]
Closed bug.



CR Number                     : 12567
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsgw
Short Description             : HYP: dfs_login does not build
Reported Date                 : 10/7/94
Found in Baseline             : 1.1b21
Found Date                    : 10/7/94
Severity                      : B
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b22
Affected File(s)              : file/gateway/dfs_login/dfs_login.c
Sensitivity                   : public

[10/7/94 public]

dfs_login is a program that is in the source tree but not built on a
regular basis.  It is built by copying the sources into a vanila
kerberos source tree and compiling it there.  The last drop of this
code from HP introduced a syntax error which has gone undetected until
now.  It is a simple undefined variable.  The fix is simple.



CR Number                     : 12535
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Incorporate dcecp commands
Reported Date                 : 10/6/94
Found in Baseline             : 1.0.3
Found Date                    : 10/6/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/6/94 public]
References to the existing control programs (e.g., acl_edit and rgy_edit)
need to be updated to refer to dcecp across the DFS Administration Guide
and Reference.  This defect pertains to that change.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/14/94 public]
I updated all of the necessary DFS doc files to replace the old DCE control
programs with their dcecp alternatives.  The following files were affected:
 
./src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
./src/dce_books/dfs_admin_gdref/gd/adminkey.gpsml
./src/dce_books/dfs_admin_gdref/gd/backup.gpsml
./src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./src/dce_books/dfs_admin_gdref/gd/ftmgmt.gpsml
./src/dce_books/dfs_admin_gdref/gd/issues.gpsml
./src/dce_books/dfs_admin_gdref/gd/overview.gpsml
./src/dce_books/dfs_admin_gdref/ref/man8dfs/bos.8dfs
./src/dce_books/dfs_admin_gdref/ref/man8dfs/bos_addkey.8dfs
./src/dce_books/dfs_admin_gdref/ref/man8dfs/bos_gckeys.8dfs
./src/dce_books/dfs_admin_gdref/ref/man8dfs/bos_genkey.8dfs
./src/dce_books/dfs_admin_gdref/ref/man8dfs/bos_lskeys.8dfs
./src/dce_books/dfs_admin_gdref/ref/man8dfs/bos_rmkey.8dfs
 
I verified all of the changes at least twice.  This one can be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1doc'

[10/27/94 public]
Closed bug.



CR Number                     : 12452
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : HYP:fts lsquota output missing newline
Reported Date                 : 9/30/94
Found in Baseline             : 1.1b20
Found Date                    : 9/30/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b20
Affected File(s)              : ./file/userInt/fts/fts.sams
Sensitivity                   : public

[9/30/94 public]

BUILD: bl-20.1

fts lsquota output should be on 2 lines, not 1. The fts tests that
rely on the 2 line output fail with:

Name: bad number

Logged as an A0 since it causes functional tests to fail ...

[10/3/94 public]
Added HYP: to short description to designate as on the hypercritical list

[10/3/94 public]
This bug is believed to be a dup.  If a change is required, it's low
risk.  Will be in by 10/4.

[10/4/94 public]

Added newline to string is sams file.



CR Number                     : 12352
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa,rs6000
S/W Ref Platform              : hpux,aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : No IDIR for dfscmd.cat
Reported Date                 : 9/27/94
Found in Baseline             : 1.1
Found Date                    : 9/27/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b21
Affected File(s)              : ./file/episode/salvage/sal.sams ./file/ftutil/ftu.sams ./file/tools/cmd/Makefile ./file/tools/cmd/cmd.sams
Sensitivity                   : public

[9/27/94 public]

The file dfscmd.cat is being installed in

	intall/{hp800,rios}/opt/dce1.1/_MISSING_IDIR_

This is in bl19 as well as last night's build

[9/27/94 public]
The old cmd.sams file needs to be defuncted in this directory
as well.  Re-assigned to ruby.



CR Number                     : 12328
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : svc
Short Description             : fts output w/ extra newlines
Reported Date                 : 9/23/94
Found in Baseline             : 1.1b18
Found Date                    : 9/23/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b19
Affected File(s)              : 
	./file/userInt/fts/fts.sams
	./file/userInt/fts/volc_main.c
	./file/userInt/fts/volc_queue.c
	./file/userInt/fts/volc_vldbsubr.c
	./file/userInt/fts/volc_misc.c
	./file/userInt/fts/volc_vldbint.c
	./file/userInt/fts/volc_volsint.c
Sensitivity                   : public

[9/23/94 public]

With bl-18.2, you get:

root@cobbler> fts lsheader -server cobbler
Total filesets on server cobbler aggregate m1.aggr1 (id 1): 1
root.dfs                 0,,1 
RW      9 K alloc
      9 K quota 
      On-line

Total filesets on-line 1; total off-line 0; total busy 0

There are no filesets on aggregate m1.aggr2 (id 2) of server cobbler.

Total filesets on server cobbler aggregate /u0 (id 3): 1
/dev/lv00                0,,10 
RW  74516 K alloc
74516 K quota 
On-line

Total filesets on-line 1; total off-line 0; total busy 0

Total number of filesets on server cobbler: 2

Used to look a lot nicer ...

Actually, not just fts lsheader, fts lsfldb doesn't look so good either:

cobbler.u0  
        readWrite   ID 0,,10 
 valid
        readOnly    ID 0,,11 
 invalid
        backup      ID 0,,12 
 invalid
number of sites: 1
flags                aggr     siteAgeprincipal owner          
Y@    XK?H
cobbler.osf.org     RW       /u0     0:00:00 hosts/cobbler  
<nil>               

largo.u0  
        readWrite   ID 0,,4 
 valid
        readOnly    ID 0,,5 
 invalid
        backup      ID 0,,6 
 invalid
number of sites: 1
flags                aggr     siteAgeprincipal owner          
Y@    XK?H
largo.osf.org       RW       /u0     0:00:00 hosts/largo    
<nil>               

root.dfs  
        readWrite   ID 0,,1 
 valid
        readOnly    ID 0,,2 
 invalid
        backup      ID 0,,3 
 invalid
number of sites: 1
flags                aggr     siteAgeprincipal owner          
Y@    XK?H
cobbler.osf.org     RW       m1.aggr1 0:00:00 hosts/cobbler  
<nil>               

spanky.usr  
        readWrite   ID 0,,7 
 valid
        readOnly    ID 0,,8 
 invalid
        backup      ID 0,,9 
 invalid
number of sites: 1
flags                aggr     siteAgeprincipal owner          
Y@    XK?H
spanky.osf.org      RW       /usr/users 0:00:00 hosts/spanky   
<nil>               
----------------------
Total FLDB entries that were successfully enumerated: 4 (0 failed; 0 wrong aggr type)



CR Number                     : 12311
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 12179
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfsbind spins
Reported Date                 : 9/22/94
Found in Baseline             : 1.1b17
Found Date                    : 9/22/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b20
Affected File(s)              : see CR 12179
Sensitivity                   : public

[9/22/94 public]
 
Moving 9/14 data from CR 11913:
 
Well - dfsbind is awful busy on dce8 - using top, it showed it was using
CPU constantly, sometimes as much as 80% and had logged 408 minutes of
time since the cell was started 48 hours ago).
 
TTY   PID USERNAME  PRI   NI  SIZE  RES  STATE   TIME   %WCPU  %CPU  COMMAND
?     8187 root    241   20 10428K  352K  wait  409:39  85.01  84.86/opt/dceloca
l/bin/dfsbind
 
I used xdb to attach to it to see if I could tell what was going on - can't set
any breakpoints (not built
correctly?) but I can send SEGV signals get a trace and continue! Can
we trust any of this?! Second 2 traces are the same anyway ...
 
 0 _doprnt@libc + 0x0000019c (0x4001bec8, 0x400da138, 0, 0)
 1 vsprintf@libc + 0x00000040 (0x4001bec8, 0x7aff1388, 0, 0)
 2 cma_sprintf@libdce + 0x000001c4 (0x409a64b0, 0x7afb3110, 0x13, 0x40149c24)
 3 asn1_text_time@libdce + 0x0000031c (0x409a64b0, 0x40149a20, 0x1, 0)
 4 asn1_gen_time@libdce + 0x0000003c (0x409a64b0, 0x40149a20, 0, 0)
 5 krb5_timestamp2asn1_field@libd + 0x00000084 (0x4014989c, 0x409a4624, 0x409a46
   0c, 0)
 6 krb5_kdc_req2KDC_REQ_BODY@libd + 0x00000108 (0x40149884, 0x409a4600, 0xb, 0xa)
 7 krb5_encode_generic@libdce + 0x0000008c (0x40149884, 0x401498d4, 0x7af387ea,0x48)
 8 krb5_send_tgs@libdce + 0x000006ac (0, 0x409a3664, 0x40140001, 0x40140002)
 9 krb5_get_cred_via_tgt@libdce + 0x000001b0 (0x401496f0, 0, 0x40140001, 0x40140002)
10 krb5_get_cred_from_kdc@libdce + 0x00000984 (0x40277420, 0x409a3650, 0x40149660, 0x409a3650)
11 krb5_get_credentials@libdce + 0x00000178 (0x10, 0x40277420, 0x409a3650, 0x401495e0)
12 sec_krb_get_cred@libdce + 0x00000268 (0x409a37b0, 0x4099a610, 0x5, 0x2)
13 rpc__krb_get_tkt@libdce + 0x000003d8 (0x409a34e8, 0x5, 0x4099eb60, 0x409a3538)
14 rpc__krb_bnd_set_auth@libdce + 0x00000670 (0x4099eb60, 0x5, 0x409a37b0, 0x2)
15 rpc_binding_set_auth_info@libd + 0x00000448 (0x4054e0e8, 0x4099eb60, 0x5, 0x1)
16 rca_setup_handle@libdce + 0x000005e0 (0x4099fc98, 0x401482d4, 0x401482d4, 0)
17 rca_site_bind_nsi_cell_if@libd + 0x000001dc (0x4099fc98, 0x40999a70, 0x7afa7920, 0x401482d4)
18 rca_site_bind_nsi_cell@libdce + 0x000000d0 (0x4099fc98, 0x40999a70, 0x401482d4, 0)
19 rca_nsi_binding_import@libdce + 0x000001ec (0x4099fc98, 0x401482d4, 0x401487a8, 0x401482d4)
20 rca_site_bind@libdce + 0x0000059c (0x401483a0, 0x401476b0, 0x3, 0)
21 sec_rgy_site_bind@libdce + 0x00000068 (0x40147ed0, 0x401476b0, 0x401476c4, 0x401482d4)
22 sec_rgy_site_bind_query@libdce + 0x00000048 (0x40147ed0, 0x401476b0, 0x401476c4, 0x401482d4)
23 sec_krb_sec_parse_name@libdce + 0x0000013c (0, 0x5, 0x4013d0b4, 0x40147650)
24 cred_from_cred_rep (credrep = 0x4014757c, cred = 0x40147540, cred_context =0x4014753c, cache_slot = 0x40147594)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 686]
25 handle_sec_krb_dg_build_message (inptr = 0x4013d0ed, inlen = 133, outbuf = 0x4013e090)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 807]
26 do_auth_request (inptr = 0x4013d094, inlen = 133, outbuf = 0x4013e090, outsize = 0x4013d080)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 1183]
27 ProcessRequest + 0x000000f4 (0x4013d078, 0x29c6b013, 0x1000000, 0x4ac)
28 service_thread + 0x00000128 (0x4001af88, 0x7af51478, 0x400da138, 0x5)
29 cma__thread_base@libdce + 0x0000021c (0x400da138, 0, 0, 0)
30 cma__thread_start1@libdce + 0x0000004c (0x400da138, 0, 0, 0)
31 cma__thread_start0@libdce + 0x00000008 (0x400da138, 0, 0, 0)
32 _rpc_createerr@libc + 0x0000031c (Bad access to child process (UE308)
-------------------------------------------------------------------------
 0 read@libc + 0x00000008 (hp-ux export stub)
 1 cma_read@libdce + 0x000009a4 (0xa, 0x4099e3a0, 0x4, 0)
 2 krb5_fcc_read@libdce + 0x00000048 (0x408c7ab8, 0x4099e3a0, 0x4, 0x401c3ba4)
 3 krb5_fcc_read_int32@libdce + 0x0000003c (0x408c7ab8, 0x4099e3a0, 0x1a, 0x401c38f4)
 4 krb5_fcc_read_data@libdce + 0x00000040 (0x408c7ab8, 0x4099e3a0, 0x2, 0)
 5 krb5_fcc_read_principal@libdce + 0x000001c0 (0x408c7ab8, 0x401c37ac, 0x1, 0)
 6 krb5_fcc_next_cred@libdce + 0x000001cc (0x408c7ab8, 0x401c37a0, 0x401c37a8, 0x401c37a8)
 7 krb5_fcc_retrieve@libdce + 0x000001c0 (0x408c7ab8, 0x21, 0x401c3724, 0x40998a20)
 8 krb5_get_credentials@libdce + 0x000000f8 (0x10, 0x408c7ab8, 0x40998a20, 0x401c36a0)
 9 sec_krb_get_cred@libdce + 0x00000268 (0x409b6078, 0x409850c0, 0x4, 0x2)
10 cred_from_cred_rep (credrep = 0x401c354c, cred = 0x401c3548, cred_context = 0x401c3564, cache_slot = 0x401c3568)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 690]
11 handle_sec_krb_get_cred (inptr = 0x4013f0f5, inlen = 81, outbuf = 0x401400a4)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 755]
12 do_auth_request (inptr = 0x4013f0a8, inlen = 81, outbuf = 0x401400a4, outsize = 0x4013f094)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 1183]
13 ProcessRequest + 0x000000f4 (0x4013f08c, 0x29c6b013, 0x1000000, 0xffffffff)
14 service_thread + 0x00000128 (0x4001afa0, 0x7af51478, 0x400ddb38, 0x9)
--------------------------------------------------------------------------
 0 read@libc + 0x00000008 (hp-ux export stub)
 1 cma_read@libdce + 0x000009a4 (0xa, 0x409aacac, 0x4, 0)
 2 krb5_fcc_read@libdce + 0x00000048 (0x409850c0, 0x409aacac, 0x4, 0x401aa934)
 3 krb5_fcc_read_int@libdce + 0x0000003c (0x409850c0, 0x409aacac, 0x4, 0)
 4 krb5_fcc_read_authdatum@libdce + 0x0000007c (0x409850c0, 0x409aaca8, 0x4, 0)
 5 krb5_fcc_read_authdata@libdce + 0x00000170 (0x409850c0, 0x401aa828, 0x1, 0)
 6 krb5_fcc_next_cred@libdce + 0x00000334 (0x409850c0, 0x401aa7e0, 0x401aa7e8, 0x401aa7e8)
 7 krb5_fcc_retrieve@libdce + 0x000001c0 (0x409850c0, 0x21, 0x401aa764, 0x409a5bc8)
 8 krb5_get_credentials@libdce + 0x000000f8 (0x10, 0x409850c0, 0x409a5bc8, 0x401aa6e0)
 9 sec_krb_get_cred@libdce + 0x00000268 (0x409be648, 0x409940f0, 0x4, 0x2)
10 cred_from_cred_rep (credrep = 0x401aa57c, cred = 0x401aa540, cred_context = 0x401aa53c, cache_slot = 0x401aa594)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 690]
11 handle_sec_krb_dg_build_message (inptr = 0x40141115, inlen = 133, outbuf = 0x401420b8)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 807]
12 do_auth_request (inptr = 0x401410bc, inlen = 133, outbuf = 0x401420b8, outsize = 0x401410a8)    [/project/dce/build/dce1.1-snap/src/security/helper/auth_helper.c: 1183]
13 ProcessRequest + 0x000000f4 (0x401410a0, 0x29c6b013, 0x1000000, 0x4a4)
14 service_thread + 0x00000128 (0x4001af88, 0x7af51478, 0x400dccb8, 0x8)
 
Assigning to Ron - we need to get icl.bind logs for spinning dfsbinds to see what's really
going on - he suspects that the kernel is just making a lot of requests of dfsbind and that
they are getting queued up. I have also seen a cdsclerk be busy, right alongside the dfsbind
but attaching to the cdsclerk showed that it was going through normal code paths to read
attributes.

[9/23/94 public]
CR 12313 dup'd to this one since the following symptoms are believed to be
a direct result of dfsbind spinning (which in turn may be the result of
DCE core performance problems?):
 
dfs: rpc errors (code 382312543) from the fx server 130.105.201.7 in cell cobble
r_cell.qadce.osf.org
dfs: set auth binding failed (code 382312711); running unauthenticated.
dfs: server disk quota exceeded
dfs: server disk quota exceeded
dfs: set context failed: code 691089613
dfs: lost contact with the fx server 130.105.201.7 in cell cobbler_cell.qadce.os
f.org
dfs: file server 130.105.201.7 in cell cobbler_cell.qadce.osf.org back up!
 
where:
stcode_hp 382312711
382312711 (decimal), 16c9a107 (hex): Helper process catatonic (dce / rpc)
 
stcode_hp 691089613
691089613 (decimal), 293130cd (hex): status 0x293130cd (dfs / xvl)
 
stcode_hp 382312543
382312543 (decimal), 16c9a05f (hex): Who are you failed (dce / rpc)
 
This output from running fts tests 12 + 14.

[9/23/94 public]
I don't know where this 691089613 error code is coming from, but it's
disturbing me that it's there at all!  It's clearly in the DFS ``xvl''
code space (ranging from 691089408 to (691089408+4095 = 691093503)).
However, it's outside the VOLERR_TRANS_LOWEST..VOLERR_TRANS_HIGHEST
and VOLERR_PERS_LOWEST..VOLERR_PERS_HIGHEST ranges.  It looks to have
been added by somebody who didn't particularly care what the CM--at
least the 1.0.3a CM--did when it received this error!
My guess is that the problem at hand is intimately related to the
generating of this error code.

[9/23/94 public]
Don't ask me how I found this, but under certain circumstances, 
cm_ConnAndReset() will return CM_RECURSIVE_TSR_ERROR which 
translates in cm.h to 
 
#define CM_RECURSIVE_TSR_ERROR (VOLERR_TRANS_HIGHEST+4)
 
which I believe translates to 691089613.  It's all been this
way since at least 1.0.3a.  Was this a quick hack to avoid putting
the entry in the xvolume.et file?  There's also 
 
#define CM_REP_ADVANCED_AGAIN (VOLERR_TRANS_HIGHEST+3)
 
in cm.h.  If someone else uses these codes in vol_errs.et, you
end up with codes that mean multiple things.

[9/26/94 public]
Well, there's egg on my face--excuse me while I try to clean up.
 
As an aside, the number of codes claimed in xvolume.et should be
pretty well static; there are lots of spares.  The idea behind
CM_REP_ADVANCED_AGAIN was to grab something in that approximate number
space, a code value for internal communication in the CM itself that
doesn't cross the wire, but one that's yet in the right number space
to be represented in a compact fashion in the cm_volume structure.
I don't think this is a big problem in that the CM both generates
and recognizes this error code specially.
 
But, unfortunately, I recognized this CM_RECURSIVE_TSR_ERROR at Ron's
nudging, and remember that I put it into the 1.0.3a stuff.  Shortly
after 1.0.3a closed we ran into problems with it and took it out
again, though, which is a lousy excuse for why I didn't remember it at
all.  The scenario is one in which some CM thread did an RPC, but
something took so long or whatever that the FX had declared the CM
down, and the CM had to do TSR on that server.  In doing TSR, though,
things weren't going any more quickly, so after doing its initial
AFS_SetContext call, the TSR process itself discovered that the CM had
again been declared down by the FX, so the CM would have to restart
TSRing.  Well, we didn't really understand the problem too well at
that point (a double network failure, after all) and wanted to declare
something like a disaster for that CM/FX communication, rather than
deadlocking or panic()ing or overflowing a stack , which we had been
doing when recursively grabbing a lock.  So I had the CM use this
CM_RECURSIVE_TSR_ERROR error code for the purpose.  However, the cleaner
solution that we adopted almost immediately afterward was to return
some error code that would get the stack popped down to the point
where TSR could start again from the beginning.  This involved returning
TKN_ERR_NEW_NEEDS_RESET rather than CM_RECURSIVE_TSR_ERROR if we
were recursing on setCtxProcID, returning rather than blocking if
some other thread were already doing TSR on this server, and making
one fewer call to AFS_SetContext.
 
This is all embedded in a relatively large delta.  I don't know how
helpful it would really be to you, unfortunately, or whether it would
be helpful to the close-down-1.1 efforts.

[9/26/94 public]
As you mention, it's probably too late to integrate and test the 
delta you mention in 1.1 (Thanks for offering it, though).  Our time 
is probably better spent tracking down why the network is gumming up 
to cause this condition in the first place.  When you saw this
condition at Transarc, was it usually associated with a dfsbind spin?
I'm trying to figure out if it's worth my while to grovel through
the ICL log from a spinning dfsbind that we've collected.

[9/27/94 public]
I grovelled through a icl.bind from a spinning dfsbind, and pulled 
out what I thought was interesting (not really knowing what I was 
looking at).  The full logs are in ~notuser/rsarbo/ot12311/pid_*.
Anything look familiar?
 
pid_10:
 
time 614.534955, pid 10: ProcessRequest: entry
time 614.554410, pid 10: sec_id_parse_name: entry name=/.../dfs_tests
time 620.100369, pid 10: sec_id_parse_name: exit code:387063930
time 620.100615, pid 10: rpc_ns_entry_inq_resolution: entry
time 620.958151, pid 10: rpc_ns_entry_inq_resolution: exit code:282109010
time 620.960604, pid 10: ProcessRequest: exit code:2
 
387063930 = sec_rgy_object_not_found
282109010 = DNS_ROOTLOST
 
pid_1:
 
filled with:
 
time 420.851600, pid 1: helper read: entry
time 420.852496, pid 1: helper read: exit code:68
 
sequences
 
pid_5:
 
Occasional instances of:
 
time 61.666497, pid 5: helper write: exit code:-1
 
pid_6:
 
Occasional instances of:
 
time 840.151232, pid 6: helper write: exit code:-1
 
pid_7:
 
Occasional instances of:
 
time 661.730898, pid 7: helper write: exit code:-1
 
pid_8:
 
Occasional instances of:
 
time 195.196345, pid 8: helper write: exit code:-1
 
pid_9:
 
time 814.076222, pid 9: sec_id_parse_name: entry name=/.../dfs_test/..+
time 814.106564, pid 9: sec_id_parse_name: exit code:0
time 814.144053, pid 9: rpc_ns_entry_inq_resolution: entry
time 903.862538, pid 9: rpc_ns_entry_inq_resolution: exit code:382312643
time 903.863288, pid 9: ProcessRequest: exit code:2
 
382312643 = rpc_s_partial_results
 
pid_11:
 
time 817.393987, pid 11: ubik_GetServerList returns (groupName=/.../dfs_test, gr
oupSize=0, errorcode=0)
 
time 818.205258, pid 11: no flservers available
 
time 818.206122, pid 11: ProcessRequest: exit code:21
 
time 815.041480, pid 11: sec_id_parse_name: entry name=/.../dfs_test/..+
time 815.041855, pid 11: sec_id_parse_name: exit code:0
time 815.042061, pid 11: rpc_ns_entry_inq_resolution: entry
time 903.992028, pid 11: rpc_ns_entry_inq_resolution: exit code:382312643
time 904.142194, pid 11: ProcessRequest: exit code:2
 
occasional:
 
time 802.745037, pid 11: helper write: exit code:-1
 
pid_12:
 
time 819.938317, pid 12: ubik_GetServerList returns (groupName=/.../dfs_test/fs,
 groupSize=3, errorcode=0)
 
time 285.739404, pid 12: sec_id_parse_name: entry name=/.../dfs_test/..+
time 304.865682, pid 12: sec_id_parse_name: exit code:0
time 304.901142, pid 12: rpc_ns_entry_inq_resolution: entry
time 305.338214, pid 12: rpc_ns_entry_inq_resolution: exit code:382312643
 
Occasional:
 
time 994.332778, pid 12: helper write: exit code:-1

[9/27/94 public]
  pid_10:
 
  time 614.534955, pid 10: ProcessRequest: entry
  time 614.554410, pid 10: sec_id_parse_name: entry name=/.../dfs_tests
  time 620.100369, pid 10: sec_id_parse_name: exit code:387063930
  time 620.100615, pid 10: rpc_ns_entry_inq_resolution: entry
  time 620.958151, pid 10: rpc_ns_entry_inq_resolution: exit code:282109010
  time 620.960604, pid 10: ProcessRequest: exit code:2
 
  387063930 = sec_rgy_object_not_found
  282109010 = DNS_ROOTLOST
 
Are these reasonable errors for lookups of a cell calling itself
/.../dfs_tests?  Notice that it took 5.5 seconds for the lookup, which
is pretty good.  ENOENT was returned, saying that there really isn't
any such name.
 
  pid_1:
 
  filled with:
 
  time 420.851600, pid 1: helper read: entry
  time 420.852496, pid 1: helper read: exit code:68
 
This is OK: as you can see from reading dfsbind code, the ``exit code''
is just the return value from write().  Here, 68 bytes were written OK.
 
  pid_5:
 
  Occasional instances of:
 
  time 61.666497, pid 5: helper write: exit code:-1
 
This is an error from write(), probably because the request block in
the kernel has already timed out and given the helper_catatonic error
to the in-kernel caller.  The reasonable thing to do is to look back
to see when this PID got its request, to see if it has really taken 5
minutes to do its job.  Alternatively, this thread might not have
gotten its request in a timely fashion because the reader thread (the
one that reads the krpch device) may have been blocked waiting for a
buffer into which it could do the read.  (As I mentioned to you.)  In
this case, the 5 minutes starts when the kernel enqueues the message,
not when the dfsbind reader thread reads it.  Hm--in looking at your
pid_1 file, though, it seems that there isn't ever very much of a
delay between the time that a read() has been done and when the next
read() is begun.  But the do_auth_request calls usually took 300
seconds or more, causing the kernel to time out the requests.
 
Same situation in other PIDs, e.g. 6, 7, 8, 11.
 
  pid_9:
 
  time 814.076222, pid 9: sec_id_parse_name: entry name=/.../dfs_test/..+
  time 814.106564, pid 9: sec_id_parse_name: exit code:0
  time 814.144053, pid 9: rpc_ns_entry_inq_resolution: entry
  time 903.862538, pid 9: rpc_ns_entry_inq_resolution: exit code:382312643
  time 903.863288, pid 9: ProcessRequest: exit code:2
 
  382312643 = rpc_s_partial_results
 
I don't know--is this reasonable?  Looks like dfsbind is returning an
ENOENT code.  And it's taking 89 seconds to do the
rpc_ns_entry_inq_resolution call.  Is the final ``..'' a literal quote?
Why would this be being passed to dfsbind?
 
Same situation in pids 11 and 12.
 
  pid_11:
 
  time 817.393987, pid 11: ubik_GetServerList returns (groupName=/.../dfs_test, groupSize=0, errorcode=0)
  time 818.205258, pid 11: no flservers available
  time 818.206122, pid 11: ProcessRequest: exit code:21
 
This is pretty basic, certainly; that's the error straight from Ubik.
Dfsbind is turning it into EISDIR, as it should.  It needs /.../dfs_test/fs
to get a real result.
 
  pid_12:
 
  time 819.938317, pid 12: ubik_GetServerList returns (groupName=/.../dfs_test/fs, groupSize=3, errorcode=0)
 
This is a successful lookup.

[9/27/94 public]
OK, that helped a lot.  Thanks.  Yes those "/.../dfs_test/..+" strings 
were copied exactly as they appeared in the logs.  Perhaps it was just
a typo on the tester's part.  Alternatively, it could be lack of 
a null terminator, but I would think that problem would have shown 
up before.
 
Seems the problem we should be attacking is the fact that 
do_auth_request() is taking a long time to complete. There were a 
total of 73 instances of:
 
  time 61.666497, pid 5: helper write: exit code:-1
 
type messages in the log.  The few that I glanced through
all showed greater than 300 seconds in the corresponding 
do_auth_request.

[9/27/94 public]
Adding dce-ot-sec to interest list.

[9/27/94 public]
Next question: how big are the credential files getting?  Every time a
ticket is refreshed, an entry gets appended to one of the dcecred_xxxx
files.  If these files are getting big, it can take a very long time
to paw through all the entries; they're pawed through from the beginning
each time.  Are you using the hosts/XXXXX/self entry for authentication?
If so, how big are the dcecred_ffffffff.* files?  If not, do you have a
refresh interval set to a small value, or is the user logged in for
a long time?
 
I ask this because I looked at the top of this report and saw that
dfsbind was reading a file in two out of the three samples.

[9/27/94 public]
We need to figure out where the extra ..+ comes from on the end of the
names above.  The rpc_ns_inq_resolution() call *should* be returning
partial results errors for that string.
Craig makes a good point above:  What are these tests using for ticket exp
values?  (I assume dfsbind is running as root and picking up the machine's
creds, so checking the size of the dcecred_ffff* files is the best suggestion
yet).
We had a similar problem here that happened because the DFS group cell here
had set their max & default tkt lifetimes to a week.  BUT, a ptgt lifetime
is still 2hrs, so every SVC tkt (and PTGT) was a max of 2hrs.  So, tickets
would fill up the credentials cache and a refresh was only happening once
a week, so they would never get cleaned up.  Eventually the cache would get
so big that it took so long to go through it, that dfsbind would start having
problems.  I don't know why I didn't think correlate the two problems, other
than I just assumed that your tests would have been trying smaller lifetimes
rather than larger ones, and thus shouldn't run into the problem (because
when you do a refresh of a TGT, it wipes out the cred cache files and creates
new ones with the same data, but no TKTs yet).  Added Rick Kissel, Steve
Moriarty and Daryl Kinney to the CC list because for some reason there's
no one from the DFS group here getting DFS CR updates.

[9/27/94 public]
Yep, it's big:
 
> root@garlic $ ls -l dcecred_fffffff*
> -rw-------   1 root     root     1738071 Sep 27 16:44 dcecred_ffffffff
> -rw-------   1 root     root         889 Sep 26 12:02 dcecred_ffffffff.data
> -rw-rw-rw-   1 root     root        1015 Sep 26 15:53 dcecred_ffffffff.nc
 
another clue: klist as root takes forever.
 
> rgy_edit=> pr
>   Properties:
>     Properties for Registry at:               /.../dfs_test
>     Registry is NOT read-only
>     Certificates to this server may be generated at any site.
>     Encrypted passwords are hidden
>     Unix IDs ARE embedded in PGO UUIDs
>     Low UID for principal creation:           12449
>     Low UID for group creation:               100
>     Low UID for org creation:                 100
>     Maximum possible UID:                     32767
>     Minimum certificate lifetime              5m
>     Default certificate lifetime              2d

[9/27/94 public]
Well, looks like a winner.  Now to figure what to do about it.
 
You have to watch out for the sizes of other credentials besides that
of hosts/FOO/self, though.  My understanding is that one of the jobs
that dfsbind does is to read the credentials of DFS users and pass the
contents into the kernel.

[9/27/94 public]
Great, that's at least one of the causes of the problem!!!
Other credentials probably aren't a problem here because they belong to
test scripts that are probably creating/deleting them occasionally during
tests...   Given that it's so late in the game, and that it's probably set
to two days so that the tests can finish without something having to refresh
somewhere, I won't ask you to change the 2d limit, BUT! 2days for a machine
context is too long based on what we've found here and what you've run into
there (because dfsbind is running as the machine context and handling ops
for *lots* of other identities, the machine context is going to get more tkts
than any of the others...).

SO, for every machine in the test cell where you have a very high (>1day)
maximum ticket lifetime and default ticket lifetime, do the following:

rgy_edit=> change -p hosts/<machinename>/self -mcl 10h
Change account "hosts/<machinename>/self none none" [y/n/g/q]? y

(to match the default used for config, so we'll be testing what people
should be using).  Then, run the tests.  If this fixes the problem, then
maybe this should be reassigned to be a doc defect for at least the release
notes to warn not to increase the ticket lifetime of machine principals
because of the known limitation that cred caches are only cleaned up on
a refresh.

[9/27/94 public]
Just for the record, I generally do NOT extend the default cert life,
just the max cert life. To run the CHO tests, I do a:

	kinit -l 4d

where 4d is my max cert life. I do this so that the normal ticket
refreshes occur. I'm installing bl-19, running fts test 12+14 and
recording the creds file sizes tonight ...

[9/28/94 public]
We know that one of the things that slows down dfsbind almost beyond use
is too many tickets in a credential cache.  We also know that there was a
bug (CR12179) causing new tickets to be obtained for every new binding
handle (no reuse).  We also know that obtaining tickets for every new handle
was slowing down all of DCE in general.
CR12179 has just been fixed.  We're hoping that it solves all of the problems
seen here, and if so we'll dup this one to it.  For now, this is only marked
dependent on 12179 in case that doesn't fix all the problems.

[10/3/94 public]
I ran the fts tests and did NOT get the busy dfsbind symptoms at all ... BUT
it was in an HP-only cell so perhaps that's significant. ALSO, to run the
dfs.read_write_all test, I have to change the DEFAULT ticket liftetime to
something > 2d, in this case I changed it to 4d (in both cells, I changed
MAX certificate lifetime to 4d).

In my 2 HP, 1 RIOS cell where I ran the dfs.read_write_all test, dfsbind
has been VERY busy:

RIOS (where 2 filesets and 2 clients lived, 1 of which got Conn Timeout):
    root 13274     1   0   Sep 29      - 549:43 /opt/dcelocal/bin/dfsbind 

HP (where 1 fileset and 1 client lived)
    root  1841     1  0  Sep 29  ?       179:57 /opt/dcelocal/bin/dfsbind

HP (where 1 fileset and 1 client lived)
    root  1555     1  0  Sep 29  ?       200:00 /opt/dcelocal/bin/dfsbind

I can't say that it has been SPINNING at any one time though ... going to
look at the creds situation now. Note that the read_write_all test creates
the filesets, logs in the users and each runs a "busy" script in their own
fileset for the duration of the test - there is not a lot of dce_login
activity or changing of acls. I'm going to run this test in my HP-only cell
because I haven't any other ideas ...

[10/3/94 public]
The creds situation appears to be okay but the cell is pretty slow and
as a result dfs is now reporting "who are you" failures. Lots of Time provider
timeouts and dts comm failures too (12315). There are no tests running in
the cell - only 1 test has been run in the cell since it was last
rebooted (read_write_all) and that test ended 18 or so hours ago... Getting
traces now - if anything interesting, will update.

Okay - here's what's in one of my icl.bind's:

time 842.686504, pid 5: do_auth_request: exit code:336760869
time 842.686745, pid 5: ProcessRequest: exit code:0
time 842.710389, pid 5: helper write: entry
time 842.712926, pid 5: helper write: exit code:8
time 842.713228, pid 5: ProcessRequest: entry
time 842.713394, pid 5: do_auth_request: entry
time 842.818016, pid 5: do_auth_request: exit code:336760869
time 842.818256, pid 5: ProcessRequest: exit code:0
time 842.818427, pid 5: helper write: entry
time 842.820896, pid 5: helper write: exit code:8
time 842.821201, pid 5: ProcessRequest: entry
time 842.821368, pid 5: do_auth_request: entry
time 842.998686, pid 7: ProcessRequest: entry
time 842.999066, pid 7: do_auth_request: entry
time 843.123520, pid 7: do_auth_request: exit code:336760869
time 843.123760, pid 7: ProcessRequest: exit code:0
time 843.123932, pid 7: helper write: entry
time 843.126507, pid 7: helper write: exit code:8
time 843.126812, pid 7: ProcessRequest: entry
time 843.126976, pid 7: do_auth_request: entry
time 843.479918, pid 7: do_auth_request: exit code:0
time 843.480157, pid 7: ProcessRequest: exit code:0
time 843.480326, pid 7: helper write: entry
time 843.491269, pid 7: helper write: exit code:636
time 843.573164, pid 9: ProcessRequest: entry
time 843.573398, pid 9: do_auth_request: entry
time 72.860907, pid 0: current time: Mon Oct  3 13:50:00 1994
time 72.860907, pid 6: do_auth_request: exit code:0
time 72.957321, pid 6: ProcessRequest: exit code:0 
time 72.957545, pid 6: helper write: entry
time 72.958420, pid 6: helper write: exit code:-1
time 249.643982, pid 5: do_auth_request: exit code:0
time 249.672186, pid 5: ProcessRequest: exit code:0
time 249.672394, pid 5: helper write: entry
time 249.687592, pid 5: helper write: exit code:-1
time 270.188521, pid 9: do_auth_request: exit code:0
time 270.189018, pid 9: ProcessRequest: exit code:0
time 270.189216, pid 9: helper write: entry
time 270.293831, pid 9: helper write: exit code:-1

where the error code is:
336760869 (decimal), 14129025 (hex): Clock skew too great (dce / krb)

but the clocks (as reported by the "date" command at least) are w/in
a second of each other:

root@wolfboy> date
Mon Oct  3 14:10:47 EDT 1994

root@dce11> date
Mon Oct  3 14:10:50 EDT 1994

root@dce8> date
Mon Oct  3 14:10:49 EDT 1994

However - I've just been kindly reminded of the instructions
included above (rgy_edit) for keeping the self ticket lifetimes
at the default so refreshes happen - I believe this is probably
the problem - restarting the cell and rerunning the test - we'll
know in 48 hours :-)

BTW - I will make sure the core system test people ALSO run with
this setting if their default ticket liftimes are set > 2d for
their tests as well...

[10/04/94 public]

The impact of a big cred file, dcecred_ffffffff, is really substantial.

In order to quantify the impact I ran a profiling "klist" before and after 
a refresh and got the following numbers:

	* Before the refresh, the cred file was about 34 KB, and 
	  there were 121 krb5_fcc_next_cred calls using 2.03 seconds.

	*  AFter the refresh, the cred file is about four KB, and
	   there were only 29 krb5_fcc_next_cred calls using 0.47 second
	   of CPU.

So, if the cred file grows to 340 MB the time in reading the cred file
can be as much as 20 seconds when executing a "klist".

[10/4/94 public]
NOTE! Jean made a typo above.  I assume he meant to say 340KB, not MB.  If
DCE creds files grow to 340MB, then I'd suggest throwing DCE away and using
ONC+ ;^)

[10/05/94 public]

Mike, thanks for make the correction. It is 340 KB.

Here is another data point. A dce/dfs cell with two
HP's is as follows. 

	*dce10 --- security server, cds server and dts server
	*toaster ---- dce client, dfs client and dts server
	*Both dts servers sync every two minutes.
	*dce-ptgt ticket renew time is 10 minutes.

At the update time of the cred file, dcecred_ffffffff, "toaster"
was ~100% busy for as long as 30 seconds ---- at that time
the size of this file was just 92,417 bytes. 

This scenario repeats every 10 minutes(renew interval) but 
with longer ~100% busy time as dcecred_ffffffff is getting bigger. 

Since the renew time is so short, the cred file grew to about
340 KB before the 10h refresh. And, both dtsd experienced
some "need 2 servers" failures.

In gmd's test her cell has many dfs servers and many dts servers, it's 
very likely that the cred file, dcecred_ffffffff, can grow very
quickly to 50 KB or more(Gail, any data on this?) in just a few updates 
before the 10h refresh, and the 50 KB size will certainly casue the
system into ~100% busy for ~15 seconds. Mixing with rw-test and
others, comm or other failures are very likely to happen
during this busy period.

[10/05/94 public]
I have 3 dts servers and 3 dfs server machines, each with flservers,
ftservers, etc. I'm afraid I didn't capture the cred file size data
from the incorrect rgy setup (self = default = 4d) but I believe the
size of the self cred file was something like 499680 ... I've rerun
the dfs.read_write_all.main test with all the self principals set
back to 10h (default = 4d) - the test run was not as successful as
I'd hoped but times on dfsbind are down and the self cred file is
45010. I think we can close this one, the fix for 12179 (combined
with correct rgy setup :-) seems to have done the trick - what
d'ya think?

[10/05/94 public]
>the 50 KB size will certainly casue the
>system into ~100% busy for ~15 seconds.

Can the same be said of this OT? :)  It's almost at 37k right now...

[10/5/94 public]
> dfsbind spins
Big CRs make my head spin :-)

WRT Gail's last comment above:  It has been proven (48hr tests) that dfsbind
is no longer spinning with the fix for 12179 and the fix to the configuration
(10h machine creds).  Since that was what this CR was reporting then I'd ask
gmd to cancel this CR if she's confident that she can't reproduce the dfsbind
spin with a correctly configured cell.  If there were new problems in the
test that are showing up now that we're getting farther, then open a new CR
for those.

[10/5/94 public]
Closing this CR. I'll open separate CRs for any other problems I hit
and for the dfs.read_write_all.main README to note the correct rgy
setup.



CR Number                     : 12298
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : missing a #include..., broke
libdce
Reported Date                 : 9/22/94
Found in Baseline             : 1.1b18
Found Date                    : 9/22/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b19
Affected File(s)              : dfs_dce_acl.h
Sensitivity                   : public

[9/22/94 public]

cc: "/u3/devobj/sb/nb_ux/export/hp800/usr/include/dce/dfs_dce_acl.h", line 158: error 1000: Unexpected symbol: "*".
cc: "/u3/devobj/sb/nb_ux/export/hp800/usr/include/dce/dfs_dce_acl.h", line 158: error 1573: Type of "pacListPP" is undefined.

[9/22/94 public]
I've got a 2 line fix.  I'm building all of file and security
to be sure it's clean then I'll submit.

[9/22/94 public]
I understand this is in dfs.  BUT we should still have the fact
that libdce broke in the summary line.  So, I am adding that.
Otherwise it is hard to tell from this bug why or how the dfs
component impact the core components.

thanks
annie

[09/26/94 public]
Closed.



CR Number                     : 12280
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bos
Short Description             : bos output incorrect
Reported Date                 : 9/21/94
Found in Baseline             : 1.1b18
Found Date                    : 9/21/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b19
Affected File(s)              : ./file/userInt/bos/bos_main.c,
                                             ./file/userInt/bos/bos.sams
Sensitivity                   : public

[9/21/94 public]

The bos stat command is missing output.

In 1.0.3a:


 bos stat -server /.:/hosts/barge -long
Instance upserver, (type is simple) currently running normally.
    Process last started at Sun Sep 18 04:00:13 1994 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/upserver                  -path /opt/dcelocal/var/dfs/admin.bos                  /opt/dcelocal/var/dfs/admin.fl                  /opt/dcelocal/var/dfs/admin.ft                 /opt/dcelocal/var/dfs/admin.up                  /opt/dcelocal/var/dfs/admin.bak'

Instance flserver, (type is simple) currently running normally.
    Process last started at Sun Sep 18 04:00:14 1994 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/flserver'

.
.
.

In 1.1b18:

bos stat -server /.:/hosts/shotz -long
Instance upserver, (type is simple) currently running normally.
    Parameter 1 is '/opt/dcelocal/bin/upserver                  -path /opt/dcelocal/var/dfs/admin.bos                   /opt/dcelocal/var/dfs/admin.fl                  /opt/dcelocal/var/dfs/admin.ft                  /opt/dcelocal/var/dfs/admin.up                  /opt/dcelocal/var/dfs/admin.bak'

Instance flserver, (type is simple) currently running normally.
    Parameter 1 is '/opt/dcelocal/bin/flserver'

Instance bakserver, (type is simple) currently running normally.
    Parameter 1 is '/opt/dcelocal/bin/bakserver'

Instance ftserver, (type is simple) currently running normally.
    Parameter 1 is '/opt/dcelocal/bin/ftserver'
.
.
.

Notice that the "process last started" line is missing for each entry.

[9/30/94 public]

The problem was that the messages with the attribute 'notice' were not
being output. The reason was that they were not registered (svc_register).
It had not been noticed that notice messages were not being output prior
to this. I don't know if something had changed recently; nonetheless it
was incorrect not to register the 'notice' messages.

I have put registration for messages with 'notice' attribute in all the
initialize_svc() routines in all the files where this initialization is
accomplished. Additionally, I had made a note to myself to write an OT
for two messages in bos.sams that needed a newline in the message text,
so I have included the fix in this OT instead, hence bos.sams is included
in the fix.

Additional files where registration of 'notice' messages was added are,
(all with the prefix ./file/):

  afsd/afsd.c and vsys.c
  dfsbind/dfsbind_test.c and main_helper.c
  ftserver/ftserver_main.c
  gateway/dfsgwd.dgsgwd.c
  gtx/cb_test.c, gtxtest.c, object_test.c, and screen_test.c
  pxd/gsys.c and pxd.c
  update/client.c and server.c
  userInt/cm/cm.c
  userInt/fts/volc_main.c
  xaggr/agclient.c and export.c

The bos fvt test was run prior to submission for this fix, and it ran,
so I am closing this OT.



CR Number                     : 12279
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : salvage
Short Description             : salvager not reporting messages
Reported Date                 : 9/21/94
Found in Baseline             : 1.1
Found Date                    : 9/21/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b19
Affected File(s)              : src/file/episode/slv.sams
Sensitivity                   : public

[9/21/94 public]
The salvager in 1.1b18 is not reporting messages that it did in 
1.0.3a:

	# cd /project/dce/build/dce1.0.3a/install/rios/opt/dce1.0/bin
	# ./salvage /dev/lv00
	Salvaging /dev/lv00
	Will run recovery on /dev/lv00
	Processed 1 vols 139 anodes 17 dirs 115 files 1 acls
	Done.  /dev/rlv00 checks out as Episode aggregate.
	# ./salvage /dev/lv00
	Salvaging /dev/lv00
	Will run recovery on /dev/lv00
	Processed 1 vols 139 anodes 17 dirs 115 files 1 acls
	Done.  /dev/rlv00 checks out as Episode aggregate.
	# /opt/dcelocal/bin/salvage /dev/lv00
	# /opt/dcelocal/bin/salvage /dev/lv00
	#

[9/22/94 public]

 Changed message severities from notice to warning

[09/26/94 public]
Closed.



CR Number                     : 12260
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : vnode
Short Description             : Failure in test_vnops tests
Reported Date                 : 9/20/94
Found in Baseline             : 1.1b18
Found Date                    : 9/20/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/file/episode/vnops/test_vnodeops.c
Sensitivity                   : public

[9/20/94 public]

The following test fails in test_vnops.

###  lookup by foreign user

root fid is (1,1)
New uid: 100
New gid: 200
Setting mode to 774
name foo, looked up fid (2,1)
root fid is (1,1)
New realm: 34
name foo, looked up fid (2,1)
Code error:  code = 0, should be 13
Test mode was FAILED
1.5u 0.5s 0:02 97%


It can be reproduced by running ./test_vnodeops mode.test

the sources to the test can be found in
src/file/episode/vnops/test_vnodeops

[9/25/94 public]
Fixed.  Also fixed uninitialized memory problem in mount.test
while I was in there.

[09/26/94 public]
Closed.



CR Number                     : 12259
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : make sec_acl interface work w/DFS delegation
Reported Date                 : 9/20/94
Found in Baseline             : 1.1b18
Found Date                    : 9/20/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b19
Affected File(s)              : dfs_dce_acl.c, dfs_dce_acl.h
Sensitivity                   : public

[9/20/94 public]
The glue between sec_acl and DFS acl's needs some minor changes
to understand and deal with DFS/delegation.

[9/21/94 public]
Fixed.

[09/26/94 public]
Closed.



CR Number                     : 12209
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : Unable to set foreign_user_delegate
Reported Date                 : 9/15/94
Found in Baseline             : 1.1b17
Found Date                    : 9/15/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b19
Affected File(s)              : dacl_sec_acl.c
Sensitivity                   : public

[9/15/94 public]

When setting a foreign_user_delegate entry on a episode file,
user_delegate gets set instead.


dcecp> touch /:/bar
dcecp> acl show /:/bar
{user_obj rw-c--}
{group_obj rw----}
{other_obj rw----}
dcecp> acl mod /:/bar -add {foreign_user_delegate /.../shotz_cell/user_a rw}
dcecp> acl show /:/bar                                                       
{mask_obj rw----}
{user_obj rw-c--}
{group_obj rw----}
{other_obj rw----}
{user_delegate 000001f4-cf27-21cd-b900-08000932c194 rw----}
dcecp> 

same happens with foreign_group_delegate:

dcecp> acl mod /:/bar -add {foreign_group_delegate /.../shotz_cell/group_a1 rw}
dcecp> acl show /:/bar                                                       
{mask_obj rwx-id}
{user_obj rw-c--}
{group_obj rw----}
{other_obj rw----}
{user_delegate 000001f4-cf27-21cd-b900-08000932c194 rw----}
{group_delegate 000003f2-cf28-21cd-b901-08000932c194 rw----}

foreign_other_delegate and any_other_delegate report as a invalid acl
entries, they should be valid.

Similar problems exist in acl_Edit, you just have to exit and reenter
to see the improper entries.  dcecp must refresh the cached acls from
disk after it makes a change.

[9/21/94 public]
Fixed.

[09/26/94 public]
Closed.



CR Number                     : 12145
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : Output corrupted in cm whereis
Reported Date                 : 9/12/94
Found in Baseline             : 1.1b17
Found Date                    : 9/12/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b19
Affected File(s)              : cm.c
Sensitivity                   : public

[9/12/94 public]

The output of the cm whereis command is not formatted properly.  I am
assuming this is due to the svc work....

Pre-svc:

mckeen@tool(mckeen) $ cm whereis .
The file '.' resides in the cell 'sisyphus.osf.org', in fileset 'user.mckeen', 
on host seagrams.osf.org.

Post-svc:

mckeen@shotz(cell_admin) $ cm whereis .
The file '.' resides 
in the cell 'booboo_cell', 
in fileset 'root.dfs', 


on host booboo.osf.org.

[9/12/94 public]

There are also problems with cm lscellinfo

[9/21/94 public]

The problems have been fixed. There were some dce_svc_printfs that were
printing extra lines. Also, two coding bugs were fixed (missing parens
which caused a partial evaluation of an expression, but no compiler error -
and a bad compare that missed the newline for lscellinfo. The missing
parens fixed whereis).
There was one file affected, .../file/userInt/cm/cm.c

[09/26/94 public]
Closed.



CR Number                     : 12017
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000,hppa
S/W Ref Platform              : aix,hpux
Component Name                : dfs
Subcomponent Name             : /file/ftserver/ftserver_main.c
Short Description             : missing includes
Reported Date                 : 9/2/94
Found in Baseline             : 1.1
Found Date                    : 9/2/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b17
Affected File(s)              : /file/Makefile
Sensitivity                   : public

[9/2/94 public]
[ /file/ftserver at 22:34 (PM) Thursday ]
makepath ftserver/. && cd ftserver &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -Dunix -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/ftserver -I/project/dce/build/dce1.1-snap/src/file/ftserver  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/ftserver/ftserver_main.c
ftserver_main.c: 238: Unable to find include file 'dfstsvmac.h'.
ftserver_main.c: 239: Unable to find include file 'dfstsvsvc.h'.
ftserver_main.c: 240: Unable to find include file 'dfstsvmsg.h'.
*** Error code 1

c89 -c    -D_SHARED_LIBRARIES   -Dunix -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/ftserver -I/project/dce/build/dce1.1-snap/src/file/ftserver  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/ftserver/ftserver_trans.c
ftserver_trans.c: 151: Unable to find include file 'dfstsvmac.h'.
ftserver_trans.c: 152: Unable to find include file 'dfstsvsvc.h'.
ftserver_trans.c: 153: Unable to find include file 'dfstsvmsg.h'.
*** Error code 

c89 -c    -D_SHARED_LIBRARIES   -Dunix -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/ftserver -I/project/dce/build/dce1.1-snap/src/file/ftserver  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/ftserver/ftserver_vprocs.c
ftserver_vprocs.c: 291: Unable to find include file 'dfstsvmac.h'.
ftserver_vprocs.c: 292: Unable to find include file 'dfstsvsvc.h'.
ftserver_vprocs.c: 293: Unable to find include file 'dfstsvmsg.h'.
*** Error code 1

[09/27/94 public]
Closed.



CR Number                     : 11998
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : wrong nosupport path?
Reported Date                 : 9/1/94
Found in Baseline             : 1.1b16
Found Date                    : 9/1/94
Severity                      : D
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b18
Affected File(s)              : src/file/nosupport/memusage.c, src/file/nosupport/Makefile
Sensitivity                   : public

[9/1/94 public]
There is a dce1.1/src/nosupport that has lots of stuff in it.
There is also a dce1.1/src/unsupported that has only:

  absolut dce1.1/src$ ls unsupported/file/tools
  Makefile     memusage.c
 
Should this be moved to nosupport?

Rational for D1:
D: I thought it was little more important than an E.  :-)
1: This should be looked at before we ship and I don't know if DFS is only
   looking at P1 bugs yet.

As always feel free to change these.

[09/26/94 public]
Closed.



CR Number                     : 11956
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : EXPDIR Missing from dfs Makefiles
Reported Date                 : 8/30/94
Found in Baseline             : 1.1
Found Date                    : 8/30/94
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b17
Affected File(s)              : file/gateway/dfsgw/Makefile, file/gateway/dfsgwd/Makefile
Sensitivity                   : public

[8/30/94 public]

Andy,
   Here is a list of files you need to check.  They are trying to export some 
files while not definine EXPDIR in the Makefile.  The result is that they are 
put in a place that is inaccessable to your builds.

[ /file/gateway/dfsgwd/Makefile ]
*** makefile error: Can't export dfsgwymsg.h because EXPDIR is not defined in the Makefile.
[ /file/gateway/dfsgw/Makefile ]
*** makefile error: Can't export dfsgwymsg.h because EXPDIR is not defined in the Makefile.
[ /file/xcred/Makefile ]
*** makefile error: Can't export xcred_kint.h because EXPDIR is not defined in the Makefile.
*** makefile error: Can't export xcred_errs.h because EXPDIR is not defined in the Makefile.
*** makefile error: Can't export xcred.h because EXPDIR is not defined in the Makefile.
[ /file/config.klib/Makefile ]
*** makefile error: Can't export libkcommondata.a because EXPDIR is not defined in the Makefile.
[ /file/util.klib/Makefile ]
*** makefile error: Can't export libkafsutil.a because EXPDIR is not defined in the Makefile.
[ /file/bomb.klib/Makefile ]
*** makefile error: Can't export libkbomb.a because EXPDIR is not defined in the Makefile.
[ /file/flserver.klib/Makefile ]
*** makefile error: Can't export libkfldb.a because EXPDIR is not defined in the Makefile.

[9/1/94 public]

  I have fixed the problem in the gateway Makefiles by not exporting
dfsgwymsg.h at all (it didn't need to be exported).  The other
directories do not have any problems in them.  The EXPDIR directive is
specified individualy for each file (see the Makefiles in question).

[09/26/94 public]
Closed.



CR Number                     : 11948
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : s12y
Short Description             : garbled and incorrect msg in FtLog
Reported Date                 : 8/30/94
Found in Baseline             : 1.1b16
Found Date                    : 8/30/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b17
Affected File(s)              : n/a
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[8/30/94 public]

While running fts tests 12 and 14, the following written to FtLog:

1994-Aug-29 18:12:12 Creating fileset ft1.2857 (0,,224) on aggr 2
1994-Aug-29 18:12:12 Failed to open 2:0,,224, code = 572833799 (vols_DumpDir: Du
mped name is longer than allowed on this system (%1$u bytes vs. %2$u bytes allow
ed) (dfs / ftu))
1994-Aug-29 18:12:13 ftserver_CreateVolume: created ft1.2857 as 0,,224 on aggr 2
1994-Aug-29 18:13:31 Creating fileset repfs.dce11 (0,,233) on aggr 2
1994-Aug-29 18:13:31 Failed to open 2:0,,233, code = 572833799 (vols_DumpDir: Du
mped name is longer than allowed on this system (%1$u bytes vs. %2$u bytes allow
ed) (dfs / ftu))
1994-Aug-29 18:13:32 ftserver_CreateVolume: created repfs.dce11 as 0,,233 on agg
r 2

I suspect that the message shouldn't be appearing at all - can you please
correct and check? This was found with the current LGB, bl-16.2

[8/31/94 public]
I have submitted code last night for ftutil, xaggr, tools/cmd et al... to
fix a problem with overlaid message catalogues. You might rerun the tests
12 and 14 to determine if the problem has been fixed.

The problem is coming out of the ftutil routines. However, I have checked
everything I can think of, and it is not apparent there is a problem (other
than the catalog).The code itself appears to be functioning "normally".
(The %1$u %2$u syntax is the XPG expanded I18N syntax. I believe it is in the
catalogues but shouldn't be in the incore message tables.)

The only other thing I can think of is that perhaps on RIOS there is a problem.
I will try to run a program on a RIOS and see if it gives the same results.
(The program works on HP, so the conversions are done properly. A test for
correct conversion hasn't been tried (by me) yet on RIOS.)

[8/31/94 public]
Gail Marie, Have you investigated the cause of this error message?
You are obviously having some problems getting it printed correctly,
but there is likely also a DFS bug here.  Are you getting any
failures from the testcases??  I'll mail you a copy of a defect
I opened a while back related to this error.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[8/31/94 public]
I tried a test for correct conversion on RIOS and it worked. The conversions
were done correctly - so I puzzled over how an unformatted message could have
been issued and have convinced myself that the reason is because the message
is being retrieved out of the wrong catalog.

Gail Marie, I think that if you rerun the tests on a system that has
my last night's submissions in it, you will get a different third part
to the message.

To wit:
Instead of the seventh message fragment/piece  retrieved being:
 vols_DumpDir: Dumped name is longer... (%1$u bytes vs. %2$u bytes ...)
 
it should have been:
  Fileset does not exist.

Similarly, in a previous submission,
dfsexport: Failed to attach /dev/lv01:m1.aggr1 (can't deplete %1$d:
%2$s (dfs / ftu)).  Ignoring it.,
the (can't deplete %1$d: %2$s (dfs /ftu)) portion should have been:

 (Aggregate may need to be recovered (dfs / ftu))

This does not mean you don't have a problem, only that you should get
the correct message if you do.

Happy days,
Ron...



CR Number                     : 11947
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_app_gdref
Short Description             : Correct/clarify DFS APIs
Reported Date                 : 8/29/94
Found in Baseline             : 1.0.3
Found Date                    : 8/29/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[8/29/94 public]
I made various changes to the DFS APIs for the FTSERVER* and VL* calls.  I
corrected some incorrect types and parameters, and I clarified some descriptive
text.  The bulk of the changes were made to bring the documentation a bit more
closely in line with the source code.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[8/29/94 public]
I made the updates to all necessary DFS API files.  The changes were suggested
by, discussed with, and reviewed by David VanRyzin (with a timely assist from
the top of the mount by The Burning Bush of development, Dr. E).  The following
files were affected:
 
./src/supp_docs/redistrib/dfs_app_gdref/chapters/filesets.gpsml
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_AbortTrans.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_AggregateInfo.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_Clone.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_CreateTrans.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_CreateVolume.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_DeleteTrans.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_DeleteVolume.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_Dump.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_Forward.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_GetFlags.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_GetOneVolStatus.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_GetStatus.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_ListAggregates.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_ListVolumes.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_Monitor.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_ReClone.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_Restore.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_SetFlags.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_SetStatus.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/FTSERVER_SwapIDs.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_AddAddress.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_AlterServer.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_ChangeAddress.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_CreateEntry.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_CreateServer.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_DeleteEntry.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_ExpandSiteCookie.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GenerateSites.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetCEntryByID.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetCEntryByName.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetCNextServersByID.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetCNextServersByName.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetCellInfo.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetEntryByID.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetEntryByName.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetNewVolumeId.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetNewVolumeIds.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetNextServersByID.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetNextServersByName.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetSiteInfo.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_GetStats.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_ListByAttributes.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_ListEntry.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_Probe.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_ReleaseLock.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_RemoveAddress.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_ReplaceEntry.3dfs
./src/supp_docs/redistrib/dfs_app_gdref/man3dfs/VL_SetLock.3dfs
 
It really wasn't as bad as it looks.  However, I am no less pleased that it
can now be closed....
 
Filled in Interest List CC with `david_vanryzin@transarc.com, cfe@transarc.com' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1doc' 
Changed Transarc Status from `open' to `closed'

[09/01/94 public]
Closed bug.



CR Number                     : 11825
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : delegation
Short Description             : implement dfs delegation
Reported Date                 : 8/23/94
Found in Baseline             : 1.1
Found Date                    : 8/23/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b16
Affected File(s)              : many
Sensitivity                   : public

[8/23/94 public]
This is a placeholder for submitting the new DFS/delegation functionality.

[9/15/94 public]
This has been submitted for some time now. Fixed.

[09/26/94 public]
Closed.



CR Number                     : 11801
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/ftutil
Short Description             : don't know how to make ftu_errs.msf
Reported Date                 : 8/22/94
Found in Baseline             : 1.1
Found Date                    : 8/22/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b16
Affected File(s)              : src/file/ftutil/Makefile
Sensitivity                   : public

[8/22/94 public]

[ /file/ftutil at 19:39 (PM) Saturday ]
makepath ftutil/. && cd ftutil &&  exec make MAKEFILE_PASS=AUTOGEN     build_all
ftutil: created directory
make: don't know how to make ftu_errs.msf (continuing)

[8/22/94 public]
Fixed.  Added ftu_errs.msf as target to existing dependency rule 
for ftutil.h in file/ftutil/Makefile

[09/26/94 public]
Closed.



CR Number                     : 11772
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : /file/episode/anode
Short Description             : Unsatisfied symbols ftu_GetRawDeviceName
Reported Date                 : 8/19/94
Found in Baseline             : 1.1
Found Date                    : 8/19/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : /file/ftutil/RIOS AT486 HPUX
Sensitivity                   : public

[8/19/94 public]
[ /file/episode/anode at 22:27 (PM) Thursday ]
makepath anode/. && cd anode &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/calcLogSize.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/findlog.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/newaggr.c
c89 -c    -D_SHARED_LIBRARIES -g  -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/runtest.c
c89 -c    -D_SHARED_LIBRARIES -g  -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/test_anode.c
c89 -c    -D_SHARED_LIBRARIES -g  -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/test_vm.c
gencat dfsepi.cat epi_errs.msf
gencat dfszea.cat epi_trace.msf
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o calcLogSize.X calcLogSize.o  -llogbuf -lasync  -lafsutil -lcom_err -ltools -losi -ldfstab -lcmd  -ldce -lBSD  
mv calcLogSize.X calcLogSize
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o findlog.X findlog.o  -ledsk -lanode -llogbuf -lasync  -lbomb -lafsutil -lcom_err -ltools -ldfstab -lcmd  -lftutil -lafssys -licl -losi -ldce -lBSD  
/bin/ld: Unsatisfied symbols:
   ftu_GetRawDeviceName (code)
*** Error code 1

[7/19/94]

The missing symbol is defined in a file in a machine dependent directory
contained under the ftutil directory. It appears that the machine dependent
directories did not get copied as part of the build process.

[8/18/94 public ]

fixed snap problem to include missing files
should build OK now.

[09/27/94 public]
Closed.



CR Number                     : 11771
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/aggr
Short Description             : Unsatisfied symbols
Reported Date                 : 8/19/94
Found in Baseline             : 1.1
Found Date                    : 8/19/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : /src/file/ftutil/RIOS AT486 HPUX
Sensitivity                   : public

[8/19/94 public]
[ /file/xaggr at 22:18 (PM) Thursday ]
makepath xaggr/. && cd xaggr &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/xaggr -I/project/dce/build/dce1.1-snap/src/file/xaggr  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/xaggr/agclient.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/xaggr -I/project/dce/build/dce1.1-snap/src/file/xaggr  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/xaggr/export.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/xaggr -I/project/dce/build/dce1.1-snap/src/file/xaggr  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/xaggr/aggrDesc.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/xaggr -I/project/dce/build/dce1.1-snap/src/file/xaggr  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/xaggr/volDesc.c
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o agclient.X agclient.o   dfsxagsvc.o dfsxagmsg.o xag_svc.o  -lafssys -ldce -lBSD  
mv agclient.X agclient
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o dfsexport.X export.o aggrDesc.o volDesc.o  dfsxagsvc.o dfsxagmsg.o  xag_svc.o  -lvolc -lftserver -lnubik -licl -lfldb -lncompat -lafs4int -ldauth -lrep -lcommondata -ldacl  -ldacllfs -ltpq  -ldfsncs -lbomb -lafsutil -lcmd -lftutil -lpipe  -lafssys -ldfstab -losi -lcom_err -ldce -lBSD  
/bin/ld: Unsatisfied symbols:
   ftu_GetRawDeviceName (code)
   _ftu_Unmount (code)
   _ftu_Mount (code)
*** Error code 1

[8/19/94]

The unresolved symbols are contained in machine specific directories under
the ftutil directory. It seems that these directories were not copied as
part of the build process.

[8/19/94 public]

The snap find error was due to a permissions problem in the dynamic
tree. It has been resolved. Tonights snap should proceed normally
and correct these errors.

[09/27/94 public]
Closed.



CR Number                     : 11747
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000,hppa
S/W Ref Platform              : aix,hpux
Component Name                : dfs
Subcomponent Name             : file/ncscompat/compat_osi.c
Short Description             : able to find include file 'dfsncsmsg.h
Reported Date                 : 8/18/94
Found in Baseline             : 1.1
Found Date                    : 8/18/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b16
Affected File(s)              : src/file/Makefile, src/file/ncompat/Makefile
Sensitivity                   : public

[8/18/94 public]

c89 -c    -D_SHARED_LIBRARIES  +z  -Dunix -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/ncscompat -I/project/dce/build/dce1.1-snap/src/file/ncscompat  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/ncscompat/compat_osi.c
compat_osi.c: 43: Unable to find include file 'dfsncsmac.h'.
compat_osi.c: 44: Unable to find include file 'dfsncssvc.h'.
compat_osi.c: 45: Unable to find include file 'dfsncsmsg.h'.
*** Error code 1

[8/22/94 public]
This was fixed by Jon Ruby last week.

[09/26/94 public]
Closed.



CR Number                     : 11664
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Need to use two spares
in afsRecordLock for 64-bits interop
Reported Date                 : 8/12/94
Found in Baseline             : 1.1
Found Date                    : 8/12/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/file/config/common_data.idl
Sensitivity                   : public

[8/12/94 public]

There is a need to use the two spares in afsRecordLock for
64-bits interop. The proposed changes of the two spares are: 

314,315c314,315
<       unsigned32   l_start_pos_ext;
<       unsigned32   l_end_pos_ext;
---
>       unsigned32   l_spare0;
>       unsigned32   l_spare1;

[8/12/94 public]

Submit as proposed.

Build against dce1.1 bl-13.

Tests include:

	connectathon/basic test
	One writer/two readers data integrity test

[8/18/94 public]

Change "fix" to "closed"



CR Number                     : 11654
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Additional permissions required to rename a directory
Reported Date                 : 8/11/94
Found in Baseline             : 1.0.3
Found Date                    : 8/11/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[8/11/94 public]
The following two points need to be added to the DFS ACL chapter:
 
1. If you rename a directory, you also need to have w permission on the
   directory being renamed.
 
2. If the target object (the new name) already exists, the existing object
   is deleted.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[8/12/94 public]
I added the appropriate information to the file aclgroup.gpsml.  The changes
consist of two small additional pieces of text, both of which are enclosed in
the necessary change markers.  Rajesh Agarwalla verified the text.  This one
is done.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1doc' 
Changed Transarc Status from `open' to `closed'

[08/30/94 public]
Closed bug.



CR Number                     : 11632
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : /file/libafs
Short Description             : Unresolved or undefined symbols
Reported Date                 : 8/10/94
Found in Baseline             : 1.1
Found Date                    : 8/10/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b15
Affected File(s)              : src/security/kutils/RIOS/sec_kmalloc.c
Sensitivity                   : public

[8/10/94 public]

[ /file/libafs at 06:54 (AM) Wednesday ]
makepath libafs/. && cd libafs &&  exec make MAKEFILE_PASS=BASIC     build_all
xlc -c       -D_BSD -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOfile/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/config.c
xlc           -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib  -o cfgdfs.X cfgafs.o  -lafssys -lcfg -lodm  
mv cfgdfs.X cfgdfs
rm -f ./dfscore.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscore.exp .
rm -f ./extras.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/extras.exp .
rm -f ./export.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/export.exp .
rm -f ./afs.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/afs.exp .
ld -o dfscore.ext -H8 -edfs_core_config  -bexport:dfscore.exp -bexport:afs.exp  -bI:/lib/kernex.exp  -bI:extras.exp -bI:/lib/syscalls.exp  -bI:export.exp -bloadmap:dfscore.ext.ldmap `genpath -L../RIOS `  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib dfscorecfg.o -lvolreg -lvolume -lkxcred -lkdfskutil -lkosi  -lkicl -laixexport -lxvnode -lagfs -lktkc -lktkm  -laggr -lkolddacl -lufsops -lkdacl -lkdacllfs -lktpq -lkzlc -lkbomb -lkafsutil -lcsys -lsys
0706-221 WARNING:  Import version of 'uprintf' replaced by local definition.
0706-222 WARNING:  Import version of 'brkpoint' replaced by import definition.
rm -f  dfscore.exp
rm -f extras.exp
rm -f export.exp
rm -f afs.exp
rm -f ./dfscmfx.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscmfx.exp .
rm -f ./dfscore.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscore.exp .
rm -f ./extras.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/extras.exp .
rm -f ./export.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/export.exp .
rm -f ./afs.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/afs.exp .
ld -o dfscmfx.ext -H8 -eafs_config -bI:/lib/kernex.exp  -bexport:dfscmfx.exp  -bI:extras.exp -bI:/lib/syscalls.exp  -bI:export.exp -bI:dfscore.exp -bI:afs.exp -bloadmap:dfscmfx.ext.ldmap `genpath -L../RIOS `  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib config.o -lpx -lcm -lkfldb -lhost -lktkset  -lkcommondata -lkafs4srv -lkafs4clt  -lkrepcli -lkdfsncs -lknck -lkidl -lkdes -lcsys -lsys
0706-222 WARNING:  Import version of 'brkpoint' replaced by import definition.
0706-221 WARNING:  Import version of 'setgroups' replaced by local definition.
0706-222 WARNING:  Import version of 'uprintf' replaced by import definition.
0706-317 ERROR: Unresolved or undefined symbols detected:
		 Symbols in error (followed by references) are
		 dumped to the load map.
		 The -bloadmap:<filename> option will create a load map.
.sec__cred_create_authz_handle
*** Error code 8
`build_all' not remade because of errors.
*** Error code 1
`build_all' not remade because of errors.
*** Error code 1

[8/10/94 public]
Fix submitted.  Add -lksec to link line for dfscmfx.ext.

[ holmes 8/11/94 public ]

still has unrsolved symbols

[ /file/libafs at 06:52 (AM) Thursday ]
makepath libafs/. && cd libafs &&  exec make MAKEFILE_PASS=BASIC     build_all
xlc -c       -D_BSD -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOURCE -DAIX32   -I. -I/u1/devobj/sb/nb_rios/src/file/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/cfgafs.c
xlc -c       -DAFS_DEBUG -DKERNEL -D_KERNEL -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOURCE -DAIX32   -I. -I/u1/devobj/sb/nb_rios/src/file/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscorecfg.c
xlc -c       -DAFS_DEBUG -DKERNEL -D_KERNEL -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOURCE -DAIX32   -I. -I/u1/devobj/sb/nb_rios/src/file/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/config.c
xlc           -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib  -o cfgdfs.X cfgafs.o  -lafssys -lcfg -lodm  
mv cfgdfs.X cfgdfs
rm -f ./dfscore.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscore.exp .
rm -f ./extras.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/extras.exp .
rm -f ./export.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/export.exp .
rm -f ./afs.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/afs.exp .
ld -o dfscore.ext -H8 -edfs_core_config  -bexport:dfscore.exp -bexport:afs.exp  -bI:/lib/kernex.exp  -bI:extras.exp -bI:/lib/syscalls.exp  -bI:export.exp -bloadmap:dfscore.ext.ldmap `genpath -L../RIOS `  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib dfscorecfg.o -lvolreg -lvolume -lkxcred -lkdfskutil -lkosi  -lkicl -laixexport -lxvnode -lagfs -lktkc -lktkm  -laggr -lkolddacl -lufsops -lkdacl -lkdacllfs -lktpq -lkzlc -lkbomb -lkafsutil -lcsys -lsys
0706-221 WARNING:  Import version of 'uprintf' replaced by local definition.
0706-222 WARNING:  Import version of 'brkpoint' replaced by import definition.
rm -f  dfscore.exp
rm -f extras.exp
rm -f export.exp
rm -f afs.exp
rm -f ./dfscmfx.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscmfx.exp .
rm -f ./dfscore.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscore.exp .
rm -f ./extras.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/extras.exp .
rm -f ./export.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/export.exp .
rm -f ./afs.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/afs.exp .
ld -o dfscmfx.ext -H8 -eafs_config -bI:/lib/kernex.exp  -bexport:dfscmfx.exp  -bI:extras.exp -bI:/lib/syscalls.exp  -bI:export.exp -bI:dfscore.exp -bI:afs.exp -bloadmap:dfscmfx.ext.ldmap `genpath -L../RIOS `  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib config.o -lpx -lcm -lkfldb -lhost -lktkset  -lkcommondata -lkafs4srv -lkafs4clt  -lkrepcli -lkdfsncs -lknck -lkidl -lkdes  -lksec -lcsys -lsys
0706-222 WARNING:  Import version of 'brkpoint' replaced by import definition.
0706-221 WARNING:  Import version of 'setgroups' replaced by local definition.
0706-222 WARNING:  Import version of 'uprintf' replaced by import definition.
0706-317 ERROR: Unresolved or undefined symbols detected:
		 Symbols in error (followed by references) are
		 dumped to the load map.
		 The -bloadmap:<filename> option will create a load map.
.xfree
*** Error code 8
`build_all' not remade because of errors.
*** Error code 1
`build_all' not remade because of errors.
*** Error code 1

[8/10/94 public]
Fix submitted.  Should be xmfree, not xfree.

[09/26/94 public]
Closed.



CR Number                     : 11592
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fsint.klib
Short Description             : incomplete IDINCFLAGS in fsint.klib
Reported Date                 : 8/8/94
Found in Baseline             : 1.1b14
Found Date                    : 8/8/94
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b15
Affected File(s)              : src/file/fsint.klib/Makefile
Sensitivity                   : public

[8/8/94 public]
The fsint.klib makefile has an incomplete IDLINCFLAGS line that
causes the .acf file to be silently passed over if the corresponding
.idl file is checked out in a sandbox.

[8/10/94 public]
Went through nightly build.  Closed.



CR Number                     : 11580
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cmd svc
Short Description             : .sams messages used with dceprintf need /n on the end
Reported Date                 : 8/5/94
Found in Baseline             : 1.1b14
Found Date                    : 8/5/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b14
Affected File(s)              : /file/tools/cmd/cmd.sams
Sensitivity                   : public

[8/5/94 public]

There are messages in the cmd.sams file that are printed with
dce_printf.  To properly display these must end in \n which they
currently lack.

[8/5/94 public]
I've xferred this to Jon Ruby, who svc'ized this directory.
Jon used dce_printf instead of dce_svc_printf as cmd manipulates
the messages a lot. He will place the \n in the sams file on all
but 3 messages now that dce_printf no longer issues \n for messages.

[8/9/94 public]
The fix is waiting for verification by Andy Mckeen.

[09/26/94 public]
Closed.



CR Number                     : 11562
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : dfsd
Short Description             : CacheItems' failed (code 2)
Reported Date                 : 8/4/94
Found in Baseline             : 1.1
Found Date                    : 8/4/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b15
Affected File(s)              : src/file/afsd/afsd.c
Sensitivity                   : public

[8/4/94 public]



Smoketest failed on RIOS during dce_config


S:****** Starting dfsd...
dfsd: start sweeping disk cache files ....
dfsd: init CacheItems '/opt/dcelocal/var/adm/dfs/cache/CacheItems' failed (code 2)
S:****** Exiting from dce_config.

[8/5/94 public]

Assigning to myself since I touched dce_config and dfs_config most recently :-(.
Apparently this is not due to a dce_config/dfs_config change at all - the
file exists - perhaps this is the result of serviceability changes that
dfsd does not sense that this is a brand new configuration and complains
unnecessarily? I'll look ...

[8/9/94 public]
This problem is still visible in today's multi-cell smoke test on the RIOS. 
The build was based on the 08/08/94 sources.  The error message occur during 
the multi-cell smoke test's RIOS configuration.  The error message is the 
same as above, which is as such:

...
S:****** Starting dfsd...
dfsd: start sweeping disk cache files ....
dfsd: init CacheItems '/opt/dcelocal/var/adm/dfs/cache/CacheItems' failed (code 2)
S:****** Exiting from dce_config.
...

After the configuration, the run_commands.dfs test failed at the 

    cd /:

command with this error message:

    /:: A file or directory in the path name does not exist.

The run_commands.dfs script has the wait for the tsr.  i.e sleep 180
prior to the cd /: command.

Let me know if additional information is required.

[8/10/94 public]
Not clear why but, starting dfsd with both the -debug and -verbose
switches serves as a workaround for this problem. Assigning to Ron
Sasala whose serviceability work seems to be involved here. Submitting
a workaround to the rc.dfs script to start dfsd this way (on rios) as
a temporary way to enable the dfs smoketest to be run.

[8/11/94 public]
This problem has "nothing" (other than coincidence, dear Murphy) to do
with serviceability. The dfsd code in afsd.c is  randomly checking errno
without checking the return code for non-zero. It just so happens that the
errno has a value of 2 at this point, but the return from the following:
  call_syscall()...  is good (0).

It appears that the Transarc 1.0.3 update, 1.1.16.1, on 12/7/93 overlaid the
correction that was done previously for this problem via update 1.1.7.4 on 
5/13/93.

Setting verbose or debug seems to have cleared the random errno so that
by the time it was erroneously checked, it was zero. This is because those
modes issue messages with good returns.

This clearly is not a RIOS-specific problem. It applies to all platforms. 
Perhaps a placeholder OT should be opened so the problem doesn't get lost. 
This one routine is getting fixed. Others may also need the same fixing.

[8/11/94 public]
I submitted a fix for Ron S.  Fixed.

[09/26/94 public]
Closed.



CR Number                     : 11515
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/xaggr
Short Description             : missing include
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : /src/Makefile,/src/file/Makefile,
                                            /src/file/dfsbind/main_helper.c
Sensitivity                   : public

[8/2/94 public]



[ /file/xaggr at 21:29 (PM) Monday ]
makepath xaggr/. && cd xaggr &&  exec make MAKEFILE_PASS=THIRD     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/xaggr -I/project/dce/build/dce1.1-snap/src/file/xaggr  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/xaggr/dfstab.c
dfstab.c: 95: Unable to find include file 'dfsxagmac.h'.
*** Error code 1

[8/02/94 public]
The directory, file, has been added to the SAMIDL_SUBDIRS list in /src/Makefile
for the DFS serviceability items.

The subdirectories 'xaggr, dfsbind, afsd, pxd, update, and fsprobe' have been
added to the SAMIDL_SUBDIRS list in file/Makefile. The SAMIDL_SUBDIRS statement
has also been added to the Makefile in order to get the autogen pass to
run. This pass builds the serviceability files dfsxxxyyy.h for the xxx
components in the above subdirectories. The yyy items are mac, msg, and svc.
Thus, for xaggr, the dfsxagmac.h, dfsxagmsg.h and dfsxagsvc.h files get
built.

Additionally, due to ancestor string problems in the CR 10925 submission, caused
by a bug in bsubmit, an incorrect line was included in main_helper.c in the
/file/dfsbind subdirectory. The missing value, qsize was corrected to the
value, krpch_config_params.qsize.

This also fixes CRs 11516, 11517, 11518, 11519, 11522 and 11523. The problems
noted in them are all a fallout of the above fixes for 11515.

[8/02/94 public]
(How I enjoy this!) CR 11520 is also fixed by 11515.



CR Number                     : 11482
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : /file/episode/vnops
Short Description             : Unsatisfied symbols
Reported Date                 : 7/29/94
Found in Baseline             : 1.1
Found Date                    : 7/29/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/file/episode/vnops/Makefile
Sensitivity                   : public

[7/29/94 public]


[ /file/episode/vnops at 22:27 (PM) Thursday ]
makepath vnops/. && cd vnops &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -DEPI_USE_FULL_ID -DMOUNT_EFS=5 -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/vnops -I/project/dce/build/dce1.1-snap/src/file/episode/vnops  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/vnops/efs_newvol.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -DEPI_USE_FULL_ID -DMOUNT_EFS=5 -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/vnops -I/project/dce/build/dce1.1-snap/src/file/episode/vnops  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/vnops/stubs.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -DEPI_USE_FULL_ID -DMOUNT_EFS=5 -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/vnops -I/project/dce/build/dce1.1-snap/src/file/episode/vnops  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/vnops/efs_growaggr.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -DEPI_USE_FULL_ID -DMOUNT_EFS=5 -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/vnops -I/project/dce/build/dce1.1-snap/src/file/episode/vnops  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/vnops/test_vnodeops.c
gencat dfszev.cat efs_trace.msf
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o newvol.X efs_newvol.o stubs.o  -lefsops -ldir -lanode -llogbuf -lasync -leacl  -lxcred -lbomb -lafsutil -ldfskutil  -ledsk -lftutil  -lcom_err -ltools -losi  -ldacl -ldacllfs -ldacl -licl  -ldce -lBSD  
/bin/ld: Unsatisfied symbols:
   osi_GetNGroups (code)
*** Error code 1

[7/29/94 public]
Fix submitted.  Re-ordered link command line.  No idea why this is
showing up now.

[09/26/94 public]
Closed.



CR Number                     : 11447
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : gateway
Short Description             : Integrate NFS gateway
Reported Date                 : 7/26/94
Found in Baseline             : 1.1
Found Date                    : 7/26/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b14
Affected File(s)              : many
Sensitivity                   : public

[7/26/94 public]

Integrate the DFS/NFS Gateway into the DCE 1.1 source base.



CR Number                     : 11388
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 11356
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Document prohibition on unauthenticated ACL entry
Reported Date                 : 7/21/94
Found in Baseline             : 1.0.3
Found Date                    : 7/21/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : ./src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[7/21/94 public]
Document (1) that the unauthenticated ACL entry can no longer be added to the
ACL of a DCE LFS object and (2) how to handle existing unauthenticated entries
that may exist on the ACLs of DCE LFS objects.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[7/22/94 public]
In the DFS ACL chapter, I modified the section that discusses how the
permissions granted to unauthenticated users are determined.  The section
now concludes with an extended note that discusses the status of the
unauthenticated entry on the ACLs of DCE LFS objects.  The text also
provides details about how to remove such entries.  All changes were
reviewed by Rajesh Agarwalla, who made the corresponding code change.
 
Filled in Inter-dependent CRs with `11356' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[08/30/94 public]
Closed bug.



CR Number                     : 11350
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Local root gets privilege of machine's self principal
Reported Date                 : 7/18/94
Found in Baseline             : 1.0.3
Found Date                    : 7/18/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[7/18/94 public]
Well, it's back:  Logging into a machine as local root provides a user with
privileges/permissions of the local machine's hosts/<hostname>/self principal
for access to DFS via a /... pathname.  Formerly, local root was treated as an
unauthenticated user for DFS access via the /... path.  I'm really doing it
this time, I'm making this change; stop me now; it'll be done by the end of
the day; I'm not kidding....
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[7/18/94 public]

[7/18/94 public]
The necessary references to this functionality were documented in the following
pair of files:
 
./src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
./src/dce_books/dfs_admin_gdref/gd/issues.gpsml
 
The changes were verified by Craig Everhart and Rajesh Agarawalla.  This one
can be closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Filled in Affected File with `See Description.' 
Changed Transarc Status from `open' to `closed'

[08/30/94 public]
Closed bug.



CR Number                     : 11346
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : need uniform interface
Reported Date                 : 7/18/94
Found in Baseline             : 1.1
Found Date                    : 7/18/94
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b20
Affected File(s)              : dfs.block_frag dfs.glue dfs.lock dfs.maxdir dfs.maxfile dfs.repfs
Sensitivity                   : public

[7/18/94 public]

The automated dfs system tests do not all have the same
command line interface.

[7/19/94 public]
Changes submitted

[09/27/94 public]
Closed.



CR Number                     : 11307
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : acl manager support
Short Description             : Modify DFS ACL
manager names and helpstrings as described below.
Reported Date                 : 7/14/94
Found in Baseline             : 1.1b11
Found Date                    : 7/14/94
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b17
Affected File(s)              : security/client/acl/dfs_dce_acl.c
Sensitivity                   : public

[7/14/94 public]

  Old names        New names/helpstrings
--------------    ---------------------------
DFS               {dfs           Distributed File Service server}

______________________________________

BACKGROUND:
 A polymorphic object, as in DCE 1.1, can be a principal name and a 
 principal directory and therefore have 2 ACL managers. 

 In "dcecp", the ACL manager name can be obtained by invoking the
 "acl show -managers" command.  The other "acl" commands can
 accept a -type option to specify the manager name.  Previously, 
 this ACL manager name was really only informational and not visible
 to the end user, but now it is.

 Below, I have compiled a list of the ACL manager names and their 
 "helpstrings" that will be displayed to the user.

CDS ACL Managers:
    GDA managment ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/cds-gda -managers
        {GDA              Change ACL for gda management}

    CDS Server Management ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/cds-server -managers
        {CDS Server       Change ACL for server management}

    CDS Directory entry ACL manager:
        dcecp> acl show /.:/cell-profile -managers -entry
        {Directory entry  Change ACL for CDS directory entry}

    CDS Directory ACL manager:
        dcecp> acl show /.:/hosts -managers
        {Directory        Change ACL for CDS directory}

    CDS Clearinghouse ACL manager:
        dcecp> acl show /.:/<hostname>_ch -managers
        {Clearinghouse    Change ACL for CDS clearinghouse}

    CDS Clerk management ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/cds-clerk -managers
        {CDS Clerk        Change ACL for clerk management}


Security ACL Managers:
    Security Directory ACL manager:
	dcecp> acl show /.:/sec/principal -managers
	dcecp> acl show /.:/sec/group -managers
	dcecp> acl show /.:/sec/org -managers
        {Sec Directory    Manage directory object acls.}

    Security Principal ACL manager:
        dcecp> acl show /.:/sec/principal/<name> -managers
        {Sec Principal    Manage principal object acls.}

        A polymorphic object will look like this:
	dcecp> acl show /.:/sec/principal/<princ-dir-name> -managers
	{Sec Principal    Manage principal object acls.}
	{Sec Directory    Manage directory object acls.}

    Security Group ACL manager:
        dcecp> acl show /.:/sec/group/<name> -managers
        {Sec Group        Manage group object acls.}

        A polymorphic object will look like this:
	dcecp> acl show /.:/sec/group/<group-dir-name> -managers
	{Sec Group        Manage group object acls.}
	{Sec Directory    Manage directory object acls.}

    Security Organization ACL manager:
        dcecp> acl show /.:/sec/org/<name> -managers
        {Sec Org          Manage org object acls.}

        A polymorphic object will look like this:
	dcecp> acl show /.:/sec/org/<org-dir-name> -managers
	{Sec Org          Manage org object acls.}
	{Sec Directory    Manage directory object acls.}

    Security Replica List ACL manager:
        dcecp> acl show /.:/sec/replist -managers
        {Sec Replist      Manage the replist object acl.}

    Security Policy ACL manager:
        dcecp> acl show /.:/sec/policy -managers
        {Sec Policy       Manage the policy object acl.}

    Security Extended Attribute Schema ACL manager:
        dcecp> acl show /.:/sec/xattrschema/<name> -managers
        {Sec Attr_schema  Manage the attr_schema object acl.}


DTS ACL manager:
    dcecp> acl show /.:/hosts/<hostname>/dts-entity -managers
    {dtsd             DTSD DCE Time Service daemon}


Auditing ACL manager:
    dcecp> acl show /.:/hosts/<hostname>/aud-acl -managers
    {audit            Audit daemon ACL manager}


DCED ACL managers:

    Generic ACL managers for DCED:
	dcecp> acl sh /.:/hosts/<hostname>/self -m
	dcecp> acl sh /.:/hosts/<hostname>/config -m
        {dced-mgmt        DCED manager}

        dcecp> acl show /.:/hosts/<hostname>/config/srvrconf -managers
        dcecp> acl show /.:/hosts/<hostname>/config/srvrexec -managers
	dcecp> acl show /.:/hosts/<hostname>/config/hostdata -managers
        dcecp> acl show /.:/hosts/<hostname>/config/keytab -managers
        dcecp> acl sh /.:/hosts/<hostname>/config/attr_schema -managers
        dcecp> acl show /.:/hosts/<hostname>/config/secval -managers
        {dced-managers    DCED manager Pseudo-object}

    Server configuration ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/config/srvrconf/<name> -managers
        {srvrconf         Server Configuration}

    Server execution ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/config/srvrexec/<name> -managers
        {srvrexec         Server Execution}
    
    Hostdata configuration ACL managers:
        dcecp> acl show /.:/hosts/<hostname>/config/hostdata/<name> -managers
        {hostdata         Host Data}

    Keytab configuration ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/config/keytab/<name> -managers
        {rkeytab          Key Management}

    Extended Attribute Schema configuration ACL manager:
        dcecp> acl sh /.:/hosts/<hostname>/config/xattrschema/<name> -managers
        {attr_schema      Attribute Schema}

    Secval configuration ACL manager:
        dcecp> acl show /.:/hosts/<hostname>/config/secval/<name> -managers
        {secval           Security Validation}


DFS ACL manager:
    dcecp> acl show /.:/fs/<name> -managers
    {DFS              DFS ACL manager}


Misc:
    dcecp> acl show <application-specific ACL object>  -managers
    {general          General ACL Manager}
    Note that there is nothing DCE ships that uses this ACL manager.
    This is part of the ACL manager library for application
    developers.

____________________________________________________________
NEW GUIDELINES FOR ACL MANAGER NAMES:

We would like to enforce guidelines for all DCE ACL
managers.  These are not architectural and dcecp will accept 
any valid ACL manager names, but we would like DCE-named ACL 
managers to be named consistently and in an easily
usable manner.  Since dcecp is what will make these names
visible to a user, we are using dcecp attribute naming
conventions:

ACL manager names for all of DCE must be:
1) lowercase
2) no underscores
3) no spaces
4) 16 bytes (sizeof "sec_acl_printstring_len").
5) Choose names similiar to object command names supported in
   dcecp wherever possible.
   For example, the ACL manager name "principal" refers to
   the object that contains registry information about principals;
   /.:/sec/principal.
   Note that dcecp will allow abbreviations. For example, a user
   can specify "org" for the ACL manager name "organization".
6) Names must be unique within a component's ACL manager but not
   within DCE. 
   For example, "xattrschema" can be used for a DCED Extended
   Attribute Configuration Schema ACL object and for a Security 
   Extended Registry Attribute Schema ACL object.
7) The helpstring for an ACL manager must specify the component
   that owns or manages the objects in questions, since this 
   information cannot always be derived from the ACL manager name.
____________________________________________________________
PROPOSAL:

Below are the current DCE ACL manager names/helpstrings and their 
proposed names/helpstrings following the above guidelines.

  Old names        New names/helpstrings
--------------    ---------------------------
GDA               {gda           Global Directory Agent server}
CDS Server        {cdsserver     Cell Directory Service server}
Directory entry   {entry         CDS namespace entry ACLs}
Directory         {directory     CDS directory ACLs}
Clearinghouse     {clearinghouse CDS clearinghouse ACLs}
CDS Clerk         {cdsclerk      Cell Directory Service clerk}
Sec Principal     {principal     Registry principal object ACLs}
Sec Directory     {secdirectory  Registry directory object ACLs}
Sec Group         {group         Registry group object ACLs}
Sec Org           {organization  Registry organization object ACLs}
Sec Replist       {replist       Registry replica list object ACL}
Sec Policy        {policy        Registry policy object ACL}
Sec Attr_schema   {xattrschema   Registry extended attribute schema object ACLs}
dtsd              {dts           Distributed Time Service server or clerk}
audit             {audit         Audit server}
dced-mgmt         {dced          DCED server}
dced-managers     {container     DCED container ACLs}
srvrconf          {srvrconf      DCED server configuration object ACLs}
srvrexec          {srvrexec      DCED server execution object ACLs}
hostdata          {hostdata      DCED host data object ACLs}
rkeytab           {keytab        DCED key management object ACLs}
attr_schema       {xattrschema   DCED extended attribute schema object ACLs} 
secval            {secval        DCED security validation service}
DFS               {dfs           Distributed File Service server}
general           {generic       Generic ACL server}

[9/2/94 public]

Fix submitted.

[09/26/94 public]
Closed.



CR Number                     : 11286
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : merge dce1.1a to DCE1.1
Reported Date                 : 7/13/94
Found in Baseline             : 1.1
Found Date                    : 7/13/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1
Affected File(s)              : see below
Sensitivity                   : public

[7/13/94 public]

Merged files from DCE1.1a backing tree to DCE1.1.  

The files affected are:

    src/rpc/rpc.mk
    src/test/rpc/rpc.mk
    src/file/*
    src/test/file/*
    src/systest/file/*
    src/lbe/mk/osf.std.mk
    src/rpc/kruntime/*helper.[ch]


thanks
annie

[7/13/94 public]

Files were merged to DCE1.1 last night and build has NO errors.

thanks
annie



CR Number                     : 11248
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 10766
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : replication
Short Description             : dfs.repfs fails rios -> hp
Reported Date                 : 7/11/94
Found in Baseline             : 1.1a
Found Date                    : 7/11/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b18
Affected File(s)              : ones w/ dlg debug
Sensitivity                   : public

[7/11/94 public]

Using dfs.repfs 48 CHO testing of dce1.1ab9 on m1 (rios - sec server)
to m3 (hp) appears to have caused some serious damage to the cell.
Symptoms include:

sec_cred_is_authenticated returns status=0 test=1
sec_cred_is_authenticated returns status=0 test=1
sec_cred_is_authenticated returns status=0 test=1
sec_cred_is_authenticated returns status=0 test=1
sec_cred_is_authenticated returns status=0 test=1
sec_cred_is_authenticated returns status=0 test=1
sec_cred_is_authenticated returns status=0 test=1

in the BosLogs of all 3 servers.

Core dumps on m3 of ftserver and repserver (10766)

Cdsadv on m2 hung - as well as dfsbind on m2 (hp - cds server).
(Cdsadv could be killed and restarted, dfsbind can not).

Error messages of the form:
dfs: server disk quota exceeded
dfs: set context failed: code 572616706
dfs: lost contact with the fx server 130.105.201.7 in cell cho_cell.qadce.osf.org
dfs: file server 130.105.201.7 in cell cho_cell.qadce.osf.org back up!

where:
572616706 (decimal), 22217002 (hex): Internal corruption (dfs / fsh)

repfs.rios fileset filled - unclear why.
an attempt to delete the repfs.rios fileset failed with:
fts delete repfs.rios -server cobbler -aggr m1.aggr2
AFS_SetContext(cobbler.osf.org) fails: Internal corruption (dfs / fsh)
Error in delete: Internal corruption (dfs / fsh)

repfs.rios's readonly copy out of date and generating:

dfs: fileset (0,,31) error, code 691089421, on server 130.105.202.30 in cell cho_cell.qadce.osf.org.

where:

691089421 (decimal), 2931300d (hex): replica not current enough (dfs / xvl)

[7/12/94 public]
Ron Arbo thinks the "sec_cred_is_authenticated" messages that are
filling up the BosLogs may account for a lot of this ... these 
messages were inserted to help debug delegation code in user-space
but shouldn't be "on" since this code hasn't been tested ... Rerunning
dfs.repfs now - it's possible that it will pass in isolation since
the first run was part of CHO (ie. acl, fts tests 12+14 and some
cthon tests were running simultaneously for part of the time).

[8/26/94 public]
Ron did get rid of the messages but dfs.repfs rios -> hp not passing
still - dfsbind on rios coredumped - coredump not useable :-(. Will
retry with Max Grasso's small dfsbind fix.

[8/29/94 public]
Max Grasso's dfsbind fix okay but CR 11925 causing dfsbind to coredump.
BEFORE the coredump however, dfs.repfs rios->hp reports:

**** Starting Switch Replica Location Section****
**** Create new replica and remove old replica ****
fts addsite -fileset repfs.rios -server dce11 -aggregate m3.aggr2
Added replication site dce11 m3.aggr2 for fileset repfs.rios
dce11:    SRCVV = 629    CURVV = 629
fts rmsite -fileset repfs.rios -server cobbler -aggregate m1.aggr2
Removed replication site cobbler 2 for fileset repfs.rios
Client(25634): Starting at Mon Aug 29 18:29:52 1994
(lots of filewnr/dirwrite.sh/dirread output - all PASSED)
Next Scheduled Replication: 300
Sleeping for 300 seconds starting at Mon Aug 29 19:11:18 EDT 1994
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
dce11:    SRCVV = 2406    CURVV = 1780
Replication not yet complete
ERROR (verify_rep_completed): Replication did not complete
ITERATION 1: FAILED

Lost quorum for awhile during ITERATION 2 so:
Replication not yet complete
Could not get info about site cobbler.osf.org (no quorum elected (dfs / ubk))
Error: no quorum elected (dfs / ubk)
connToServer(cobbler.osf.org): no known principal for this connection; no quorum
 elected (dfs / ubk), proceeding...
 Could not get info about site dce11.osf.org (no quorum elected (dfs / ubk))
 Error: no quorum elected (dfs / ubk)
 connToServer(dce11.osf.org): no known principal for this connection; no quorum elected (dfs / ubk), proceeding...
 dce11:    SRCVV = 3031    CURVV = 1780
 Replication not yet complete
 dce11:    SRCVV = 3031    CURVV = 1780
 Replication not yet complete
 ERROR (verify_rep_completed): Replication did not complete
 ITERATION 2: FAILED

RepLog contains this, the first of many of this sequence of messages:
94-Aug-29 18:20:08 setNextTime: 0,,231: delay 120 secs: GetIncrementalDump: abor
ted: lost WVT
94-Aug-29 18:22:48 setNextTime: 0,,231: delay 0 secs: StartVolumeRetrieval: Abou
t to publish newly-built volume
94-Aug-29 18:22:49 setNextTime: 0,,231: delay 36000 secs: ReplicaWantsAdvance: a
ll OK at 778198969
94-Aug-29 18:28:18 setNextTime: 0,,234: delay 0 secs: StartVolumeRetrieval: Abou
t to publish newly-built volume

NOTE: dfs.repfs hp->rios (same pair of machines, simultaneous runs) passed
4 iterations before the dfsbind core dump)
ITERATION 4 COMPLETED AT Mon Aug 29 21:16:13 EDT 1994
ITERATION 4: PASSED

[gmd public]
This test is currently PASSING on bl-18 - note that CR 10776 (repserver
coredump) was seen on the HP this time and not the RIOS. As luck would
have it, dfs.repfs hp->rios is now not passing (probably due to 10776)
The security bugs causing dfsbind to core dump have been fixed so I may
be able to close this one if this run completes.

[gmd public]
The above test run did not complete successfully, apparently due to CR
10776 striking the RIOS as well. I'm comfortable closing this CR since
I believe CR 10776 covers all the problems with getting this test to
pass. (There MAY be performance-related problems as well but that too
is logged under a separate CR)



CR Number                     : 11180
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Incorporate DFS/NFS Gateway documentation
Reported Date                 : 7/6/94
Found in Baseline             : 1.0.3
Found Date                    : 7/6/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[7/6/94 public]
Documentation for the new DFS/NFS Secure Gateway from H-P needs to be
incorporated into the DFS documentation.  The source files are provided by
Alan Houser at H-P.  I need to take the files and work them into the DFS
Admin Guide and Ref.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[8/25/94 public]
A number of files were added or modified for incorporation of the DFS/NFS
Secure Gateway documentation.  The following files were modified for the
Gateway documentation:
 
./dce_books/intro/intro/3k_dfs.gpsml
./dce_books/dfs_admin_gdref/README
./dce_books/dfs_admin_gdref/Description
./dce_books/dfs_admin_gdref/frontmatter/preface.gpsml
./dce_books/dfs_admin_gdref/gd/README
./dce_books/dfs_admin_gdref/gd/overview.gpsml
./dce_books/dfs_admin_gdref/gd/processes.gpsml
./dce_books/dfs_admin_gdref/ref/man4dfs/dfs_intro.4dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfs_intro.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bos_getlog.8dfs
 
The following files were added for the Gateway documentation:
 
./dce_books/dfs_admin_gdref/gd/gateway_app.gpsml
./dce_books/dfs_admin_gdref/ref/man4dfs/DfsgwLog.4dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfs_login.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfs_logout.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw_add.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw_apropos.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw_delete.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw_help.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw_list.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgw_query.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsgwd.8dfs
 
In addition, I forwarded two new definitions ("DFS/NFS Secure Gateway" and
"PAG") to Willie at OSF for inclusion in the Glossary in the Introduction to
OSF DCE document.
 
All files were reviewed by Alan Houser at HP, the appropriate developers at HP
(e.g., John Brezak), and Hal at OSF.  I believe Andy McKeen at OSF also looked
over the files.  I have incorporated all comments into the text.  Barring any
further editorial or technical comments that may arise, this one can be closed.
 
Changed Interest List CC from `arh@apollo.hp.com' to `arh@apollo.hp.com, 
 hal@osf.org' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1doc' 
Changed Transarc Status from `open' to `closed'

[09/01/94 public]
Closed bug.



CR Number                     : 11090
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : Makefiles
Short Description             : Need to turn OSDEBUG macro off in HP kernel builds
Reported Date                 : 6/27/94
Found in Baseline             : 1.1a
Found Date                    : 6/27/94
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : rpc/rpc.mk, file/hp800/machdep.mk, test/rpc/rpc.mk
Sensitivity                   : public

[6/27/94 public]

The following three makefiles need to be updated to remove -DOSDEBUG
from the HP Kernel flags.  This is needed so that the kernel
extensions built in the dce tree are compatible with the default
kernels provided by HP.

  ./file/HP800/machdep.mk
  ./rpc/rpc.mk
  ./test/rpc/rpc.mk

[6/27/94 public]

Submitted..

[09/27/94 public]
Closed.



CR Number                     : 11076
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : /file/libafs
Short Description             : Unresolved or undefined symbols detected
Reported Date                 : 6/24/94
Found in Baseline             : 1.1
Found Date                    : 6/24/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : ./rpc/runtime/uuid.c ./rpc/runtime/uuidp.h ./rpc/runtime/uuidsys.c
Sensitivity                   : public

[6/24/94 public]

[ /file/libafs at 12:54 (PM) Friday ]
makepath libafs/. && cd libafs &&  exec make MAKEFILE_PASS=BASIC     build_all
xlc -c     -O  -D_BSD -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOURCE -DAIX32   -I. -I/u1/devobj/sb/nb_rios/src/file/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/cfgafs.c
xlc -c     -O  -DAFS_DEBUG -DKERNEL -D_KERNEL -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOURCE -DAIX32   -I. -I/u1/devobj/sb/nb_rios/src/file/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscorecfg.c
xlc -c     -O  -DAFS_DEBUG -DKERNEL -D_KERNEL -D_ALL_SOURCE -DAFS_AIX32_ENV -D_ALL_SOURCE -DAIX32   -I. -I/u1/devobj/sb/nb_rios/src/file/libafs -I/project/dce/build/dce1.1-snap/src/file/libafs   -I/u1/devobj/sb/nb_rios/export/rios/usr/include -I/project/dce/build/dce1.1-snap/export/rios/usr/include   /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/config.c
xlc           -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib  -o cfgdfs.X cfgafs.o  -lafssys -lcfg -lodm  
mv cfgdfs.X cfgdfs
rm -f ./dfscore.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscore.exp .
rm -f ./extras.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/extras.exp .
rm -f ./export.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/export.exp .
rm -f ./afs.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/afs.exp .
ld -o dfscore.ext -H8 -edfs_core_config  -bexport:dfscore.exp -bexport:afs.exp  -bI:/lib/kernex.exp  -bI:extras.exp -bI:/lib/syscalls.exp  -bI:export.exp -bloadmap:dfscore.ext.ldmap `genpath -L../RIOS `  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib dfscorecfg.o -lvolreg -lvolume -lkxcred -lkdfskutil -lkosi  -lkicl -laixexport -lxvnode -lagfs -lktkc -lktkm  -laggr -lkolddacl -lufsops -lkdacl -lkdacllfs -lktpq -lkzlc -lkbomb -lkafsutil -lcsys -lsys
0706-221 WARNING:  Import version of 'uprintf' replaced by local definition.
0706-222 WARNING:  Import version of 'brkpoint' replaced by import definition.
rm -f  dfscore.exp
rm -f extras.exp
rm -f export.exp
rm -f afs.exp
rm -f ./dfscmfx.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscmfx.exp .
rm -f ./dfscore.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/dfscore.exp .
rm -f ./extras.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/extras.exp .
rm -f ./export.exp
cp /project/dce/build/dce1.1-snap/src/file/export/RIOS/export.exp .
rm -f ./afs.exp
cp /project/dce/build/dce1.1-snap/src/file/libafs/RIOS/afs.exp .
ld -o dfscmfx.ext -H8 -eafs_config -bI:/lib/kernex.exp  -bexport:dfscmfx.exp  -bI:extras.exp -bI:/lib/syscalls.exp  -bI:export.exp -bI:dfscore.exp -bI:afs.exp -bloadmap:dfscmfx.ext.ldmap `genpath -L../RIOS `  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib     -L/project/dce/build/dce1.1-snap/export/rios/usr/lib config.o -lpx -lcm -lkfldb -lhost -lktkset  -lkcommondata -lkafs4srv -lkafs4clt  -lkrepcli -lkdfsncs -lknck -lkidl -lkdes -lcsys -lsys
0706-222 WARNING:  Import version of 'brkpoint' replaced by import definition.
0706-221 WARNING:  Import version of 'setgroups' replaced by local definition.
0706-222 WARNING:  Import version of 'uprintf' replaced by import definition.
0706-317 ERROR: Unresolved or undefined symbols detected:
		 Symbols in error (followed by references) are
		 dumped to the load map.
		 The -bloadmap:<filename> option will create a load map.
.dce_get_802_addr
*** Error code 8
`build_all' not remade because of errors.
*** Error code 1
`build_all' not remade because of errors.
*** Error code 1

[6/24/94 public]
Tom and I just spoke about this.  He'll be fixing it.

[6/24/94 public]
Put the routine uuid__get_os_address() back in to uuidsys.c,
since krpc has its own version.  For runtime, it is now
a wrapper around dce_get_802_addr().

[09/27/94 public]
Closed.



CR Number                     : 11074
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : delegation
Short Description             : enable setting/getting delegate entries
Reported Date                 : 6/24/94
Found in Baseline             : 1.1a
Found Date                    : 6/24/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b15
Affected File(s)              : n/a
Sensitivity                   : public

[6/24/94 public]
Diane ifdef'ed out the user space code she wrote that deals with 
converting extended acl entries from DFS to sec format.  This
was to prevent folks from setting/getting extended acl entries
in DFS before full delegation was in place.  Problem is, I have to 
build a libdce every time I want to use this code, so I'm removing 
the ifdefs.

[09/27/94 public]
Closed.



CR Number                     : 11070
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : fxd
Short Description             : rios crashes with 1.1 secd + 103a dfs
Reported Date                 : 6/24/94
Found in Baseline             : 1.1
Found Date                    : 6/24/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b18
Affected File(s)              : src/file/px/px_subr.c
Sensitivity                   : public

[6/24/94 public]

Using 1.1 baseline 10 (BETA), I can crash the rios soldier which
is the security server, a dts server and a flserver for the cell,
within 2 hours of starting dfs.glue (on soldier)  and dfs.lock
(on west, an hp which is the cds server for the cell). I am
attempting to reproduce for the 3rd time in the hopes that the
crash dump is more useful ... if not, Mike and I will have to
come up with a plan.

[7/7/94 public]
I switched rios's (now using cobbler) and I can still consistently
crash ... maybe you'll have better luck figuring out what's going on than I
if you have master/slave kernel debugging set up ...

Here's the scoop:
1) configure a cell similar to the one I've been using - I suspect
you can use a smaller/less complex cell but maybe not
Make sure you have enough space on your dump device (/dev/hd7)
for a crash dump - you're going to need it :-)

2) install the dfs system tests

3) update the data file to reflect your machines, dfsexported
native filesystem and mount point -  also use the "root"
principal

4) run the test (syntax here is for a 4 hour run - it won't get
that far):
	cd /dcetest/dcelocal/test/systest/file
	./dfs.glue ./glue.data 4 > ./glue.log 2>&1 &

CONFIG = 1 RIOS, 3 HP's (but you can probably go smaller)
RIOS = sec, dts, fldb and file server (also happens to be fldb sync site)
HP1 = cds, dts, fldb and file server
HP2 = dts, fldb and file server
HP3 = dts server, dfs client

My datafile looks like this:
MACHINES="darkman dce8 dce11 cobbler"
TROOT="/dcetest/dcelocal/test"
RUN_CACHEMGRTESTS=FALSE
UFS_PATH="/u0/glue_dir"
DFS_PATH="/:/m1.jfs/glue_dir"
LARGE_FILE="/etc/dce_config"
PRINC=root
PW=-dce-
NUMDIRENTRIES=10
NUMFILEWRITES=100
NUMPROCPERMACH=1
MAXTIME_DFSWRITEUPDATE=30

It appears to be happen during the local read via DFS - the filewnr
program is swapped in according to the crash dump but its stack trace
looks innocent enough doesn't it?

 97 s     612f  5e19  5e19     0     0    64     0 05a51820 filewnr  
	 FLAGS: swapped_in

> trace 97
STACK TRACE:
	.e_wait ()
	.e_sleep ()
	[dfscore.ext:osi_aix_Sleep] ()
	[dfscore.ext:osi_SleepW] ()
	[dfscmfx.ext:cm_read] ()
	[dfscmfx.ext:cm_rdwr] ()
	[dfscore.ext:naix_rdwr] ()
	[dfscore.ext:xglue_rdwr] ()
	.vno_rw ()
	.rwuio ()
	.rdwr ()
	.kreadv ()

[7/13/94 public]
I was able to reproduce the problem with 2 RS/6000 machines. The process
that was causing the crash is fxd. The following is the stack trace of a
thread from fxd:

osi_PreemptionOff
px_read
px_rdwr
SAFS_FetchData
op2_ssr
rpc__dg_execute_call
cthread_call_executor
thread_boot
procentry

[7/14/94 public]
Hit this problem again with dfs.repfs - my fxd stack traces from
the crash dump do not match your trace however ... Mike Schmitz
here at OSF is familiar with the osi_PreemptionOff routine so
I'm adding him to the interest list - Please update this bug
with who is working on the problem and the progress/questions
etc. so he can help  -  Thanks.

[7/15/94 public]
Okay - I give up - hit this with dfs.lock - apparently I can't
do any dfs testing with the rios platform until this is fixed.

[7/28/94 public]
I have given Ron Arbo a new px_read function to replace the existing
version in px_subr.c. I have run the dfs.glue test and it no longer
causes the machine to panic. I have opened OT 11400 because of a
timeout problem that is occuring in rpc.

[8/11/94 public]

I have used the new code and yes, this does avoid the panic and
cause a timeout problem. We haven't been able to make any progress
on debugging the timeout problem (11400). I was under the impression
that this code change was a workaround more than a fix - if this
is NOT the case, then we need this code submitted and then this
bug can be closed (and CR 11400 will track the remaining timeout
problem).

[8/24/94 public]
Hello? Have you been able to find a real fix or did you want to
submit this workaround? Let us know asap - thanks.

[8/26/94 public]
I have been using the new code on BL16 and a panic is still occuring.
We captured the exception, which is 804779204. We are in the process
of rebuilding the rpc kruntime in order to get a stack traceback.
At first I thought I might be running with the wrong code when a
panic occured. Has something changed that is causing this second
panic to occur. The traceback is the same as the one before.

[8/29/94 public]
The value 804779204 is incorrect, that was the address of the structure
for THIS_CATCH. "THIS_CATCH" is of type EXCEPTION which had the
following values at the specified address:

	EXCEPTION
		exc_kind
			address		0x2130456
			status		0x16c9a80
		match [value/address]	0x2ff7fc60

Since the value for match appeared to be an address in the stack, I
assume that 0x16c980 (23894656 in base 10) is the exception status.
I will append this information in OT 11400.

[8/31/94 public]
0x2130456 corresponds to a value of _exc_kind_status in the 
enum _exc_kind so that part looks right.  However, I was under
the (perhaps mistaken) impression that if you retrieved the
value of THIS_CATCH->match.value in the CATCH_ALL clause, you 
could look up the value in the rpcsts.h header file to find out
what kind of exception was raised.  However, I can't seem to
match either 0x16c9a80 or 0x2ff7fc60 to valid exceptions in
the header file even after converting to base 10. 

Could someone from RPC land comment on whether this approach
for finding out the exception type in kernel is correct?  Also, Bob, I
thought when we last spoke you were also planning on changing the
RAISE macro to be a function and then setting a breakpoint
in the kernel debugger on that function to get a stack trace for 
when this occurs.  Are you still planning on doing that?  Thanks..

[9/01/94 public]
The exception is 0x16c9a80 (382312576) which is
rpc_s_fault_pipe_comm_error. We are looking into places where this
exsption is being raised.

[9/01/94 public]
We have finally been able to get a proper stack trace for this
problem. We first found that the client side was raising the
exception rpc_s_call_timeout (382312556). The stack trace is:

	rpc_ss_ndr_unmar_by_copying
	rpc_ss_ndr_u_fix_or_conf_arr
	rpc_ss_ndr_unmar_pipe
	rpc_ss_ndr_unmar_interp
	AFS_FetchData
	cm_FetchDCache
	cm_GetDOnLine
	cm_GetDLock
	bkg_Prefetch
	cm_bkgDaemon
	afscall_cm
	kafs_syscall

We then continued until we another exception was raise in a thread
of fxd. This stack trace produced the following:

	rpc_ss_xmit_iovec_if_necess
	rpc_ss_ndr_marsh_by_pointing
	rpc_ss_ndr_m_fix_or_conf_arr
	rpc_ss_ndr_ee_marsh_pipe_chunk
	px_read
	... (rest of stack is as listed earlier)

rpc_ss_xmit_iovec_if_necess contains the following code:

	rpc_call_transmit ((rpc_call_handle_t)IDL_msp->IDL_call_h,
			   (rpc_iovector_p_t)IDL_msp->IDL_iovec,
			   (unsigned32 *)&IDL_msp->IDL_status);
	if (IDL_msp->IDL_status != error_status_ok)
		RAISE (rpc_x_ss_pipe_comm_error);

When RAISE was called we hit the breakpoint we had set and found that
the value for IDL_msp->status was rpc_s_call_orpaned (382312542). We are
now in the process of running the test again to get a system dump on
the first exception that was raised by the client in order to examine
all of the threads in fxd.

[9/8/94 public]

From the discussion above, it sounds like:

    1. The client set a call timeout value in its binding handle
       (meaning it was only willing to wait a certain amount of 
       time for an RPC to complete, even if the RPC appeared to be
       healthy).
    2. The client made a call to a server, and the server was taking
       too long to execute the call.
    3. The client-side RPC runtime faulted the call with the 
       rpc_s_call_timeout status.
    4. Part of faulting the call (on the client) is to send a quit
       to the server (to orphan the call).
    5. The server received the orphan quit, and set the call status
       to rpc_s_call_orpaned.  (A cancel is then posted against the
       call thread, but this is a no-op in the kernel.)
    6. The server calls back into the runtime to transmit/receive
       more data, the runtime notices that the call was orphaned,
       and returns a status (i.e., IDL_msp->IDL_status) of 
       rpc_s_call_orpaned.
    7. The IDL runtime sees this bad status, and raises a
       rpc_x_ss_pipe_comm_error exception.

(Apparently, at one point, the px_read was causing a panic when it caught
this exception, but that was fixed/worked-around so that now we just see 
the exception.)  

The claim here, and in 11400, is that there's a bug in RPC because a call
times out.  There are many places where a call could end up wasting time,
and cause the client to timeout;  is there some evidence that the time is
being wasted in RPC.

[9/8/94 public]

Another question: does the rpc_s_call_timeout happen with the post 8/25
build (after the last MBF code drop)?

[9/8/94 public]
This is being tested on b16, so I would assume that does NOT include the
last MBF drop. As for the timeout problem, DFS has a timeout of 10 minutes.

[9/9/94 public]

Please try it out with the build which has the last MBF code drop. The
initial MBF code drop had a bug which caused some of the DG packet
reservations never cancelled (runaway packet reservation). This is not a
problem in the user-space rpc runtime since the packet rationing never
happen (well almost :-). But for KRPC, if this happens in the server and
the server manager routine makes an rpc (DFS does that), none of rpcs
proceeds and all clients get is a timeout. The runaway packet reservation
was fixed in the last MBF code drop.

Of course, the rpc_s_call_timeout still can happen if the operation takes
more than 10 mins (but that's not the rpc runtime's fault) and the various
exceptions will be raised at different places.

[9/12/94 public]
I ran the dfs.glue test on the BL17 build for 10 hours without any
errors. I assume that this probably means that the rpc error that was
occuring is now fixed. Does everyone concur?

[9/12/94 public]
Bob - I am kicking off a run of dfs.glue on bl-17 now - will let you know.
I'm a little confused by the above RPC update - regardless of whose fault
it is, I do not want the machine CRASHING due to rpc exceptions ... does
this mean your "workaround" for this problem should be submitted as a "fix"
now?

[9/12/94 public]
What was proposed as a workaround, should be submitted as a fix since
this error could still occur. I ran the BL17 code without any changes,
therefore, I know I did not encounter the rpc problem. There are
several places in the code where this problem could arise. In both
the functions px_read and px_write the following code exists:

	osi_RestorePreemption(0);
	...
	osi_PreemptionOff();

This should be changed to have a TRY - CATCH around it.

	osi_RestorePreemption(0);
	TRY {
	...
	} CATCH_ALL {
		osi_PreemptionOff(0);
	...
		osi_RestorePreemption(0);
	}
	osi_PreemptionOff();

[9/12/94 public]
Yeah! dfs.glue has been running succesfully for over 4 hours now on bl-17!
Once it has solidly passed the 10 hour mark, can I reassign this to you
Ron A. as a placeholder for the above fix submission?

[9/13/94 public]
All yours Ron - nearly 24 hours of dfs.glue and only 1 failure (not a
timeout) - please submit the IBM fix when you get a chance - thanks.

[9/15/94 public]
Downgrading to a 1 since this is no longer blocking system test. The
dfs.glue test ran 30 hours with just the 1 unrelated failure (ie.
until other problems were hit :-( Thanks for the hard work.

[9/15/94 public]
Submitted fixes to px_read and px_write to handle pipe exceptions
properly.  Fixed.  R.I.P.

[09/26/94 public]
Closed.



CR Number                     : 11023
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts lsfldb FAILED!
Reported Date                 : 6/21/94
Found in Baseline             : 1.1
Found Date                    : 6/21/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1
Affected File(s)              : do_test.dfs
Sensitivity                   : public

[6/21/94 public]

               

cdscp show dir /.:

                        SHOW
                   DIRECTORY   /.../sif_cell
                          AT   1994-06-21-12:56:43
            RPC_ClassVersion = 0100
                     CDS_CTS = 1994-06-21-11:35:28.291310100/00-00-c0-c7-49-44
                     CDS_UTS = 1994-06-21-11:36:07.118279100/00-00-c0-c7-49-44
              CDS_ObjectUUID = 002c734d-d080-1e06-9adb-0000c0c74944
                CDS_Replicas = :
          Clearinghouse UUID = 0007dd8e-d07d-1e06-9adb-0000c0c74944
                       Tower = ncacn_ip_tcp:130.105.5.25[]
		       Tower = ncadg_ip_udp:130.105.5.25[]
                Replica type = master
          Clearinghouse Name = /.../sif_cell/sif_ch
                 CDS_AllUpTo = 0
             CDS_Convergence = medium
                CDS_InCHName = allowed
        CDS_DirectoryVersion = 3.0
            CDS_ReplicaState = on
             CDS_ReplicaType = master
               CDS_LastSkulk = 1994-06-21-11:35:28.291310100/00-00-c0-c7-49-44
              CDS_LastUpdate = 1994-06-21-16:57:56.271423100/00-00-c0-c7-49-44
             CDS_RingPointer = 0007dd8e-d07d-1e06-9adb-0000c0c74944
                   CDS_Epoch = 002c734c-d080-1e06-9adb-0000c0c74944
          CDS_ReplicaVersion = 3.0
cdscp list dir /.:/hosts
                        LIST
                   DIRECTORY   /.../sif_cell/hosts
                          AT   1994-06-21-12:56:48
hosts
fts lsfldb
fts lsfldb FAILED!
cd /:

[6/21/94 public]

Budapest does not appear to be running dfs at all - was
there a problem during configuration?

[6/22/94 public]

Resolved DFS Smoketest startup problem by setting
new environment variables DCE_ADMIN, DCE_ADMIN_PW 
for Toms' dce.clean(dce_shutdown) routine which
is used in the DFS Smoketests scripts.



CR Number                     : 10926
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : nscubik
Short Description             : remove workaround for 10082
Reported Date                 : 6/9/94
Found in Baseline             : 1.1a
Found Date                    : 6/9/94
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b18
Affected File(s)              : ubik.p.h
Sensitivity                   : public

[06/9/94 public]

This ot is to serve as a reminder to remove the workaround for the
incorrect ticket expiration error code (10082) from ubik.p.h whenever this
problem is fixed in the security component.

#define UBIK_RPC_POSTWRAP(error_status)                                 \
          if ((error_status == rpc_s_auth_tkt_expired) || (error_status==336760973L)) {


In the above macro definition you will need to remove the check for
error_status == 336760973L.

[6/9/94 public]

[9/15/94 public]

Removed workaround.

[09/26/94 public]
Closed.



CR Number                     : 10887
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : memory leak in efs_getacl()
Reported Date                 : 6/7/94
Found in Baseline             : 1.1
Found Date                    : 6/7/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/file/episode/efs_vnodeops.c
Sensitivity                   : public

[6/7/94 public]
The following from efs_getacl() looks like a memory leak to me:

>            if ( ! evp->vd_acl ) {
>                dacl_AllocateAcl (&accessAclP);
>                accessAclP = (dacl_t *) osi_Alloc (sizeof (dacl_t));
>                code = dacl_ReadAclFromAnode (accessAclP, ap, fstat.acl,
>                                              EPIF_AUX_IS_ACL, &episodeAclMgmtUu
>id);

dacl_AllocateAcl() simply calls osi_Alloc() which initializes accessAclP
to point to newly allocated memory.  That pointer is subsequently overwritten
by the call to osi_Alloc() directly.

[09/27/94 public]
Closed.



CR Number                     : 10869
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : support delegate ACL entries in DFS
Reported Date                 : 6/6/94
Found in Baseline             : 1.1
Found Date                    : 6/6/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : file/security/dacl/*
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/6/94 public]
As part of the delegation work for 1.1, we need to add support
in the DFS acl mgr for the new delegate entries.

[6/6/94 public]
Filled in Interest List CC with `jeff@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[06/08/94 public]

The code is submitted, but we've turned off the ability to
create the delegate ACLs via acl_edit until DFS fully supports
delegation.

[09/27/94 public]
Closed.



CR Number                     : 10842
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/systest/file
Short Description             : incorrect comment leaders
Reported Date                 : 6/2/94
Found in Baseline             : 1.1
Found Date                    : 6/2/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1
Affected File(s)              : None
Sensitivity                   : public

[6/2/94 public]

Hi gail,

I called you about the fact that the following files have an
incorrect comment leader:

./test/systest/file/README.RWALL        #
./test/systest/file/dfs.read.write.data #
./test/systest/file/dfsacltst/README    #
./test/systest/file/dfsacltst/acldriver.awk     #
./test/systest/file/dfsacltst/acldriver.ksh     #
./test/systest/file/dfsacltst/aclgen.chk        #
./test/systest/file/dfsacltst/acltestgen.awk    #
./test/systest/file/dfsacltst/acltestgen.ksh    #
./test/systest/file/dfsacltst/data/test1.aclset #
./test/systest/file/dfsacltst/data/test1.setup  #
./test/systest/file/dfsacltst/dfsacl.functions  #
./test/systest/file/dfsacltst/dfsacl_registry_setup.exec        #
./test/systest/file/dfsacltst/dfsacl_registry_setup.main        #
./test/systest/file/do.ksh      #
./test/systest/file/profile     #

They have '#' instead of '# '.  ODE seems to require a space after 
the '#'.  I have talked to you and will be fixing these for 1.1a.

[06/03/94 public]

Fixed the above files in /project/dce/build/dce1.1.
These files WERE NOT in /project/dce/build/dce1.1a because
it is a shared sandbox.  Once /project/dce/build/dce1.1a
gets the 'new' target, it will then see the fixes.  

The fix does not IMPACT the code execution because there only
involve the comments.



CR Number                     : 10825
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Some of the text for the scout program is wrong
Reported Date                 : 6/1/94
Found in Baseline             : 1.0.3
Found Date                    : 6/1/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[6/1/94 public]
The following information about the scout program needs to be corrected:
 
o Scout tracks number of fetches, not amount of data fetched.
 
o Scout tracks number of stores, not amount of data stored.
 
o Scout tracks connections from principals, not from clients.
 
o Scout can truncate machine and aggregate names with asterisks.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[6/1/94 public]
I modified the following files to correct the scout documentation:
 
./dce_books/dfs_admin_gdref/gd/overview.gpsml
./dce_books/dfs_admin_gdref/gd/scout.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/scout.8dfs
 
The changes were verified by Vijay Anand.  This one can be closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[08/30/94 public]
Closed bug.



CR Number                     : 10776
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : HYP: repserver error exit due to signal 6
Reported Date                 : 5/25/94
Found in Baseline             : 1.1
Found Date                    : 5/25/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.1b21
Affected File(s)              : rep_main.c
Sensitivity                   : public

[5/25/94 public]
 
Placeholder CR while I try to gather more information. I believe
this repserver error exit has been seen on HP as well but for now
just listing the RIOS that hit it this past weekend during CHO
that includes dfs.repfs test.
 
The RIOS dfs.repfs test had been backgrounded and had stopped for tty output
on:
Sat May 21 04:46:09 1994
 
The RIOS repserver coredumped:
Sun May 22 08:52:55 1994
 
The HP dfs.repfs test USING the RIOS repserver continued successfully
throughout the whole weekend.
 
The BosLog contains an untimestamped message:
 
assertion failed: line 373, file /project/dce/build/dce1.1a-snap/src/file/osi/os
i_misc.c
 
Between messages timestamped:
Sun May 22 04:00:15 1994
and
Mon May 23 10:48:25 1994
 
so this could be very well be the culprit ...

[5/25/94 public]
If this is NOT seen during June regression testing, I will cancel.

[7/11/94 public]
June regression testing is finally happening and yes, this can
still be reproduced ... unfortunately the core is still not useful - 
next step = getting a debuggable repserver from dev - Andy?!
I'll reproduce and add info from USEFUL core once a debuggable
repserver for the hp is available ...
 
OOPS - forgot to clarify that this time, the core dump was on
the hp. I also got an icl dump from much earlier so 2 core
dumps may have occurred, one overwriting the other that
matched the icl dump. Last entries in icl dump were:
 
time 883.239979, pid 11: dfsauth_PrinName_GetName entered
time 883.240200, pid 11: dfsauth_PrinName_GetName: basename is /.../cho_cell.qadce
.osf.org/hosts/cobbler/dfs-server, adding suffix? no
time 883.240519, pid 11: dfsauth_PrinName_MakePrincipalName entered, base name is:
 /.../cho_cell.qadce.osf.org/hosts/cobbler/dfs-server
 time 883.240849, pid 11: dfsauth_PrinName_MakePrincipalName exiting, returning 0,
 constructed name is: /.../cho_cell.qadce.osf.org/hosts/cobbler/dfs-server
 time 883.241124, pid 11: dfsauth_PrinName_GetName exiting, returning principal /..
 ./cho_cell.qadce.osf.org/hosts/cobbler/dfs-server, value 0
 time 883.241401, pid 11: dfsauth_client_InitBindingAuth: about to call rpc_binding
 _set_auth_info, authn svc = rpc_c_authn_dce_private, principal name: /.../cho_cell
 .qadce.osf.org/hosts/cobbler/dfs-server
 time 883.244857, pid 11: dfsauth_client_InitBindingAuth: rpc_binding_set_auth_info
  returned
  time 883.245097, pid 11: dfsauth_client_InitBindingAuth exiting, returning Error 0
 
  time 883.245426, pid 11: fs conn before first call
  time 883.247284, pid 11: d9b75a18-9a83-11cd-8320-08000932114c@ncadg_ip_udp:130.105
  .201.7[]: u 'd9b75a18-9a83-11cd-8320-08000932114c', p 'ncadg_ip_udp', n '130.105.2
  01.7', e '', o ''
 
One last bit of info ... the ftserver coredumped as well (timestamps
not close) - both coredumps indicate that an extra "free" call may be
the problem (if these core dumps can be trusted)
 
core file from:  ftserver
Child died due to: IOT instruction
WARNING: /usr/lib/end.o was not linked with this program (UE836)
 Shared-library debugging cannot be made available (UE837)
 Procedures:      0
 Files: 0
 >trace
  0 _kill + 0x00000008 (0x19, 0x47, 0x40160fff, 0x4015d000)
  1 free + 0x00000224 (Address not found (UE302)
 
core file from:  repserver
Child died due to: IOT instruction
WARNING: /usr/lib/end.o was not linked with this program (UE836)
 Shared-library debugging cannot be made available (UE837)
 Procedures:      0
 Files: 0
 >trace
  0 _kill + 0x00000008 (0, 0, 0, 0)
  1 free + 0x00000224 (Address not found (UE302)

[8/29/94 public]
Andy - bumping this to a 1 since I don't't like my daemons core
dumping during CHO and this is STILL happening.
CR 11708 contains the latest ftserver coredump and here's the latest
repserver core dump:
 
Copyright Hewlett-Packard Co. 1985,1987-1992. All Rights Reserved.
<<<< XDB Version A.09.01 HP-UX >>>>
Core file from:  repserver
Child died due to: IOT instruction
Procedures:      0
Files: 0
 0 _kill@libc + 0x00000008 (hp-ux export stub)
 1 _raise@libc + 0x00000024 (0x40023e70, 0x4003dc90, 0, 0)
 2 _abort@libc + 0x00000078 (0x33, 0x4003dc90, 0, 0)
 3 _assert@libc + 0x00000128 (0x7aff0468, 0x3, 0x401aa420, 0)
 4 TMEM@libdce + 0x00313da0 (0x40289b30, 0, 0, 0)
 5 TMEM@libdce + 0x003145e4 (0x40289b30, 0x401a9758, 0, 0)
 6 TMEM@libdce + 0x0031d1e4 (0x40289b30, 0x401aa358, 0x401a9758, 0)
 7 TMEM@libdce + 0x003204e0 (0x400c15c0, 0x401a9758, 0x401a9758, 0x3a2f6b72)
 8 TMEM@libdce + 0x00310694 (0x400c15c0, 0x401e5dd0, 0x401a974c, 0x401a9750)
 9 TMEM@libdce + 0x003119bc (0x400c15c0, 0x1, 0, 0)
10 dfsauth_client_EstablishLocalA + 0x00000888 (0x1, 0x401a6860, 0, 0)
11 dfsauth_client_RefreshLocalAut + 0x00000018 (0x400bf5fc, 0x29ccb017, 0x1000000, 0)
12 dfsauth_client_InitAuthContext + 0x00000200 (0, 0x1, 0x1000000, 0x401eca4c)
13 dfsauth_client_InitBindingAuth + 0x0000011c (0x401a68b0, 0, 0x1, 0x401a68b4)
14 GetIncrementalDump + 0x0000023c (0x401d947c, 0x401542fc, 0x401d9038, 0x401a682c)
15 StartVolumeRetrieval + 0x00000278 (0x401d947c, 0x48, 0x401a67a0, 0x2e5d6b8b)
16 ReplicaWantsAdvance + 0x00000850 (0x401d947c, 0x401a671c, 0x401a6720, 0x2e5ec41a)
17 StartImporting + 0x000000f8 (0x4001ebc0, 0x29ef707c, 0x1041104, 0x2e5d729a)
18 BackgroundProcessThread + 0x000001c4 (0, 0x7af53230, 0x4003dc90, 0xb)
19 TMEM@libdce + 0x000d376c (0x4003dc90, 0, 0, 0)
 
CR's 11708, 11913 and this one could all be related since "localauth"
was affected by the fix to 11423 ...
 
Thanks to Andy McKeen, hidden libdce routines above show up with:
xdb -s -l ALL repserver core.repserver
 
 0 _kill@libc + 0x00000008 (hp-ux export stub)
 1 _raise@libc + 0x00000024 (0x40023e70, 0x4003dc90, 0, 0)
 2 _abort@libc + 0x00000078 (0x33, 0x4003dc90, 0, 0)
 3 _assert@libc + 0x00000128 (0x7aff0468, 0x3, 0x401aa420, 0)
 4 cache_sanity_check@libdce + 0x00000380 (0x40289b30, 0, 0, 0)
 5 sec_login_pvt_release_context@ + 0x00000090 (0x40289b30, 0x401a9758, 0, 0)
 6 sec_login_pvt_purge_refresh_co + 0x000000a4 (0x40289b30, 0x401aa358, 0x401a9758, 0)
 7 sec_login_pvt_setup_refresh_co + 0x0000058c (0x400c15c0, 0x401a9758, 0x401a9758, 0x3a2f6b72)
 8 sec_login_validate_identity@li + 0x000001f8 (0x400c15c0, 0x401e5dd0, 0x401a974c, 0x401a9750)
 9 sec_login_valid_from_keytable@ + 0x00000374 (0x400c15c0, 0x1, 0, 0)
10 dfsauth_client_EstablishLocalA + 0x00000888 (0x1, 0x401a6860, 0, 0)

[8/29/94 public]
What baselevel did you use to get the above failure?  I submitted a fix
on 8/23/94 to src/security/client/login/sec_login_pvt.c to prevent the above
cache assertion, so any build after the 24th should not be able to produce
the above traceback.  Note that instead, the refresh would have returned
an error, so even with the assertion fix, you'll still have a problem (but
now you'll be able to see what the real problem is).

[8/29/94 public]
Oops - left that out - bl-16 which is just BEFORE your fix - will
retry.

[9/20/94 public]
Reproduced on bl-18, this from the core.repserver:
 
Copyright Hewlett-Packard Co. 1985,1987-1992. All Rights Reserved.
<<<< XDB Version A.09.01 HP-UX >>>>
Core file from:  repserver
Child died due to: IOT instruction
Procedures:      0
Files: 0
 0 _kill@libc + 0x00000008 (hp-ux export stub)
 1 _raise@libc + 0x00000024 (0x40023690, 0x3, 0x4015df60, 0)
 2 _abort@libc + 0x00000078 (0x40001428, 0x4001ceb8, 0x17e, 0x4001cedc)
 3 osi_Free + 0x00000190 (0, 0, 0x10c1041, 0x40219eb8)
 4 SREP_KeepFilesAlive + 0x000005f4 (0x400e7620, 0x4015d914, 0x1, 0)
 5 op2_ssr + 0x0000019c (0x400e7620, 0x400eb930, 0x4015d53c, 0x4015d520)
 6 rpc__dg_execute_call@libdce + 0x00001dd8 (0x400eb930, 0, 0, 0)
 7 cthread_call_executor@libdce + 0x000002d4 (0x40224aa8, 0x7af511d8, 0x400ff4b8, 0x11)
 8 cma__thread_base@libdce + 0x0000021c (0x400ff4b8, 0, 0, 0)
 9 cma__thread_start1@libdce + 0x0000004c (0x400ff4b8, 0, 0, 0)

[9/20/94 public]
Just a note to say that the comments on 8/29 have nothing to do with the
rest of this CR.  That was a completely separate bug that happened to prevent
getting to the real problem using bl16, and has since been fixed, so the
entire problem seems to be within DFS, no security routines at all, thus I'm
removing myself from the CC list (having enough trouble keeping up with
many dozen CR updates per day).

[9/20/94 public]
The problem is doubtless in the clunky code that's in your copy of
SREP_KeepFilesAlive to manage the keep-alive storage.  There are a
couple of problems, all starting at the ``if (missing > 0)'' clause.
First, the given computation for NewMax may not be adequate for
the new storage needed, so replace
	NewMax = (2 * rp->NumKAs) + 5;
with
	NewMax = rp->NumKAs + MAX(8, missing);
 
Then, ``rp->KA'' may be a NULL pointer, so surround the two statements
  bcopy((char *)rp->KA, (char *)ka, (rp->NumKAs * sizeof(struct FileKA)));
  osi_Free((opaque) rp->KA,  (rp->NumKAs * sizeof(struct FileKA)));
with a check like ``if (rp->KA != NULL)''.
 
Probably the proximate cause of your most recent failure is that we're
passing a null pointer as the first argument to to osi_Free().

[9/21/94 public]
Bumping the severity and priority now that other blocking bugs
are out of the way AND I've seen it on the HP again too:

Copyright Hewlett-Packard Co. 1985,1987-1992. All Rights Reserved.
<<<< XDB Version A.09.01 HP-UX >>>>
Core file from:  repserver
Child died due to: IOT instruction
Procedures:      0
Files: 0
 0 _kill@libc + 0x00000008 (hp-ux export stub)
 1 _raise@libc + 0x00000024 (0x40023690, 0x3, 0x4015df60, 0)
 2 _abort@libc + 0x00000078 (0x40001428, 0x4001ceb8, 0x17e, 0x4001cedc)
 3 osi_Free + 0x00000190 (0, 0, 0x10c1041, 0x40219eb8)
 4 SREP_KeepFilesAlive + 0x000005f4 (0x400e7620, 0x4015d914, 0x1, 0)
 5 op2_ssr + 0x0000019c (0x400e7620, 0x400eb930, 0x4015d53c, 0x4015d520)
 6 rpc__dg_execute_call@libdce + 0x00001dd8 (0x400eb930, 0, 0, 0)
 7 cthread_call_executor@libdce + 0x000002d4 (0x40224aa8, 0x7af511d8, 0x400ff4b8, 0x11)
 8 cma__thread_base@libdce + 0x0000021c (0x400ff4b8, 0, 0, 0)

 Thanks for the fix suggestion Craig ... Something for you to do
 in your spare time Andy :-).

[10/3/94 public]
On hypercritical list.

[10/5/94 public]
Applied the fix provided by Craig, seems to have solved the problem.



CR Number                     : 10752
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Include minor technical clarifications
Reported Date                 : 5/24/94
Found in Baseline             : 1.0.3
Found Date                    : 5/24/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/24/94 public]
I have accumulated a number of small technical clarifications that need to
be incorporated into the documentation.  This defect is a wrapper for these
changes.  I'll briefly list the changes and the affected files as I make the
edits.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/24/94 public]
Updated the following files to indicate that a replication site for a fileset
from one cell cannot exist on a file server in another cell:
 
./dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_addsite.8dfs

[5/24/94 public]
Added an Exit Values section to the reference page for the dfsexport command,
./dce_books/dfs_admin_gdref/ref/man8dfs/dfsexport.8dfs.  The command's exit
values were thought to be worth mentioning.

[5/24/94 public]
Added the explicit .readonly and .backup restrictions to the discussions of
DFS fileset naming conventions.  You can specify a name that includes either
of these extensions for a new fileset.  The following files were modified:
 
./dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./dce_books/dfs_admin_gdref/gd/issues.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_create.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_crfldbentry.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_rename.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_restore.8dfs

[5/25/94 public]
Added information about database access during a Ubik election and subsequent
database distribution.  The information briefly lets the reader know what can
and cannot be done in these situations.  The only file affected was
./dce_books/dfs_admin_gdref/gd/issues.gpsml.

[5/27/94 public]
Corrected text that discussed use of the fts update command; the command is
intended for use with Scheduled Replication.  Also clarified how replicas are
distributed and how some of the replication parameters affect the distribution.
The following files were affected:
 
./dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_release.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_update.8dfs
 
Jeff Prem reviewed the changes.  This is the last miscellaneous clarification
or correction I'm likely to get to for this release, so this defect can be
closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/01/94 public]
Closed bug.



CR Number                     : 10750
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Clarify and correct Update Server documentation
Reported Date                 : 5/24/94
Found in Baseline             : 1.0.3
Found Date                    : 5/24/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/24/94 public]
The reference pages upclient.8dfs and upserver.8dfs have some mistakes in
their examples, and they are unclear in certain other respects.  This defect
is intended to address these problems.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/24/94 public]
I submitted the following pair of files in response to this defect:
 
./dce_books/dfs_admin_gdref/ref/man8dfs/upclient.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/upserver.8dfs
 
I verified the changes, which were simple technical corrections and minor
editorial rewriting.  This one can be closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[08/30/94 public]
Closed bug.



CR Number                     : 10694
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : smoke test failed
Short Description             : smoke test failed with unknown uid
Reported Date                 : 5/18/94
Found in Baseline             : 1.1a
Found Date                    : 5/18/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1a
Affected File(s)              : smoke test script (read write data file)
Sensitivity                   : public

[5/18/94 public]

Hi gail,

The smoke test failed again.  It seems to have die quite early.
It died at the check uids in the dfs.read_write.all test.

I am suspecting that this is caused by a network glitch.  
Can you verify?  It is too late to redo everything again.

In any case, just so you are aware that I have copied the install
area from yesterday's build to shades (your machine).  These
binaries were the ones causing the smoke test to die with a FAIL
message.

thanks
annie


[5/23/94/public]
Annie - we seem to have lost all the comments to this ot with your
last update. Also - you changed the state to closed without explaining
why - did we solve this problem (ie. was it lack of disk space on the
machine that caused the smoketest to fail)? Did the smoketest pass
friday? Please add the details here - thanks.


[5/23/94/public]
Oops - didn't realize you'd logged separate CRs for the smoketest
failures. The error reported in THIS CR was the result of an incorrect
data file being in place at the time of the smoketest. There are now
2 datafiles in the directory for convenience, the smoketest one:
rwall.data and the rwall.data.rerun one for rerunning the smoketest
when the registry and filesets are already in place.

[05/23/94 public]
See Gail's message above.



CR Number                     : 10690
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Provide more information for executable files
Reported Date                 : 5/18/94
Found in Baseline             : 1.0.3
Found Date                    : 5/18/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : ./dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/18/94 public]
I need to add a note stating that unless the user, group, or other mode bits
provide the x mode bit, an executable file cannot be executed.  This is a small
point that has caused some recent confusion.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/18/94 public]
I added the necessary paragraph to the file aclgroup.gpsml.  The change was
verified by myself and Rajesh Agarwalla.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/06/94 public]
Closed bug.



CR Number                     : 10688
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Add missing configuration steps for Tape Coordinator
Reported Date                 : 5/18/94
Found in Baseline             : 1.0.3
Found Date                    : 5/18/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/18/94 public]
This defect addresses the need to add some missing steps to the configuration
of a Tape Coordinator machine for use with the DFS Backup System.  The missing
information applies only to configuration of DCE clients that are not also DFS
server machines of some type.  Basically, a client-only machine needs to have
a DFS server principal (hosts/<hostname>/dfs-server) created for it, and this
principal needs to be added to the appropriate DFS administrative group
(subsys/dce/dfs-admin).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/19/94 public]
The additional steps were added to the appropriate section, and necessary
cross-references were added to the appropriate related sections. The
following files were modified:
 
./dce_books/dfs_admin_gdref/gd/backup.gpsml
./dce_books/dfs_admin_gdref/gd/backrest.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/bak.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/butc.8dfs
 
The bulk of the changes (the new material) were made to the file backup.gpsml.
All changes were verified by Abhijit Khale, Steve Lammert, and myself (the
missing information was promptly provided by Abhijit).  This one can be closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/06/94 public]
Closed bug.



CR Number                     : 10687
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : make KRPC debug printfs work again
Reported Date                 : 5/18/94
Found in Baseline             : 1.1
Found Date                    : 5/18/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b18
Affected File(s)              : src/rpc/runtime/rpcsvc.h, src/rpc/runtime/rpcdbg.h, src/rpc/runtime/rpcdbg.c, src/rpc/kruntime/Makefile, src/rpc/kruntime/HP800/machdep.mk, src/rpc/kruntime/svc_krpc.c
Sensitivity                   : public

[5/18/94 public]
The debug macros in RPC have been converted to use the
new serviceability.  This OT is a placeholder to track the 
kernel-specific work required to maintain minimal debug
capability in KRPC.  This means at least providing the ability to
set debug levels in the new serviceability in krpc and 
getting debug output as kernel printfs for 1.1.  An additional 
level of functionality which will not be addressed in 1.1 is
to record the KRPC debug output into the ICL buffer and read it out 
of the kernel using dfstrace.

[09/26/94 public]
Closed.



CR Number                     : 10665
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bld
Short Description             : smoke test failed
Reported Date                 : 5/16/94
Found in Baseline             : 1.1a
Found Date                    : 5/16/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1a
Affected File(s)              : smoke test script to remove stale files in /tmp area
Sensitivity                   : public

[5/16/94 public]

Smoke testing of dfs component failed.  The log file is in
berlin:/u1/1.1a_smoketest.  It is called driver.out.  A partial
output with the failures is as followed:

...
Entering users_homefs_setup ...
cp /u1/1.1a_smoketest/profile /:/dfsusers/dfs_1/.profile
cp /u1/1.1a_smoketest/do.ksh /:/dfsusers/dfs_1/do.ksh
chown -R dfs_1 /:/dfsusers/dfs_1
chmod -R 755 /:/dfsusers/dfs_1
chgrp -R dfs /:/dfsusers/dfs_1
cp /u1/1.1a_smoketest/profile /:/dfsusers/dfs_2/.profile
cp /u1/1.1a_smoketest/do.ksh /:/dfsusers/dfs_2/do.ksh
chown -R dfs_2 /:/dfsusers/dfs_2
chmod -R 755 /:/dfsusers/dfs_2
chgrp -R dfs /:/dfsusers/dfs_2
... Exiting users_homefs_setup
Entering dcelogin_users_to_readnwrite ...
user_home = /:/dfsusers/dfs_1
Ready to dce_login user - dfs_1 on machine - tinker
PIDS => 4582
user_home = /:/dfsusers/dfs_2
Ready to dce_login user - dfs_2 on machine - berlin
PIDS => 4582 4587
... Exiting dcelogin_users_to_readnwrite
Entering wait_and_see ...
FAILURE suspected - setting PASS_FAIL to 1
... Exiting wait_and_see
dfs.read_write_all.main FAILED
Exiting /u1/1.1a_smoketest/dfs.read_write_all.main with 1 - program terminated at Mon May 16 14:55:57 EDT 1994
Test 7   ----   /u1/1.1a_smoketest/dfs.read_write_all.main -f /u1/1.1a_smoketest/rwall.data -t 1 FAILED!

(I do not know the problem off-hand.  However, the last submission was:

  ...
  Submitted by Gail Marie Driscoll; User name: gmd
  Date: 5/13/94; Time: 15:33
  Number of files: 1; Defect number: 10377.
  ...
[ ./rpc/runtime/dglsn.c ]
Partial fix for 10377, enable general cancelability when
blocking on a private socket read.
[19:32:50  gmd]
... 

Prior to that, the smoke test was passing.)

[5/17/94 public]

This appears to be a problem with disk space on the smoketest machine BERLIN.
When attempting to rerun the smoketest by hand, received errors that the
root partition was full. Annie has added code to her scripts to remove the
trash that is built up in /tmp by dce_config/dfs_config ie.

	dce_config.log
	rc.dfs*
	sh*
	xyz.*

[05/23/94 public]

smoke test scripts have been updated to remove temporary files so that
the disk will NOT be full again.  This seems to solve the problem.  The
disk is currently at 98% full.



CR Number                     : 10653
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : low/util
Short Description             : fs_DetermineFSType fails w/ dfs and /bin/posix/sh
Reported Date                 : 5/16/94
Found in Baseline             : 1.1
Found Date                    : 5/16/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1a
Affected File(s)              : test/file/util/fs_FunctionsScript
						test/file/util/fs_SetSysVar
						test/file/low/runtests
Sensitivity                   : public

[5/16/94 public]

Running performance tests for CHO evaluation, noticed large discrepancy
between RIOS and HP test times. Scanning HP output, noticed it reported
"local" filesystem rather than "dfs". Code in script for fs_DetermineFSType
relies on expansion of /: symbolic link to /.../<cellname>/fs by `pwd`
but /bin/posix/sh built-in pwd does not expand /: . Suggested fix is to
force use of /bin/pwd rather than built-in pwd.

[05/17/94 public]
Files submitted.

[05/23/94 public]
Builds fine - closing.



CR Number                     : 10576
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Need to discuss default cell earlier in ACL chapter
Reported Date                 : 5/7/94
Found in Baseline             : 1.0.3
Found Date                    : 5/7/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : ./dce_books/dfs_admin_gdref/gd/aclgroup.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/7/94 public]
I need to find a way to discuss an ACL's default cell much earlier in the DFS
chapter that discusses DCE ACLs with DFS objects.  My current approach at this
description is pretty misleading because it fails to consider the default cell
when it initially presents the concepts of local and foreign cells and users
with respect to ACLs.  An understanding of the default cell is integral to a
proper understanding of the various ACL entries and how they are interpreted.
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/13/94 public]
I did a lot of reorganization to the chapter to present the ACL default
cell description much earlier in the chapter.  I also made some additional
corrections and edits to my previous work for defect 8118.  In general, I
spent a lot of time reorganizing this chapter for purposes of improved
clarity.  Rajesh Agarwalla reviewed all of my changes (in addition to the
remainder of the chapter), providing many helpful comments and suggestions.
 
Changed Interest List CC from `bob@transarc.com' to `bob@transarc.com, 
 rajesh@transarc.com' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/08/94 public]
Closed bug.



CR Number                     : 10552
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Fix inconsistent index entries
Reported Date                 : 5/5/94
Found in Baseline             : 1.0.3
Found Date                    : 5/5/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/5/94 public]
After printing the index for the new DFS Admin Guide and Ref, it became clear
that a number of entries were pretty inconsistent between the former DFS Guide
and DFS Reference documents.  This defect addresses correction of some of the
more glaring inconsistencies.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/6/94 public]
I repaired what I think were the serious index entry inconsistencies between
the former guide and reference documentation.  I modified the following files:
 
./dce_books/dfs_admin_gdref/gd/backup.gpsml
./dce_books/dfs_admin_gdref/gd/issues.gpsml
./dce_books/dfs_admin_gdref/gd/overview.gpsml
./dce_books/dfs_admin_gdref/gd/processes.gpsml
./dce_books/dfs_admin_gdref/gd/scout.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/cm_statservers.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_delfldbentry.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_dump.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/scout.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/udebug.8dfs
 
I formatted and printed the index to verify the changes.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/08/94 public]
Closed bug.



CR Number                     : 10551
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : test/systest/file
Short Description             : dcetest/dce1.1/test/systest/file does not exist in DFS smoketest
Reported Date                 : 5/5/94
Found in Baseline             : 1.1
Found Date                    : 5/5/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1
Affected File(s)              : dce_config
Sensitivity                   : public

[5/5/94 public]

During the RaT multi_machine smoketest, when running "dce_config" in
the RIOS DFS server "mudslide" the following complaint occurs, hanging
the smoketest:

/mnt/tailor/dcetest/dce1.1/test/systest/file: A file or directory in the \
  path name does not exist.


Here's the output:


dfsd: All DFS daemons started.
+ cd /usr/rrizzo/RAT_tools
+ /usr/opt/dcelocal/bin/dce_login cell_admin -dce- -exec
/usr/rrizzo/RAT_tools/run_commands.dfs
Password must be changed!
+ grep FAIL /usr/rrizzo/RAT_tools/run_commands.dfs.log
+ [ 1 != 1 ]
+ [ ! -d /dcetest/dcelocal/test/systest/file ]
+ cd /mnt/tailor/dcetest/dce1.1/test/systest/file
/mnt/tailor/dcetest/dce1.1/test/systest/file: A file or directory in the
path name does not exist. 


/tmp/dce_config.log shows:


S:****** Starting dfsd...
D:         Executing: dfsd
DEBUG:     Executing: enable_in_dfs_rc(dfsd)
V:         Successfully modified rcfile /etc/rc.dfs for "dfsd"
D:         Executing: ls /.: 2>/dev/null | grep /.../sif_cell >/dev/null
2>\&1
D:         Executing: rm -f /.: > /dev/null 2>\&1
D:         Executing: ln -s /.../sif_cell /.: >/dev/null 2>\&1
D:         Executing: ls /: 2>/dev/null | grep /.../sif_cell >/dev/null
2>\&1
D:         Executing: rm -f /: > /dev/null 2>\&1
D:         Executing: ln -s /.../sif_cell/fs /: >/dev/null 2>\&1

[05/05/94 public]
Hmm - dcetest/dce1.1/test/systest/file DOES exist in the install tree
of nightly builds ... have you changed the build machines or mount points
recently? Is it possible the test build hasn't completed at the time
you run this? If test build completion is an issue, we may want to switch
to using a local copy of the dfs.read_write_all.main test (as the
DFS 1.1a smoketest does).

[05/20/94 public]
This hasn't shown up for 2 weeks - my suspicion is still that the test build had
not completed - lowering the severity/priority for now - will close if this 
doesn't show up before BETA.

[06/30/94 public]
Closing - standing by theory - not seen since.



CR Number                     : 10527
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Complete structure of DFS docs for new organization
Reported Date                 : 5/4/94
Found in Baseline             : 1.0.3
Found Date                    : 5/4/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[5/4/94 public]
This defect addresses completion of the reorganization effort to take the DFS
portions of the DCE Administration Guide and DCE Command Reference and use them
to create a new DCE DFS Administration Guide and Reference.  Completion of this
defect requires that I create all necessary frontmatter and configuration
files.  I will list the files when I verify the defect.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[5/5/94 public]
The restructuring of the DFS documentation to create the new "OSF DCE DFS
Administration Guide and Reference" is complete.  I verified all of the files
that I modified or created for the new organization.
 
o I modified the following files:
 
./src/dce_books/dfs_admin_gdref/README
./src/dce_books/dfs_admin_gdref/gd/README
./src/dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
./src/dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./src/dce_books/dfs_admin_gdref/gd/ftmgmt.gpsml
./src/dce_books/dfs_admin_gdref/gd/overview.gpsml
./src/dce_books/dfs_admin_gdref/parts/gd_part.gpsml
./src/dce_books/dfs_admin_gdref/parts/ref_part.gpsml
 
o I created the following files and directories:
 
./src/dce_books/dfs_admin_gdref/DESCRIPTION
./src/dce_books/dfs_admin_gdref/Headers
./src/dce_books/dfs_admin_gdref/Headers/header.man
./src/dce_books/dfs_admin_gdref/Headers/header.mm
./src/dce_books/dfs_admin_gdref/chapters
./src/dce_books/dfs_admin_gdref/chapters/chapter.4dfs
./src/dce_books/dfs_admin_gdref/chapters/chapter.8dfs
./src/dce_books/dfs_admin_gdref/frontmatter
./src/dce_books/dfs_admin_gdref/frontmatter/preface.gpsml
./src/dce_books/dfs_admin_gdref/frontmatter/title.gpsml
./src/dce_books/dfs_admin_gdref/ref/README
 
I scanned portions of much of the output to verify that all formats as desired
and expected.  I believe all of the files are correct.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/08/94 public]
Closed bug.



CR Number                     : 10492
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : repserver
Short Description             : fts addsite fails with rpc timeout
Reported Date                 : 4/29/94
Found in Baseline             : 1.1
Found Date                    : 4/29/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : userInt/fts/volc_vldbsubr.c
Sensitivity                   : public

[4/29/94 public]

Running dfs.repfs on a RIOS (cobbler) using dce1.1ab3, failed during the local
portion of the test with the following:

 fts addsite -fileset repfs.test -server cobbler -aggregate m1.aggr1
Warning: Couldn't talk to repserver on cobbler.osf.org: call timed out (dce / rpc)
Added replication site cobbler m1.aggr1 for fileset repfs.test

The repserver on cobbler was up and running at the time - unclear why the
rpc would have timed out but the operation did not succeed:

fts lsreplicas: Cannot get status of repserver cobbler.osf.org: Given volume is not a replica
(dfs / rep)

Note that the call time out is considered a warning since, if the repserver
was down and restarted, the repserver would have read the fldb and created
the replica (at least this is the behavior we've seen).

[5/3/94 public]
I'm seeing similar behavior using dce1.1ab2 when adding a replica site
on a HP (the read/write is on a HP also).  The replica is eventually
created, but after a delay.  I couldn't find any reason for the
communications failure.  

From the dfs.repfs log:
fts addsite -fileset another.rep -server dce13 -aggregate dce13_aggr
Added replication site dce13 dce13_aggr for fileset another.rep

The RepLog on the replica site:

94-May-03 09:47:58 setNextTime: 0,,91: delay 30 secs: TryForWVTRead: interleaved revoke
94-May-03 09:49:05 setNextTime: 0,,91: delay 120 secs: GetIncrementalDump: can't open remote source ft: communications failure (dce / rpc)
94-May-03 09:51:31 setNextTime: 0,,91: delay 0 secs: StartVolumeRetrieval: About to publish newly-built volume

The FtLog from the machine with the read/write fileset:

1994-May-03 09:38:25 Log file initialized as
/opt/dcelocal/var/dfs/adm/FtLog
1994-May-03 09:38:25 Ftserver starting
1994-May-03 09:47:05 Dumping 1:another.rep (0,,90)
1994-May-03 09:47:16 Restoring fileset 0,,88/1
1994-May-03 09:47:16 Restoring 1:another.rep.clone (0,,88)
1994-May-03 09:47:26 vols_Restore: returning 0
1994-May-03 09:47:26 Restored fileset 0,,88/1: returned code 0
1994-May-03 09:51:21 Dumping 1:another.rep (0,,90)

[06/7/94 public]

One way to reproduce this is by replicating a large fileset.  While
the fileset is being forwarded, issue and fts addsite or rmsite.  If
you've got the timing just right, the the rmsite or addsite call will
timeout.

I'm assuming that Transarc's threaded repserver will reduce the likelihood
of this occurring?  However, we'd like to modify the vldb_TellRepAboutNewEntry
routine to retry the REP_AllCheckReplicationConfig when it gets an
rpc timeout or comm failure.   The retry would occur only if we got
a timeout/comm failure and we couldn't contact any of the relevant
repservers successfully.

[09/27/94 public]
Closed.



CR Number                     : 10485
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : dfsexport core dumps on hp
Reported Date                 : 4/28/94
Found in Baseline             : 1.1
Found Date                    : 4/28/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/file/xaggr/export.c
Sensitivity                   : public

[4/28/94 public]
Setting up CHO for dce1.1ab3 (end of April regression testing) - dfsexport core
dumps on HP when trying to export lfs aggregates - stack trace only contains:
Procedures:     16
Files: 3
>run -all
Starting process 1666:  "dfsexport -all"

bus error (no ignore) at 0x800b2860
(file unknown): _end +0x4008cf5c: (line unknown)
>trace
 0 _end + 0x4008cf5c (0, 0, 0, 0)
 1 ftu_AggrSyscall + 0x00000128 (0, 0, 0, 0)
>
Still researching if my setup script somehow contributed to this. Rebooting does NOT
help, nor does retyping the dfstab file. Exact same script loop run on RIOS is fine.

[04/28/94 public]
CORRECTION: NOT lfs aggregate specific - occurs with BOTH native and lfs
UPDATE: NOT caused by my setup script

[05/01/94 public]
Observations:
* dfsexport on HP can only handle 1 aggregate (native or lfs) before
coredumping 
* dfsexport cannot detach an aggregate once attached

This is severely hampering my ability to setup and run CHO - adding
Diane, Mike and Andy to the interest list in the hopes that one can
volunteer to fix this SOON! Thanks!

[05/04/94 public]
Gail - The problem was memory being freed with free() though it
was alloc'ed with osi_Alloc().  dce8 now has a 
/opt/dcelocal/bin/dfsexport.new with the fix.  I'll be submitting 
later today..

[05/04/94 public]
Fixed.

[09/27/94 public]
Closed.



CR Number                     : 10440
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd. command_ref
Short Description             : Work user-level DFS doc into admin-level DFS doc
Reported Date                 : 4/22/94
Found in Baseline             : 1.0.3
Found Date                    : 4/22/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[4/22/94 public]
This defect addresses moving the necessary DFS User's Guide information into
the DFS Admin Guide.  Lisa Zahn completed the preliminary work for this defect.
I will now complete the process to make sure we convert everything we need and
to ensure that I get everything done properly.  Lisa has already briefed me all
of her work, so I have a good handle on where we are at and what remains to be
done.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[4/23/94 public]
I submitted the following set of files in response to this defect:
 
./src/dce_books/admin_gd/dfs/dfs/1_overview_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/8_cachemgr_dfs.gpsml
./src/dce_books/command_ref/man8dfs/bak.8dfs
./src/dce_books/command_ref/man8dfs/bak_apropos.8dfs
./src/dce_books/command_ref/man8dfs/bos.8dfs
./src/dce_books/command_ref/man8dfs/bos_apropos.8dfs
./src/dce_books/command_ref/man8dfs/cm.8dfs
./src/dce_books/command_ref/man8dfs/cm_apropos.8dfs
./src/dce_books/command_ref/man8dfs/cm_help.8dfs
./src/dce_books/command_ref/man8dfs/cm_statservers.8dfs
./src/dce_books/command_ref/man8dfs/cm_whereis.8dfs
./src/dce_books/command_ref/man8dfs/dfs_intro.8dfs
./src/dce_books/command_ref/man8dfs/dfstrace.8dfs
./src/dce_books/command_ref/man8dfs/dfstrace_apropos.8dfs
./src/dce_books/command_ref/man8dfs/fts.8dfs
./src/dce_books/command_ref/man8dfs/fts_apropos.8dfs
./src/dce_books/command_ref/man8dfs/fts_help.8dfs
./src/dce_books/command_ref/man8dfs/fts_lsquota.8dfs
 
This submission represents the first of two submissions I'll make for this
defect; it includes most of the files I need to edit.  Most of the files
include change bars for defect 10440.  Some do not include change bars for
this defect; in such cases, the changes I made to the files fall into one
or more of the following categories:
 
o Changes that, while associated with the converion of user text to admin
  text, were too small to really legitimize inclusion of change bars.
o Small editorial changes to correct previous style, format, or writing
  mistakes, most of which were made by Keith or me in previous submissions.
o Small editorial changes for consistency with other files and changes.
 
Please let me know of any questions or comments regarding any of my changes.
Note that the bulk of my effort has been made much easier by previous work
done recently at OSF by Lisa Zahn and Bob Mathews.

[4/25/94 public]
I submitted the final DFS file in which former User's Guide information needed
to be included.  The file is named
 
./src/dce_books/admin_gd/dfs/dfs/3_aclgroup_dfs.gpsml
 
All changes are indicated with the appropriate change bars.  I verified the
changes manually.  The bulk of the changes were to include wording and
organization from the User's Guide where I thought it was superior to its
counterpart in the Admin Guide.  I could drag this out ad infinitum, but I
think I picked up the important information.
 
This wraps up this defect from my end.  Please feel free to close off the
defect whenever you get the chance.  Thanks.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/19/94 public]
Closed bug.



CR Number                     : 10439
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, command_ref
Short Description             : Fix cross-references in DFS documentation
Reported Date                 : 4/22/94
Found in Baseline             : 1.0.3
Found Date                    : 4/22/94
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[4/22/94 public]
This defect covers making all of the cross-references in the DFS documentation
conform to the new structure.  Because of the current schedule, I will probably
address this defect in two stages.  I will fix the references I can for now;
once we decide on book or two for the DFS documentation, I will fix the
references to the documenmt titles.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[4/28/94 public]
I submitted the following files in response to this defect:
 
./src/dce_books/admin_gd/dfs/dfs/1_overview_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/2_issues_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/3_aclgroup_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/4_adminkey_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/5_processes_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/8_cachemgr_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/9_backup_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/10_backrest_dfs.gpsml
./src/dce_books/admin_gd/dfs/dfs/11_scout_dfs.gpsml
./src/dce_books/command_ref/man4dfs/CacheInfo.4dfs
./src/dce_books/command_ref/man4dfs/dfs_intro.4dfs
./src/dce_books/command_ref/man8dfs/bos_status.8dfs
./src/dce_books/command_ref/man8dfs/fts.8dfs
./src/dce_books/command_ref/man8dfs/fts_create.8dfs
./src/dce_books/command_ref/man8dfs/fts_crmount.8dfs
./src/dce_books/command_ref/man8dfs/fts_setrepinfo.8dfs
./src/dce_books/command_ref/man8dfs/fxd.8dfs
 
I fixed all internal and external cross-references in all of the documents.
As an added bonus, I also fixed all document references to use the SML macros
defined in the bookdefs.mac file.  Previously, such references spelled out the
names of the documents; as Hal pointed out, this approach led to problems when
we needed to rename a document.  (I should have done this long ago.)
 
Anyway, I visually verified output for all of the affected files.  This one
can be closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/19/94 public]
Closed bug.



CR Number                     : 10418
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : command_ref
Short Description             : Remove lingering references to dfsatab file
Reported Date                 : 4/20/94
Found in Baseline             : 1.0.3
Found Date                    : 4/20/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[4/20/94 public]
While working on some other issues, I found two reference pages that still
mention the dfsatab file, which was officially nuked for 1.0.3.  I need to
remove the references to the dfsatab file.  It's a quick edit.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[4/21/94 public]
The final references to the dfsatab(4dfs) file were removed from the following
two files:
 
src/dce_books/command_ref/man4dfs/dfs_intro.4dfs
src/dce_books/command_ref/man4dfs/dfstab.4dfs
  
I verified the virtually editorial changes myself.  This one can be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[09/19/94 public]
Closed bug.



CR Number                     : 10398
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 10224
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : No longer able to find mounted
.backup filesets
Reported Date                 : 4/19/94
Found in Baseline             : 1.1a
Found Date                    : 4/19/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : cm_subr.c, cm_tokens.c
Sensitivity                   : public

[4/19/94 public]
 
This defect seems to be a side effect of the fix applied for OT 10224.
When I do the following:
 
fts clone foobar
fts crmount /:/test foobar
fts crmount /:/test/.backup foobar.backup
ls /:/test
 
I get the following:
 
dfs: fileset (0,,133) error, code 691089421, on server 130.105.4.203 in
cell sisyphus.osf.org.
 
where:
 
691089421 = replica not current enough (dfs / xvl)
 
Which was added as part of the fix to OT 10224.
This happens with any .backup fileset.

[4/19/94 public]
This behavior doesn't happen here, so I can't debug it.  (Without a
contract, I can't even look at your sources...)  It looks like you've
got an old fileserver (the one housing ``foobar''), one that doesn't
have the jdp-db5146-use-REPSERVER_MGD-in-px_SetSync work.  This would
cause you grief since it would think that a backup fileset was a
way-out-of-date read-only fileset.
Check whether px_SetSync() (in file/px/px_subr.c) tests only
VOL_REPSERVER_MGD, or whether it tests VOL_REPFIELD for being
VOL_REP_NONE/VOL_REP_RELEASE/VOL_REP_LAZY.  I believe that it should
be testing only VOL_REPSERVER_MGD.
Changed Interest List CC from `jdp@transarc.com' to `jdp@transarc.com, 
 cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `mckeen' 
Changed Resp. Engr's Company from `tarc' to `osf'

[4/19/94 public]
I have checked px_SetSync and it seems to have the proper code in it.  I
have also retested against new and old fileservers and the problem happens
on both.  
 
I am concerned that the patch will cause old fileservers not to work with
new clients.  This would seem unacceptable.  If we can't get the patch to
work correctly with both old servers and new servers it should be backed
out.

[4/19/94 public]
I'm glad you re-assigned this to Diane, since I don't know for sure what's
happening in your particular situation.  I was guessing about the exporter
code and the new/old version stuff.  My expectation is that if you're getting
this new error code with the new client, that you'd be getting an infinite
loop with an old client in the same situation.  But clearly something else
isn't working here, and it's going to take some debugging to figure out what
that is.  The ICL kernel traces may yet be enough, to figure out what kind
of fileset-currency-age value is being returned from the exporter and thus
isolate where the problem is.

[4/19/94 public]
 
I've checked px_setsync and cm_CheckVolSync and I didn't see any merge
problems, however that doesn't mean there aren't any :-).
 
What we are getting is that cm_CheckVolSync is failing the test 
where it's looking for Volp->timeWhenVVCurrent+volp->hardMaxTotalLatency <= Now 
This is the case where syncp->VV and volp->latestVVSeen are equal.
 
Here's the icl tracing:
 
time 883.739418, pid 4027748: nh_dolookup dvp 263a27c, name cloneit.backup
time 883.739419, pid 4027748: gettokens vp 263a27c, rights.low 0x404
time 883.739423, pid 4027748: cm_GetScache vp 2645bbc, volume.low 0x1, vnode 0x6c
time 883.739426, pid 4027748: found fid e125b9b.1.6c.96 (hex)
time 883.739427, pid 4027748: lookup crossing mount point
time 883.739428, pid 4027748: gettokens vp 2645bbc, rights.low 0x404
time 883.739432, pid 4027748: in EvalMountPoint, vp 2645bbc
time 883.739433, pid 4027748: gettokens vp 2645bbc, rights.low 0x404
time 883.739439, pid 4027748: cm_ConnByMHosts server type 20000
time 883.739441, pid 4027748: cm_ConnByHost server type 0x20000, serverp 256210c
time 883.739442, pid 4027748: cm_ConnByHost using conn 2591e4c, service 0x20000
time 883.739443, pid 4027748: begin AFS_LookupRoot
time 883.749200, pid 4022692: in AFS_LookupRoot Volume 0.9
time 883.749204, pid 4022692: AFS_LookupRoot returning Vnode 1, Unique 1, code 0
time 883.749205, pid 4027748: end AFS_LookupRoot, code 0
time 883.749207, pid 4027748: cm_Analyze: conn 2591e4c, code 691089421, user 1107296254
time 883.749208, pid 4027748: cm_Analyze: volerr subcode 12
time 883.749210, pid 4027748: cm_ConnByMHosts server type 10000
time 883.749211, pid 4027748: cm_ConnByHost server type 0x10000, serverp 2572f0c
time 883.749212, pid 4027748: cm_ConnByHost using conn 2591acc, service 0x10000
time 883.749213, pid 4027748: begin VL_GetEntryByID
time 883.829200, pid 4027748: end VL_GetEntryByID, code 0
time 883.829201, pid 4027748: cm_Analyze: conn 2591acc, code 0, user 0
time 883.829203, pid 4027748: Install vol entry for volume ID 9
time 884.839206, pid 4027748: cm_Analyze: check loc of volume 9: result 0, gotNewLoc 0
time 887.849208, pid 4027748: cm_ConnByMHosts server type 20000
time 887.849210, pid 4027748: cm_ConnByMHosts: all filesets bad
time 887.849211, pid 4027748: cm_Analyze: conn 0, code -1, user 1107296254
time 887.849212, pid 4027748: mount point lookup failed
time 887.849213, pid 4027748: inactive vp 2645bbc
time 887.849214, pid 4027748: checkerror returning code 19

[4/19/94 public]
 
This information would probably help:
both hardMaxTotalLatency and timeWhenVVCurrent are 0.

[4/20/94 public]
I'm unable to find the problem here, given that I can't see your sources.
Can Diane send me a copy of your px_SetSync and cm_CheckVolSync procedures?
Thanks.

[4/20/94 public]
I'm checking on it...will let you know hopefully later today

[4/20/94 public]
 
okay here's what we've got.  I'm not certain if this is a merge
error or we are just missing something essential in our code base.
 
in cm_CheckVolSync  there's some code which goes something like:
 
if the syc VV and our vol VV are the same 
then
   if the VL_LAZYREP flag is set
   then
       update our memory of when this version was last current
   end if
   code = 0
/* The following line is what fails for the backup vol case */
   if volp->timeWhenVVCurrent + volp->hardMaxTotallatency <= Now
   then
        code = VOLERR_PERS_AGEOLD
   endif
   return
endif
 
Are we supposed to be checking VVCurrent and hardMaxTotalLatency for
a backup volume?  It seems that VVCurrent only gets initialized and updated for
replicated filesets.
 
When I move the offending check for VVCurrent+hardMaxTotalLatency <= now
to occur inside of the if VL_LASYREP block, the problem goes away.
 
Anybody got any clues?

[4/20/94 public]
Thanks, Diane.  I see the problem.  There were apparently two deltas for our
bug number 4423 and it looks like I gave you only one.  I'll forward the
other one to you immediately.  It's a simple fix, that causes the test
that you pointed to to be conditional on whether volp->hardMaxTotalLatency
is zero.  (There's a related fix that is included, also.)

[4/20/94 public]
Though the point is moot for this bug, I checked and Transarc 
does have permission to look at 1.1 sources.

[09/27/94 public]
Closed.



CR Number                     : 10378
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts addsite always returns 0
Reported Date                 : 4/14/94
Found in Baseline             : 1.1
Found Date                    : 4/14/94
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : volc_vldbsubr.c
Sensitivity                   : public

[4/14/94 public]

Diane, is this expected behavior?

fts addsite returns 0 even when the command returns an error message.  Is
the communications failure message just a warning?

The cell was configured with dce1.1ab2. 
root@cobbler> fts addsite rep.test dce8 m2.aggr2
Couldn't talk to repserver on dce8.osf.org: communications failure (dce /rpc)
Added replication site dce8 m2.aggr2 for fileset rep.test
root@cobbler> echo $?
0
root@cobbler> fts lsheader /.:/hosts/dce8
Total filesets on server /.:/hosts/dce8 aggregate /u2 (id 1): 1
/dev/dsk/c201d5s0        0,,1 RW 231908 K alloc 231908 K quota On-line
Total filesets on-line 1; total off-line 0; total busy 0

There are no filesets on aggregate m2.aggr2 (id 2) of server
/.:/hosts/dce8.

Total number of filesets on server /.:/hosts/dce8: 1
root@cobbler> fts lsfldb -fileset rep.test
        readWrite   ID 0,,46  valid
        readOnly    ID 0,,47  valid
        backup      ID 0,,48  invalid
number of sites: 2
  Sched repl: maxAge=0:20:00; failAge=1d0:00:00; reclaimWait=18:00:00;
minRepDelay=0:02:00; defaultSiteAge=0:05:00
   server           flags     aggr   siteAge principal      owner
   
cobbler.osf.org     RW,RO    m1.aggr2 0:05:00 hosts/cobbler  <nil>
    
dce8.osf.org        RO       m2.aggr2 0:05:00 hosts/dce8     <nil>

[04/18/94 public]

On the question of expected behavior - well sort of.
The fts command, in addition to updating the information in
the FLDB with the new replication site, will attempt to
contact the repserver to tell it to re-read the configuration
for that volume, since there is now new information in the FLDB.
It seems that the FLDB update was successful, but the repserver
interaction was not.  The error message should probably be issued
as "warning: couldn't contact the repserver" or something like
that.  In this case the fact that we couldn't talk to the repserver
probably isn't a serious error for the addsite command and should be 
treated as a warning. 

It is confusing to see the error about the repserver
and then see the command report success, so we should probably
fix the error message.

[09/27/94 public]
Closed.



CR Number                     : 10294
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : compilation errors on the RIOS
Short Description             : build failed on the RIOS
Reported Date                 : 4/5/94
Found in Baseline             : 1.1a
Found Date                    : 4/5/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1a
Fixed In Baseline             : 1.1a
Affected File(s)              : osi_alloc.h
Sensitivity                   : public

[4/5/94 public]

We got build errors on the RIOS with the header file osi_alloc.h as 
followed:

"/u1/devobj/sb/nb1_1a_rios/export/rios/usr/include/dcedfs/osi_alloc.h", line 190.0: 1506-215 (S) Too many arguments for macro osi_caller.
*** Error code 1

and

0706-317 ERROR: Unresolved or undefined symbols detected:
                 Symbols in error (followed by references) are
                 dumped to the load map.
                 The -bloadmap:<filename> option will create a load map.
.osi_caller

Look in /project/dce/build/nb1_1a_rios/logs/build.log.04-04-94
for more information.

thanks
annie



CR Number                     : 10281
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/file/rep
Short Description             : incorrect grep usage in file/rep/rtest3
Reported Date                 : 4/4/94
Found in Baseline             : 1.1a
Found Date                    : 4/4/94
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b11
Affected File(s)              : test/file/rep/rtest3
Sensitivity                   : public

[4/4/94 public]
 
Running file/rep/rtest3 yields the following error messages:
	usage:
	 grep  [-E|-F] [-cbilnqsvx] [-e expression [-e expression] ... | -f file]
	 [expression] [file ...]
	 egrep [-cbilnqsvx] [-e expression [-e expression] ... | -f file]
	 [expression] [file ...]
	 fgrep [-cbilnqsvx] [-e expression [-e expression] ... | -f file]
	 [expression] [file ...]
	./rtest3: On-line: not found

[4/25/94 public]
This has a trivial fix.  I'll mail it to Andy and Ron today, just in case
they haven't fixed it already.  (Our DB 5159.)

[4/25/94 public]
Already submitted the fix.  Sorry for the delay in updating the CR.

[8/11/94 public]
Rep functional tests successfully run since so closing - can not be
run currently due to 11423



CR Number                     : 10279
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : acl
Short Description             : misplaced quotes in driver script
Reported Date                 : 4/4/94
Found in Baseline             : 1.1
Found Date                    : 4/4/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1
Affected File(s)              : test/file/acl/scripts/accheck/driver.sh
Sensitivity                   : public

[4/4/94 public]

The file src/test/file/acl/scripts/accheck/driver.sh contains:

      su acluou "-c $DCE_LOGIN $USER_OBJ_USER $COMMON_USER_PASSWD -e $PROG"
      su $2 "-c $DCE_LOGIN $2 $COMMON_USER_PASSWD -e $PROG"

which fail because su attempts to interpret the "-e" option (which
belongs to the $DCE_LOGIN command) and reports "bad option(s)"

[04/05/94 public]
Submitted.

[5/23/94 public]
Builds and test runs fine now - closed.



CR Number                     : 10239
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 10224
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm spins when RO exceeds hardMaxLatency
Reported Date                 : 3/29/94
Found in Baseline             : 1.1
Found Date                    : 3/29/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b2
Affected File(s)              : cm_subr.c, cm_tokens.c, vol_errs.et
Sensitivity                   : public

[3/29/94 public]

the cm spins in cm_GetTokensRange when the RO volume it is requesting
tokens on exceeds hardMaxTotalLatency for that volume.  cm_HaveTokens
always returns failure in this case, which keeps the cm looping
trying to get the same tokens.  It would be nice if some how the cm
could realize (either via seeting an error flag in the cached volume
structure or something) that there is something wrong with this volume
and it should give up.

This can be reproduced by moving the RW volume, which causes the
repserver to fail to update the RO version (ot 10224); 

Suggestions for fixes anyone?


Apended is the icl trace log for this problem:

time 913.750008, pid 4022692: cm_ConnByMHosts server type 20000
time 913.750010, pid 4022692: cm_ConnByHost server type 0x20000, serverp 267120c
time 913.750011, pid 4022692: cm_ConnByHost using conn 25ee7cc, service 0x20000
time 913.750012, pid 4022692: begin AFS_GetToken for vp 260c5fc, rights 0x400
time 913.760000, pid 4022692: end AFS_GetToken, code 0
time 913.760001, pid 4022692: cm_Analyze: conn 25ee7cc, code 0, user 1107296251
time 913.760004, pid 4022692: mergestatus vp260c5fc, tokenp 7ffe6e94, flags 0x0
time 913.760006, pid 4022692: mergestatus ctime OK for merging, local mod flags 0x0
time 913.760009, pid 4022692: havetokens: vol 275 known 2d920557: past hard limit 86400
time 913.760013, pid 4022692: havetokens: vol 275 known 2d920557: past hard limit 86400
time 914.770008, pid 4022692: cm_ConnByMHosts server type 20000
time 914.770010, pid 4022692: cm_ConnByHost server type 0x20000, serverp 267120c
time 914.770011, pid 4022692: cm_ConnByHost using conn 25ee7cc, service 0x20000
time 914.770012, pid 4022692: begin AFS_GetToken for vp 260c5fc, rights 0x400
time 914.780000, pid 4022692: end AFS_GetToken, code 0
time 914.780001, pid 4022692: cm_Analyze: conn 25ee7cc, code 0, user 1107296251
time 914.780004, pid 4022692: mergestatus vp260c5fc, tokenp 7ffe6e94, flags 0x0
time 914.780006, pid 4022692: mergestatus ctime OK for merging, local mod flags 0x0
time 914.780009, pid 4022692: havetokens: vol 275 known 2d920557: past hard limit 86400
time 914.780013, pid 4022692: havetokens: vol 275 known 2d920557: past hard limit 86400
time 915.790012, pid 4022692: cm_ConnByMHosts server type 20000
:


[03/32/94 public]

Appending some stack traces; we keep alternating between the following
two stack traces:

breakpoint at 0x022c817c
}t
 0 cm_HaveTokens (0x2625c8c, 0x29c9c0b0, 0x1040000, 0)
 1 cm_MergeStatus + 0x00000878 (0x2628d4c, 0x7ffe6dc8, 0x7ffe6e94, 0)
 2 cm_GetTokensRange + 0x00000704 (0x2628d4c, 0x7ffe6d28, 0x7ffe6cd0, 0)
 3 cm_GetTokens + 0x00000078 (0x2628d4c, 0x400, 0x7ffe6cd0, 0x1)
 4 cm_getattr + 0x000001cc (0x2628d4c, 0x7ffe6bb0, 0, 0x25f1800)
 5 nux_getattr + 0x00000090 (0x2628d4c, 0x7ffe6bb0, 0x25f1800, 0x2)
 6 xglue_getattr + 0x00000100 (0x2628d4c, 0x7ffe6bb0, 0x25f1800, 0x2)
 7 vno_stat + 0x00000048 (0x262727c, 0x3ccc34, 0x25f1800, 0x40020784)
 8 stat1 + 0x0000006c (0x40013a11, 0x1, 0x7b033db8, 0x7ffe6000)
 9 lstat + 0x00000010 (0x3d0a00, 0x1fc000, 0, 0x1fc800)
10 syscall + 0x000001a4 (0xcf8f4, 0x1fc000, 0x320f, 0x40000000)
11 $syscallrtn ()
}breakpoint at 0x022c817c
}t
 0 cm_HaveTokens (0x2645638, 0x7ffe6dc8, 0x7ffe6e94, 0)
 1 cm_GetTokensRange + 0x00000170 (0x2628d4c, 0x7ffe6d28, 0x7ffe6cd0, 0)
 2 cm_GetTokens + 0x00000078 (0x2628d4c, 0x400, 0x7ffe6cd0, 0x1)
 3 cm_getattr + 0x000001cc (0x2628d4c, 0x7ffe6bb0, 0, 0x25f1800)
 4 nux_getattr + 0x00000090 (0x2628d4c, 0x7ffe6bb0, 0x25f1800, 0x2)
 5 xglue_getattr + 0x00000100 (0x2628d4c, 0x7ffe6bb0, 0x25f1800, 0x2)
 6 vno_stat + 0x00000048 (0x262727c, 0x3ccc34, 0x25f1800, 0x40020784)
 7 stat1 + 0x0000006c (0x40013a11, 0x1, 0x7b033db8, 0x7ffe6000)
 8 lstat + 0x00000010 (0x3d0a00, 0x1fc000, 0, 0x1fc800)
 9 syscall + 0x000001a4 (0xcf8f4, 0x1fc000, 0x320f, 0x40000000)
10 $syscallrtn ()
}yu
PID    UTIME     STIME    WHAT
2367        0        22    ls
}breakpoint at 0x022c817c
}

[3/24/94 public]

Craig has supplied us a fix for this problem.

[09/27/94 public]
Closed.



CR Number                     : 10229
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : can fts crmount with illegal fileset name
Reported Date                 : 3/25/94
Found in Baseline             : 1.0.3a
Found Date                    : 3/25/94
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1ab2
Affected File(s)              : userInt/fts/volc_main.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[3/25/94 public]
If you mistype the fileset name during a fts crmount, the mount point is
still created. This is annoying since the mount point is not useful and
will produce an error every time listed.
fts crmount -dir /:/m3.lfs.low.hp800 -fileset /m3.lfs.low.hp800 
fts crmount: warning: error checking for fileset /m3.lfs.low.hp800 in cell cho_bl5.qadce.osf.org: FLDB: fileset name is illegal (dfs / vls)
root@dce11> ls /:/m3.lfs.low.hp800
dfs: dce errors (code 676372487) from the fileset location server 130.105.201.7 in cell cho_bl5.qadce.osf.org
/:/m3.lfs.low.hp800 not found
Assigned to Diane since I know she was considering working on fts
commands in general ...

[3/28/94 public]
I believe this is intentional function so that the administrator
isn't forced to create the fileset before the mount point (i.e.
the two events can happen in either order).  Although I'm not sure
how often this is explcitly used, I think it is useful in the case
of mountpoints for .backup filesets.  For example, when a user's
home directory is created, a OldFiles mount point can be created
in it for a .backup that doesn't exist yet.  When the nightly
clonesys job runs, the .backup is created and the mount becomes
usable.  AFS works the same way.
This should be viewed as an enhancement, not a bug, and I'd like
to see some mechanism put in place for reviewing/prioritizing
enhancements for the 1.1 timeframe.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[3/28/94 public]
Filled in Interest List CC with `jeff@transarc.com'

[03/28/94 public]
I believe that the complaint isn't that the fileset doesn't exist, but
that the fileset name is illegal and can never exist - even at a later
point. Fixing this won't preclude the ability to create mountpoints for
non-existant filesets. Thanks for the warning however, we'll be sure
to doublecheck this in our testing.

I believe the existing enhancements have been reviewed and that the OT
database is in the process of being updated to reflect this. One method
of conveying your opinions on prioritization would be to update those
CRs.

[3/28/94 public]
Thanks for the additional info, I didn't understand the distinction
you were making between invalid fileset name and non-existant fileset.
This sounds fine to me.

[09/27/94 public]
Closed.



CR Number                     : 10224
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rep, fx
Short Description             : repserver loops with zero-valued VVafter RW copy is moved.
Reported Date                 : 3/24/94
Found in Baseline             : 1.1
Found Date                    : 3/24/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1ab2
Affected File(s)              : px_subr.c
Sensitivity                   : public

[3/24/94 public]
 
Seen on both 1.1 and the final 1.0.3a
 
Moving the RW fileset of a replicated fileset, whether we move to
a different aggregate on the same server or are moving to a different
server creates many problems after the RW is moved.
 
The access to the moved fileset from cm on the master is continously looping reporting the following:
 
time 852.219227, pid 10161: havetokens: vol 2 known 2d1bf83c: past hard limit 86
400
time 852.219320, pid 10161: havetokens: vol 2 known 2d1bf83c: past hard limit 86
400
time 852.224609, pid 17087: end AFS_GetToken, code 0
time 852.224658, pid 17087: cm_Analyze: conn 5e0790c, code 0, user 0
time 852.224758, pid 17087: mergestatus vp5e1b254, tokenp 2ff97bc0, flags 0x0
time 852.224837, pid 17087: mergestatus ctime OK for merging, local mod flags 0x
: end AFS_GetToken, code 0
time 853.257547, pid 10161: cm_Analyze: conn 5e0788c, code 0, user 0
time 853.257646, pid 10161: mergestatus vp5e1b254, tokenp 2ff97530, flags 0x0
time 853.257730, pid 10161: setting online token ID for vp 5e1b254, dcp 5e14f00
time 853.257834, pid 10161: mergestatus ctime OK for merging, local mod flags 0x
time 853.257894, pid 10161: havetokens: vol 2 known 2d1bf83c: past hard limit 86
400
time 853.257988, pid 10161: havetokens: vol 2 known 2d1bf83c: past hard li
I have not found a way to stop this other than rebooting the machine.
The ftserver on the primary site reports the following:
 
root@barge $ pg FtLog
1994-Mar-24 08:56:45 Log file initialized as /opt/dcelocal/var/dfs/adm/FtLog
1994-Mar-24 08:56:46 Ftserver starting
1994-Mar-24 09:22:25 Failed to open 2:0,,1, code = 18 (Cross-device link)
1994-Mar-24 09:22:27 Cloning 1:root.dfs (0,,1) into root.dfs.move-temp (0,,116)
1994-Mar-24 09:22:27 Creating fileset root.dfs.move-temp (0,,116) on aggr 1
1994-Mar-24 09:22:27 Failed to open 1:0,,116, code = 572833799 (Fileset does not
 exist (dfs / ftu))
1994-Mar-24 09:22:28 Failed to open 2:0,,117, code = 572833799 (Fileset does not
 exist (dfs / ftu))
1994-Mar-24 09:22:28 Creating fileset root.dfs (0,,117) on aggr 2
1994-Mar-24 09:22:28 Failed to open 2:0,,117, code = 572833799 (Fileset does not
 exist (dfs / ftu))
1994-Mar-24 09:22:28 ftserver_CreateVolume: created root.dfs as 0,,117 on aggr 2
1994-Mar-24 09:22:30 Dumping 1:root.dfs.move-temp (0,,116)
1994-Mar-24 09:22:39 Restoring fileset 0,,117/2
1994-Mar-24 09:22:39 Restoring 2:root.dfs (0,,117)
1994-Mar-24 09:25:20 vols_Restore: returning 0
1994-Mar-24 09:25:20 Restored fileset 0,,117/2: returned code 0
1994-Mar-24 09:25:20 Destroying 1:root.dfs.move-temp (0,,116)
1994-Mar-24 09:25:21 Dumping 1:root.dfs (0,,1)
1994-Mar-24 09:25:23 Restoring fileset 0,,117/2
1994-Mar-24 09:25:23 Restoring 2:root.dfs (0,,117)
1994-Mar-24 09:25:23 vols_Restore: returning 0
1994-Mar-24 09:25:23 Restored fileset 0,,117/2: returned code 0
1994-Mar-24 09:25:24 Destroying 1:root.dfs (0,,117)
The repserver on the secondary site is constantly calling ReplicaWantsAdvance
and not getting anywhere:
94-Mar-24 10:44:29 Disk scan: attaching 0,,2 (aggr 1, backing 0,,0) to rep 'root.dfs'
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
94-Mar-24 10:44:35 setNextTime: 0,,2: delay 0 secs: ReplicaWantsAdvance: Trying to get VV from primar
y
---
 
I put Craig on the interest list because maybe he already knows what is
going wrong here. Hopefully there is a workaround for this?

[3/24/94 public]
 
more info:  Aparently the repserver is calling AFS_FetchBulkVV to get
the volume version for this fileset and AFS_FetchBulkVV is always returning
0 for the VV, which triggers the looping in the repserver.  On the primary
site, lsft says that the version is non-zero (If I'm interpreting the output
correctly?) 
 
root@barge $ fts lsft -fileset root.dfs -server barge -localauth
_____________________________________________
root.dfs 0,,1 RW LFS Scheduled  states 0x10010205 On-line
    barge.osf.org, aggregate epi01 (ID 2)
    Parent 0,,0       Clone 0,,2       Backup 0,,3
    llBack 0,,0       llFwd 0,,0       Version 0,,9690
       1048576 K alloc limit;       9244 K alloc usage
         20000 K quota limit;       9244 K quota usage
    Creation Thu Mar 24 09:22:28 1994
    Last Update Thu Mar 24 09:25:24 1994
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  valid
        backup      ID 0,,3  invalid
number of sites: 2
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDela
y=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner
barge.osf.org       RW       epi01   0:00:00 hosts/barge    <nil>
seagrams.osf.org    RO       lfs01   0:30:00 hosts/seagrams <nil>
_____________________________________________
root@barge $

[3/24/94 public]
This all looks pretty mysterious.  Jeff Prem has been looking in this
general area and might have some clue as to what's going wrong.
 
Indeed, the root.dfs (R/W) version is non-zero.  The repserver assumes that
its attempt to get a VV will result in a non-zero result.  I think that the
repserver's ICL log will be the thing that will tell us what happens when
the repserver tries to read the version of the R/W root.dfs.
 
Ah, now I remember.  Fileset motion sets the VOL_REPFIELD even in the R/W
fileset, whereas it's not set normally (a different bug; setrepinfo should
be setting it).  This causes px_SetSync to check the VOL_IS_COMPLETE bit
in the volume states, which is never set in a R/W fileset since it's never
managed by the repserver.  If that's not set, px_SetSync always returns
a very large age for the fileset, since it assumes that it's being asked
about an incomplete repserver-managed fileset, about which no assumptions
could be made.
 
Jeff has a fix for this, but it's part of some larger work.  I'll let him
fill you in on it.
Changed Interest List CC from `cfe@transarc.com' to `cfe@transarc.com, 
 jdp@transarc.com'

[3/29/94 public]

I'm going to move the cache mgr problems over to another ot and Leave
this one for the repserver looping continuously when it get the 0 VV.

[3/31/94 public]

Craig has given us a fix for AFS_BulkFetchVV returning the 0 VV

[09/26/94 public]
Closed.



CR Number                     : 10199
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : build errors in test/file on the RIOS
Short Description             : build errors in test/file on the RIOS
Reported Date                 : 3/22/94
Found in Baseline             : 1.1a
Found Date                    : 3/22/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1a
Fixed In Baseline             : 1.1a
Affected File(s)              : test/file/recovery/*, test/file/scavenge/*, test/file/ravage/*
Sensitivity                   : public

[3/22/94 public]
There were a bunch of build problems on the RIOS platform related
to the Makefile.  These were:

     src/test/file/recovery/Makefile
     src/test/file/ravage/Makefile
     src/test/file/scavenge/Makefile

There were a total of 15 error messages during the test build BUT the
problem is only related to these three Makefiles.

The install test phase also failed with errors coming from these three
Makefiles.

thanks
annie

[3/23/94 public]
Good WORK diane.  Thanks for the quick turn around!!



CR Number                     : 10191
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : RIOS' build failed
Short Description             : build of dfs failed on RIOS
Reported Date                 : 3/21/94
Found in Baseline             : 1.1a
Found Date                    : 3/21/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1a
Fixed In Baseline             : 1.1a
Affected File(s)              : Nightly build script - driver.sh on tinker, RIOS
Sensitivity                   : public

[3/21/94 public]

Build of dfs on RIOS failed.  This is because vertigo went down
and was being upgraded.  At that time, the build has already started.
So in the middle of it, it died.

There is really NO two way around this problem.  Maybe the upgrade
of vertigo should have been performed on Saturday.

I have updated the nightly build script to minimize this sort of
failures.  In addition, the cleanup of the build machine has been
changed to be after the mount point.  This way, if any mount points
failed, we still have the previous night's build which MIGHT HAVE
work rather than nothing.

thanks
annie



CR Number                     : 10190
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : HPUX's build failed
Short Description             : build of dfs failed on HPUX
Reported Date                 : 3/21/94
Found in Baseline             : 1.1a
Found Date                    : 3/21/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1a
Fixed In Baseline             : 1.1a
Affected File(s)              : Nightly Build script - driver.sh on HPUX, zagreb
Sensitivity                   : public

[3/21/94 public]

Build of dfs on HPUX failed.  This was due to the fact that
the HPUX server was upgraded to a newer version.  Because of
this, when the Nightly Build tries to mount the snapped area
from the server, it got an 'NFS stale files' error and failed.

The fix is to unmount the required mount points in the Nightly 
Build script and remount them every time rather than relying
in the fact that they are there.  This means that common mount
points shared between the DCE 1.1 group such as /usr/ode/bin
cannot be used; othewise unmounting it will cause the other 
project's build to die.  The other mount points are o.k.

In addition to the above fix, I have moved the cleanup step
to be after the mount step.  This way, if the mount failed,
we can still rely on the previous night's build which MIGHT
have succeeded rather than nothing.

The build is going, so the fix has been verified.  Afterall, 
the server is now up and running.



CR Number                     : 10176
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : file/libafs
Short Description             : don't know how to make libknck.a
Reported Date                 : 3/18/94
Found in Baseline             : 1.1
Found Date                    : 3/18/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : combind.c
Sensitivity                   : public

[3/18/94 public]

[ /file/libafs at 06:19 (AM) Friday ]

makepath libafs/. && cd libafs &&  exec make MAKEFILE_PASS=BASIC
build_all
make: don't know how to make libknck.a (continuing)
`extras.exp' is up to date.
`export.exp' is up to date.
`libvolreg.a' is up to date.
`libvolume.a' is up to date.
`libkxcred.a' is up to date.
`libkdfskutil.a' is up to date.
`libkosi.a' is up to date.
`libaixexport.a' is up to date.
`libxvnode.a' is up to date.
`libagfs.a' is up to date.
`libktkc.a' is up to date.
`libktkm.a' is up to date.
`libaggr.a' is up to date.
`libkolddacl.a' is up to date.
`libufsops.a' is up to date.
`libkdacl.a' is up to date.
`libkdacllfs.a' is up to date.
`libktpq.a' is up to date.
`libkzlc.a' is up to date.
`libkicl.a' is up to date.
`libkbomb.a' is up to date.
`libkafsutil.a' is up to date.
`dfscore.exp' is up to date.
`extras.exp' is up to date.
`afs.exp' is up to date.
`dfscore.exp' is up to date.
`libcm.a' is up to date.
`libpx.a' is up to date.
`libkfldb.a' is up to date.
`libhost.a' is up to date.
`libktkset.a' is up to date.
`libkcommondata.a' is up to date.
`dfscore.exp' is up to date.
`extras.exp' is up to date.
`afs.exp' is up to date.
`dfscmfx.exp' is up to date.
`export.exp' is up to date.
`dfscmfx.exp' is up to date.

[3/18/94 public]

Still broken, but the RPC bug in OT 10173 has gone away.  Here are the new
error messages:

[ /file/libafs ]

0706-222 WARNING:  Import version of 'brkpoint' replaced by import
definition.
0706-221 WARNING:  Import version of 'setgroups' replaced by local
definition.
0706-222 WARNING:  Import version of 'uprintf' replaced by import
definition.
0706-317 ERROR: Unresolved or undefined symbols detected:
                 Symbols in error (followed by references) are
                 dumped to the load map.
                 The -bloadmap:<filename> option will create a load map.
.rpc__tower_flr_to_rpc_prot_id
*** Error code 8
`build_all' not remade because of errors.
*** Error code 1
`build_all' not remade because of errors.
*** Error code 1

[03/21/94 public]
I placed the offending routine in combind.c under #ifndef _KERNEL.

[09/27/94 public]
Closed.



CR Number                     : 10125
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfsbind, krpc_helper
Short Description             : make dfsbind buffers configurable
Reported Date                 : 3/11/94
Found in Baseline             : 1.1
Found Date                    : 3/11/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : dfsbind/main_helper.c, kruntime/krpc_helper.c, kruntime/krpc_helper.h
Sensitivity                   : public

[3/11/94 public]

This is a 1.1 integration issue; in the new world we are
going to need to make the upcall buffers a configureable size
because things are getting bigger (EPACS).

[03/17/94 public]

After inquring with Mike Burati, it's now not
clear whether we really need to do this.
We can re-open this ot if it turns out later that
this is necessary.

[04/22/94 public]

As it turns out, we do need to do this after all.

[09/27/94 public]
Closed.



CR Number                     : 9986
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bosserver
Short Description             : DfsServerKeyManagerThread needs better retry algorithm
Reported Date                 : 2/22/94
Found in Baseline             : 1.1a
Found Date                    : 2/22/94
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : bosserver/bossvr_main.c
Sensitivity                   : public

[2/22/94 public]
The DfsServerKeyManager Thread retries every second if the
sec_key_mgmt_manage_key routine fails.  This results in
huge log files which fill up the filesystem. The DfsServerKeyManger
thread should probably have some kind of backoff algorithm with
some maximum retry interval, say 5 minutes.
 
This problem can be reproduced by causing a time skew between a bosserver host
and the security server host.
Filled in Interest List CC with `cfe@transarc.com'

[09/27/94 public]
Closed.



CR Number                     : 9960
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : stale host error causes raceing revoke
Reported Date                 : 2/15/94
Found in Baseline             : 1.0.3a
Found Date                    : 2/15/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : rep_main.c
Sensitivity                   : public

[2/15/94 public]
 
Configuration: 2 servers, 1 HPUX one RIOS
 
Master replica resides on the HPUX, readonly on the RIOS.
 
Replication parameters: all defaults except default site age which
is 10 minutes.
 
What we are seeing is that after about the second time we replicate,
it takes too long for the replica to get updated.  It can take on
the order of hours or overnight. The neither the replica nor the
master is busy during any of this. The update consists of createing
an empty directory on the master fileset.
 
What we have noticed is that the background thread in the repserver
Calls AFS_GetToken and gets a Stale host error.  It then resets
the binding and does an AFS_SetContext which causes the server
to initiate a TKN_InitTokenState back to the repserver, which 
of course, changes some of the repserver internal state information
for that replica.  In the meantime, the background thread has tried
GetToken a second time and then checks to see if everything is
okay (TryForWVTRead) however it hits the token-revocation race
case:
 
Here's the relevant part of the log for the repserver (which repeats itself
for a long time). 
---------------------
 
time 7.004382, pid 11: bkg: now 761356295, tgt 761356295, timed out: on time.
 
time 7.011021, pid 11: 0,,5421: Attempting to get token (0x404) for 0,,5420 on primary
time 7.053721, pid 11: GetToken result: 572616705
time 7.056216, pid 11: 0,,5421: TryForWVTRead-2/getFSConn(secId 0,,5421): connecting to 1
30.105.4.204
time 7.059287, pid 11: dfsauth_client_InitBindingAuth entered
time 7.059613, pid 11: dfsauth_client_InitAuthContext entered
time 7.059926, pid 11: dfsauth_client_GrabLock entered
time 7.060593, pid 11: dfsauth_client_GrabLock exiting, returning Error 0
time 7.060942, pid 11: dfsauth_client_ReleaseLock entered
time 7.061304, pid 11: dfsauth_client_ReleaseLock exiting, returning Error 0
time 7.061666, pid 11: dfsauth_EstablishLocalAuthContext entered
time 7.061984, pid 11: dfsauth_client_GrabLock entered
time 7.062381, pid 11: dfsauth_client_GrabLock exiting, returning Error 0time 7.063078, pid 11: dfsauth_PrinName_MakePrincipalName entered, base name is: hosts/al
catraz
time 7.063501, pid 11: dfsauth_PrinName_MakePrincipalName exiting, returning 0, construct
ed name is: hosts/alcatraz/dfs-server
time 7.063895, pid 11: dfsauth_EstablishLocalAuthContext: about to call sec_login_refresh
_identity with name: `hosts/alcatraz/dfs-server'
time 7.286108, pid 11: dfsauth_EstablishLocalAuthContext: sec_login_refresh_identity retu
rned
time 7.286428, pid 11: dfsauth_EstablishLocalAuthContext: about to call sec_login_valid_f
rom_keytable
time 9.566816, pid 11: dfsauth_EstablishLocalAuthContext: returned from sec_login_valid_f
rom_keytable
time 9.567140, pid 11: dfsauth_EstablishLocalAuthContext: about to call sec_login_set_con
text
time 9.700849, pid 11: dfsauth_EstablishLocalAuthContext: sec_login_set_context returned
time 9.701190, pid 11: dfsauth_client_ReleaseLock entered
time 9.701557, pid 11: dfsauth_client_ReleaseLock exiting, returning Error 0
time 9.701915, pid 11: dfsauth_EstablishLocalAuthContext exiting, returning Error 0
time 9.702281, pid 11: dfsauth_client_InitAuthContext exiting, returning Error 0
time 9.702620, pid 11: dfsauth_client_InitBindingAuth: about to call rpc_binding_inq_auth
_info
 
time 9.702988, pid 11: dfsauth_PrinName_GetName entered
time 9.703295, pid 11: dfsauth_PrinName_GetName: basename is /.../mine_field.osf.org/host
s/explorer/dfs-server, adding suffix? no
time 9.703930, pid 11: dfsauth_PrinName_MakePrincipalName entered, base name is: /.../min
e_field.osf.org/hosts/explorer/dfs-server
time 9.704426, pid 11: dfsauth_PrinName_MakePrincipalName exiting, returning 0, construct
ed name is: /.../mine_field.osf.org/hosts/explorer/dfs-server
time 9.704869, pid 11: dfsauth_PrinName_GetName exiting, returning principal /.../mine_fi
eld.osf.org/hosts/explorer/dfs-server, value 0
time 9.705317, pid 11: dfsauth_client_InitBindingAuth: about to call rpc_binding_set_auth
_info, authn svc = rpc_c_authn_dce_private, principal name: /.../mine_field.osf.org/hosts
/explorer/dfs-server
time 9.706204, pid 11: dfsauth_client_InitBindingAuth: rpc_binding_set_auth_info returned
 
time 9.706542, pid 11: dfsauth_client_InitBindingAuth exiting, returning Error 0time 9.706973, pid 11: fs conn before first call
time 9.709804, pid 11: 0058b06a-5ac8-1d61-bfb8-02608c2f6c55@ncadg_ip_udp:130.105.4.204[]:
 u '0058b06a-5ac8-1d61-bfb8-02608c2f6c55', p 'ncadg_ip_udp', n '130.105.4.204', e '', o '
'
 
time 9.794868, pid 16: repserver: STKN_InitTokenState() called
time 9.795261, pid 16: repserver: STKN_InitTokenState about to get global lock
time 9.795655, pid 16: repserver: STKN_InitTokenState got global lock
time 9.796360, pid 16: 0,,5421: LoseWVT: Racing revocation at 761356297: 0 -> 0xffffffff
time 9.796864, pid 16: repserver: STKN_InitTokenState released global lock and returns
time 9.828562, pid 11: fs conn after first call
time 9.831952, pid 11: 0058b06a-5ac8-1d61-bfb8-02608c2f6c55@ncadg_ip_udp:130.105.4.204[12
03]: u '0058b06a-5ac8-1d61-bfb8-02608c2f6c55', p 'ncadg_ip_udp', n '130.105.4.204', e '12
03', o ''
 
time 9.908070, pid 11: GetToken result: 0
time 9.908725, pid 11: 0,,5421: GetToken returned 0x404 token, ID 761339361,,192.
time 9.909680, pid 11: 0,,5421: TryForWVTRead: types 0xffffffff already revoked, leaving
time 9.910466, pid 11: NeedSetVol figuring VV
time 9.910823, pid 11: NeedSetVol TRUE: new PingCurr time 761355995.527307 -> 761356297.7
94583
time 9.911162, pid 11: UpdateLocal calling AGOPEN
time 9.912118, pid 11: UpdateLocal: calling SetOpenVol
time 9.912424, pid 11: SetOpenVol calling SETVV
time 9.922555, pid 11: SetOpenVol: Set VV for 0,,5421: flags 1000, mask 3000, curr 761354
994.114226, pc 761356297.794583, TO 761356377.000000
time 9.923107, pid 11: SetOpenVol returning 0
time 9.923422, pid 11: UpdateLocal: calling CLOSE
time 9.924195, pid 11: UpdateLocal: returning 0
time 9.925901, pid 11: [0,,5421: delay 300 secs: TryForWVTRead: interleaved revoke]
  
-------
The next step is to determin why we are getting the Stale host error.

[2/16/94 public]
It's not unreasonable for the host to be called Stale, once in a
while, since in a single-threaded repserver the host object might not
be refreshed often enough.  (A single FTSERVER_Forward() command can
easily take more than the two-minute host timeout.)
 
What I don't understand is why this is a repeating pattern.  I'd
imagine that the host would be made non-stale once, then would be OK
when it was next used.
 
Maybe what's happening is that the token-refresh loop isn't seeing
this host as one that requires refreshing, since it doesn't believe
that it holds a token.  In that case, the repserver won't make any
RPCs to the exporter, the host object in the exporter will age, and
the exporter won't bother with making an RPC to cause revocations.
 
This fails if the exporter's token manager doesn't get any revocation
requests for the WVT for the first two minutes, but before the five
minute interval expires, the WVT is revoked.  After a while, this
won't be the case, and the repserver will have a still-alive host
object to work with.
 
A couple of work-arounds come to mind, assuming this is the problem.
 
  (a) Force the interleaved-revoke timeout to be smaller than two
      minutes (the hostLifetime interval) by some fraction.  This
      300 second wait is letting the host object die.  There is supposed
      to be code that does this in the repserver, in LoseWVT, when it's
      the initial token loss that's revoking a held token.  There could
      be comparable code in TryForWVTRead, where it records the
      ``interleaved revoke'' message in a call to setNextTime() with
      the timeout of rp->remoteBusyWait.  This timeout could be cut back
      to a few seconds (like 15, say) if code==0 (interleaved revoke)
      and WVT_TypesRevoked==(-1,,-1) (because a TKN_InitTokenState
      intervened, knocking out all tokens, rather than a simple revocation,
      which would have knocked out only specific tokens).  Then again,
      it might make as much sense to cut the timeout just based on code==0
      since any revocation will leave the repserver not doing its keep-alive
      job on the px host object.
 
  (b) Trick the token-keep-alive function in the repserver to keep
      host objects alive (with periodic AFS_GetTime calls) even if it
      appears that no token is being held.  This could be done by setting
      some still-wishing-to-be-active bit in the repserver's host object
      and checking that in the keep-alive loop.
Filled in Interest List CC with `cfe@transarc.com, jdp@transarc.com'

[2/16/94 public]

Thanks, I will try option A, since this seems to be
the simpler thing.  

I am seeing that the AFS_GetTime calls stop running regularly
after we get the STKN_TokenRevoke call when the master is updated.
We then see the bkg thread sleep for serveral minutes with the
message "StartImporting", by the time it wakes up to do the AFS_GetToken 
it's been too long and the host information is already stale.  The 
AFS_SetContext is then called, to reset things after the stale host
error and then the race begins.  So the bkg thread retries the AFS_GetToken
and is successful, but notices that revokes have occured so it
returns and sleeps for 5 minutes, causing the host information to
go stale by the next time it runs.  After several hours, it fixes
itself, but we don't know why.

[09/27/94 public]
Closed.



CR Number                     : 9931
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : IDL files
Short Description             : Need fault_status comm_status
Reported Date                 : 2/10/94
Found in Baseline             : 1.1
Found Date                    : 2/10/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : ./bakserver/budb.idl, ./bosserver/bbos_ncs_interface.idl, ./bubasics/butc.idl, ./icl/icl_rpc.idl, ./security/dfsauth/authtest.idl
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[2/10/94 public]
All IDL files that are part of libdce should be compiled with ACF files
that have the [fault_status,comm_status] attributes.  Without this, a
failing RPC could cause client code to core dump when there is a perfectly
good error_status_t parameter available.  When adding the ACF files, it is
important to make sure that no API changes.  As most routines probably
already have an "error_status_t *st" parameter this will probably not be
an issue; routines that don't will have to be handled on a case-by-case,
probably by adding a new routine that DOES have the status param and
changing our existing code to call that routine.

[3/30/94 public]
I believe this bug applies to the following DFS
IDL interfaces.  I have checked the others and
believe them to be correct.  The first four i/f's
below define only comm_status, not fault_status.  Of
these, the return is already defined as error_status_t,
except for bbos_ncs_interface.idl which returns
errors through an error_status_t argument.  The
comm_status attribute applies directly to that
argument in that case.

The last interface is a test interface.  It defines
no .acf at all.  I've not touched this one because
it's not built and it appears to be a developer tool
only.

./bakserver/budb.idl
./bosserver/bbos_ncs_interface.idl
./bubasics/butc.idl
./icl/icl_rpc.idl
./security/dfsauth/authtest.idl

I have added the fault_status attribute in the
appropriate places in the appropriate .acf's, built 
new servers and clients, and exercised pieces of the
interfaces. All seems to work as before.  Should I be
concerned about checking for new error codes in the
client as a result of these changes?  If so, which
error codes?

[3/30/94 public]
At one point, there were some rpc exceptions which could not
be converted to either comm_status or fault_status error codes.
Does anyone know if this is still the case?  Its probably worth
looking at this in the context of this defect to determine if
adding these acf files is a complete solution.  If there are
exceptions which can still surface we probably also need some
TRY/CATCH blocks to handle those.

We recently saw a situation where the bos create command
core dumped due to an unhandled exception and added a TRY/CATCH
block around the cmd_Dispatch call in bos_main.c:main() to give
us one last chance to catch these things.  Your solution (adding
fault_status) is a better one, but the TRY/CATCH may still be
necessary??

FYI, if you want a good test scenario, try sending a -cmd parameter
to the bos create command that is longer than 256 characters.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[3/30/94 public]
Thanks for the test scenario.  I get an exception even with 
the fault_status included in the acf file:

Exception: status 16c9a07d (dce / rpc) (= rpc_s_fault_invalid_bound)
Abort(coredump)

So it looks like this is an incomplete solution, as you suggest.   
I'll submit the .acf changes under this OT.  Are you planning on 
opening a new OT for this problem you pointed out?

[3/31/94 public]
Opened OT 10258 for the second part of this problem.
Its at the same severity level, etc as this one, change
it if appropriate.

[4/18/94 public]
Fixed the first part of this problem.

[09/27/94 public]
Closed.



CR Number                     : 9896
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa,rs6000
S/W Ref Platform              : hpux,aix
Component Name                : dfs
Subcomponent Name             : src/test/file
Short Description             : DFS test Makefiles had pre-ODE
2.3 include line format
Reported Date                 : 2/7/94
Found in Baseline             : 1.1
Found Date                    : 2/7/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1
Affected File(s)              : src/file/HP800,RIOS/machdep.mk
Sensitivity                   : public

[2/7/94 public]

Pre-ODE 2.3 angle brackets for include lines in DFS test Makefiles caused
the following error:

[ /test/file/itl at 23:41 (PM) Friday ]

makepath itl/. && cd itl &&  exec make MAKEFILE_PASS=FIRSTB     build_all
"/project/dce/build/dce1.1-snap/src/test/file/itl/Makefile", line 190:
Could no\
t find ../../..//file/HP800/machdep.mk
Fatal errors encountered -- cannot continue
*** Error code 1

A fix is being submitted, which converts the include lines to double
quotes format.

[3/4/94 public]
These fixes were submitted on 2/7 and worked fine.  This defect should have
been closed long before this....



CR Number                     : 9878
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : revert changes in dfs printfs
Reported Date                 : 2/3/94
Found in Baseline             : 1.0.3a
Found Date                    : 2/3/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/file/xaggr/aggr.h, src/file/userInt/fts/volc_main.c, src/file/butc/lwps.c
Sensitivity                   : public

[2/3/94 public]
The text of a couple of the messages output by DFS were changed
in 1.0.3a.  This OT is to revert the text to the original to
avoid documentation impacts on existing DFS products.  This was
requested by IBM.

[2/4/94 public]
Submitted. Closed.



CR Number                     : 9870
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : open/lock argument mismatch in filewnr.c
Reported Date                 : 2/1/94
Found in Baseline             : 1.0.3a
Found Date                    : 2/1/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/file/filewnr.c
Sensitivity                   : public

[2/1/94 public]
The last submission of filewnr.c contained a path where an exclusive
write lock was being requested for a file opened for read-only access.
This lock request always failed, causing all dfs.lock iterations to
fail.

[2/4/94 public]
Fix tested and submitted - closing.



CR Number                     : 9859
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : osi
Short Description             : afscall_plumber needs some work
Reported Date                 : 1/31/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/31/94
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : osi_misc.c
Sensitivity                   : public

[1/31/94 public]

It would be better if afscall_plumber walked the osi_allocrec_head
chain and returned this information rather than the osi_memhash chain.
The osi_allocrec_head contains summaries of allocation histories
for various callers which is very useful and easier to use for
memory leak tracing than the osi_memhash information.

[2/22/94 public]

Forgot to mark this as fix.  Note though, that osi_caller still isn't
implemented on HPUX.

[09/27/94 public]
Closed.



CR Number                     : 9851
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : integrate 1.0.3a chgs into 1.1
Reported Date                 : 1/28/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/28/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/file/episode/vnops/HP800/efs_hpux_subr.c src/file/episode/logbuf/buffer.c
Sensitivity                   : public

[1/28/94 public]
This is just a placeholder to track integration of 1.0.3a changes
back into 1.1.  It is also a reminder to include the unintegrated
changes in OT9560 and OT8564.

[2/7/94 public]

1.0.3a code was submitted to 1.1 on 2/4/94.  The nightly build went fine
and is now being smoke tested.  OT 9560 was integrated into the merge,
however 8564 was left to be merged in later by hand.

[3/22/94 public]
8564 now merged.  fixed.

[09/27/94 public]
Closed.



CR Number                     : 9840
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : submit unintegrated fixes
Reported Date                 : 1/28/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/28/94
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/unintegrated
Sensitivity                   : public

[1/28/94 public]
Just a place holder to submit unintegrated fixes to 1.0.3a.

[1/28/94 public]
Fixed.

[02/04/94 public]
Closed.



CR Number                     : 9837
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Install misses some DFS header files.
Reported Date                 : 1/28/94
Found in Baseline             : 1.1, 1.0.3a
Found Date                    : 1/28/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : file/Makefile, file/{config,flserver,osi,xvnode}/Makefile
Sensitivity                   : public

[1/28/94 public]

We have someone trying to use the documented DFS API (in the Prentice-Hall
books, or the man pages).  He has found that the set of header files
installed in /usr/include/dcedfs (linked to
/opt/dcelocal/share/include/dcedfs) is not complete.  The following header
files could not be found:

	dcedfs/param.h
	dcedfs/stds.h
	dcedfs/afsvl_s2c.h
	dcedfs/afsvl_data.h
	dcedfs/osi.h
	dcedfs/lock.h
	dcedfs/queue.h
	dcedfs/osi_param.h
	dcedfs/osi_net.h
	dcedfs/xvfs_vnode.h

There may be others.  Here is a set of diffs to fix these.  The changed
files are:

  src/file/config/Makefile
  src/file/flserver/Makefile
  src/file/osi/Makefile
  src/file/xvnode/Makefile
  src/file/Makefile

[3/8/94 public]
Submitted fixed to dce1.1a shared sandbox

[09/27/94 public]
Closed.



CR Number                     : 9832
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest
Short Description             : test other parameters of replication in dfs.repfs
Reported Date                 : 1/27/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/27/94
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1a
Affected File(s)              : test/systest/file/dfs.repfs, test/systest/file/repfs.data
Sensitivity                   : public

[1/27/94 public]
The following parameters of replication should be tested:

MaxAge -- see if the cache is updated 
MaxSiteAge -- use values other than the DefaultSiteAge
              This value is set in the fts addsite command so there can be
              a different value at each site of the replica.
FailAge
ReclaimWait

Verification of the replication can include flushing the cache of the
fileset and then accessing it again.  The updates should then be accessible
soon after the MinRepDelay instead of the sum of MinRepDelay and MaxAge.
Currently the verification is done by checking the information from the fts
lsreplicas command, not by accessing the replicated fileset.

[5/6/94 public]
Submitted a new version that includes the following enhancements:
Verifies MaxAge and FailAge.
Reports the number of iterations that passed/failed.
Moves the read/write fileset
Creates a replica on the same machine as the read/write fileset
Creates a new replica, removes the old replica, verifies access to the
  replica



CR Number                     : 9828
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : put copyright headers files
Reported Date                 : 1/27/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/27/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : 57 files, look in bsubmit.log
Sensitivity                   : public

[1/27/94 public]

There were NO COPYRIGHT EXPANSION in some of the DCE1.0.3 
src/file and src/test/file that we are going to ship.

So, we will be expansing this, and put this in DCE1.0.3a.

thanks
annie

[1/29/94 public]

This has been performed on Friday.  All files needed from the
DCE1.0.3 tree without the copyright expansion have been checked
into the DCE1.0.3a tree.

thanks
annie



CR Number                     : 9791
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : newaggr
Short Description             : newaggr inits HP UFS partition
without warning
Reported Date                 : 1/21/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/21/94
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : 
file/episode/anode/edbcheck.c , file/episode/anode/newaggr.c
Sensitivity                   : public

[1/21/94 public]

newaggr will initialize a LFS aggregate over a HP UFS partition without
warining even when -overwrite is not specified.  This happened when I
accidently specified the device for my root partition instead of my second
drive.  This did not bring down the system and did not corrupt the
filesystem.  No ill effects were seen until I tried to reboot.

I have seen newaggr refuse to init a partition with a valid UFS filesystem
on it in the past, but it did not catch it this time.  I would hazard a
guess that it is because my root partition contains swapspace or boot
records that newaggr did not think it was a valid UFS partition.

[1/24/94 public]
Defer to 1.1. Assign to Kinney since it requires knowledge of
hpux native fs on-disk format.  There's some generic code for
recognizing a BSD filesystem in newaggr.c, but I suspect the
HP layout is different because it has some SysV heritage.

[01/25/94 public]
This has also happened to me.

A few weeks ago, I tried a simple change to newaggr (in
MakeAggregate()) to check that the specified aggregate wasn't
already mounted before proceeding.  Just as an experiment ...
independent of whether -overwrite was specified.

This seemed to "work" but I meant to get back to it eventually
and somehow check for devices currently in use for swapping.

If you really want something soon, I can give you what I have.
Although it might be better than  nothing, it may differ from
a long-term, final solution.  In the mean time, I'll try to find
what the on-disk fields look like to see if newaggr can recognize
a valid hpux disk (either file system or device swap).  The other
direction would be nice as well: arranging for HPUX to refuse to
mount or swap off of an aggregate that had been taken by Episode.

[1/28/94  public]
I've just submitted two changes which
  (a)  cause newaggr to recognize an existing hpux file system and refuse
       to proceed unless -overwrite is specified
  (b)  cause newaggr to refuse to proceed if the specified partition is
       currently mounted

[02/04/94 public]
Closed.



CR Number                     : 9770
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : dump/restore
Short Description             : incremental restore from jfs to lfs fails
Reported Date                 : 1/18/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/18/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/file/xvnode/RIOS/xvfs_vfs2aix.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[1/18/94 public]
In completing the admin checklist, an incremental dump and restore
of a jfs fileset to an lfs fileset appears to fail since updates
made between the full and incremental dumps do NOT appear in the
restored fileset. This is true regardless of whether I'm restoring
to lfs on an hp server or a rios server.
 
steps:
 - fts dump -fileset soldier.home -time 0 -file /tmp/home.dump.full
 - fts restore -fileset new.home -server dce11 -aggr dce11_aggr
(where new.home doesn't yet exist on dce11)
 
 - edit file in soldier.home fileset
 - fts dump -fileset soldier.home -time "mm/dd/yy hh:hh" -file /tmp/home.dump.inc
(where timestamp is between original dump and file edit)
 - fts restore -fileset new.home -server dce11 -aggr dce11_aggr -file /tmp/home.dump.inc
 
Then check for edits - I can't find mine ... Perhaps I'm misundertanding
the timestamp?

[1/19/94 public]
The timestamp should be of a time before the initial dump was made.  Picking
a time after the dump was made may or may not work.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[01/19/94 public]
Thanks Craig - but even that doesn't seem to work. In the scenario below,
notice that the dump is taken ~12:40, the timestamp used is 12:30 and the
original version of the file "l" is from yesterday, the new version is
created between the full and the incremental dump, in fact, the file "l"
is the only change between the full and the incremental dump. The 
native filesystem is restored to a lfs fileset that is not accessed until
AFTER both the full and incremental have been applied. All steps were done
sequentially in the order listed below.

1) ls -l /home/gmd/l
-rw-r--r--   1 root     system       201 Jan 18 18:02 /home/gmd/l

2) fts dump -fileset soldier.home -time 0 -file /tmp/home.full_dump
ls -l /tmp/home.full_dump
-rw-r--r--   1 root     system    365159 Jan 19 12:42 /tmp/home.full_dump

3) vi /home/gmd/l
4) ls -l /home/gmd/l
-rw-r--r--   1 root     system       146 Jan 19 12:43 /home/gmd/l

5) fts dump -fileset soldier.home -time "1/19/94 12:30" -file /tmp/home.inc_dump
6) ls -l /tmp/home.inc_dump
-rw-r--r--   1 root     system     10972 Jan 19 12:45 /tmp/home.inc_dump

7) fts restore -ftname home.restore -server dce11 -aggr dce11_aggr -file /tmp/home.full_dump
Fileset home.restore does not exist, creating it in dce11_aggr of dce11.osf.org
        readWrite   ID 0,,52  valid
        readOnly    ID 0,,53  invalid
        backup      ID 0,,54  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
dce11.osf.org       RW       dce11_aggr 0:00:00 hosts/dce11    <nil>            
Fileset 0,,52 created on aggregate dce11_aggr of dce11
Restored fileset home.restore on dce11 dce11_aggr from /tmp/home.full_dump

8) fts restore -ftname home.restore -server dce11 -aggr dce11_aggr -file /tmp/home.inc_dump -overwrite
fts restore: The fileset home.restore (0,,52) already exists in the FLDB.
fts restore: Overwriting the existing entry...
Restored fileset home.restore on dce11 dce11_aggr from /tmp/home.inc_dump

9) fts crmount /:/root_dfs/home_restore -fileset home.restore
10) ls -l /:/root_dfs/home_restore/gmd/l
-rw-r--r--   1 root     system       201 Jan 18 18:02 /:/root_dfs/home_restore/gmd/l

[1/25/94 public]
Fixed.

[2/1/94]
Verified fix by doing the steps outlined above.  Closing this CR.



CR Number                     : 9763
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs.lock
Short Description             : intermittent dfs.lock failures
Reported Date                 : 1/18/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/18/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/test/systest/file/filewnr.c, src/test/systest/file/dfs.lock
Sensitivity                   : public

[1/18/94 public]
This is a placeholder to track dfs.lock failures.  We've seen a variety
including the wrong client obtaining the lock, and clients taking too
long to read the data file.  Additionally, the stdout and stderr streams
get jumbled in the log files making debug difficult.

[1/26/94 public]
Fixed.  Beefed error reporting, re-directed stdout, stderr of filewnr
to local file rather than piping back through the remsh since that
jumbled output and caused bugs.

[02/04/94 public]
Closed.



CR Number                     : 9753
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : hpux executes wrong executable
Reported Date                 : 1/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/17/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_subr.c
Sensitivity                   : public

[1/17/94 public]

Excuting a modified executable but getting the old result.

This bug happened when executing an executable on a dfs client
directory on HP.

Re-producible scenarios:

	create a file test.c in /:/dfstest on "toaster", dfs client
	
		main(){
			
			printf("This is test1\n");
		}
	cc test.c -o test in the same dir

	execute ./test in  /:/dfstest on "toaster"

	execute ./test in /:/dfstest on "dce10", dfs server+client

	modify test.c in  /:/dfstest on "toaster"

		main(){
			/* change test1 to test2 */
			printf("This is test2\n");
		}

	cc test.c -o test in the same dir

	execute ./test in  /:/dfstest on "toaster"
	--- you may get the old result. But, if "cp 
	test /tmp/test" and execute /tmp/test, you
	will get the expected result, "This is test2".

	execute ./test in /:/dfstest on "dce10", dfs server+client
        --- you may get the old result. But, if "cp
        test /tmp/test" and execute /tmp/test, you
        will get the expected result.

	Note : If execute test in /u1/dfstest you'll get the expected
	result, "This is test2".

Config

	dce1.0.3a

	dce10/HP	dfs server + dfs client
			/u1 exported UFS

	toaster/HP	dfs client

[1/18/94 public]
I've reproduced this problem using Jean's instructions.  After 
compiling the new version of "test", I checksummed the test binary
on both machines and got the identical (new) checksum value
in spite of the fact that the client continued to execute
the old binary.  This would lead me to believe that the old binary
is resident in memory on the client and the VM system isn't
recognizing (or isn't being told) that the data on disk that 
is backing those pages has changed.  Eventually the client rights
itself and starts executing the updated binary.

Jean, were you truly able to determine this problem is HP
specific?  I thought we couldn't compile things in DFS
since the RIOS uses memory mapping.

[01/20/94 public]

I have verified this bug ... it is HPUX specific.

The fix, which I have also verified, is to call xrele() from
cm_FlushText() in cm/cm_subr.c .  Spefically, near the bottom
while still under the not-AFS_FTXT ifdef, just add:

#ifndef AFS_OSF11_ENV
  ...
#else
  ...
#endif  /* AFS_OSF11_ENV */

#endif  /* AFS_OSF_ENV */

#ifdef AFS_HPUX_ENV                  <=== ADDED
    xrele(scp);                      <=== ADDED
#endif                               <=== ADDED

#endif  /* AFS_FTXT */
    lock_ReleaseWrite(&cm_ftxtlock);
}

I'll work on getting this down to OSF asap.

[1/21/94 public]

The suggested fix worked. So, Darl, go ahead to submit the fix.

[1/22/94 public]
I've just submitted the fix I detailed above down to the 1.0.3a tree
at OSF.

[02/04/94 public]
Closed.



CR Number                     : 9734
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : low/test14
Short Description             : infinite loop in test14
Reported Date                 : 1/13/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/13/94
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : low/prog14.c
Sensitivity                   : public

[1/13/94 public]


Test14 runs forever after it trips over a fileset quota overrun
even aftet the quota problem has been fixed.  I have tried
cm flush, cm check to try to clear out the state of things 
but I am continually getting error 88 back from cm_write.

cm_write vp 5ec900c, flag 0x2
time 194.225610, pid 21596: checkerror returning code 88
time 194.225893, pid 21596: cm_read vp 5e95bf8, nolock 0
time 194.226017, pid 21596: gettokens vp 5e95bf8, rights.low 0x400
time 194.226192, pid 21596: checkerror returning code 0
time 194.226549, pid 21596: cm_write vp 5ec900c, flag 0x2
time 194.226668, pid 21596: checkerror returning code 88
time 194.226954, pid 21596: cm_read vp 5e95bf8, nolock 0
time 194.227067, pid 21596: gettokens vp 5e95bf8, rights.low 0x400
time 194.227245, pid 21596: checkerror returning code 0
time 194.227602, pid 21596: cm_write vp 5ec900c, flag 0x2
time 194.227721, pid 21596: checkerror returning code 88

The output from fts aggrinfo and fts lsheader show that
there is lots of room on this fileset and the aggregate.
As far as I know test14 creates holey files, which don't
actually consume large amunts of disk space. 

explorer.epi1 0,,23594 RW LFS     states 0x10010005 On-line
    explorer.osf.org, aggregate epi1 (ID 1)
    Parent 0,,0       Clone 0,,23595   Backup 0,,23596
    llBack 0,,0       llFwd 0,,0       Version 0,,3820663
    4294967232 K alloc limit;       6498 K alloc usage
         70000 K quota limit;       6498 K quota usage
    Creation Tue Jan 11 09:31:28 1994
    Last Update Thu Jan 13 12:45:23 1994

LFS aggregate epi1 (/dev/dsk/c201d4s0): 299766 K free out of total 321272 (3240 reserved)

[1/13/94 public]

It seems we are infintely looping in the read_it function in test14.
The only way I can see this happening is if the read system call
returns 0 for the number of bytes read.  The program never checks
for this and assumes that this case will never happen.

[1/17/93 public]

Well after all this, it turns out there is a coding error in test14
which caused the infinite loop.  I think it just happened that
this occurred that the same time we got quota errors and they
don't really have anything to do with each other.

The following line in read_it could use some extra paranthesis
to give the assignment statement precedence over the relational operators:

 if (rf = read(fd,buf,(RWA3)BUFSIZE) < 0) errex("read(1)");

[1/17/93 public]


fix submitted

[02/04/94 public]
Closed.



CR Number                     : 9724
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/systest/file
Short Description             : dfs.sec.cross_bind_checklist needs corrections
Reported Date                 : 1/12/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/12/94
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/file/dfs.sec.cross_bind_checklist
Sensitivity                   : public

[1/12/94 public]
The following fixes or changes are needed for correctness or clarity:
- change the cell names to be generic
- clarify wording in Section B about root owning the fileset
- set the umask to 077 at the beginning of the checklist
- put in comment in section 5 about being someone other than root

[1/26/94 public]
Made the above fixes and changes.  The wording about root owning the
fileset was actually in Section D not B.
Filled in Section G -- Check ACL Inheritance.

[02/04/94 public]
Closed.



CR Number                     : 9715
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : admin_ref, admin_gd
Short Description             : BOSSVR_Exec API doesn't exist
Reported Date                 : 1/11/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/11/94
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : supp_docs/redistrib/dfs_app_gdref/man3dfs/BOSSVR_Exec.3dfs
Sensitivity                   : public
Transarc Status               : closed
Transarc Deltas               : 
Transarc Herder               : 

[1/11/94 public]
Though BOSSVR_Exec is documented in a man page, it does not exist in the
code.  It should either cease to be documented, or implemented.
Changed Interest List CC from `arh@ch.hp.com' to `arh@ch.hp.com, 
 jeff@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[1/11/94 public]
This is a documentation defect.  Documentation of the BOSSVR_Exec API should
have been removed from the DFS API docs one or two releases ago.
Changed CR in Code, Doc, or Test? from `code' to `doc' 
Changed Subcomponent Name from `bosserver' to `admin_ref, admin_gd' 
Changed Interest List CC from `arh@ch.hp.com, jeff@transarc.com' to 
 `arh@ch.hp.com' 
Changed Severity from `C' to `B' 
Changed Priority from `3' to `2' 
Changed Responsible Engr. from `rsarbo' to `jeff@transarc.com' 
Changed Resp. Engr's Company from `osf' to `tarc' 
Filled in Transarc Status with `open'

[6/1/94 public]
This defect was pretty trivial to fix.  It appears that the BOSSVR_Exec()
reference page was not defuncted when all references to the function were
removed from the DFS API documentation.  The only work required for this
defect was to defunct the BOSSVR_Exec.3dfs reference page.  That being done,
this defect can now be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Filled in Affected File with 
 `supp_docs/redistrib/dfs_app_gdref/man3dfs/BOSSVR_Exec.3dfs' 
Changed Transarc Status from `open' to `closed'

[09/19/94 public]
Closed bug.



CR Number                     : 9712
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ncsubik
Short Description             : recovery.c does not compile when ubik_concurrent_rpc is turned off
Reported Date                 : 1/11/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/11/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : recovery.c
Sensitivity                   : public

[1/11/94 public]

recovery.c does not compile if "ubik_concurrent_rpc" is turned
off.  I am guessing that the non-tpq code is still supposed to
work since it is ifdef'd that way.   The problem is the use
of the variable inAddr in the "#else" portion of the 
"#ifdef ubik_concurrent_rpc"

[02/04/94 public]
Closed.



CR Number                     : 9711
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : reference TROOT after reading datafile
Reported Date                 : 1/10/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/10/94
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : dfs.read_write_all.main
Sensitivity                   : public

[1/10/94 public]
Small gotcha - apparently I've always had TROOT defined in my environment
but it's actually defined in the datafile and the datafile should be
read first - then TROOT referenced to find check_RC.ksh ie.

move sourcing of $TROOT/check_RC.ksh AFTER
read_datafile_export_variables

OTHERWISE test exits complaining that it can't find /check_RC.ksh.

[01/17/94 public]
Tested,submitted, built - closing.



CR Number                     : 9706
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 9130
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/file
Short Description             : organize functional tests
Reported Date                 : 1/10/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/10/94
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : epsiode/rcx/*, episode/scavenge/*, episode/ravage/*, episode/Makefile, test/file/Makefile
Sensitivity                   : public

[1/10/94 public]
Inadvertently, 9130 was never completely addressed. Here's what appears
to remain:

	- the rcx tests are in the code rather than the functional test tree
	(perhaps the directory can be renamed from rcx to recovery to make
	their purpose more obvious)

	- the salvage tests (and tools?) are also in the code rather than
	the functional test tree. the ravage directory also contains tools
	for these tests and should therefore be moved as well
	(perhaps ravage can be made a subdirectory of salvage to make the
	relation more clear)

	- the test_anode tests are also in the code rather than the functional
	test tree. if these are considered "unit" rather than "functional",
	perhaps this is the correct location?
	the same question holds for the following "test" files in the code tree:
		token_test/tktest_tkcsetHost.c
		security/dacl/testacl.c
		episode/vnops/test_vnodeops.c
		episode/logbuf/defer-io.3.test
		dfsbind/dfsbind_test.c
		bakutil/budb_test.c
		episode/dir/dtest.c
		episode/dir/test_dir.c
		tpq/tpq_pardo_test.c
		xcred/xcred_ktest.c

[09/27/94 public]
Closed.



CR Number                     : 9688
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/acl
Short Description             : change acl tests to run sequentially
Reported Date                 : 1/6/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/6/94
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/file/acl/driver.sh
Sensitivity                   : public

[1/6/94 public]


The default mode of operation for the ACL tests is to
kick off all six tests simultaneously.  This has the
effect of placing way too much stress on a system,
especially when combined with other tests such as DFS CHO.

We'd like the default behaviour for the tests to be to
run them sequentially.  (The ideal solution would be
to have an option to crank up the load on these tests,
but we are not going to do that for 1.0.3a.).  Anyway,
what's the hurry?  Running all 6 at once bogs down the
system so much that it probably does not complete much
faster than running them 1 at a time.

[1/6/94 public]

fix submitted.

[02/04/94 public]
Closed.



CR Number                     : 9684
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs.lock
Short Description             : filewnr has tight read loop
Reported Date                 : 1/5/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/5/94
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/test/systest/file/filewnr.c
Sensitivity                   : public

[1/5/94 public]
Using 1.0.3ab2 (latest weekly build) in the large cell (Config G),
10 DFS machines (3 flservers, 2 file servers, 5 clients) and
dfs.lock test with 7 clients to an episode fileset on a rios 
file server, I only got through 1 iteration before the fxd
seemed to hang up. 

Evidence:
none of the client machines can do a "ls /:/dfs_1" (all hang on this command -
can be interrupted with ctl-c)
when 1 or more clients attempt the above ls, top.aix on the file server
shows:

load averages:   0.00,  0.00,  0.00               10:16:01
89 processes:  1 runable, 1 swapable
Cpu states:  4.2% user, 85.3% kernel,  9.9% idle,  0.4% wait
2emory: 3 MB Free , 32 MB Total, 65 MB Swap Free, 104 MB Swap Total

  PID USERNAME PRI NICE   SIZE   RES STATE   TIME   %CPU COMMAND
17944 root     120   20    48K   24K run    19:20 79.44% thr 
  514 root     127   41    24K    8K run  1137:28 11.13% wait
17687 root      61   20    32K   16K sleep   3:27  1.63% LSN
12201 root      60   20  1520K  576K sleep   1:42  1.30% rpcd
12723 root      61   20  1536K  468K sleep   6:46  1.20% cdsadv
18201 root      60   20    48K   24K sleep  20:14  1.06% thr"
 5692 root      60   20   760K  120K sleep   0:07  0.00% snmpd
 3115 root      60   20   264K   84K sleep   0:01  0.00% syslogd
 3567 root      60   20   440K   72K sleep   0:00  0.00% errdemon
 
ie. lots of kernel!

a dfstrace dump during an ls ends with:
time 900.867200, pid 17944: vnm_WaitForBusyVnode: evp 0x5dc800c, lock 0x5c8cf10
time 900.867996, pid 17944: vnm_VnodeBusy: evp 0x5de4e0c, flags 0
time 900.950882, pid 17944: vnm_VnodeNotBusy: evp 0x5dfac0c, flags 0x4
time 900.954394, pid 17944: vnm_VnodeBusy: evp 0x5dfac0c, flags 0
time 900.954518, pid 17944: vnm_VnodeNotBusy: evp 0x5dfac0c, flags 0x4
time 900.954610, pid 17944: vnm_VnodeBusy: evp 0x5dc800c, flags 0x2
time 900.954692, pid 17944: vnm_VnodeNotBusy: evp 0x5dc800c, flags 0x6

note that the pid in the dfstrace dump matches the busy thread pid
in the top.aix output.

NOTE: I must take down this cell in the next hour to create smaller
cells for parallel hand testing! Please advise on further debug
options asap - Thanks!

[1/5/94 public]
OK - the only further debug options are if the master-slave debug setup
has been done beforehand so stack traces of the fxd threads can be
taken with the kdbx or llbd AIX kernel debuggers. Unfortunately,
this master-slave debug setup was NOT done prior to testing - largely
because we have never had much luck with it here at OSF.

SO ... to reproduce this problem at Transarc where the master-slave
debug setup has been done (and used successfully), do the following:

1) setup an episode fileset on a rios file server (not nec. an fldb server)
with a quota large enough to handle test's file writes (2 or 3 X default of
5000)
2) allow access to that fileset by a DCE principal and list that principal
(and assoc'd password) in the lock.data datafile.
3) further edit the lock.data datafile such that:
CLI_MACHINES="<names of all your dfs machines in cell>"
NCLIENTS=7
HOURS=8 (or greater)
NUMFILEWRITES=1000
TROOT=/dcetest/dcelocal/test/systest/file (must contain filewnr executable)
STDEXC=/dcetest/dcelocal/test/systest/tools (must contain conf_util.sh)
DFS_PATH="<mount point to episode fileset referenced in step 1)>"
SYNCH_DIR=$DFS_PATH
LOG_DIR=$DFS_PATH
READY_WAIT=5
LOCK_WAIT=10
EXIT_WAIT=15
SLEEPRANGE=10
4) login as the DCE principal referenced in step 2 (and the lock.data
file), cd to $TROOT and type:
	./dfs.lock -f ./lock.data > lock.log 2>&1 
Note that you can start dfs.lock from any machine that has rsh/remsh
access to all the other machines listed in CLI_MACHINES.

You'll want rev 1.1.8.2 of dfs.lock, 1.1.5.4 of lock.data - currently
in the 1.0.3 tree (ode error - should be in 1.0.3a). These newer versions
do not change the test but do fix small errors (like not defining STDEXC)
that made it more difficult to run.

In the meantime, I'll verify that I can reproduce this problem in a
smaller cell with the above settings as well.

Please let me know if you have any questions.

[1/6/94 public]
Using the above steps, I believe I have reproduced the problem in the smaller
(CHO matching) configuration A of system test. HOURS_TO_RUN=8 is NOT big
enough however - it took 14 hours or so for the problem to appear AND the
symptoms are not identical (yet anyway). 

Evidence:
COMPLETED dfs.lock ITERATION 7 AT Thu Jan  6 07:04:37 EST 1994
COMPLETED dfs.lock ITERATION 8 AT Thu Jan  6 07:38:53 EST 1994
COMPLETED dfs.lock ITERATION 9 AT Thu Jan  6 11:18:43 EST 1994

ie. Iteration 8 took 34 minutes, Iteration 9 took 3 hours, 50 minutes.

After letting it sit for 2.5 hours, I was still able to ls the episode
fileset from the different clients - HOWEVER, top.aix reports that again,
the system is VERY busy in kernel code and the dfstrace dump contains
the same vnm type messages.

Using crash on the running system, I was unable to get stack traces of
the busiest threads (as reported by top.aix). The trace of the running
proc/thread (taken twice ~10 minutes apart):
	.uiomove
	.mmrw
	.rdevread (2nd time started here)
	.cdev_rdwr
	.spec_rdwr
	.vno_rw
	.rwuio
	.rdwr
	.kreadv

fxd traces looked normal as did dfsd. epidaemon traces were:
	.e_wait
	.e_sleep
	[dfscore.ext:osi_aix_Sleep]
	[dfscore.ext:Lock_Obtain]
	[dfslfs.ext:epia_Async]
	[dfslfs.ext:afscall_episode]
and
	.e_wait
	.e_sleep
	.e_sleepl
	.sleepx
	[dfslfs.ext:get_bioreq]
	[dfslfs.ext:efs_BioDaemon]
	[dfslfs.ext:afscall_episode]

[1/6/94 public]
More observations (of problem reproduced in smaller cell) - apparently this
instance of the problem is not fatal or irrecoverable - by letting the test
continue crawling along, I observed that kernel CPU usage and iteration
times returned to "normal" again. I've listed the interval times here in
the hopes that a pattern may appear:
BEGINNING dfs.lock ITERATION 0 AT Wed Jan  5 17:08:34 EST 1994
COMPLETED dfs.lock ITERATION 0 AT Wed Jan  5 19:03:44 EST 1994	2 hours

BEGINNING dfs.lock ITERATION 1 AT Wed Jan  5 19:03:44 EST 1994
COMPLETED dfs.lock ITERATION 1 AT Wed Jan  5 19:14:20 EST 1994	11 minutes!

BEGINNING dfs.lock ITERATION 2 AT Wed Jan  5 19:14:20 EST 1994
COMPLETED dfs.lock ITERATION 2 AT Wed Jan  5 20:49:09 EST 1994	1 hour 35 min

BEGINNING dfs.lock ITERATION 3 AT Wed Jan  5 20:49:09 EST 1994
COMPLETED dfs.lock ITERATION 3 AT Thu Jan  6 00:58:00 EST 1994	4 hours 9 min!

BEGINNING dfs.lock ITERATION 4 AT Thu Jan  6 00:58:00 EST 1994
COMPLETED dfs.lock ITERATION 4 AT Thu Jan  6 02:43:06 EST 1994	2 hours 45 min

BEGINNING dfs.lock ITERATION 5 AT Thu Jan  6 02:43:07 EST 1994
COMPLETED dfs.lock ITERATION 5 AT Thu Jan  6 05:58:34 EST 1994	3 hours 5 minutes

BEGINNING dfs.lock ITERATION 6 AT Thu Jan  6 05:58:34 EST 1994
COMPLETED dfs.lock ITERATION 6 AT Thu Jan  6 06:12:10 EST 1994	14 minutes

BEGINNING dfs.lock ITERATION 7 AT Thu Jan  6 06:12:10 EST 1994
COMPLETED dfs.lock ITERATION 7 AT Thu Jan  6 07:04:37 EST 1994	52 minutes

BEGINNING dfs.lock ITERATION 8 AT Thu Jan  6 07:04:37 EST 1994
COMPLETED dfs.lock ITERATION 8 AT Thu Jan  6 07:38:53 EST 1994	34 minutes

BEGINNING dfs.lock ITERATION 9 AT Thu Jan  6 07:38:54 EST 1994
COMPLETED dfs.lock ITERATION 9 AT Thu Jan  6 11:18:43 EST 1994	3 hours 40 min

BEGINNING dfs.lock ITERATION 10 AT Thu Jan  6 11:18:43 EST 1994
COMPLETED dfs.lock ITERATION 10 AT Thu Jan  6 13:49:58 EST 1994	2 hours 31 min

BEGINNING dfs.lock ITERATION 11 AT Thu Jan  6 13:49:58 EST 1994
COMPLETED dfs.lock ITERATION 11 AT Thu Jan  6 15:08:18 EST 1994	1 hour 19 min

BEGINNING dfs.lock ITERATION 12 AT Thu Jan  6 15:08:18 EST 1994
COMPLETED dfs.lock ITERATION 12 AT Thu Jan  6 15:23:42 EST 1994	15 minutes

BEGINNING dfs.lock ITERATION 13 AT Thu Jan  6 15:23:42 EST 1994
(crawling again)
machine taken down at 16:41 - ITERATION 13 had not yet completed 1 hour 20 min +

[01/07/94 public]
Adding Carl to the interest list - any insight you can add here Carl
would be greatly appreciated.

[01/09/94 public]
The demail1 phone line is down - switching to other email address for Carl - thanks
for the info Dawn.

[01/09/94 public]
Neither email address works for Carl but he was very helpful over the phone, plan
of attack is:
	- run dfs.lock in an exported jfs filesystem long enough to
	notice/rule out similar performance swings
	(Not that dfs.glue run on an exported jfs filesystem did NOT
	exhibit performance swings. dfs.glue uses same test program w/o 
	file locking steps)

	- turn off all icl tracing and rerun as above to see if
	problem still appears (dfstrace setset -dormant) - Diane had
	also recommended this - I have just not attempted to reproduce
	a 3rd time since system test schedule being impacted greatly.

	- report above results to Carl - we can possibly use tprof to
	determine who's most active in kernel and where (what procs and
	functions most active).

None of these steps are yet scheduled. Plan is to continue with system testing
for additional exposure - especially while CHO is not yet passing (ie. equally
troubling problems known to be in current code base).

[01/12/94 public]
Step 1 taken - dfs.lock on an exported jfs filesystem. Iteration times were as follows:
BEGINNING dfs.lock ITERATION 0 AT Tue Jan 11 19:38:18 EST 1994
COMPLETED dfs.lock ITERATION 0 AT Tue Jan 11 21:35:18 EST 1994	1 hr 57 min

BEGINNING dfs.lock ITERATION 1 AT Tue Jan 11 21:35:18 EST 1994
COMPLETED dfs.lock ITERATION 1 AT Wed Jan 12 01:29:41 EST 1994	3 hr 54 min !

BEGINNING dfs.lock ITERATION 2 AT Wed Jan 12 01:29:41 EST 1994
COMPLETED dfs.lock ITERATION 2 AT Wed Jan 12 01:40:30 EST 1994	11 minutes!

BEGINNING dfs.lock ITERATION 3 AT Wed Jan 12 01:40:30 EST 1994	2 hr 1 min
COMPLETED dfs.lock ITERATION 3 AT Wed Jan 12 03:41:17 EST 1994

BEGINNING dfs.lock ITERATION 4 AT Wed Jan 12 03:41:17 EST 1994
COMPLETED dfs.lock ITERATION 4 AT Wed Jan 12 05:05:39 EST 1994	1 hr 24 min

BEGINNING dfs.lock ITERATION 5 AT Wed Jan 12 05:05:39 EST 1994
ITERATION 5: FAILED
COMPLETED dfs.lock ITERATION 5 AT Wed Jan 12 09:13:39 EST 1994 	4 hr 8 min !

BEGINNING dfs.lock ITERATION 6 AT Wed Jan 12 09:13:39 EST 1994
COMPLETED dfs.lock ITERATION 6 AT Wed Jan 12 09:46:12 EST 1994	33 min

SO ... a similar range of ITERATION completion times exists with both episode
and native filesystems. Note that a failure does take a LITTLE longer to detect
in the test (test waits full WAIT period for message - largest WAIT period set
for this test was 15 minutes) but does not account for the huge time difference.

[1/13/93 public]

I've also seen 1 occurence of this in the CHO cell with test 14.
The configuration was a RIOS client to an HPUX server (episode).
It appears that the prog14 process is getting cpu time and
moving along but it has now taken more than 3 hours to run this
test, which normally requires less than 10 minutes.

[1/13/94 public]
The filewnr program in the dfs.lock test contains a read loop checking 
the contents of a file in DFS for messages used to synchronize with 
other test clients.  The read loop contains no sleeps so if the 
message it is looking for has not yet been written by the dfs.lock 
program, it immediately attempts another read.  Needless to say,
this is not a recommended programming practice.  In large cells,
when you have many (in this case 7) clients all performing this
tight read loop on the file, the process attempting to do the write
can hang for long periods of time (we've seen up to ~4 hours).  
Presumably, the cm is attempting to fetch the write token on the
file, but there is so much contention that it is not possible so
it continues to retry.  I've added a sleep to the tight loop in
the filewnr program and re-run the test in a large (7 client) cell.
The completion times were:

0	10m34s
1	10m31s
2	10m30s
3	10m30s
4	13m32s
5	13m33s
6	13m23s

Apparently, there is still some small anomaly in the test 
completion times, but it is miniscule compared to the previous
completion times seen with this test (2 - 4 hours!).

[1/13/94 public]
Fixed.

[02/04/94 public]
Closed.



CR Number                     : 9660
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : ufsops
Short Description             : volume ops (dump, restore)
fail horribly on HP-UX UFS filesets
Reported Date                 : 12/28/93
Found in Baseline             : 1.0.3
Found Date                    : 12/28/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ufsops/ufs_volops.c
					     file/ufsops/HPUX/ufs_mach.h
					     file/ufsops/HPUX/hpux_volops.c
Sensitivity                   : public

[12/28/93 public]
The UFS volume ops code in src/file/ufsops/HPUX doesn't even come close to
working for HPUX.  Attempting to do a "fts dump" or "fts restore" crashes
a machine outright ... much less doing anything intelligent.

I've made the appropriate fixes to the following files here at HP and
am prepared to push them back to OSF when they decide they're ready for
them.  The changes are almost entirely to HPUX specific code and is
low-risk.

	ufsops/HPUX/hpux_volops.c
	ufsops/ufs_volops.c
	ufsops/ufs.h
	ufsops/HPUX/ufs_mach.h
	ftserver/ftserver_vprocs.c

[12/29/93 public]
We agreed these would be nice to get into 103a - especially since
the admin_checklist system test includes fts dump and restore. The
103a tree is open and drb requests are being approved for "fixby 103a"
fixes - you may want to doublecheck with the RATs here about what's
required to switch from "103" to "103a" before attemting to submit.

[01/17/94 public]
Problem reproduced as part of the admin checklist ie. machine automatically
rebooted when the following command was attempted:

fts dump -fileset dce11.u2 -time 0 -file /tmp/dce11.u2.dump

where dce11.u2 is a dfsexported native filesystem.
(command executed on dce11, dce11 bounced)

[1/18/94 public]
We have the fix from HP, thanks Daryl - mbs will be integrating.

[1/31/94 public]

Integrated and tested the fix -- many changes to volops implementation,
especially in ufsops/HPUX/hpux_volops.c which was replaced in its
entirety.

[2/1/94]
Verified fix using fts dump commands.  Closing this CR.



CR Number                     : 9651
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa,rs6000
S/W Ref Platform              : hpux,aix
Component Name                : dfs
Subcomponent Name             : test/file/*/Makefile
Short Description             : can't find <platform>/machdep.mk
Reported Date                 : 12/23/93
Found in Baseline             : 1.1
Found Date                    : 12/23/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b1
Affected File(s)              : test/file/*/Makefile
Sensitivity                   : public

[12/23/93 public]

All test Makefiles for DFS complain that they can't find the common
Makefile "<platform>/machdep.mk".  A sample from the HP tests build log:

[ /test/file/util at 02:02 (AM) Thursday ]

makepath util/. && cd util &&  exec make MAKEFILE_PASS=BASIC install_all
"/project/dce/build/dce1.1-snap/src/test/file/util/Makefile", line 143: \
Could not find ../../..//file/HP800/machdep.mk
Fatal errors encountered -- cannot continue
*** Error code 1

These Makefiles define the path to "machdep.mk" as:

	${MAKETOP}/file/${TARGET_MACHINE}/machdep.mk

For example, the HP "machdep.mk" source is in:

	src/file/HP800/machdep.mk

[ rrizzo 12/23/93 public ]

Corrected mistakes in the short and full descriptions of the problem.

[12/28/93]

We changed the .include syntax for machdep.mk's which are specified
using MAKETOP to use "" rather than <>'s

Fix submitted.

[09/26/94 public]
Closed.



CR Number                     : 9650
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 9284
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Allow different-sized partitions in rcx tests in HP
Reported Date                 : 12/23/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/rcx/copyaggr.c
Sensitivity                   : public
Transarc Deltas               : rajesh-db4799-rcx-copyaggr-optional-checksum
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/23/93 public]
Due to the restriction that there can be only one partition per disk, and
the problematic nature of finding identically-sized disks to mount on one
machine for the RCX tests, we have relaxed the requirement that all three
partitions be the same size.  There are two new environment variables:
  COPYAGGR_NOCHECKSUM specifies that copyaggr won't do checksumming.  This
    is needed because the checksum algorithm looks at the whole partition,
    not just what's in use by the aggregate.
  RCX_SRCAGGR_NEWAGGR_SPEC specifies that the compare-aggr phase should
    look at only as many blocks in each destination aggregate as there were
    in the source aggregate.
It's still necessary to insure that the source partition is the smallest of
the three.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/29/93 public]
Changed component to test, defer to 1.1.  This is a candidate for
the 1.0.3a unintegrated tree.

[1/5/94 public]
Defect Closure Form
-------------------
--Verification procedure below--
The verification procedure described for OT CR 9284 was used.  The three
disks (partitions) used were of different sizes.
Associated information:
Tested on TA build:  
dfs-osf-1.12
Tested with backing build:  
dce1.0.3ab1
Filled in Transarc Deltas with `rajesh-db4799-rcx-copyaggr-optional-checksum' 
Changed Transarc Status from `open' to `export'

[1/19/94 public]
Changed Status from `defer' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Filled in Affected File with `file/episode/rcx/copyaggr.c' 
Changed Transarc Status from `export' to `submit'

[02/04/94 public]
Closed.



CR Number                     : 9645
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : inaccurate output from fts aggrinfo command
Reported Date                 : 12/22/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/22/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/file/ufsops/ufs_agops.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : open

[12/22/93 public]
The amount of free space of a HP native filesystem reported from an 
fts aggrinfo command does not agree with the information from a bdf 
command.  On the RS-6000 the amount of free space listed from both 
commands is the same.
On a HP:
% fts aggrinfo -aggr /u0 -server largo
Non-EFS aggregate /u0 (/dev/dsk/c201d5s0): 4278610001 K free out of total
4278870971 (31987 reserved)
% bdf /u0  
Filesystem           kbytes    used   avail capacity Mounted on
/dev/dsk/c201d5s0    319866  260970   26909    91%   /u0
On a RS-6000:
% fts aggrinfo -aggr /u1 -server soldier
Non-EFS aggregate /u1 (/dev/lv00): 160812 K free out of total 204800
% df /u1
Filesystem     Total KB     free %used   iused %iused Mounted on
/dev/lv00        204800   160812   21%    1959     3% /u1

[12/28/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[12/29/93 public]
Assigning to Kinney at HP since this is HP specific. This fix is a candidate
for the unintegrated tree of source code that will be shipped with 103a if
it comes in time.

[1/5/94 public]
Changing the priority to 1 because dfs.maxfile fails due to getting and
trying to use the inaccurate information.

[1/10/94 public]
Fix submitted on 1/7.

[1/25/94]
Verified fix by successfully running dfs.maxfile and using the command.



CR Number                     : 9633
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 9674
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Some minor problems with fts test21
Reported Date                 : 12/20/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/file/fts/runtests
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot9633-fts-test21-problems
Transarc Herder               : jaffe@transarc.com

[12/20/93 public]
Some minor problems with fts/test21 (and fts/runtests):
- definition of "tests" in fts/runtests is not used.
- fts/test21 should be trying to use two different servers, and if that fails,
  use the same server, rather than the other way around; as it is, when 
  SERVER1 and SERVER2 are identical, fts/runtests sets AGGRNAME_3 to "invalid"
  and fts/test21 issues a warning message about how it's finding aggregates
  on different servers, when it really isn't.
- In fts/test21 there are two forked processes that are supposed to test
  whether the two aggregates are still accessible after the failed fts moves.
  Instead of one starting by cd'ing to $mp1 and the other by cd'ing to $mp2,
  they both start by cd'ing to $mp1, so they only test one of the aggregates.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/6/94 public]
Inadvertently, I included the fix for OT CR 9674 in the delta that fixes
this bug (OT CR 9633).
Defect Closure Form
-------------------
--Verification procedure below--
Run fts test21 using two aggregates on the same server, and look at the log.
Suppose the server's name is "foo"; you used to see an informational message
like: "Using two aggregates on *different* servers."  Now you will see
"Using two aggregates on the same server."
.
There is no verification procedure for the other two minor problems
(superfluous definition of "tests" and incorrect choice of a mountpoint on
which to test fileset accessibility).
.
To test whether OT CR 9674 is fixed (see above), run fts test21 and monitor
its progress.  If the filler file it creates on /tmp is later deleted, and
if the filler file's size is exactly 1<<10, you win.  Also, run fts test21 in
an environment where /tmp is already full (less than 1MB free).  It should
complain and exit.
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-osf-1.12
Tested with backing build:  
dce1.0.3ab1
Filled in Inter-dependent CRs with `9674' 
Filled in Transarc Deltas with `bwl-ot9633-fts-test21-problems' 
Changed Transarc Status from `open' to `export'

[1/6/94 public]

[1/6/94 public]
Edited verification procedure (see above).

[1/19/94 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Filled in Affected File with `test/file/fts/runtests' 
Changed Transarc Status from `export' to `submit'

[02/04/94 public]
Closed.



CR Number                     : 9631
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : panic while trying to attach bad aggregate
Reported Date                 : 12/20/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/20/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_agops.c
Sensitivity                   : public
Transarc Deltas               : bwl-ot9631-check-volume-open
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/20/93 public]
While trying to get the rcx tests to run on HP/UX, we (inadvertently of course)
created an aggregate in which one of the volumes had a damaged header.
epimount'ing the volume returns EIO, but attaching the aggregate causes a
panic, because there is an MBZ after the call to epiv_GetStatus in
ag_efsVolInfo.  (How this could get an error, after the previous call to
vnv_IsAVolume did not, I haven't yet figured out.)
.
-------------------------
.
I said an MBZ, but it was actually the older form, osi_assert.
.
The code looks like this:
.
    if (!vnv_IsAVolume (aggh, index))
        return ENOENT;
.
    (void) epig_OpenVolume (aggh, index, &volh);
.
    code = epiv_GetStatus (volh, &vstat);
    osi_assert (!code);
.
The result of the epig_OpenVolume is being thrown away.  This is wrong; it
probably dates back to an earlier era when vnv_IsAVolume (or the equivalent
in-line code) called the same function.
.
The correct thing to do is to convert the result of the epig_OpenVolume to a
Unix error code and return it to the caller.  (Probably the only legitimate
code at this point would be EIO.)  Having done this, it would probably be
correct to assert on the result of the epiv_GetStatus.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/29/93 public]
Well, this may be an unpopular decision but ... I'm bumping the priority
and severity of this. I don't believe the system should panic or otherwise
become unusable if an aggregate is bad - it seems reasonable to report
that the aggregate is bad (suggest steps to repair?), abandon the attach
attempt and move on. While the case described here may be "rare", it 
sounds as if the problem is fairly reproducable so the fix can easily be
verified. With the limited functional, system and self-host testing we
do, it's hard for me to judge how "rare" bad aggregates (of this
particular format?) will be at customer sites so I'd rather the
result be less catastrophic. Please clarify if I've misunderstood
the above problem description.

[12/29/93 public]
This is an A0 if the panic can occur during normal/recommended attach
procedures (see rc.dfs). If the "attempt attach/salvage/re-attempt attach"
scenario does not panic the machine when there's a bad aggregate, then
severity and priority are lower (B 1).

[12/29/93 public]
After talking it over with others, I've been convinced that this is
not an A0. My apologies for the false alarm.

[1/6/94 public]
Defect Closure Form
-------------------
--Verification procedure below--
The testing procedure at the highest level is:
(1) Create an aggregate with at least one fileset.
(2) Damage the fileset in just the right way.
(3) Try to attach (dfsexport) the aggregate.
.
In step (2), "just the right way" in this case means modifying the fileset ID
in the AVL anode but not modifying the fileset ID in the fileset header.
To do this I made a copy of src/file/episode/scavenge/chftids.pl, which
modifies all fileset ID's in the aggregate, and commented out the line that
modifies the fileset ID's in the fileset headers.  I then ran this copy on
my aggregate:
   cd <tree>/obj/rios/file/episode/scavenge
   perl -I . ./chftids.bogus /dev/epi3
.
The damaged fileset will cause old Episodes to panic in dfsexport.  But with
the fix in place, the dfsexport succeeds.  (In fact there isn't even a warning
message about a bad fileset.  This is unfortunate.)
.
--Other explanation below--
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Transarc Deltas with `bwl-ot9631-check-volume-open' 
Changed Transarc Status from `open' to `export'

[1/6/94 public]
Bruce - it seems better that bad aggregates be recognized at attach time - the panic
was extreme but at least it would have occurred most frequently at system startup - your
fix sounds like we've just postponed the problem of a damaged aggregate to some
later, random, access. Please clarify - thanks.

[1/7/94 public]
What happens now (after the fix) is that AG_VOLINFO, called from 
ag_RegisterVolumes, returns EIO.  Note that ag_RegisterVolumes throws this
value away but does a "continue", which means that it doesn't try to register
the fileset in the fileset registry.
.
So if you try to do something like fts lsquota, or try to access the fileset
through a mountpoint, you will error codes and messages.  You can't crash the
machine this way.
.
However, it could appear mysterious.  There should be an error message of some
kind.  ag_RegisterVolumes is already printing a message to the console in
another case.  (It might be more useful to pass the error code back out to
the process and let the process print a message on stderr, but this would
require a change to the interface, since more than one fileset in a single
aggregate can fail to register.)

[1/19/94 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Filled in Affected File with `file/episode/vnops/efs_agops.c' 
Changed Transarc Status from `export' to `submit'

[02/04/94 public]
Closed.



CR Number                     : 9629
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : #include's with double quotes
					     can lead to unexpected results
					     when building in ODE environment
Reported Date                 : 12/17/93
Found in Baseline             : 1.0.3
Found Date                    : 12/17/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : 332 files are affected
Sensitivity                   : public

[12/17/93 public]

When double quotes are used to specify an include file from within 
a second include file, the file found may not be the one expected.
In more detail:

	Suppose a developer's sandbox directory contains file foo.h.

	The corresponding directory in the backing tree contains
	files foo.h and bar.h and file.c.

	Partial contents of file.c:

		#include "bar.h"

	Partial contents of bar.h:

		#include "foo.h"

	When file.c is compiled, bar.h is found in the backing tree.  When
	bar.h includes foo.h, the foo.h in the backing tree will be used.
	This is probably not the developer's intention.  This situation does
	not arise when using angle brackets instead of double quotes.

332 files are affected.  These files have a total of 1005 double quote includes.

[12/22/93 public]
defer to 1.1

[12/23/93 public]
This problem seemed familiar so I forwarded it to the 'Rat' mailing list
for comment.  Here is the response I got from Alan Hamilton:
--
This is a language tools bug.  Every 'cc' command should have:
  o A list of "-I"s to be searched for '#include ""'.
  o A "-I-".
  o A list of "-I"s to be searched for '#include <>'.
 If you don't use the "-I-", our beloved gcc (like many other C
compilers) will look for the included file first in the directory
where it found the including file -- WRONG behavior for a sandbox
environment.

I thought this had been fixed throughout DCE, but perhaps not.  Or
perhaps the default DCE "cc" rule is being overridden with incorrect
behavior.  Does the compile line have the expected "-I" options?
>From "man cc":

|     -I-  Indicates that any directories specified with -I
|          options before the -I- flag are searched only for the
|          case of #include "file"; they are not searched for
|          #include <file>.
|          If additional directories are specified with -I options
|          after the -I-, these directories are searched for all
|          #include directives. (Ordinarily all -I directories
|          are used this way.)
|          In addition, the -I- flag inhibits the use of the
|          current directory as the first search directory for
|          #include "file".  Therefore, the current directory is
|          searched only if it is requested explicitly with -I..
|          Specifying both -I- and -I. allows you to control pre-
|          cisely which directories are searched before the
|          current one and which are searched after.
---

To add to Alan's comments:  after a casual inspection of some DFS makefiles
it seems the the appropriate code is in file.mk:

  _ansi_GENINC_= -I- ${x:L:!${GENPATH} -I.!}
  _traditional_GENINC_= -I- ${x:L:!${GENPATH} -I.!}
  _writable_strings_GENINC_= -I- ${x:L:!${GENPATH} -I.!}
(You are compiling with and 'ansi' CCTYPE.)

lbe/mk/osf.std.mk then includes GENINC as part of the entire compilation
invocation. (See the valuation of _CCFLAGS_)

Unfortunately, this doesn't seem to work.  At least, in some experiments in
my sandbox I don't get an -I-.

I have seen that the current behavior matches the DCE1.0.2 behavior.  This
is as far as I'm going to take it.

[12/23/93 public]
Forgot to mention:  are you sure this happens on all platforms?  I would
think that this is a problem just with gcc.

[4/18/94 public]
Upgrading priority.  This one's been around for a while, and it's 
about to become a big pain with the larger code changes going on
soon.

[mbs 4/29/94 public] 
The simplest fix is to avoid double quotes altogether.  I changed
all the files to use the angle brackets.

[09/27/94 public]
Closed.



CR Number                     : 9628
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000,hppa
S/W Ref Platform              : hpux,aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : comment needed in
block_frag.data about minimum block size
Reported Date                 : 12/17/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/17/93
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : block_frag.data
Sensitivity                   : public

[12/17/93 public]

The minimum block size for RS6000s has changed to 4096.  A comment
needs to be added to the data file for dfs.block_frag so that the
BLOCKSIZE_RANGE is properly set.

[12/22/93 public]
Added comment to the data file about the minimum block size for RS-6000s.



CR Number                     : 9623
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : butc inconsistency with multitape restores
Reported Date                 : 12/16/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bubasics/butc_errs.et, file/butc/lwps.c
Sensitivity                   : public
Transarc Deltas               : vijay-db4775-butc-handle-inconsistency-in-multi-tape-restores
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/16/93 public]
1. The function that reads data from tape and passes it to the restorer could
   sometimes erroneously return an empty buffer.
2. The error codes from the restorer routine are sometimes meaningless, eg. -1.
   Error codes from the error table are substituted instead.
Added field Transarc Deltas with value 
 `vijay-db4775-butc-handle-inconsistency-in-multi-tape-restores' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/20/93 public]
submitted 12/20/93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Changed Transarc Status from `open' to `submit'

[02/04/94 public]
Closed.



CR Number                     : 9622
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Spurious checkForBackingUp failures panic fts move
Reported Date                 : 12/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Herder               : andi@transarc.com
Transarc Status               : export

[12/16/93 public]
**Description Text**
 From Bruce Leverett, diagnosing a panic that Dawn Stokes is seeing while
 doing a fileset move while copying new files into a fileset:
 
 The file in question is the last file in the fileset (that is, it has the
 highest index).  So the sequence of volume ops at the end of the first
 dump/restore was something like:
   VOL_SCAN
   VOL_CREATE
   VOL_COPYACL
   VOL_SETATTR
 As we know, VOL_SETATTR leaves behind a non-phantomized vnode.  So when the
 second dump/restore starts, if it is actually going to restore anything, it
 had better call vnm_StopUse first.  checkForBackingUp is supposed to handle
 this.  BUT:  VOL_COPYACL is vol_efsCopyAcl which called vol_efsScan, which
 called vol_efsSeek, which called checkForBackingUp, which reset the volume's
 "current slot" pointer to the index of the vnode whose ACL was being copied.
 And so, the "current slot" pointer is not up to date at the end of the
 first dump/restore.  The second dump/restore has exactly one file to restore,
 which is the last file, and the sequence of ops is:
   VOL_SCAN
   VOL_DELETE
   VOL_CREATE
 VOL_SCAN calls checkForBackingUp, but because the "current slot" pointer is
 wrong, this fails to call vnm_StopUse.  And so VOL_DELETE encounters a
 non-phantom vnode representing the file it is deleting.  The first to notice
 the error is efs_inactive, which panics.
**Solution Text**
 Delta: cfe-db3765-no-backing-up-in-copyacl
 Tested on dfs-carl-1.11.

[12/16/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/16/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9620
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : ftutil
Short Description             : ftu_LookupAggrByDevice() needs to stat raw dev.
Reported Date                 : 12/15/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/15/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftutil/ftu_aggrreg.c
Sensitivity                   : public
Transarc Deltas               : berman-o-ot9620-make-LookupAggrByDevice-compare-rawdevs
Transarc Status               : submit
Transarc Herder               : 

[12/15/93 public]
ftu_LookupAggrByDevice() Gets passed in a raw device name from
routine ftu_LockAggrDevice() and stats it.  It then enumerates
all aggrs and stats their aggrP->dvname which is a block device
name.  It then compares the stat structures st_rdev fields.
On HPUX raw and block devices have different st_rdevs.  The 
Lookup fails and Lock assumes the aggr is not attached, and grants
the lock erroneously.  This was found by running fset tests.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[12/15/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Run fset_test -I scripts/lock00.itl.  Assertions will fail.
--Verification procedure below--
Run fset_test -I scripts/lock00.itl.  Test succeeds
--Other explanation below--
HPUX uses different r_dev values for block and char devices.
New code only compares stat() of char devices.
Associated information:
Tested on TA build:  
dfs-osf-1.12
Tested with backing build:  
dce1.0.3bl10
Filled in Transarc Deltas with 
 `berman-o-ot9620-make-LookupAggrByDevice-compare-rawdevs' 
Changed Transarc Status from `open' to `export'

[12/16/93 public]
Priority changed to 1 since all functional tests must pass for 1.0.3a release.

[12/16/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Changed Transarc Status from `export' to `submit' 
Added field Transarc Herder with value `'

[12/17/93 public]
Closed.



CR Number                     : 9619
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : flserver
Short Description             : flserver deadlock
Reported Date                 : 12/15/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3ab3
Affected File(s)              : file/ncsubik/remote.c
Sensitivity                   : public

[12/15/93 public]


The flserver deadlocked during our weekend testing.
Unfortunately this is on the RIOS and we haven't got
any threads-aware debugging tools for AIX; therefore
there is not much information so far. So, I understand
that Transarc has seen this too, so that's why I'm
passing the problem along to them.

[12/30/93 public]

This has happened in 2 out of 3 CHO runs.  Here's the dbx
info:


Script command is started on Thu Dec 30 10:43:20 EST 1993.# r dbx
dbx -a 15863
Waiting to attach to process 15863 ...
Successfully attached to flserver.
warning: Directory containing flserver could not be determined.
Apply 'use' command to initialize source path.

dbx version 3.1 for AIX.
Type 'help' for help.
reading symbolic information ...

attached in cma__io_available at 0xd0369338 ($t2)
0xd0369338 (cma__io_available+0x3c0) 80410014          l   r2,0x14(r1)
(dbx) where
cma__io_available(0x0, 0x0, 0x200ff998) at 0xd0369338
cma___null_thread(0x0) at 0xd036619c
cma__thread_base(0x200da928) at 0xd037cf84
(dbx) suer      use /opt/dcelocal/bin
(dbx) threads 
 thread  state  substate   held  priority  yield  preempt  function
 $t1     susp   cond         no  12(thru)  0      0        cma__dispatch       
>$t2     run                 no   0(idle)  0      0        cma__io_available   
 $t3     susp   mutex        no  12(thru)  0      0        cma__dispatch       
 $t4     susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t5     susp   mutex        no  12(thru)  0      0        cma__dispatch       
 $t6     susp   mutex        no  12(thru)  0      0        cma__dispatch       
 $t7     susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t8     term                no  12(thru)  0      0        cma__dispatch       
 $t9     susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t10    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t11    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t12    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t13    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t14    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t15    susp   mutex        no  12(thru)  0      0        cma__dispatch       
 $t16    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t17    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t18    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t19    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t20    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t21    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t22    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t23    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t24    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t25    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t26    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t27    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t28    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t29    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t30    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t31    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t32    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t33    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t34    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t35    susp   cond         no  12(thru)  0      0        cma__dispatch       
 $t36    susp   cond         no  12(thru)  0      0        cma__dispatch       
(dbx) mutex
 mutex  type  lock  owner  mutex_name            num_wait  waiters
 $m1    fast    no         default attr's mutex         0  
 $m2    fast    no         attr sequence object         0  
 $m3    fast    no         known attr list              0  
 $m4    fast    no         mutex sequence objec         0  
 $m5    fast    no         known mutex list             0  
 $m6    recu   yes  $t17   global lock                  3  $t15 $t3 $t6 
 $m7    fast    no         global's internal lo         0  
 $m8    fast    no         VM, small                    0  
 $m9    fast    no         VM, medium                   0  
 $m10   fast    no         VM, large                    0  
 $m11   fast    no         VM, pool                     0  
 $m12   fast    no         per-thread context           0  
 $m13   fast    no         cond sequence object         0  
 $m14   fast    no         known cond list              0  
 $m15   fast    no         mutex for delay              0  
 $m16   fast    no         atfork queue                 0  
 $m17   fast    no         one time init                0  
 $m18   fast    no         stack mutex                  0  
 $m19   fast    no         stack sequence objec         0  
 $m20   fast    no         thread sequence obje         0  
 $m21   fast    no         for a TCB                    0  
 $m22   fast    no         for a TCB                    0  
 $m23   fast    no         null thread                  0  
 $m24   fast    no         for a TCB                    0  
 $m25   fast    no         io database                  0  
 $m26   fast    no         dynamic io init              0  
 $m27   fast    no         file mutex                   0  
 $m28   fast    no         file mutex                   0  
 $m29   fast    no         file mutex                   0  
 $m30   fast    no         sigwait                      0  
 $m31   fast    no         debugger client regi         0  
 $m32   fast    no         for a one time init          0  
 $m33   fast   yes  $t3                                 1  $t5 
 $m34   fast    no                                      0  
 $m35   fast    no                                      0  
 $m36   fast    no         for a TCB                    0  
 $m37   fast    no         for a TCB                    0  
 $m38   fast    no                                      0  
 $m39   fast    no                                      0  
 $m40   fast    no                                      0  
 $m41   fast    no         file mutex                   0  
 $m42   fast    no                                      0  
 $m43   fast    no                                      0  
 $m44   fast    no                                      0  
 $m45   fast    no                                      0  
 $m46   fast    no                                      0  
 $m47   fast    no         for a one time init          0  
 $m48   fast   yes  $t17                                0  
 $m49   fast    no         for an attr                  0  
 $m50   fast    no         for an attr                  0  
 $m51   fast    no                                      0  
 $m52   fast    no         file mutex                   0  
 $m53   fast    no         for a one time init          0  
 $m54   fast    no                                      0  
 $m55   fast    no         file mutex                   0  
 $m56   fast    no         for a one time init          0  
 $m57   fast    no                                      0  
 $m58   fast    no         for a one time init          0  
 $m59   fast    no                                      0  
 $m60   fast    no         for a one time init          0  
 $m61   fast    no                                      0  
 $m62   fast    no                                      0  
 $m63   fast    no         for a one time init          0  
 $m64   fast    no                                      0  
 $m65   fast    no         for a one time init          0  
 $m66   fast    no                                      0  
 $m67   fast    no         for a one time init          0  
 $m68   fast    no                                      0  
 $m69   fast    no         for a TCB                    0  
 $m70   fast    no         for a TCB                    0  
 $m71   fast    no         for a one time init          0  
 $m72   fast    no         file mutex                   0  
 $m73   fast    no         for a TCB                    0  
 $m74   fast    no         for a TCB                    0  
 $m76   fast    no         for a TCB                    0  
 $m77   fast    no         for a TCB                    0  
 $m78   fast    no         for a one time init          0  
 $m79   fast    no         for an attr                  0  
 $m80   fast    no         for an attr                  0  
 $m81   fast    no                                      0  
 $m82   fast    no                                      0  
 $m83   fast    no                                      0  
 $m84   fast    no                                      0  
 $m85   fast    no                                      0  
 $m86   fast    no                                      0  
 $m87   fast    no                                      0  
 $m88   fast    no                                      0  
 $m89   fast    no                                      0  
 $m90   fast    no                                      0  
 $m91   fast    no                                      0  
 $m92   fast    no                                      0  
 $m93   fast    no                                      0  
 $m94   fast    no                                      0  
 $m95   fast    no                                      0  
 $m96   fast    no                                      0  
 $m97   fast    no                                      0  
 $m98   fast    no                                      0  
 $m99   fast    no                                      0  
 $m100  fast    no                                      0  
 $m101  fast    no                                      0  
 $m102  fast    no                                      0  
 $m103  fast    no                                      0  
 $m104  fast    no                                      0  
 $m105  fast    no                                      0  
 $m106  fast    no                                      0  
 $m107  fast    no                                      0  
 $m108  fast    no                                      0  
 $m109  fast    no                                      0  
 $m110  fast    no                                      0  
 $m111  fast    no                                      0  
 $m112  fast    no                                      0  
 $m113  fast    no                                      0  
 $m114  fast    no                                      0  
 $m115  fast    no                                      0  
 $m116  fast    no                                      0  
 $m117  fast    no                                      0  
 $m118  fast    no                                      0  
 $m119  fast    no                                      0  
 $m120  fast    no                                      0  
 $m121  fast    no                                      0  
 $m122  fast    no                                      0  
 $m123  fast    no                                      0  
 $m124  fast    no                                      0  
 $m125  fast    no                                      0  
 $m126  fast    no                                      0  
 $m127  fast    no                                      0  
 $m128  fast    no                                      0  
 $m129  fast    no                                      0  
 $m130  fast    no                                      0  
 $m131  fast    no                                      0  
 $m132  fast    no                                      0  
 $m133  fast    no                                      0  
 $m134  fast    no                                      0  
 $m135  fast    no                                      0  
 $m136  fast    no                                      0  
 $m137  fast    no                                      0  
 $m138  fast    no                                      0  
 $m139  fast    no                                      0  
 $m140  fast    no                                      0  
 $m141  fast    no                                      0  
 $m142  fast    no                                      0  
 $m143  fast    no                                      0  
 $m144  fast    no                                      0  
 $m145  fast    no                                      0  
 $m146  fast    no                                      0  
 $m147  fast    no                                      0  
 $m148  fast    no                                      0  
 $m149  fast    no                                      0  
 $m150  fast    no                                      0  
 $m151  fast    no                                      0  
 $m152  fast    no                                      0  
 $m153  fast    no                                      0  
 $m154  fast    no                                      0  
 $m155  fast    no                                      0  
 $m156  fast    no                                      0  
 $m157  fast    no                                      0  
 $m158  fast    no                                      0  
 $m159  fast    no                                      0  
 $m160  fast    no                                      0  
 $m161  fast    no                                      0  
 $m162  fast    no                                      0  
 $m163  fast    no                                      0  
 $m164  fast    no                                      0  
 $m165  fast    no                                      0  
 $m166  fast    no                                      0  
 $m167  fast    no                                      0  
 $m168  fast    no                                      0  
 $m169  fast    no                                      0  
 $m170  fast    no                                      0  
 $m171  fast    no                                      0  
 $m172  fast    no                                      0  
 $m173  fast    no                                      0  
 $m174  fast    no                                      0  
 $m175  fast    no                                      0  
 $m176  fast    no                                      0  
 $m177  fast    no                                      0  
 $m178  fast    no                                      0  
 $m179  fast    no                                      0  
 $m180  fast    no                                      0  
 $m181  fast    no                                      0  
 $m182  fast    no                                      0  
 $m183  fast    no                                      0  
 $m184  fast    no                                      0  
 $m185  fast    no                                      0  
 $m186  fast    no                                      0  
 $m187  fast    no                                      0  
 $m188  fast    no                                      0  
 $m189  fast    no                                      0  
 $m190  fast    no                                      0  
 $m191  fast    no                                      0  
 $m192  fast    no                                      0  
 $m193  fast    no                                      0  
 $m194  fast    no                                      0  
 $m195  fast    no                                      0  
 $m196  fast    no                                      0  
 $m197  fast    no                                      0  
 $m198  fast    no                                      0  
 $m199  fast    no                                      0  
 $m200  fast    no                                      0  
 $m201  fast    no                                      0  
 $m202  fast    no                                      0  
 $m203  fast    no                                      0  
 $m204  fast    no                                      0  
 $m205  fast    no                                      0  
 $m206  fast    no                                      0  
 $m207  fast    no                                      0  
 $m208  fast    no                                      0  
 $m209  fast    no         file mutex                   0  
 $m210  fast    no         for a one time init          0  
 $m211  fast    no         for an attr                  0  
 $m212  fast    no                                      0  
 $m213  fast    no         for a one time init          0  
 $m214  fast    no         for an attr                  0  
 $m215  fast    no                                      0  
 $m216  fast    no         for a one time init          0  
 $m217  fast    no         file mutex                   0  
 $m218  fast    no         for a one time init          0  
 $m219  fast    no         for an attr                  0  
 $m220  fast    no                                      0  
 $m221  fast    no         for a one time init          0  
 $m222  fast    no                                      0  
 $m223  fast    no         file mutex                   0  
 $m224  fast    no                                      0  
 $m225  fast    no         for a one time init          0  
 $m226  fast    no                                      0  
 $m227  fast    no                                      0  
 $m229  fast    no         for a one time init          0  
 $m230  fast    no                                      0  
 $m231  fast    no         for a one time init          0  
 $m232  fast    no                                      0  
 $m233  fast    no                                      0  
 $m234  fast    no                                      0  
 $m236  fast    no         for a TCB                    0  
 $m237  fast    no         for a TCB                    0  
 $m238  fast    no                                      0  
 $m242  fast    no         file mutex                   0  
 $m243  fast    no         for a TCB                    0  
 $m244  fast    no         for a TCB                    0  
 $m245  fast    no         for a TCB                    0  
 $m246  fast    no         for a TCB                    0  
 $m247  fast    no         for a TCB                    0  
 $m248  fast    no         for a TCB                    0  
 $m249  fast    no         for a TCB                    0  
 $m250  fast    no         for a TCB                    0  
 $m251  fast    no                                      0  
 $m252  fast    no                                      0  
 $m253  fast    no                                      0  
 $m254  fast    no                                      0  
 $m255  fast    no         file mutex                   0  
 $m256  fast    no         for a one time init          0  
 $m257  fast    no                                      0  
 $m258  fast    no                                      0  
 $m259  fast    no         for a TCB                    0  
 $m260  fast    no         for a TCB                    0  
 $m261  fast    no         for a TCB                    0  
 $m262  fast    no         for a TCB                    0  
 $m263  fast    no         file mutex                   0  
 $m264  fast    no         file mutex                   0  
 $m265  fast    no                                      0  
 $m266  fast    no                                      0  
 $m268  fast    no                                      0  
 $m269  fast    no         for an attr                  0  
 $m270  fast    no         for a TCB                    0  
 $m271  fast    no         for a TCB                    0  
 $m272  fast    no         for a TCB                    0  
 $m273  fast    no         for a TCB                    0  
 $m276  fast    no         for a TCB                    0  
 $m277  fast    no         for a TCB                    0  
 $m278  fast    no         for a TCB                    0  
 $m279  fast    no         for a TCB                    0  
 $m280  fast    no         for a TCB                    0  
 $m281  fast    no         for a TCB                    0  
 $m282  fast    no         for a TCB                    0  
 $m283  fast    no         for a TCB                    0  
 $m284  fast    no         for a TCB                    0  
 $m285  fast    no         for a TCB                    0  
 $m286  fast    no         for a TCB                    0  
 $m287  fast    no         for a TCB                    0  
 $m288  fast    no         for a TCB                    0  
 $m289  fast    no         for a TCB                    0  
 $m290  fast    no         for a TCB                    0  
 $m291  fast    no         for a TCB                    0  
 $m292  fast    no         for a TCB                    0  
 $m293  fast    no         for a TCB                    0  
 $m294  fast    no         for a TCB                    0  
 $m295  fast    no         for a TCB                    0  
 $m296  fast    no         for a TCB                    0  
 $m297  fast    no         for a TCB                    0  
 $m298  fast    no         for a TCB                    0  
 $m299  fast    no         for a TCB                    0  
 $m300  fast    no         for a TCB                    0  
 $m301  fast    no         for a TCB                    0  
 $m302  fast    no         for a TCB                    0  
 $m303  fast    no         for a TCB                    0  
 $m304  fast    no         for a TCB                    0  
 $m305  fast    no         for a TCB                    0  
 $m306  fast    no         for a TCB                    0  
 $m307  fast    no         for a TCB                    0  
 $m308  fast    no         for a TCB                    0  
 $m309  fast    no         for a TCB                    0  
 $m310  fast    no         for a TCB                    0  
 $m311  fast    no         for a TCB                    0  
 $m312  fast    no         for a TCB                    0  
 $m313  fast    no         for a TCB                    0  
 $m314  fast    no         for a TCB                    0  
 $m315  fast    no         for a TCB                    0  
 $m937  fast    no                                      0  
 $m1806  fast    no                                      0  
 $m1807  fast    no                                      0  
 $m3530  fast    no         for a TCB                    0  
 $m3531  fast    no         for a TCB                    0  
 $m3560  fast    no                                      0  
 $m4116  fast    no                                      0  
 $m4120  fast    no                                      0  
 $m4126  fast    no                                      0  
 $m4130  fast    no                                      0  
 $m4134  fast    no                                      0  
 $m4139  fast    no                                      0  
 $m4142  fast    no                                      0  
 $m4146  fast    no                                      0  
 $m4151  fast    no                                      0  
 $m4155  fast    no                                      0  
 $m4158  fast    no                                      0  
 $m4164  fast    no                                      0  
 $m4167  fast    no                                      0  
 $m4170  fast    no                                      0  
 $m4176  fast    no                                      0  
 $m4179  fast    no                                      0  
 $m4183  fast    no                                      0  
 $m4187  fast    no                                      0  
 $m4188  fast    no                                      0  
 $m4193  fast    no                                      0  
 $m4194  fast    no                                      0  
 $m4196  fast    no                                      0  
 $m4197  fast    no                                      0  
 $m4198  fast    no                                      0  
 $m4200  fast    no                                      0  
 $m4201  fast   yes  $t17                                0  
 $m4202  fast   yes  $t3                                 0  
(dbx) condition
 cv      obj_addr    cv_name             num_wait  waiters
 $c1     0x200d24d8  cond for delay      0         
 $c2     0x200d2740  for a TCB           0         
 $c3     0x200d27f0  for a TCB           0         
 $c4     0x200db950  null thread         0         
 $c5     0x200dba00  for a TCB           0         
 $c6     0x20125068  file read cv        0         
 $c7     0x201250c0  file write cv       0         
 $c8     0x20125170  file read cv        0         
 $c9     0x201251c8  file write cv       0         
 $c10    0x20125278  file read cv        0         
 $c11    0x201252d0  file write cv       0         
 $c12    0x20125520                      0         
 $c13    0x20125578                      0         
 $c14    0x201265f8                      0         
 $c15    0x20127678  for a TCB           0         
 $c16    0x20127728  for a TCB           0         
 $c17    0x20128960  file read cv        0         
 $c18    0x201289b8  file write cv       0         
 $c19    0x20128a10                      0         
 $c20    0x20128a68                      0         
 $c21    0x20128ac0                      0         
 $c22    0x20128b18                      0         
 $c23    0x20128b70                      0         
 $c24    0x20128bc8                      0         
 $c25    0x20128c20                      0         
 $c26    0x20128c78                      0         
 $c27    0x20128cd0                      0         
 $c28    0x20128d28                      0         
 $c29    0x20128d80                      0         
 $c30    0x20128dd8                      0         
 $c31    0x20128f90                      1         $t7 
 $c32    0x2012b1b8                      0         
 $c33    0x2012b210                      1         $t1 
 $c34    0x2012b2c0  file read cv        0         
 $c35    0x2012b318  file write cv       0         
 $c36    0x2012b440                      0         
 $c37    0x2012b4f0  file read cv        1         $t4 
 $c38    0x2012b548  file write cv       0         
 $c39    0x2012ba08                      0         
 $c40    0x2012bb30                      0         
 $c42    0x2012bbe0                      0         
 $c43    0x2012cc60  for a TCB           0         
 $c44    0x2012cd10  for a TCB           0         
 $c49    0x2012bb88  file read cv        0         
 $c50    0x2012cee8  file write cv       0         
 $c51    0x2012cf98  for a TCB           0         
 $c52    0x2012d048  for a TCB           0         
 $c61    0x2012d1a8  for a TCB           0         
 $c62    0x2012d200  for a TCB           0         
 $c63    0x2012d4e0                      0         
 $c64    0x2012d590                      1         $t9 
 $c65    0x2012d640                      0         
 $c66    0x2012d6f0                      0         
 $c67    0x2012d7a0                      0         
 $c68    0x2012d850                      0         
 $c69    0x200e5980                      1         $t16 
 $c70    0x200e5a30                      2         $t10 $t14 
 $c71    0x200e5ae0                      0         
 $c72    0x200e5b90                      0         
 $c73    0x200e5c40                      0         
 $c74    0x200e5cf0                      0         
 $c75    0x200e5da0                      0         
 $c76    0x200e5e50                      0         
 $c77    0x200e5f00                      0         
 $c78    0x200e5fb0                      0         
 $c79    0x200e6060                      0         
 $c80    0x200e6110                      0         
 $c81    0x200e61c0                      0         
 $c82    0x200e6270                      0         
 $c83    0x200e6320                      0         
 $c84    0x200e63d0                      0         
 $c85    0x200e6480                      0         
 $c86    0x200e6530                      0         
 $c87    0x200e65e0                      0         
 $c88    0x200e6690                      0         
 $c89    0x200e6740                      0         
 $c90    0x200e67f0                      0         
 $c91    0x200e68a0                      0         
 $c92    0x200e6950                      0         
 $c93    0x200e6a00                      0         
 $c94    0x200e6ab0                      0         
 $c95    0x200e6b60                      0         
 $c96    0x200e6c10                      0         
 $c97    0x200e6cc0                      0         
 $c98    0x200e6d70                      0         
 $c99    0x200e6e20                      0         
 $c100   0x200e6ed0                      0         
 $c101   0x200e6f80                      0         
 $c102   0x200e7030                      0         
 $c103   0x200e70e0                      0         
 $c104   0x200e7190                      0         
 $c105   0x200e7240                      0         
 $c106   0x200e72f0                      0         
 $c107   0x200e73a0                      0         
 $c108   0x200e7450                      0         
 $c109   0x200e7500                      0         
 $c110   0x200e75b0                      0         
 $c111   0x200e7660                      0         
 $c112   0x200e7710                      0         
 $c113   0x200e77c0                      0         
 $c114   0x200e7870                      0         
 $c115   0x200e7920                      0         
 $c116   0x200e79d0                      0         
 $c117   0x200e7a80                      0         
 $c118   0x200e7b30                      0         
 $c119   0x200e7be0                      0         
 $c120   0x200e7c90                      0         
 $c121   0x200e7d40                      0         
 $c122   0x200e7df0                      0         
 $c123   0x200e7ea0                      0         
 $c124   0x200e7f50                      0         
 $c125   0x200e8000                      0         
 $c126   0x200e80b0                      0         
 $c127   0x200e8160                      0         
 $c128   0x200e8210                      0         
 $c129   0x200e82c0                      0         
 $c130   0x200e8370                      0         
 $c131   0x200e8420                      0         
 $c132   0x200e84d0                      0         
 $c133   0x200e8580                      0         
 $c134   0x200e8630                      0         
 $c135   0x200e86e0                      0         
 $c136   0x200e8790                      0         
 $c137   0x200e8840                      0         
 $c138   0x200e88f0                      0         
 $c139   0x200e89a0                      0         
 $c140   0x200e8a50                      0         
 $c141   0x200e8b00                      0         
 $c142   0x200e8bb0                      0         
 $c143   0x200e8c60                      0         
 $c144   0x200e8d10                      0         
 $c145   0x200e8dc0                      0         
 $c146   0x200e8e70                      0         
 $c147   0x200e8f20                      0         
 $c148   0x200e8fd0                      0         
 $c149   0x200e9080                      0         
 $c150   0x200e9130                      0         
 $c151   0x200e91e0                      0         
 $c152   0x200e9290                      0         
 $c153   0x200e9340                      0         
 $c154   0x200e93f0                      0         
 $c155   0x200e94a0                      0         
 $c156   0x200e9550                      0         
 $c157   0x200e9600                      0         
 $c158   0x200e96b0                      0         
 $c159   0x200e9760                      0         
 $c160   0x200e9810                      0         
 $c161   0x200e98c0                      0         
 $c162   0x200e9970                      0         
 $c163   0x200e9a20                      0         
 $c164   0x200e9ad0                      0         
 $c165   0x200e9b80                      0         
 $c166   0x200e9c30                      0         
 $c167   0x200e9ce0                      0         
 $c168   0x200e9d90                      0         
 $c169   0x200e9e40                      0         
 $c170   0x200e9ef0                      0         
 $c171   0x200e9fa0                      0         
 $c172   0x200ea050                      0         
 $c173   0x200ea100                      0         
 $c174   0x200ea1b0                      0         
 $c175   0x200ea260                      0         
 $c176   0x200ea310                      0         
 $c177   0[rsarbo 1/6/94 public]
Re-assign to delgado.

[1/11/94 public]

This is more easily reproduced when one tries to issue
"dfstrace" commands to the flserver to repeatedly clear its logs 
and dump its logs.

I also saw a cma assertion warning regarding relocking
a locked mutex.  Right around that time the flserver on that
machine hung.

[1/27/94 public]

Fixed SUBIKDISK_Probe to call ubik_CheckAuth with cancelability
disabled.

[02/04/94 public]
Closed.



CR Number                     : 9616
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : dfsbind has memory leak
Reported Date                 : 12/14/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/14/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : security/helper/auth_helper.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 
	[delgado 12/14/93 public]
	Dfsbind is growing slowly(?) over time.
We have one machine where it started out at around 700k
and grew to 12388K over a 5 day period.
We ran connectathon to an episode fileset and saw
it double its size in roughly 5 hours.
This is the on the new Transarc drop.

[12/17/93 public]
It was mentioned that ot9353 might be the same as this.
I've looked at ot9353 and there seem to be serveral
problems mentioned in this ot, one of which is the
growth in dfsbind.  However, I'm not certain that
the solution provided in the OT report is
going to fix our problem.
We turned on debugging for libauthelper and
noticed that it freqently invalidates valid
cache entries in the case where a user
has tickets to more than one DFS server.
This results in sec_login_import context
getting called more often than it needs to
be.
I've send Bill Sommerfeld mail describing
what we've seen and have asked him to
comment on this and ot 9353 as well.
By the way, we modified libauthelper to
correct the invalidation problem and
it now seems to grow more slowly.

[12/20/93 public]
This dfsbind growth is also being seen in the system test
cell, config A, which matches the CHO cell configuration.
Here's the ps output for dfsbind on the rios taken after
the machine had been up for 5 days and I was starting a
dfs.read_write_all run:
ps -eF ucomm,pid,ppid,vsize,rssize,pagein,cputime,etime,pcpu,pmem
COMMAND    PID  PPID   VSZ   RSS PAGEIN        TIME     ELAPSED  %CPU  %MEM
dfsbind  14810     1 15996  4468  78577    03:06:46  5-03:31:08   2.5  14.0
and then 15 hours later:
ps -eF ucomm,pid,ppid,vsize,rssize,pagein,cputime,etime,pcpu,pmem
COMMAND    PID  PPID   VSZ   RSS PAGEIN        TIME     ELAPSED  %CPU  %MEM
dfsbind  14810     1 18324  7516 139617    03:32:48  5-17:40:55   2.6  23.0
The dfs.read_write_all test failed due to lack of swap space.

[12/21/93 public]
I've made a fix to libauthelper which seems to reduce the growth rate
by about half as much and have run this in the CHO cell.

[12/21/93 public]
One thing I noticed when I was looking at core leaks in DFS code was
that pages associated with the ICL logs didn't appear in the ps
listing (at least on a RIOS) until the page was actually touched.
This made it look like there was a leak when there really wasn't.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[12/21/93 public]

This probably explains some things we are seeing, however
we see the problem on HP as well and we've seen that growth
appears to be unbounded.  How big are these icl
logs anyway?  We had one dfsbind on a RIOS grow to 13.6meg
(from 700k ).  I hope the logs are not that big!
I would expect with the case you pointed out with ICL that
the growth should level off as some point since the log
is supposedly of fixed size?

Also, a multi server configuration will grow much more
rapidly than a single server configuration, and it was
the multi-server case which produced the 13.6 meg dfsbind.

	Dfsbind is growing slowly(?) over time.
We have one machine where it started out at around 700k
and grew to 12388K over a 5 day period.
We ran connectathon to an episode fileset and saw
it double its size in roughly 5 hours.
This is the on the new Transarc drop.

[12/22/93 public]

fix submitted.

[12/22/93 public]

I did a verification before this submission with two parallel connectathon
tests, each having a single iteration and less than 2 1/2 minutes elapsed:

        cd /:/connectathon/basic/HP     #dce10/HP as the DFS server
        test.dfs.sh &

        cd /:/u1_toaster/connectathon/basic/HP #toaster/HP as the DFS server
        test.dfs.sh &

Without her fix it grew about 44 Kbytes, with the fix it grew zero bytes.

Note that there is another dfsbind memory leak that exists even when the 
DFS client is idle, and Diane's fix is not for fixing this (idle) leak.

[1/3/94 public]

[1/3/93 public]


Of the leaks remaining, we  have observed that  jumps in size 
occur during ticket refresh.  We don't know yet if it's
in libauthhelper or the security/krb stuff.

[02/04/94 public]
Closed.



CR Number                     : 9604
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : performance hit when extending a file
Reported Date                 : 12/10/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/10/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_dcache.c
Sensitivity                   : public
Transarc Deltas               : jaffe-ot9604-fix-performance-in-cm_GetDOneLine
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/10/93 public]
DFS 1.0.2 had a serious data corruption bug where data from a hole
could be read as non-zero in some circumstances.
This situation is caused when a chunk is about to be fetched when a
file is truncated.  The truncate first executes, setting the file's
local size to 0, but not changing the file at the server yet.  The
client then prefetches the chunk, bringing in the old, untruncated
data from the server.  A write to the middle of the chunk then exposes
untruncated data.
Our fix for this problem was to cm_GetDOnLine.  Before fetching a
chunk, or even setting the fetching flag on a chunk, we check to see
if there's a cached operation that would affect the file's length, and
if there is one, we synchronously perform this operation before
fetching the chunk.
Unfortunately, this fix is also triggered when a file is *extended*,
since that also sets the file's length in the cache.  Thus, in 1.0.3,
when extending a file, many store data calls are preceded by a store
status call that changes the file's length, which noticibly slows
writes down.
There are a number of possible fixes.  First, consider the case where
MODLENGTH is set but MODTRUNCPOS is not set.  This means that we're
going to update the length, but that no local length-modifying
operation shrunk the file's length below what it was when the
operation started.  This means that the file has only grown from
what's there at the server.  In this case, we can fetch the data
safely; at worst, we fetch data from past the EOF.
This fix can be done most easily by modifying the check in
cm_GetDOnLine where we decide to call cm_SyncSCache.  Right now, it
calls it if CM_MODTRUNCPOS or CM_MODLENGTH is set, but by the above
argument, it can be modified to only call cm_SyncSCache if
cm_MODTRUNCPOS is set.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/10/93 public]
It works.  Times on the HP platform are now significantly improved.  The 
RIOS also looks better.
Filled in Transarc Deltas with `jaffe-ot9604-fix-performance-in-cm_GetDOneLine' 
Changed Transarc Status from `open' to `export'

[12/13/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 9603
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : osi
Short Description             : Fail to initialize uio_fpflags
Reported Date                 : 12/10/93
Found in Baseline             : 1.0.3a
Found Date                    : 12/10/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/osi/HPUX/osi_uiuo_mach.h
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : submit
Transarc Herder               : 

[12/10/93 public]
By failing to initialiaze uio_fpflags in osi/HPUX/osi_uio_mach.h
we can cause synchronous disk transfers which slows throughput.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[12/10/93 public]
this change makes a major improvement in DFS performance.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Changed Transarc Status from `open' to `submit' 
Added field Transarc Herder with value `'

[12/13/93 public]

[12/17/93 public]
Closed.



CR Number                     : 9600
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : bakserver should reject duplicate UseTapes
Reported Date                 : 12/8/93
Found in Baseline             : 1.0.2
Found Date                    : 12/8/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/file/bakserver/procs.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : submit
Transarc Herder               : 

[12/8/93 public]
When the bakserver receives a BUDB_UseTape, it should check to make sure that the 
Tape name received is not a duplicate. This means getting the dump id, walking
down the list of tapes in the dump and throwing out the new tape if its a duplicate 
(same name as a previous tape). This will prevent problems when restoring if
duplicate tapes are used. 
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[12/20/93 public]
submitted 12/20/93
Changed Defect or Enhancement? from `enh' to `def' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Filled in Affected File with `src/file/bakserver/procs.c' 
Changed Transarc Status from `open' to `submit' 
Added field Transarc Herder with value `'

[02/04/94 public]
Closed.



CR Number                     : 9599
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : See description below
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/18/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 There are several problems in the bakserver which lead to inconsistency in
 the text list information. These result in the database becoming unusable
 because of loops in text chains and other forms of inconsistency. Some of
 the problems noted are listed below.
 
 1. BUDB_SaveText rpc does not write the data block if the text information
    to be written is 0 bytes. The bad thing here is, the block header is also
    not written and might contain inconsistent data. This causes two problems
    a. creation of orphan blocks
    b. loops in the text block list
 
 2. pointer arithmetic in computing the linkOffset and linkValue variables in
    BUDB_SaveText very risky because it assumes certain machine dependent
    alignment restrictions.
 
 3. No return code checks for several calls to disk block manipulation routines
    such as freeOldBlockChain, dbwrite etc in db_text.c
 
 4. several problems in lock usage around routines such as AllockBlock, 
    FreeBlock, AllocStructure, FreeStructure. These routines should be called
    with the db lock held for write, but in some cases only a shared lock is
    held.
 
 5. if set_header_word or dbwrite calls fail inside FreeBlock, the in-memory
    value of the block header elemens would differ from on-disk ones.
 
 6. Block should be bzeroed when returning from AllocBlock in the case where
    the db is extended.
 
 There could be more, but at present this is all I have. My automated backup
 tests hit this problem every once in a while, but apparently others and other
 tests have never seen it. I will fix these problems, run my tests, and if
 they fail, diagnose further. What is required is a complete code audit to
 catch other similar problems in the dump/tape/vol structure block handling
 code.
 vijay-Thu, 18 Nov 1993 17:29:33
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixes were made for the above defects. They are in
 vijay-db4712-bakserver-database-text-handling-problems 1.2vijay-Tue, 23 Nov 1993 14:58:18
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9598
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Header files repser.h and rep_trace.h are getting installed in ...install/sparc_solaris2/opt/dce1.0/bin
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/18/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 DFS header files rep_trace.h and repser.h are getting installed in 
 install/sparc_solaris2/opt/dce1.0/bin/  directory. Appropriate place is
 install/sparc_solaris2/opt/dce1.0/share/include/dcedfs.
 aswini-Mon, 18 Oct 1993 16:10:22
**Solution Text**
 shl-db4511-install-rep-header-files-in-share
 
 Create _IDIR entries pointing to /share/include/dcedfs/ for
 repser.h and rep_trace.h (otherwise, they end up in the default IDIR=/bin/).
 Steve Lammert
 Tue Oct 19 16:54:50 1993
 shl-Tue, 19 Oct 1993 16:54:53
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9597
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : test_anode test fset3b panics in reclone
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/28/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 Ted captured the spirit of the striver fileset load fsetStream1 with
 file load AIMIII in a test_anode test. Running this test_anode
 test panics in a reclone (in MV_GetOps).
 
 The first level indirect block address I in the anode A of the R/W
 fileset F indicates that it is being backed by the corresponding
 indirect block in the backing anode AB1 in its backing fileset FB1.
 The backing fileset FB1 is being backed by another backing fileset FB2
 and the first level indirect block address I1 in anode AB1 is being
 backed by the corresponding indirect block I2 in anode AB2 in fileset
 FB2, i.e. I1 = COW + I2. But I != I1 and hence an assert fails
 resulting in a panic inside MV_GetOps while recloning F to FB1.
 
 While recloning FB1 to FB2, we copy the data block addresses from the
 indirect blocks in AB1 to those in AB2 and then set the indirect block
 addresses in AB1 to those in AB2 and set the COW bit on in the
 indirect block addresses in AB1 and free the indirect blocks that were
 being used by AB1. Ted determined that this is indeed wrong as A is
 still pointing to the indirect blocks that were in AB1 before reclone
 of FB1 to FB2 and that were freed in the reclone FB1 to FB2.
 
 Hence next time we do a reclone from F to FB1, we could hit the assert
 in MV_GetOps.
 rajesh-Fri, 29 Oct 1993 08:50:16
**Solution Text**
 rajesh-db4585-correct-reclone-of-indirect-blocks
 rajesh-Fri, 05 Nov 1993 12:08:30
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9596
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fix handling of multiple iterations to complete an unclone
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/26/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 While deleting the backing fileset of a fileset on which AIM III was
 running, with fileset test fsetStream running on the backend, as
 assertion failure was encountered in the depths of the anode layer.
 
 The stack trace is 
 
 panic(0xfc50364c,0xfc4f76b4,0xc3b,0xc3b,0x6,0xfc4f76b4) + 1c
 MV_GetOps(0x80003584,0xffffffff,0xfc59b818,0x1,0x7,0xffffffff) + 170
 RecursiveOps(0x2cce26,0xfc6cf08c,0xfc6cb288,0x0,0x7ad,0x8) + 600
 MoveAnodes(0x200c,0xfc6cf08c,0xfc6cb288,0x1,0xfc6f7628,0xfc7006dc) + 524
 epix_MoveData(0x2cce26,0xfc6cf08c,0x0,0xfc6cb2c0,0x0,0xfc6cf0c4) + 568
 epia_Clone(0x1,0xfc6cf08c,0xfc6cb288,0x0,0xfc4f77f8,0x0) + 348
 efsSomeClone(0x0,0xfc78580c,0x0,0x0,0x21b60005,0x4e) + 764
 afscall_volser(0x1a,0x2,0xfc78580c,0x20000,0x0,0x0) + 1e0c
 afs_syscall(0xf0453e90,0x4,0xf04539d8,0xf0453920,0xfc567260,0xfc52ea88) + f4
 syscall(0xf04539d8) + 3b4
 
 Its an assertion failure while uncloning the backing fileset into the
 R/W fileset, before deleting the backing fileset.
 
 rajesh-Wed, 27 Oct 1993 09:09:20
 
 After recovering the aggregate, the salvage reported an error in the visible
 space/quota usage of the fileset.
 
 [src] salvage -recover /dev/dsk/epi1
 Will run recovery on /dev/dsk/epi1
 recovery statistics:
         Elapsed time was 2000 ms
         79 log pages recovered consisting of 1515 records
         Modified 15 data blocks
         994 redo-data records, 0 redo-fill records
         28 undo-data records, 0 undo-fill records
 Ran recovery on dev 1/1
 [src] salvage -verify /dev/dsk/epi1
 Verifying /dev/dsk/epi1
 In volume e1.f1.back1 100,,3 (avl #5)
   Volume allocated/visible quota mismatched: usage 14112/16726, computed 14112/16
 720
   Volume is marked as inconsistent, not walked
 Processed 3 vols 215 anodes 56 dirs 48 files 0 acls
 Done.  Some inconsistencies found verifying /dev/rdsk/epi1
 
 An error in the visible quota usage, was also encountered while running
 STRIVER with file load AIMIII and fileset load fsetReclone2, which is described
 in Sybase defect 4509.rajesh-Thu, 28 Oct 1993 10:11:10
 
 With Ted's assistance the bug has been found. The error is due to an
 existing error in the unclone path. During unclone, if cow block is
 backed by a block in the clone, and that block in the clone is being
 backed by another clone, i.e.  the chain is (b, b, {a | [b ...]}),
 then a partial unclone due to EPI_E_NOTDONE, will result in (b, e, {a
 | [b ...]}) for this block. On the next iteration to make further
 progress, we do not start off where we left off, but we begin the
 operation from scratch (The idea is that for all blocks processed till
 now, no real activity will need to be done). The system currently does not
 allow for this case and returned MV_Error for (b,e) pair during
 unclone which caused an assertion failure in MV_GetOps.
 rajesh-Thu, 04 Nov 1993 15:54:43
**Solution Text**
 rajesh-db4570-handle-repeated-iterations-for-unclone-correctly
 
 The system now allows a (b, e) pair during unclone, if its not the first
 iteration for the unclone. (Well, this is not completely true, for details
 refer to comments for macro VALIDATE_OPS in anode/fixed_anode.c).
 rajesh-Thu, 04 Nov 1993 15:55:03
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9595
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_ReturnOpenToken can reference undefined vbl
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 12/01/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9594
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm checkfilesets gratuitously generates new inode #s
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 12/01/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 The "fs" node in the name space gets a new inode number when a checkfilesets
 operation is done.  This confuses pwd when it first encounters this
 node from the bottom, at first.
 Mike Kazar
 Wed Dec  1 09:26:03 1993
 kazar-Wed, 01 Dec 1993 09:26:29

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9593
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_lookup can panic/deadlock on volroot processing
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/08/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 cm_lookup holds one vnode lock while calling cm_GetSCache in its "."
 and '..'  processing code.  This problem has been extremely
 exacerbated by the new cm_ConnAndReset function, which locks all vnodes
 during its processing.
 
 Mike Kazar
 Mon Nov  8 10:56:39 1993
 kazar-Mon, 08 Nov 1993 10:56:46
**Solution Text**
 kazar-db4643-fix-cm-lookup-deadlock 
 Mike Kazar
 Mon Nov  8 17:40:56 1993
 kazar-Mon, 08 Nov 1993 17:41:01
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9592
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm can forget to invalidate clean pages on token revoke
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/04/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 The CM, in its FlushDCache logic, assumes that we can flush a chunk
 having clean pages, but not one having dirty pages.
 
 On the other hand, the code in the CM's data token revoke function assumes
 that if a chunk doesn't exist, then there are no pages, clean or dirty,
 that need to be invalidated.
 
 Thus, FlushDCache can leave some clean pages around after removing a dcp,
 and then the RevokeDataToken function will fail to zap those pages when
 a token is revoked.
 Mike Kazar
 Thu Nov  4 17:48:42 1993
 kazar-Thu, 04 Nov 1993 17:48:50
**Solution Text**
 kazar-db4614-remove-clean-pages-on-dcp-recycle
 Mike Kazar
 Fri Nov  5 16:12:10 1993
 kazar-Fri, 05 Nov 1993 16:12:16
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9591
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : getpage can fail to set dirty/haspages flags
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/03/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Solution Text**
 kazar-db4602-mark-multiple-dcps-in-getpage
 Mike Kazar
 Fri Nov  5 16:05:23 1993
 kazar-Fri, 05 Nov 1993 16:05:27
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9590
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : read() system call returns ESTALE, even when no servers go down
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/02/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Solution Text**
 kazar-db4596-fix-estale-on-delete-race 
 Mike Kazar
 Fri Nov  5 16:02:34 1993
 kazar-Fri, 05 Nov 1993 16:02:38
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9589
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FLDB
Short Description             : Multiple fts cause dead lock.
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/09/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 The testing cell configured by DFS 3.30 and BPG 09.010 has three machines 
 and a flserver is running on each machine. The fts stress test has three 
 "fts create" running on each machine while a "fts move" running between 
 two machine at a time. "fts move" operates on an old fileset. "fts create" 
 creates, say 100 filesets on each machine. There is no synchronization 
 among the operations.
 
 The test did create some filesets on each machine and got several old
 filesets moved. But at some point all fts commands on all machines hung. 
 
 The stack traces for fts commands are given below.
 
 The three machines as the test running are testlab20 (sync site), montana
 (core server), and diablo.
 
 BOS STAT
 --------
 
 testlab20# bos stat /.:/hosts/testlab20
 Instance flserver, currently running normally.
 Instance upclient, currently running normally.
 Instance ftserver, currently running normally.
 testlab20#  bos stat /.:/hosts/montana
 Instance flserver, currently running normally.
 Instance upserver, currently running normally.
 Instance ftserver, currently running normally.
 testlab20# ^montana^diablo
 bos stat /.:/hosts/diablo
 Instance flserver, currently running normally.
 Instance upclient, currently running normally.
 Instance ftserver, currently running normally.
 testlab20#
 
 diablo.transarc.com# !u
 udebug /.:/fs testlab20
 return code 668147743 from UBIKVOTE_Debug
 
 MONTANA: command 
  fts move fileset.9 diablo aggrh testlab20 aggre
 
   [9] VL_ReplaceEntry(0x6, 0xefffe658, 0x94a, 0x2, 0xfffffffe, 0x6), at 0x5956c
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(ubikclient.o)' doesn't match image in executable. S
 kipping stabs.
   [10] ubik_Call(0x2, 0x226e68, 0x0, 0x0, 0x0, 0x0), at 0xd60e4
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vldbint.o)' doesn't match image in executable.
 Skipping stabs.
   [11] vldb_ReplaceEntry(0x0, 0x7, 0x0, 0x7, 0xefffee68, 0x0), at 0x4a7c0
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vprocs.o)' doesn't match image in executable. S
 kipping stabs.
   [12] VC_MoveVolume(0x0, 0x0, 0x0, 0xffffffff, 0x5, 0x1), at 0x302dc
   [13] volc_Call(0x2dab4, 0x0, 0x0, 0x61676772, 0xeffff9cc, 0x8), at 0x2a754
   [14] fts_DoMoveVolume(0x199160, 0x19ed98, 0x19eda8, 0x19edb8, 0x19edc8, 0x19ed
 d8), at 0x1edf8
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libcmd.a(cmd.o)' doesn't match image in executable. Skipping s
 tabs.
   [15] cmd_Dispatch(0xeffffb44, 0xeffffc54, 0x16f000, 0x0, 0x199660, 0x199160),
 at 0x1123e4
   [16] main(0x7, 0xeffffb44, 0xeffffb64, 0xeffffb44, 0x0, 0x0), at 0x27248
 (/opt/SUNWspro/bin/dbx)
 
 MONTANA: command
  fts create fts.stress.6709 .65 montana aggrb
 
   [9] VL_GetSiteInfo(0xe, 0xe, 0x10, 0xefffe5c0, 0x2, 0xefffe532), at 0x51940
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(ubikclient.o)' doesn't match image in executable. S
 kipping stabs.
   [10] ubik_Call(0x2, 0x226e68, 0xc037cf17, 0x0, 0xefffeca8, 0x0), at 0xd60e4
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vldbint.o)' doesn't match image in executable.
 Skipping stabs.
   [11] vldb_GetSiteInfo(0xeffffaa8, 0xefffeca8, 0x18b2a0, 0x0, 0x0, 0x18582c), a
 t 0x4b110
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vprocs.o)' doesn't match image in executable. S
 kipping stabs.
   [12] connToServer(0xeffffaa8, 0x0, 0x186000, 0x80, 0x0, 0xeffffaa8), at 0x2a22
 c
   [13] volc_Call(0x3ea10, 0xeffffaa8, 0x0, 0xeffff09c, 0xeffff0b4, 0xeffffaa8),
 at 0x2a6b8
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_misc.o)' doesn't match image in executable. Ski
 pping stabs.
   [14] GetAggr(0x1, 0xeffffaa8, 0x0, 0x1, 0x28, 0x6d), at 0x41d88
   [15] fts_DoCreateVolume(0x190068, 0x0, 0x190068, 0x0, 0x0, 0x19ed88), at 0x1d3
 d4
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libcmd.a(cmd.o)' doesn't match image in executable. Skipping s
 tabs.
   [16] cmd_Dispatch(0xeffffc1c, 0xeffffcf4, 0x16f000, 0x0, 0x190568, 0x190068),
 at 0x1123e4
   [17] main(0x5, 0xeffffc1c, 0xeffffc34, 0xeffffc1c, 0x0, 0x0), at 0x27248
 
 We have another fts create running on diablo bu I can not get the stack trace.
 
 TESTLAB20: command
 fts create fts.stress.2157 8.73 testlab20 aggre
 
    [9] VL_CreateEntry(0x96d, 0x96d, 0x2f, 0x93e, 0xefffd6e4, 0xefffd718), at 0x5
 81e8
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(ubikclient.o)' doesn't match image in executable. S
 kipping stabs.
   [10] ubik_Call(0x2, 0x226e68, 0x0, 0x0, 0x0, 0x0), at 0xd60e4
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vldbint.o)' doesn't match image in executable.
 Skipping stabs.
   [11] vldb_CreateEntry(0xefffe390, 0x18dfb0, 0xefffebe0, 0xffffed68, 0x3, 0x0),
  at 0x4a2c8
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vldbsubr.o)' doesn't match image in executable.
  Skipping stabs.
   [12] vldb_CreateVldbEntry(0x1bb8, 0x0, 0xffffed68, 0x3, 0xffffed68, 0x3), at 0
 x45594
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libvolc.a(volc_vprocs.o)' doesn't match image in executable. S
 kipping stabs.
   [13] VC_CreateVolume(0x2278a0, 0xefffef18, 0xefffed50, 0x18dfb0, 0x0, 0x1), at
  0x2a98c
   [14] volc_Call(0x0, 0x61676772, 0x65000000, 0x5, 0x18dfb0, 0x2278a0), at 0x2a7
 1c
   [15] fts_DoCreateVolume(0x190068, 0x0, 0x190068, 0x0, 0x0, 0x19ed88), at 0x1d4
 e0
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libcmd.a(cmd.o)' doesn't match image in executable. Skipping s
 tabs.
   [16] cmd_Dispatch(0xeffffc14, 0xeffffcf0, 0x16f000, 0x0, 0x190568, 0x190068),
 at 0x1123e4
   [17] main(0x5, 0xeffffc14, 0xeffffc2c, 0xeffffc14, 0x0, 0x0), at 0x27248
 (/opt/SUNWspro/bin/dbx)
 Here is flserver thread information on TESTLAB20
 
 =>[1] icl_GetLogSpace(0x1883dc, 0xebb7b, 0x7800, 0x77f9, 0x7, 0x0), at 0x86284
   [2] icl_AppendRecord(0x1883dc, 0x2, 0x2, 0x9, 0x0, 0x1), at 0x86a70
   [3] icl_Event4(0x18f8fc, 0x0, 0xffffffff, 0x1, 0x0, 0x1), at 0x85fcc
   [4] icl_Event3(0x18f8fc, 0x29f6a004, 0x1102080, 0xef3c66d8, 0x60be4, 0x2616c0)
 , at 0x8606c
   [5] ContactQuorum(0x60be4, 0x2616c0, 0x0, 0x0, 0x0, 0x1d5210), at 0x56eb4
   [6] ubik_Write(0x2616c0, 0x139740, 0x2, 0x400, 0x0, 0x1f10), at 0x5b250
   [7] vlwrite(0x2616c0, 0x20074, 0x138740, 0x2f10, 0x188, 0x138740), at 0x2e1ac
   [8] PutSite(0x188424, 0x2, 0x0, 0x0, 0xee8f0b20, 0x0), at 0x5161c
   [9] quotaCheck(0xffffffff, 0xffffffff, 0x1388c8, 0xffffffff, 0x100, 0xffffffff
 ), at 0x51b10
   [10] EnsureAddrEntry(0x1388c8, 0x188, 0x138740, 0x1388c8, 0x0, 0x2), at 0x51f2
 0
   [11] vldbentry_to_vlentry(0x3, 0xee8f11b0, 0x1, 0x0, 0xee8f11c8, 0x0), at 0x4d
 ce0
   [12] VL_CreateEntry(0x267f68, 0x188424, 0x2850a005, 0x2850a005, 0x0, 0x13d000)
 , at 0x343ec
   [13] op13_ssr(0xee8f1bcc, 0xee8f1cac, 0xee8f1b98, 0xee8f1cdc, 0x0, 0x1d4140),
 at 0x24d3c
 
   [14] rpc__dg_execute_call(scall_ = 0x2618e8, call_was_queued = 0), line 793 in
  "/afs/transarc.com/project/alpine/dev/rel/09.011/src/rpc/runtime/dgexec.c"
   [15] cthread_call_executor(cthread = 0x212980), line 604 in "/afs/transarc.com
 /project/alpine/dev/rel/09.011/src/rpc/runtime/comcthd.c"
   [16] myStartRoutine(tcb = 0x212668), line 1072 in "/afs/transarc.com/project/a
 lpine/dev/rel/09.011/src/sol_pth/pthread.c"
   [17] _thread_start(0x212668, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef33d4f4
 (/opt/SUNWspro/bin/dbx)
 -------------
 
   [8] lock_ObtainWrite(0x2, 0x1b, 0x6c, 0x1179a4, 0x117a10, 0x117a10), at 0xe450
 0
   [9] icl_AppendRecord(0x1883dc, 0x29d13039, 0x83042, 0x271ad8, 0xee8ad0d0, 0xff
 ffffff), at 0x86504
   [10] icl_Event4(0x188424, 0x7, 0xffffffff, 0x1, 0x0, 0x1), at 0x85fcc
   [11] VL_GetEntryByID(0x2, 0xee8ad0d8, 0xffffffff, 0xee8ad0d8, 0xee8adcdc, 0x0)
 , at 0x35ea8
   [12] op0_ssr(0xef4ba748, 0xee8adcac, 0x10, 0xee8adcac, 0xee8adcac, 0x1), at 0x
 1aaf8
   [13] rpc__dg_execute_call(scall_ = 0x216800, call_was_queued = 0), line 793 in
  "/afs/transarc.com/project/alpine/dev/rel/09.011/src/rpc/runtime/dgexec.c"
   [14] cthread_call_executor(cthread = 0x212a20), line 604 in "/afs/transarc.com
 /project/alpine/dev/rel/09.011/src/rpc/runtime/comcthd.c"
   [15] myStartRoutine(tcb = 0x212768), line 1072 in "/afs/transarc.com/project/a
 lpine/dev/rel/09.011/src/sol_pth/pthread.c"
   [16] _thread_start(0x212768, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef33d4f4
 (/opt/SUNWspro/bin/dbx) (/opt/SUNWspro/bin/dbx)
 ---------------
 
   [5] lock_ObtainWrite(0x2, 0x1e, 0x78, 0x1179a4, 0x117a1c, 0x117a1c), at 0xe450
 0
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(lock.o)' doesn't match image in executable. Skippin
 g stabs.
   [6] ubik_hold(0x115594, 0xee93fb98, 0xee93fbcc, 0xee93fba0, 0x0, 0x216ebf10),
 at 0x5e774
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(beacon.o)' doesn't match image in executable. Skipp
 ing stabs.
   [7] ubeacon_Interact(0x0, 0x0, 0x115400, 0x0, 0x2, 0x0), at 0x7898c
   [8] myStartRoutine(tcb = 0x1cd720), line 1072 in "/afs/transarc.com/project/al
 pine/dev/rel/09.011/src/sol_pth/pthread.c"
   [9] _thread_start(0x1cd720, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef33d4f4
 
 -------------------
 
   [5] lock_ObtainWrite(0x2, 0x1b, 0x6c, 0x1179a4, 0x117a10, 0x117a10), at 0xe450
 0
   [6] icl_AppendRecord(0x1883dc, 0x29f6a03e, 0x101000, 0xef3c66d8, 0x1f, 0x0), a
 t 0x86504
   [7] icl_Event4(0x18f8fc, 0x7, 0xffffffff, 0x1, 0x0, 0x1), at 0x85fcc
   [8] icl_Event2(0x18f8fc, 0x29f6a03e, 0x1101000, 0xef3c66d8, 0x1f, 0xee918cb4),
  at 0x860c0
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(recovery.o)' doesn't match image in executable. Ski
 pping stabs.
   [9] urecovery_Interact(0x1, 0x0, 0x0, 0x1, 0x0, 0x0), at 0x7a5cc
   [10] myStartRoutine(tcb = 0x213dc0), line 1072 in "/afs/transarc.com/project/a
 lpine/dev/rel/09.011/src/sol_pth/pthread.c"
   [11] _thread_start(0x213dc0, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef33d4f4
 -----------------
   [5] lock_ObtainWrite(0x2, 0x1b, 0x6c, 0x1179a4, 0x117a10, 0x117a10), at 0xe450
 0
   [6] icl_AppendRecord(0x1883dc, 0x29ccb0ca, 0x0, 0x0, 0x0, 0x0), at 0x86504
   [7] icl_Event4(0x18aa3c, 0x19, 0xffffffff, 0x1, 0x0, 0x1), at 0x85fcc
   [8] icl_Event0(0x18aa3c, 0x29ccb0ca, 0x1000000, 0xee847e57, 0x0, 0xee847e55),
 at 0x8615c
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libdauth.a(dfsauth_server.o)' doesn't match image in executabl
 e. Skipping stabs.
   [9] dfsauth_server_CheckAuthorizationAclOptional(0x0, 0x0, 0x18aa3c, 0x18aa3c,
  0x2, 0xfceac600), at 0xc8d64
   [10] dfsauth_server_CheckAuthorization(0xee84786c, 0x0, 0x0, 0x0, 0x2, 0x58a97
 cc), at 0xc9ea8
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(remote.o)' doesn't match image in executable. Skipp
 ing stabs.
   [11] ubik_CheckAuth(0x267cf8, 0x1, 0x0, 0xee847b78, 0xee847a68, 0xee847a68), a
 t 0x71f74
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(vote.o)' doesn't match image in executable. Skippin
 g stabs.
   [12] SUBIKVOTE_Beacon(0x267cf8, 0x0, 0x2c8e0358, 0xee847b68, 0x2c8e0358, 0xee8
 47b78), at 0x5c9f0
 dbx: warning: Objectfile '/afs/tr/fs/dev/integ/dfs-103/dfs-103-3.30/export/sparc
 _solaris2/usr/lib/libnubik.a(ubikvote_proc_sstub.o)' doesn't match image in exec
 utable. Skipping stabs.
   [13] op0_ssr(0x1748bc, 0x2c8e0358, 0x1748c0, 0x1748c0, 0x2c8ddf72, 0x1748c4),
 at 0x7056c
   [14] rpc__dg_execute_call(scall_ = 0x216fc8, call_was_queued = 0), line 793 in
  "/afs/transarc.com/project/alpine/dev/rel/09.011/src/rpc/runtime/dgexec.c"
   [15] cthread_call_executor(cthread = 0x1d57b8), line 604 in "/afs/transarc.com
 /project/alpine/dev/rel/09.011/src/rpc/runtime/comcthd.c"
   [16] myStartRoutine(tcb = 0x212c58), line 1072 in "/afs/transarc.com/project/a
 lpine/dev/rel/09.011/src/sol_pth/pthread.c"
   [17] _thread_start(0x212c58, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef33d4f4
 --------------
 
   [5] lock_ObtainWrite(0x2, 0x1b, 0x6c, 0x1179a4, 0x117a10, 0x117a10), at 0xe450
 0
   [6] icl_AppendRecord(0x1883dc, 0x29ccb0ca, 0x0, 0x0, 0x0, 0x0), at 0x86504
   [7] icl_Event4(0x18aa3c, 0x19, 0xffffffff, 0x1, 0x0, 0x1), at 0x85fcc
   [8] icl_Event0(0x18aa3c, 0x29ccb0ca, 0x1000000, 0xee836e57, 0x0, 0xee836e55),
 at 0x8615c
   [9] dfsauth_server_CheckAuthorizationAclOptional(0x0, 0x0, 0x18aa3c, 0x18aa3c,
  0x2, 0xfcedacc0), at 0xc8d64
   [10] dfsauth_server_CheckAuthorization(0xee83686c, 0x0, 0x0, 0x0, 0x0, 0x29eb6
 454), at 0xc9ea8
   [11] ubik_CheckAuth(0x269270, 0x1, 0x0, 0xee836b78, 0xee836a68, 0xee836a68), a
 t 0x71f74
   [12] SUBIKVOTE_Beacon(0x269270, 0x0, 0x2c8e0358, 0xee836b68, 0x2c8e0358, 0xee8
 36b78), at 0x5c9f0
   [13] op0_ssr(0x14bcdc, 0x2c8e0358, 0x14bce0, 0x14bce0, 0x2c8ddf72, 0x14bce4),
 at 0x7056c
   [14] rpc__dg_execute_call(scall_ = 0x266888, call_was_queued = 0), line 793 in
  "/afs/transarc.com/project/alpine/dev/rel/09.011/src/rpc/runtime/dgexec.c"
   [15] cthread_call_executor(cthread = 0x1d57e0), line 604 in "/afs/transarc.com
 /project/alpine/dev/rel/09.011/src/rpc/runtime/comcthd.c"
   [16] myStartRoutine(tcb = 0x212c98), line 1072 in "/afs/transarc.com/project/a
 lpine/dev/rel/09.011/src/sol_pth/pthread.c"
   [17] _thread_start(0x212c98, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef33d4f4
 (/opt/SUNWspro/bin/dbx)
 The dead lock may be caused by the embedded icl function calls.
 
 jess-Thu, 09 Sep 1993 09:31:13
 Needs to be reproduced by BPG before a diagnosis can be made.
 Pervaze Akhtar
 Mon Sep 13 16:18:01 1993
 pakhtar-Mon, 13 Sep 1993 16:18:09
 ----------------------------------------
 We've seen this twice in the last two days in the DFS self host cell.
 The flserver on vespers starts spinning trying to get ICL log space.
 Of course, it holds a lock on the log, so threads with ICL-ized
 functions eventually pile up waiting for the lock.  Here is a stack
 trace for the spinning thread:
 
   [1] icl_GetLogSpace(logp = 0x1b039c, minSpace = 9), line 127 in "/afs/transarc.com/project/fs/dev/integ/dfs-103/dfs-103-3.33/obj/sparc_solaris2/file/icl/../../../../src/file/icl/icl_log.c"
   [2] icl_AppendRecord(logp = 0x1b039c, op = 704028795, types = 4, p1 = 1076908, p2 = 0, p3 = 0, p4 = 0), line 278 in "/afs/transarc.com/project/fs/dev/integ/dfs-103/dfs-103-3.33/obj/sparc_solaris2/file/icl/../../../../src/file/icl/icl_lo
g.c"
   [3] icl_Event4(setp = 0x1b03e4, eventID = 704028795, lAndT = 17825792, p1 = 1076908, p2 = 0, p3 = 0, p4 = 0), line 68 in "/afs/transarc.com/project/fs/dev/integ/dfs-103/dfs-103-3.33/obj/sparc_solaris2/file/icl/../../../../src/file/icl/i
cl_event.c"
   [4] icl_Event1(setp = 0x1b03e4, eventID = 704028795, lAndT = 17825792, p1 = 1076908), line 105 in "/afs/transarc.com/project/fs/dev/integ/dfs-103/dfs-103-3.33/obj/sparc_solaris2/file/icl/../../../../src/file/icl/icl_event.c"
   [5] SUBIKVOTE_Beacon(rxcall = 0x1d04e0, astate = 1, astart = 750658385, avers = 0xedfcdb68, atid = 0xedfcdb70, voteTime = 0xedfcdb78), line 629 in "/afs/transarc.com/project/fs/dev/integ/dfs-103/dfs-103-3.33/obj/sparc_solaris2/file/ncsu
bik/../../../../src/file/ncsubik/vote.c"
   [6] op0_ssr(h = 0x1d04e0, call_h = 0x21cff8, elt = 0xedfcdcac, drep = 0xedfcdcdc, transfer_syntax = 0xef734e3c, mgr_epv = 0x140640, st_p = 0xedfcdcd4), line 99 in "/afs/transarc.com/project/fs/dev/integ/dfs-103/dfs-103-3.33/obj/sparc_so
laris2/file/ncsubik/ubikvote_proc_sstub.c"
   [7] rpc__dg_execute_call(scall_ = 0x21cff8, call_was_queued = 0), line 793 in "/afs/transarc.com/project/alpine/dev/rel/09.014/src/rpc/runtime/dgexec.c"
   [8] cthread_call_executor(cthread = 0x1ddbe0), line 604 in "/afs/transarc.com/project/alpine/dev/rel/09.014/src/rpc/runtime/comcthd.c"
   [9] myStartRoutine(tcb = 0x1ddd90), line 1072 in "/afs/transarc.com/project/alpine/dev/rel/09.014/src/sol_pth/pthread.c"
   [10] _thread_start(0x1ddd90, 0x0, 0x0, 0x0, 0x0, 0x0), at 0xef2ad364
 I bet we have some sort of memory corruption bug that corrupts the ICL
 log--it is a big target.  Since icl_GetLogSpace() is completely
 dependent on the fact that the log is self-describing, a corrupted log
 can easily cause icl_GetLogSpace() to enter an infinite loop.
 
 There is another bug here, which is that the other flservers in the
 cell don't deal with the situation correctly.  This is really a
 separate bug, so I'm filing a new defect report to describe this more
 completely.
 
 Jeffrey Prem
 Fri Oct 15 08:25:18 1993
 jdp-Fri, 15 Oct 1993 08:25:26
 The second-order bug is described in DB 4500.
 Jeffrey Prem
 Fri Oct 15 08:42:52 1993
 After some poking around in UBIKVOTE_Beacon(), I noticed that we use
 inet_ntoa() all over the place, and in particular, in calls icl_Trace
 calls.   Inet_ntoa() is not a thread-save call, and one could imagine
 all sorts of badness, including ICL log corruption, that would result
 from racing callers of this function.
 Jeffrey Prem
 Fri Oct 15 10:14:30 1993
 jdp-Fri, 15 Oct 1993 10:14:35
**Solution Text**
 jdp-db4294-thread-safe-inet_ntoa
 Jeffrey Prem
 Fri Oct 15 14:27:36 1993
 jdp-Fri, 15 Oct 1993 14:27:40
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9588
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : bos test12 doesn't check remote file
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/16/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Solution Text**
 Have bos test12 actually check for the existence of the NoAuth
 file on a remote server.  I tested this between 2 RIOS and RIOS 
 to HP.
 comer-Tue, 16 Nov 1993 16:31:20

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9587
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOS
Short Description             : bos test0 and runtests script have minor problems
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/11/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Solution Text**
 Made a couple of minor fixes to the bos tests:
 
    runtests was trying to use ksh's $() command substitution in a
    /bin/sh script.  I tested this fix by hand.  We don't typically
    see this because we set TESTTOP explicitly in the invocing shell.
 
    test0 was running some inappropriate tests in -noadmin mode and
    was subsequently failing.  I successfully ran test0 in -admin and
    in -noadmin mode.  We hadn't seen this since we only run the full
    test suite, not the "quickie" test.
 comer-Thu, 11 Nov 1993 10:11:08

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9586
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Stack overflow/watchdog reset in initiating TSR
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/09/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 Stack overflow caused a watchdog reset.  Basically, cm_ConnAndReset called
 cm_RecoverTokenState, which called cm_GetHereToken, which called
 cm_ConnAndReset again, which called cm_RecoverTokenState again.  My best
 guess is that the two cm_ConnAndReset calls were separated by several
 minutes.  My expectation for a fix is to refuse to push the stack deeper,
 but have the recursive cm_ConnAndReset call issue some transient failure.
 cfe-Tue, 09 Nov 1993 13:57:13
**Solution Text**
 Delta: cfe-db4649-no-recursive-TSR
 Backed by dfs-103 3.38
 cfe-Wed, 10 Nov 1993 11:22:27
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9585
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : See description below
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 01/29/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
**Solution Text**
 Delta: cfe-db4577-mutex-when-dephantomizing
 cfe-Thu, 28 Oct 1993 16:54:19
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9584
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Volops need to do HOLD/RELE to ward off inactive processing
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 Bruce Eppinger was running the job generator and moving filesets around.
 This produced EIO and EBADF errors in doing the moves.  The EBADF error is
 what the restoration process generates when its input dump pipe is prematurely
 closed (as the process reading the RPC pipe gets EOF and closes the local
 pipe), so EBADF is a consequence of the EIO error that the dump process
 encountered.  EIO comes from the episode/vnops layer, converting 565575707
 (EPI_E_NOENT) to EIO at two locations: once in vnm_VInit and once in
 vol_efsGetAcl, both when epia_Open fails to open a stashed epifid value.
 
 Conjecture is that there's a process holding a zero-link-count file open
 all through the vnm_StopUse call while a fileset is being taken off-line,
 the volops are dephantomizing that vnode in the course of one or more
 operations, the process calls VN_RELE to free its reference, the inactive
 processing deletes the underlying anode, and subsequent volops fail because
 there's no anode to connect with.
 
 The design for the fix is to have vol_efsScan do a VN_HOLD on the scanned-to
 vnode and to have all relevant places do a VN_RELE.  This will prevent
 the inactive processing from changing the state of a vnode while it's being
 dumped or restored, while the vnode glue will prevent all other operations
 from changing the vnode.
 cfe-Tue, 26 Oct 1993 17:15:37
**Solution Text**
 Delta: cfe-db4568-hold-rele-for-volops
 cfe-Thu, 28 Oct 1993 17:00:45
**Validation Text**

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9583
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Should use uiomove instead of copyout, copyin
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/17/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 Functions epix_GetInline, WriteData, and elbb_ModifySource are trying to
 do uiomove "by hand".  Instead of calling uiomove to copy data, they look
 at the segment type and use it to choose between bcopy and copyout (or between
 bcopy and copyin).
 
 This doesn't work if there are other possibilities for the segment type besides
 USER and SYS.  In particular, when we are processing page faults in HP/UX, we
 call vn_rdwr with segment type PAGEIN.  The switch statement in HP's uiomove
 has at least three branches, and the branch for PAGEIN does something different
 from the branches for USER and SYS.  It doesn't call bcopy or copyout/in.
 
 The most general way to handle this, so as to be portable, is not to try to
 simulate uiomove, but to construct a uio structure on the fly, plug the
 segment type into it, and call uiomove.
 bwl-Wed, 17 Nov 1993 14:37:01
**Solution Text**
 Delta bwl-o-db4703-use-uiomove-more 1.2.
 bwl-Tue, 23 Nov 1993 10:42:51

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9582
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : self-deadlock in writing from mapped memory
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/17/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 Here is an interesting self-deadlock problem arising from the HP/UX VM
 structure.  I don't think it is directly relevant to any of our other
 platforms, but it's something to think about.
 
 A process does a write system call, specifying a buffer in mapped memory.
 (In fact this happened while a core file was being created due to some other
 problem in the mmap test.)  Since the file being written is in Episode, the
 write system call eventually devolves to a call to copyin from
 elbb_ModifySource.  At this time, the buffer lock is held.  The mapped memory
 is not current and so copyin causes a page fault.  The mapped file is also
 in Episode, so the page fault eventually devolves to a call to efs_rdwr from
 vn_rdwr from efs_pagein.  efs_rdwr calls epia_Read, which calls
 buffer_ReadGeneral, which hangs trying to obtain the buffer lock.
 
 The solution is presumably to drop the buffer lock while calling copyin
 (and presumably also copyout).
 bwl-Wed, 17 Nov 1993 12:57:52
 
 In fact, if the file being written to were the same as the mapped file, there
 would be a potential for deadlock on various per-file locks, too.
 
 This leads me to wonder whether the CM has the same problem.
 bwl-Wed, 17 Nov 1993 13:09:18
**Solution Text**
 Delta bwl-o-db4702-hp-buffer-lock-deadlock 1.1.
 bwl-Tue, 23 Nov 1993 10:41:02

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9581
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Make Episode work on AIX
Reported Date                 : 12/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[12/7/93 public]
**Description Text**
 In the course of getting Episode running on SunOS, we have broken
 the AIX version in various ways, particularly with respect to
 vnode synchronization and inactive processing.  The time has come
 to hammer it back into shape.
 
 blake-Thu, 28 Oct 1993 15:29:15

[12/7/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9568
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Description of pathname traversal is wrong in 18.1.5.
Reported Date                 : 12/1/93
Found in Baseline             : 1.0.2
Found Date                    : 12/1/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[12/1/93 public]
In 18.1.5 (pg. 18-8) of the admin guide it is stated that:
	When the Cache Manager traverses a pathname to locate
	a file, it begins at the top-level fileset in your cell
	and accesses the read-only versions of filesets whenever
	possible. However, if any fileset in the pathname has no
	read-only versions, the Cache Manager accesses the read-write
	version and never accesses a read-only version for as long as it
	traverses the remainder of the path.
This is not correct and in direct contradiction with 18.6.1.1 (pg. 18-44):
	When a moint point names a fileset with a .readonly or .backup
	extension, the Cache Manager uses only the specified fileset.
	The Cache Manager never accesses the read-write version of a
	fileset when it encounters a mount point that names a read-only
	or backup version;

[2/28/94 public]
Assigned this CR to Keith Uher (Transarc DFS writer).

[2/28/94 public]
I should change this back to me for the time-being.
Changed Responsible Engr. from `kdu@transarc.com' to `jeff@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[2/28/94 public]
Actually, this sounds correct.  Section 18.1.5 is talking about
the traversal of regular mount points and the bias in the cache
manager toward a R/O fileset if it exists.  Section 18.6.1.1 is
talking about creating an explicit mount point for the .backup or
.readonly (instead of a regular mount point for the R/W as in 18.1.5).
For example, if root.dfs is replicated then access to /.:/fs will
always go to the R/O.  However if an explicit mount point exists,
say /.:/fs/u/dstokes/root.dfs which was created by:
fts crm /.:/fs/u/dstokes/root.dfs root.dfs.readonly
then there is no decision to be made on the part of the cache
manager, it always goes directly to the readonly regardless
of what the parent directory was.

[5/19/94 public]
I edited the descriptions of fileset traversal for the different types of
mount points and filesets.  I found that a considerable amount of rewriting
was indeed possible.  I think the changes introduce much more consistency
and clarify the details a bit more.  The following files were affected:
 
./dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_crmount.8dfs
 
The changes were reviewed by Craig Everhart.  This one is done.
 
Changed Subcomponent Name from `admin_gd' to `dfs_admin_gdref' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Filled in Affected File with `See Description' 
Changed Transarc Status from `open' to `closed'

[09/19/94 public]
Closed bug.



CR Number                     : 9564
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs_config
Short Description             : Uncomments two dfsd
					     invokations for on disk cache case
Reported Date                 : 12/1/93
Found in Baseline             : 1.0.3
Found Date                    : 12/1/93
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/config/rc.dfs, src/config/dfs_config
Sensitivity                   : public

[12/1/93 public]
dfs_config does "enable_in_dfs_rc memcache" for the memory cache case, and
"enable_in_dfs_rc dfsd" for the disk cache case.  The intent is to
uncomment one of two lines in rc.dfs.  If you look at rc.dfs, however,
you'll notice that both lines contain the "dfsd" string.  So the disk cache
case actually ends up uncommenting *both* lines.  The saving grace is that
daemonrunning checks to make sure the daemon isn't already running *and*
the disk cache invokation appears first.  So the memory cache invokation
returns from daemonrunning without doing anything.  I'd suggest some unique
string be placed in a comment at the end of the disk cache invokation line,
and this string be used as the argument to enable_in_dfs_rc for the disk
cache case.

[4/28/94 public]
 Fixed.  To verify Configure a DFS client with on
disk cache.  Verify only one dfsd command was uncommented in
/etc/rc.dfs

[09/27/94 public]
Closed.



CR Number                     : 9563
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : fsync at client does not cause fsync at server
Reported Date                 : 12/1/93
Found in Baseline             : 1.0.2a
Found Date                    : 12/1/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/file/cm
Sensitivity                   : public
Transarc Deltas               : bwl-ot9563-fsync-client-server
Transarc Herder               : jaffe@transarc.com
Transarc Status               : export

[12/1/93 public]
From Ron Arbo:
  It looks like fsync() is broken on DFS.  The chunks get flushed 
  back to the server, but the call to the underlying vnode layer 
  write routine on the server explicitly nulls the ioflag so the
  IO_SYNC flag is unset and the data could remain in the buffer 
  cache (or vm) on the server.
From Bruce Leverett:
  While looking at the code, I noticed that there is a flag, AFS_FLAG_SYNC, in
  the flags argument to AFS_StoreData, which the PX interprets as meaning that
  it should do an fsync after storing the data.  However, there is only one
  call to AFS_StoreData in all the CM (from cm_StoreDCache), and it never sets
  that flag.
From Mike Kazar:
  Yes, it should do storebacks with AFS_FLAG_SYNC set when you're doing an 
  FSYNC.  It doesn't today, but that's simply a bug.  We should really keep 
  track of whether we need to use the AFS_FLAG_SYNC flag on the last chunk 
  storeback, and make sure that we do that when cleaning all dirty chunks.
  Then the fsync vnode operation should set this vnode flag.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/20/93 public]
Defect Closure Form
-------------------
--Other explanation below--
The basic method for determining whether the server is doing an fsync is to
set a breakpoint in the kernel debugger.  Sorry, I don't know of any easier
method.
.
There are three cases of interest:
- The client has modified data.
- The client doesn't have modified data, but has modified status
  (i.e. a call to ftruncate has taken place).
- The client doesn't have modified data or status.  (This doesn't mean
  the client hasn't modified anything; it just means that all its modifications
  have already been passed to the server.)
.
The first case can be exercised by editing a file using vi, which calls
fsync when you save the file and exit.
.
The second case can be exercised by a short C program that calls ftruncate,
and then calls fsync.  Note that the file must not be closed between the
ftruncate and the fsync, because the close will pass all modified status back
to the server, dropping us into the third case (see below).
.
The third case can be exercised by writing a short C program that calls fsync
on an unmodified file.
.
In all three cases a breakpoint set at the server file system's VOP_FSYNC
function (efs_fsync for Episode, xix_fsync for AIX JFS, ufs_fsync for
HP/UX UFS) should be hit.
Associated information:
Tested on TA build:  
dfs-osf-1.12
Tested with backing build:  
dce1.0.3ab1
Filled in Transarc Deltas with `bwl-ot9563-fsync-client-server' 
Changed Transarc Status from `open' to `export'

[4/20/93 public]
Is Transarc willing to contribute this fix to the cause?  
Otherwise, we'll be looking at it here.  Thanks.

[4/20/94 public]
It would be no problem to send you a delta, but I have to wonder if it
would be helpful.  Bruce's work touched many CM procedure interfaces
(it had to, to add flag parameters to several different CM
procedures), so I'm leery of merge conflict stuff.  But if you want
the raw delta, I can ship it over.

[4/20/94 public]
Our fix was completed before the end of 1993, so we should have submitted it
in the last round of individual submits in early 1994.  But apparently it
slipped through the cracks.  Sorry!
.
I am sending the diffs down "under separate cover" so that you can try to make
the same fix there that we did here.  Your sources are probably not too far
divergent from the ones that we made the changes to, so these diffs might not
be too hard to use.  I have to warn you, however, several files were modified,
some functions have new arguments, etc.--it's not a trivial change.
.
BTW, we fixed the problem of fsync at the client causing an fsync at the server,
but the broader problem of fsync at the client causing all other clients to
dump data and status to the server was not addressed.  Perhaps a separate
OT CR?  The obvious thing to do would be for cm_fsync to get a token before
calling cm_SyncDCache.

[4/29/94 public]
I've merged in the changes.  Thanks.  As for the problem you mention
above, it sounds like this additional change is warrants another CR 
if the goal is for DFS to implement true single site semantics. I
guess one would have to know how fsync() is used (or will be used)
by applications in the real-world to understand how important this 
is to them.  If I understand correctly, this problem only applies 
to one client having dirty status and another having dirty data.  The 
token management ensures that you'll never have more than one 
client with dirty status or more than one client with dirty data
for the same file.

[09/27/94 public]
Closed.



CR Number                     : 9559
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_ConnAndReset returns without unlocking server lock
Reported Date                 : 11/30/93
Found in Baseline             : 1.0.3
Found Date                    : 11/30/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_conn.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot9559-cm-unlock-server-lock-on-return-from-ConnAndReset
Transarc Herder               : jaffe@transarc.com

[11/30/93 public]
A recursive call to cm_ConnAndReset can return without unlocking the 
server lock.  Do it.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/30/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Verifying in CHO tests.
Associated information:
Tested on TA build:  
	dfs-osf-1.12
Tested with backing build:  
	dce1.0.3bl10
Filled in Transarc Deltas with 
 `comer-ot9559-cm-unlock-server-lock-on-return-from-ConnAndReset' 
Changed Transarc Status from `open' to `export'

[11/30/93 public]
Is there a more isolated test for uncovering this problem? Or can you explain
why the CHO test is the only one that uncovers it? How long into the CHO
test before it "hits"? Was it consistent? I thought I'd heard you'd achieved
72 hours with CHO so that's why I'm checking - thanks.

[12/7/93 public]
Delta is imported into dfs-osf-1.12 and is in the OSF 120793 drop.
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9555
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_setattr checks access when truncating file
Reported Date                 : 11/24/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/24/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : cm_subr.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[11/24/93 public]
cm_TruncateFile checks that our credentials give us rights to write the file.
This is not right for most of our platforms.  In the AIX, SunOS, and OSF/1
kernels, truncation is checked above the vnode interface.  Checking it below
the vnode interface means that if you open a file with 0 mode, write some
amount of data, and then truncate it using ftruncate, the ftruncate fails.
The following C program tests this.  It succeeds when run on the local file
system under AIX or SunOS, but fails, issuing the error message:
  ftruncate: Permission denied
when run on the local file system under HP/UX.  When run in DFS, it fails on
any platform.  The HP/UX behavior and the DFS behavior are both bugs, since the
man page for ftruncate specifies that ftruncate should succeed if the file is
open for writing.
#include <fcntl.h>
#include <errno.h>
main(){
char buf[8192];
int i;
int f=open("xxx.test",O_RDWR|O_CREAT|O_TRUNC,0000);
if (f==-1) perror("open");
i=write(f,buf,8192);
if (i==-1) perror("write");
i=ftruncate(f,4096);
if (i==-1) perror("ftruncate");
i=close(f);
if (i==-1) perror("close");
}
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/24/93 public]
Is Transarc willing to contribute this to the 1.1 cause?
If not, we'll be looking at it here.  Thanks.

[09/27/94 public]
Closed.



CR Number                     : 9552
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : px
Short Description             : /bin/cp fails if target is on
an exported UFS fileset (unless are root or cell_admin)
Reported Date                 : 11/23/93
Found in Baseline             : 1.0.3
Found Date                    : 11/23/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/xvnode/HPUX/xvfs_vfs2os.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[11/23/93 public]
If you're dce_login'd in as someone other than cell_admin (or, presumably,
root), /bin/cp with a target in an exported UFS fileset fails with a
permission denied (EACCESS) error.
The problem is that /bin/cp creates the target with mode bits of 0, copies
data to it, closes it and then chmod()'s it.  At close time, the CM sends
data along with a request to set the length of the file.  Unfortunately,
the HPUX ufs_setattr() call refuses to change the file length (va_size)
since the "caller" doesn't have write access to the file.
I've implemented a fix in xvnode/HPUX/xvfs_vfs2os.c (xufs_setattr()) which
seems to work:  break out any change to a file's .va_size and do that as
root instead of as the actual client.

[11/24/93 public]
It's not easy to reproduce this at Transarc because our PX optimizes out
most calls of VOPX_SETATTR.
Also, we couldn't reproduce it using ftruncate because of another bug; see
OT CR 9555.
However the following short C program does the trick:
#include <fcntl.h>
#include <errno.h>
main(){
int i;
char buf[128*1024];
int f=open("9552.test",O_RDWR|O_CREAT|O_TRUNC,0000);
if (f==-1) perror("open");
i=write(f,buf,128*1024);
if (i==-1) perror("write");
i=close(f);
if (i==-1) perror("close");
}
If run in a ufs fileset exported from an HP/UX server, both the write and
the close fail due to "Permission denied".
The best way to fix this is not in xvnode, but in the PX.  Calls to
VOPX_SETATTR for the purpose of changing the file's length should be passed
root credentials (osi_setucred()), rather than the credentials they are
given (acredp), on HP/UX only.
Filled in Subcomponent Name with `px' 
Filled in Interest List CC with `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/24/93 public]
Changed fixby field to 103a for tracking purposes. The fixby should only be
set to 1.1 if we've made the decision to set status to "defer" ie. not to
be done in 1.0.3a. The 103a schedule is tight but it would be really nice
to have this fix - what's the consensus?

[12/22/93 public]
xarc thinks they've fixed this; a look at the OSF code supports this;
reassigning to me to verify by testing.

[1/11/94 public]
HP has the fix - we're working with them to get it submitted to
103a (possibly unintegrated tree).  Leaving assigned to me for
tracking.

[1/18/94 public]
We have the fix from HP, thanks Daryl - mbs will be integrating.

[1/31/94]

Integration and test complete.

[2/1/94 public]
Verified fix.  Closing this CR.



CR Number                     : 9531
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Incorrect test for sync IO request for HP
Reported Date                 : 11/22/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot9531-hp-synch-write-test
Transarc Herder               : jaffe@transarc.com

[11/22/93 public]
In HP/UX, to determine whether or not a write request is synchronous, VOP_RDWR
must look in two places.  One is in the ioflag argument, the other is in the
u area.  Episode is only looking in the ioflag argument.  This causes our
version of low/test3 to fail in prog3a.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/22/93 public]
I meant "the uio structure", not "the u area".

[11/24/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Run low/test3 on locally mounted Episode in HP/UX.  If it fails due to
"sync write etime check", the bug isn't fixed.
Associated information:
Tested on TA build:  
dfs-osf-1.11
Tested with backing build:  
dce1.0.3bl10
Filled in Transarc Deltas with `bwl-o-ot9531-hp-synch-write-test' 
Changed Transarc Status from `open' to `export'

[12/7/93 public]
this is in the code drop to the OSF.
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9522
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test
Short Description             : dfs.repfs_checklist needs modifications
Reported Date                 : 11/19/93
Found in Baseline             : 1.0.3
Found Date                    : 11/19/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/test/systest/file/dfs.repfs_checklist
Sensitivity                   : public

[11/19/93 public]

Item B. Verify

 The "fts lsreplicas -fileset epi.1 -all" command should probably be re-run
 with new output listed in the file.  Both the IBM machine and the HP
 machine show (lfs_aggr3) instead of (lfs_aggr1) as the aggregate following
 the src 0,,7 item.

Item C. Test Release Replication

 Might want to add a note in the initial setup paragraph that the user will
 need to do some fts crmounts to get those filesets mounted on /:/epi_1 and
 /:/epi_2.  Might not be obvious to everyone.

Item D. Check Scheduled Replication

 The "fts lsheader -aggregate lfs_aggr3 -server server1" command should
 probably be re-run and the new output listed in the file.  Both the IBM
 machine and the HP machins show (id 5):2 instead of (id 5):3 in the first
 line of the output.

Item E. Disable and Re-enable Read-Only Fileset

 The series:
  cm check
  cd /:/epi_2_ro 
 indicates that if should not be possible to addess the scheduled replica
 because the repserver is down.  This in fact is not true and the repserver
 has nothing to do with being able to access that replica.  As a matter a
 fact, this is just as permissible as the access to the previous replica.

 Also in Item E. before the dfsexport "/dev/rz1e -detach command", a note
 must be made to indicate that the proper amount of time must have elapsed
 before executing this command (ie ~105 minutes after the last deletion from
 the replicated fileset on that aggregate) otherwise this will not work and
 a "Device Busy" message will appear during the detach.  NOTE:  Even this
 does not work since something in the ZLC code is broken, an OT has been
 opened against that).


 Item G.  Is this still valid or not?  You had indicated to us not to run
 this one, so you might want to either delete this entry or reword it to
 indicate what you really want to happen.

[01/19/94 public]
Made all the above changes - thanks for your excellent feedback Cindy!
Submitted so state = fix.

[02/04/94 public]
Closed.



CR Number                     : 9521
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test
Short Description             : dfs.repfldb_checklist needs modification
Reported Date                 : 11/19/93
Found in Baseline             : 1.0.3
Found Date                    : 11/19/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/test/systest/file/dfs.repfldb_checklist
Sensitivity                   : public

[11/19/93 public]

The configuration required for this test should be spelled out more
completely to indicate that in order to complete the 48 hr dfs.glue CHO
test, the DCE core services should be run on a separate machine from any of
the DFS file services.  Therefore, 4 machines will be required for this
test not the optional 3 as the script indicates, 3 FLDB servers and one DCE
core services machine.

[01/26/94 public]
The intro/config requirement section WAS unnecessarily confusing. Fixed
this and other minor errors in checklist.

[02/04/94 public]
Closed.



CR Number                     : 9520
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : wrong scanVolData functions being used
Reported Date                 : 11/19/93
Found in Baseline             : 1.0.2
Found Date                    : 11/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot9520-wrong-scanVolData-functions-being-used
Transarc Herder               : 

[11/19/93 public]
When delta sybase 4507 was imported into this build, the wrong version
of scanVolData (recoverDb.c has two copies of this function, one of which
is ifdef'ed out) was used. This leads to problems when scanning tapes. 
The fix is to ifdef out the wrong function and use the right copy of the
function.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[11/19/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
Dump a volume to tape. Then scan it with scantape. Prior to the fix, 
the scanning should say "Fileset volume empty" for any fileset on the tape. 
After the fix, it should not make this comment. 
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-osf-1.11
Tested with backing build:  
Filled in Transarc Deltas with 
 `khale-ot9520-wrong-scanVolData-functions-being-used' 
Changed Transarc Status from `open' to `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import' 
Added field Transarc Herder with value `'

[12/17/93 public]
Closed.



CR Number                     : 9506
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfsbind isn't happy when given
a path that contains what looks like a non-existant cellname; e.g.: "ls /.../foo"
Reported Date                 : 11/17/93
Found in Baseline             : 1.0.3
Found Date                    : 11/17/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot9506-dfsbind-return-ENOENT-on-unknown-cell-name
Transarc Herder               : 

[11/17/93 public]
If you type "/.../foo" or some such thing, strange things can occur.
This is because dfsbind is returning EINVAL instead of the ENOENT that DFS
would rather see.
This happens because dfsbind's call to sec_id_parse_name() is failing with
sec_rgy_object_not_found ... which dfsbind is not expecting.
I have tried a simple change to dfsbind which seems to work properly.
The fix goes as follows (new code marked by a "|"):
    sec_id_parse_name(..., &code);
    if (code == error_status_ok) {
        ...etc...
|   } else if (code == sec_rgy_object_not_found)
|       ErrnoVal = ENOENT;
    else
        ErrnoVal = EINVAL;

[11/18/93 public]
Filled in Interest List CC with `jaffe@transarc.com' 
Filled in Responsible Engr. with `comer@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/18/93 public]
Okay, I'll bite.  What strange things occur?  I tried this on an HP and on a RIOS
and got "not found" and "does not exist" resp.  One could argue that this ENOENT
should really be returned here from the TTL perspective, but is it really a problem?

[11/18/93 public]
On my HP workstation, I see the following:
	# cd /...
	# foo
	/bin/ksh: foo: Executable file incompatible with hardware
	# ./foo
	/bin/ksh: ./foo: Executable file incompatible with hardware
(although the first example comes and goes for me, the second always fails
this way)
I haven't bothered trying to track down where this is coming from.  My
off-the-cuff guess is that it perhaps has something to do with HPUX CDFs
(context dependent files).
This isn't a crucial bug.  It's more an annoyance (which people here have
noticed and commented on) that should be cleaned up at some convenient
time.  If it can't be fixed now, that's fine ... so long as it isn't
forgotten.  Is there any reason not to "fix" this when things open up again?

[11/18/93 public]
Yes, this is certainly strange.  There's no reason not to fix it.  I
was just trying to understand what 'it' was in the first place.  Does
the fix you suggested take care of the problem?  By the way, which
code base are you looking at?  In the OSF code base, there's already a
check for sec_rgy_object_not_found (to support GDS).

[11/18/93 public]

[11/18/93 public]
The fix I tried seems to fix the problem.  Although I can't make any
guarantees, I spent a few minutes looking through dfsbind and its DFS
caller to make certain there would be no problems (i.e., that dfsbind ended
up returning everything that DFS expected with an ENOENT error).
Our stuff here doesn't have X.500 change that's in the OSF base.  We took
our latest complete drop from OSF in mid-September ... which predates this
change.
Maybe this problem doesn't even exist anymore given this X.500 change
(which, in retrospect, I wish I had looked for prior to opening this OT at
all).  If this is the case, my apologies to all.
Changed Interest List CC from `jaffe@transarc.com' to `jaffe@transarc.com, 
 jean@austin.ibm.com' 
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[11/23/93 public]
This now returns ENOENT in the partial resolution case and in the
case where ROOTLOST is returned (which apparently happens when gdad
is not running).
Defect Closure Form
-------------------
--Verification procedure below--
Ran above procedure on HP and RIOS.  Now get "not found" rather than
"invalid argument".
Tested on TA build:  
	dfs-osf-1.11
Tested with backing build:  
	dce1.0.3bl10
Filled in Transarc Deltas with 
 `comer-ot9506-dfsbind-return-ENOENT-on-unknown-cell-name' 
Filled in Transarc Status with `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9479
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : xvnode/RIOS
Short Description             : RIOS: glue open w/ O_TRUNC fails to get proper tokens
Reported Date                 : 11/11/93
Found in Baseline             : 1.0.2
Found Date                    : 11/11/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-o-ot9479-rios-glue-open-tkns-on-trunc, bwl-ot9479-glue-for-open-with-truncation
Transarc Herder               : jaffe@transarc.com

[11/11/93 public]
If a file is opened through the local access path with O_TRUNC, the
local glue fails to get the write data and status tokens.  This
can lead to loss of cache consistency between the local path and a
DFS client.  For example
cd /:
cp /etc/vfs .
> /localpath/vfs
cat /:/vfs
The cat of /:/vfs will not reflect the fact the vfs has
been trucated to 0 length by the > /localpath/vfs command.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/12/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `bwl@transarc.com'

[11/12/93 public]
I see that Carl also has a delta for this.  Oops.
Though we are not supporting OSF/1 as a reference platform, I modified the
OSF/1 glue code as well, since it had the same bug.
Defect Closure Form
-------------------
--Verification procedure below--
See the original bug description, above.
--Other explanation below--
Note that using ">" as a verb is not supported under csh; I had to use sh.
Associated information:
Tested on TA build:  
dfs-osf-1.9
Tested with backing build:  
dce1.0.3bl8
Filled in Transarc Deltas with `cburnett-o-ot9479-rios-glue-open-tkns-on-trunc, 
 bwl-ot9479-glue-for-open-with-truncation' 
Changed Transarc Status from `open' to `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9472
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 9275
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : ubik doc does not mention admin lists
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.3
Found Date                    : 11/10/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[11/10/93 public]
The documented procedure for adding additional flservers does not
mention admin lists.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/13/94 public]
This defect addresses the need to mention that the DFS server principals of
database servers must be in each other's admin lists (admin.fl for Fileset
Database machines, and admin.bak for Backup Database machines).  The following
files were modified in response to this defect:
 
./dce_books/dfs_admin_gdref/gd/adminkey.gpsml
./dce_books/dfs_admin_gdref/gd/issues.gpsml
./dce_books/dfs_admin_gdref/ref/man4dfs/admin.bak.4dfs
./dce_books/dfs_admin_gdref/ref/man4dfs/admin.fl.4dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bakserver.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/flserver.8dfs
 
The changes were verified by Mike Comer, Craig Everhart, and myself.  This
one can be closed.
 
Changed Subcomponent Name from `ubik' to `dfs_admin_gdref' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Filled in Affected File with `See Description' 
Changed Transarc Herder from `jaffe@transarc.com' to `' 
Changed Transarc Status from `open' to `closed'

[09/22/94 public]
Closed bug.



CR Number                     : 9470
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak dumpinfo -ndumps <number> dumps core
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/01/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Solution Text**
 -------------------------------------------------------------------------------
 
 There was a typo in the dump enumerating function causing it to dereference a
 bad pointer. The fix is a one character fix to bak/commands.c
 
 -------------------------------------------------------------------------------
 
 This seemed like a highly important defect because of the core dump. Hence the
 immediate attention and the HIGH pri. 
 vijay-Mon, 01 Nov 1993 17:05:26
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9469
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : destination aggr id for restoredisk not correct
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 when restoring an aggregate to a different server, the destination aggregate
 ID is of the same name as the original aggregate but could be of different
 ID. The code incorrectly passes the same ID as the original aggregate to the
 restore routine and this would fail if the actual value of the destination
 aggregate's ID is different. This causes restoredisk to fail each time a 
 newserver but not a newaggregate is specified on the command line and if the
 aggregate IDs of the original and destination aggregates are different.
 
 Vijay Anand
 vijay-Sat, 23 Oct 1993 11:46:35
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed in vijay-db4557-bak-restoredisk-incorrect-dest-aggr-id 1.1
 
 Vijay Anand
 vijay-Sat, 23 Oct 1993 15:08:58
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9468
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : bakserver verifydb fails on empty db
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 bak verifydb reports database NOT OK even though there isn't any inherent
 db corruption. Here's how to reproduce it.
 
 1. kill all bakservers in the cell
 2. mv the backup database (bkdb.DB0 and bkdb.DBSYS1) somewhere else
 3. start a bakserver
 4. bak addftfam test
 5. bak rmftfam test
 6. bak verifydb        ( ...will report NOT OK)
 7. bak addftfam test   
 8. bak verifydb        ( ...will report OK)
 
 This behavior breaks a few of my backup tests.
 
 Vijay Anandvijay-Fri, 22 Oct 1993 17:34:54
**Solution Text**
 -------------------------------------------------------------------------------
 
 The fix is to check if we have am empty text data before trying to read the
 text blocks which result in failure. The fix is in ol_verify.c:verifyTextChain
 vijay-Fri, 22 Oct 1993 17:34:58
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9467
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : enable ICL in bakserver
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 It might be useful to enable ICL functionality in bakserver, atleast to trace
 any ubik problems. ICL tracing of bakserver has to be done at a later date.
 
 Vijay Anand
 vijay-Thu, 21 Oct 1993 12:41:29
**Solution Text**
 -------------------------------------------------------------------------------
 
 ICL logging now enabled in bakserver. dfstrace and kill -USR1 both will 
 generate a icl.bakserver. Please note that bakserver code itself does not
 have icl tracing, only the libraries.
 
 Delta: vijay-db4547-bakserver-enable-icl-tracing
 
 Vijay Anand
 vijay-Tue, 26 Oct 1993 12:43:17

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9466
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : two code paths in ubik don't seem to release a held lock
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 Two error code paths in remote.c don't seem to release the version lock held
 before returning from the RPC. This could lead to deadlock in the servers if
 this error path is ever taken. Thanks to Lyle for pointing this out. This 
 must have happenned by a bad cut'n paste in the editor or by a bad merge 
 because this did not show up in a lock review I did a few months ago.
 
 Vijay Anand
 vijay-Wed, 20 Oct 1993 12:38:09
**Solution Text**
 -------------------------------------------------------------------------------
 
 Both these error paths now release the version lock before return. There are
 no explicit tests to verify this fix. The bug was found by code read.
 
 Vijay Anand
 vijay-Tue, 26 Oct 1993 12:39:12

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9465
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKUTIL
Short Description             : bak command dumps core when hostname specified using /.:/hosts short form
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 When trying to add a tape coordinator using the command "bak addhost",
 bak dumps core with an assertion failure if the hostname is specified 
 using the cell-relative short name of the form /.:/hosts/orion.  The 
 core is produced in both the interactive mode as well as the regular 
 command mode of bak.
 
 A sample session that produces the core dump is shown below:
 
 ----------------------------------------------------------------------
 
 **** INTERACTIVE MODE ****
 
 orion.transarc.com# bak
 
 bak> lshosts
 bak> addhost orion 0
 Adding host 'orion.transarc.com' offset 0 to tape list...done
 bak> lshosts
 Tape hosts:
     Host orion.transarc.com, port offset 0
 bak> rmhost 0
 Deleting offset 0 from tape list...done
 bak> lshosts
 bak> addhost orion
 Adding host 'orion.transarc.com' offset 0 to tape list...done
 bak> lshosts
 Tape hosts:
     Host orion.transarc.com, port offset 0
 bak> rmhost 0
 Deleting offset 0 from tape list...done
 bak> lshosts
 
 bak> lshosts
 bak> addhost /.../bak.dce.transarc.com/hosts/orion
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 0 to tape
 list...done
 bak> rmhost 0
 Deleting offset 0 from tape list...done
 bak> addhost /.../bak.dce.transarc.com/hosts/orion 1
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 1 to tape
 list...done
 bak> lshosts
 Tape hosts:
     Host /.../bak.dce.transarc.com/hosts/orion, port offset 1
 bak> rmhost 1
 Deleting offset 1 from tape list...done
 bak> addhost /.../bak.dce.transarc.com/hosts/orion
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 0 to tape
 list...done
 bak> lshosts
 Tape hosts:
     Host /.../bak.dce.transarc.com/hosts/orion, port offset 0
 bak> rmhost  
 Deleting offset 0 from tape list...done
 bak> lshosts
 
 bak> addhost /.:/hosts/orion 0
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 0 to tape
 list...(file/osi/osi_misc.c rev. 4.60 #296 assertion failed)
 osi_Free(239834, 38) from 0: allocation size 41, from 0
 Abort (core dumped)
 
 **** REGULAR COMMAND MODE ****
 
 orion.transarc.com# bak addhost /.:/hosts/orion
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 0 to tape
 list...(file/osi/osi_misc.c rev. 4.60 #296 assertion failed)
 osi_Free(23de6c, 38) from 0: allocation size 41, from 0
 Abort (core dumped)
 orion.transarc.com# rm core
 orion.transarc.com# bak addhost /.../bak.dce.transarc.com/hosts/orion 0
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 0 to tape
 list...failed
 bak: tape coordinator id already exists (dfs / bak) ; Unable to add tape
 host
 orion.transarc.com# bak rmhost 0
 Deleting offset 0 from tape list...done
 orion.transarc.com# bak addhost /.../bak.dce.transarc.com/hosts/orion 0
 Adding host '/.../bak.dce.transarc.com/hosts/orion' offset 0 to tape
 list...done
 
 ----------------------------------------------------------------------
 nanda-Tue, 19 Oct 1993 16:53:54
**Solution Text**
 -------------------------------------------------------------------------------
 
 The size passed to osi_Alloc was more than required, and the size passed to
 osi_Free was the required amount. Corrected the size passed to osi_Alloc to
 just the required amount.
 
 Vijay Anand
 vijay-Wed, 20 Oct 1993 12:44:22
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9464
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : scantape fails on self host cell full dump
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/17/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 Fix known scantape problems.
 
 1. volumeID printed by scantape is wrong
 2. scantape is known to abort on some self-host cell tapes, yet to 
    investigate.
 
 Vijay Anand
 vijay-Sun, 17 Oct 1993 18:03:22
**Solution Text**
 -------------------------------------------------------------------------------
 
 The fileset trailers were written incorrectly by dump, which results in the
 scantape failure. In addition there were several incorrect cases in scantape.
 Both these problems were fixed.
 
 Vijay Anand
 vijay-Wed, 20 Oct 1993 12:15:04
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9463
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak savedb failed
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/09/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 I was runnig dfs 3.34 with three bakservers in the cell and one tape
 coordinator. With one butc window, I ran "bak savedb" command with
 a "null" labelled tape. The butc window tells me
 
 tape accepted
 Dump encountered errors.
 
 And the bak command returned with job 1 failed.jess-Sat, 09 Oct 1993 17:22:59
 
 More info:
 
 I am running dfs 3.33 in the self-host cell, with three bakservers --
 MINYAN and VESPERS [both 3.33] and SARATOGA [rios, IBM product release].
 When I attempt to perform a savedb with a new tape, butc responds:
 
 	Tape accepted - now dumping database
 	writeDbDump: ubik_Call_SingleServer failed
 	; communications failure (dce / rpc)
 	Database dump encountered errors
 	Database dump failed
 	; communications failure (dce / rpc)
 
 Immediately before and after this command, I am able to issue "ftinfo"
 commands which retrieve information from the database...
 Steve Lammert
 Tue Oct 12 12:33:38 1993
 shl-Tue, 12 Oct 1993 12:33:40
**Solution Text**
 -------------------------------------------------------------------------------
 
 an error in the copying of data from the unix pipe to the output buffer of the
 BUDB_DumpDB RPC causes this failure in the server stub resulting in a core 
 dump. This causes the timeout on the client. Also, the ubik_Call_SingleServer
 routine was outdated and had to be merged with the ubik_Call routine.
 
 Vijay Anand
 vijay-Wed, 20 Oct 1993 12:21:11
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9462
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FLDB
Short Description             : udebug return 0 in an error case.
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 udebug /.:/fs testlab20
 return code 382312470 from UBIKVOTE_Debug
 diablo.transarc.com# echo $status
 0
 jess-Wed, 08 Sep 1993 09:24:31

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9461
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : REPSERVER
Short Description             : fts statrepserver server -long returned rpc calltime out
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 The fts repstatserver with "-long" would cause the fts to fail with 
 a rpc time out error. The problem was that repserver tries to do an 
 ICL dump with a null string. 
 
 tu-Mon, 08 Nov 1993 12:06:19
**Solution Text**
 The delta is tu-db4610-fts-statrepserver-long-hung-on-hp 
 
 Changed the way that repserver uses the icldump.tu-Mon, 08 Nov 1993 12:16:10

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9460
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Ignore server threads umask in dummying up an initial ACL
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/21/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 The client's umask is shipped across to the server on symlink, file, dir
 creation. In each of these cases the server threads sets its
 umask to the one sent by the client, but does not restore
 its original umask setting when done.
 
 Hence if the umask at the time of a fts crmount (a symlink creation
 call) is 777, and the umask is then changed to 666, and a request is
 issued to show the initial acl on the parent dir and the parent dir
 does not have an explicit initial ACL, and this request is served by
 the server thread that serviced the fts crmount call, the dummy ACL
 corresponds to a umask setting of 777. This is very confusing for the
 general user.
 
 The ideal fix would be to modify the acl_edit interface to recognize
 that there might not be an ACL at all, i.e. modify the getacl system
 call interface to return an error indicating "Absent ACL" to acl_edit.
 rajesh-Thu, 21 Oct 1993 09:48:51
 Ted suggested modifying efsx_getacl to ignore the server thread's
 umask and use 0777 when conjuring up the initial ACL. And this is
 cleaner than using the server threads umask which has no bearing on
 the issue.
 rajesh-Thu, 21 Oct 1993 13:23:35
**Solution Text**
 rajesh-db4545-conjure-up-fixed-dummy-initial-acls-always
 rajesh-Thu, 21 Oct 1993 18:38:40
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9459
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Check log full conditions more aggressively
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 While running AIM III and concurrent fileset operations (test
 fsetReclone2 w/o modesum threads) via STRIVER, the system paniced.
 
 A reclone fileset operation in process of phantomizing a vnode, tried
 to flush the dirty data to disk. Episode needed to start a transaction
 to allocate the disk data blocks needed to hold the dirty data. The
 attempt to reserve log space for this transaction failed as all the
 log space was already reserved. Hence it tried to resolve the
 situation by trying to flush dirty metadata buffers to pull up the
 tail of the log (which would render log space available for
 reservation). It chose the oldest dirty meta data buffer that had not
 yet been written. Before it could write that buffer, it needed to
 write the log pages being pinnned by that meta data buffer to disk to
 ensure proper recovery in event of a crash. Since the log page in
 question was the current head of the log, it needs a new log page to
 serve as the new head of the log(NewLogPage).  However, all the log
 pages were pinned by dirty meta data buffers, resulting in
 unavailability of a log page to serve as the new head page of the log.
 The system asserts against such a pathological situation from arising
 and hence the panic.
 
 It raised the question about what happened to the 3 log pages that
 serve to prevent the log from really getting completely pinned and
 that are discounted for when reserving log space for transactions and
 that should trigger log full handling whenever the system gets within
 3 log pages of the log getting full.
 
 Ted analysed and found a scenario in which case the log would get
 really full. Elbl_Write, elbl_WriteExtended, WriteLocked do not
 expect the log to be full and go ahead with attempts to write to the
 log anyway.  Elbl_WriteWithBuffer checks if the record it is writing
 would cause the current log page to be filled up. If so it checks to
 see if the log would be near full (accounting for the 3 log pages that
 act as a buffer), and arranges to force log pages out and return
 LOGFULL to the caller so that the caller can do log full processing to
 free up log space. But if the record would not cause the log page to fill
 up, elbl_WriteWithBuffer avoids the log full check.
 
 Given the right timing and load, its possible that a transaction would
 end just after the current log page filled up with regular meta data
 records. Elbl_Write would get a new log page to be used as the new
 head and write the end record for the transaction in the new page. If
 this scenario repeated itself close to the log getting full, we will
 have a situation where all the log pages are really full and all the
 log pages are pinned as the tail has not been pulled up and the next
 call to NewLogPage would panic the system.
 
 The other pathological case that can cause this situation is if a a
 whole number of transactions start and end without doing anything.
 But thats more esoteric and does not need to be fixed now.
 
 Proposed solution is to check for log full condition unconditionally
 in elbl_WriteWithBuffer and fail if the log withing 3 pages of getting
 full.
 rajesh-Fri, 15 Oct 1993 18:31:43
 Defect 4508 has been opened to check for log full conditions on the end
 tran path and when that has been fixed, the changes under this 
 defect would no longer be needed.
 rajesh-Mon, 18 Oct 1993 12:19:24
**Solution Text**
  rajesh-db4506-check-for-full-log-more-aggressively
 
 rajesh-Mon, 18 Oct 1993 12:07:49
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9458
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Reclone fails buffer flags assertion in efs_strategy
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 vnm_StopUse does not pass B_FREE to efs_putpage. As a result the call to
 efs_strategy from efs_putapage fails the assertion that the B_FREE always
 be set.
 
 rajesh-Wed, 13 Oct 1993 13:12:42
**Solution Text**
 rajesh-db4485-vnm_StopUse-and-efs_strategy
 rajesh-Mon, 18 Oct 1993 12:05:40
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9457
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fix merge problem in efs_misc.c
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Fix a wrong merge.
 
 rajesh-Tue, 12 Oct 1993 09:46:50
**Solution Text**
 rajesh-db4469-fix-merge-in-efs_misc.c
 rajesh-Tue, 12 Oct 1993 11:26:05
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9456
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Record checksum length in the log
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Currently (103-3.33) we do not record the checksum length into the log.
 This causes problem when introducing a new checksum type.
 
 rajesh-Mon, 04 Oct 1993 13:10:28
 
 rajesh-Wed, 06 Oct 1993 10:15:42
 
 We need to ensure that we can handle aggregates with unknown checksum
 types and switch to a known checksum type after recovery or attach.
 Also, we should be able to handle aggregates with known chksum types
 but without the checksum length recorded on disk, and start recording the
 checksum length on disk after attach or recovery. While recovering 
 aggregates with unknown checksum we need to ignore checksum checks.
 
 rajesh-Thu, 07 Oct 1993 18:09:26
**Solution Text**
 rajesh-db4421-record-checksum-length-in-disk-log
 rajesh-Thu, 07 Oct 1993 18:09:37
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9455
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Build tool to dump ALL episode log records
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Add capability into readlog to dump the entire episode log.
 
 rajesh-Thu, 23 Sep 1993 15:27:05
**Solution Text**
 rajesh-db4362-build-tool-to-dump-ALL-log-records
 rajesh-Thu, 23 Sep 1993 15:27:10
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9454
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Add comments to code
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/29/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 I use this defect to add comments to clarify things and/or replace
 some meaningless comments.
 
 rajesh-Fri, 15 Oct 1993 09:21:35
**Solution Text**
 rajesh-db4234-add-anode-layer-comments
 rajesh-Fri, 15 Oct 1993 09:23:56
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9453
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : logfull - compute current page usage more carefully
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/27/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 ota:
 
 The new log full checking code moved for defect 4506 is implicitly
 assuming the current page is full.  This used to be true where the check
 was, but is excessively pessimistic where the check is now.
 
 The result is that all records are assumed to require an entire page
 even if they will easily fit on the current log page.  This means that
 we return LOGFULL one block too soon.
 ota-Wed, 27 Oct 1993 08:26:09
 ota-Wed, 27 Oct 1993 08:49:23
**Solution Text**
 ota:
 
 Delta: ota-db4571-consider-room-in-current-log-page
 
 Compare the space remaining on the current page with the size of the
 proposed log record.  If it fits comfortably then only check that the
 log is not overfull.  If it doesn't fit, or would force the a new page
 (because it would completely fill the current page), then calculate how
 many blocks are required all together and check that these blocks are
 available in the log.
 
 This same test is now also used to decide whether do the "easy" or
 "hard" log record insertion.
 ota-Wed, 27 Oct 1993 08:51:26
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9452
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : logfull - avoid unnecessarily returning LOGFULL
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 While investigating db4564 I realized that we might return LOGFULL
 unnecessarily if the current call to ForceAllLogPages actually
 alleviates the log full condition.  Delta
 ota-db3798-elimiate-HandleLogFull-msgs makes these spurious errors more
 of a problem.  Defect 4331 addresses the need to make HandleLogFull more
 comprehensive and it would reduce the concern about this.
 
 Further, however, calling ForceAllLogPages may often be unnecessary
 since simply waiting for Intransit log pages can be sufficient.  Some
 logic more like that in NewLogPage would be appropriate here.  We want
 to avoid writing out partial pages unless absolutely necessary.
 ota-Tue, 26 Oct 1993 14:11:50
**Solution Text**
 ota:
 
 Delta: ota-db4565-try-avoiding-LOGFULL-in-WriteWithBuffer
 
 Progressively try more aggressive technique to resolve the logfull
 condition locally before failing with LOGFULL return code.
 ota-Wed, 27 Oct 1993 16:40:41
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9451
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : logfull - off-by-one error in TRAN_..._SIZE macro
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/26/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 The macro COMPUTE_TRAN_ACTIVE_LOG_SIZE appears to add one in two places
 resulting in the reported size being too large by one.  This causes the
 log to appear to be full too early.  This causes the truncate.test
 script to fail on the 64K block size.  The bug was exposed by delta
 rajesh-db4506-check-for-full-log-more-aggressively which, as the name
 implies, checks for logfull conditions more often.
 ota-Tue, 26 Oct 1993 13:47:20
 
 ota:
 
 Defect 4571 describes an unrelated problem was diagnosed at the same
 time and also blocked the same test from succeeding.
 ota-Wed, 27 Oct 1993 08:37:00
**Solution Text**
 ota:
 
 Delta: ota-db4564-fix-TRAN_..._SIZE-off-by-1
 
 Make COMPUTE_{TRAN,DATA}_ACTIVE_LOG_SIZE have similar form, each adding
 one to the log distance to get the size.
 ota-Wed, 27 Oct 1993 08:37:04
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9450
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : fileset clone fails on full aggregate
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 When an aggregate is really full, a fileset clone will fail.  This will
 effectively prevents moving filesets to alleviate the problem.  But see
 DB 3968 which proposes a method to deal with this shortcoming.
 
 It is proposed that we implement a aggregate space withholding enforced
 (at present) by the reservation mechanism.  This will allow fileset
 clones to use this space but will prevent regular users from getting at
 it.  I propose we withhold 2M (enough for a fileset containing 8000
 files) or 1% which ever is greater.
 
 This will remove almost all uses of the sick vnode list due to
 reservation errors (see DB 4359).  But it would still be useful, at
 least in principle, for disk errors.
 
 This withholding should eventually be checked in more places so that
 directory operations, file creates, and all operations on non-VM systems
 can also preserve the withholding.  Only fileset clones would have
 access to this storage.
 ota-Wed, 20 Oct 1993 09:37:09
 
 ota:
 
 It has been suggested that a better withholding values is no more than
 10% and no less than 1%.
 ota-Wed, 20 Oct 1993 12:53:39
 
 ota:
 
 More comprehensive enforcement of the withholding postponed to DB 4542.
 ota-Wed, 20 Oct 1993 16:42:30
**Solution Text**
 Delta: ota-db4525-impose-space-withholding r1.5
 
 ota:
 
 Prevent aggregates from getting full by causing reservations to fail
 when the number of free blocks drops below a specified threashhold.
 This will still allow fileset clones even when growing files fails due
 to lack of space.
 
 We only enforce this withholding in epib_Reserve.  This has a number of
 limitations as outlined in the defect.
 
 Cause epib_Reserve to fail if the number of free blocks is less than a
 specified withholding.  The withholding is specified as a target value
 (in K) and floor and ceil values expressed as fraction of the
 aggregate size (in mills).  These default to 2000K, 1% and 10%.
 ota-Wed, 20 Oct 1993 16:42:38
 
 ota:
 
 ota:
 
 ota:
 
 Revision 1.3 changes the values reported for the aggregates free space.
 
 Here is some output illustrating this new limit.  Note that the
 aggregate in question has 2717 fragment available only in partial
 blocks.  This amounts to a little over 4% of the total maximum space in
 the aggregate.  The default setting of epib_leastWithheld was 1% (656K)
 and of epib_mostWithheld was 10% (6576K).
 
 # echo 'epib_withholding/W 0t2000' | adb -kw /dev/ksyms /dev/mem
 physmem 2df3
 epib_withholding:               0x0             =       0x7d0
 # fts lsq .
 Fileset Name          Quota    Used  % Used   Aggregate
 test.emacs            40000   31373    78%    88% = 56531/63824 (LFS)
 # fts aggrinfo galileo epi1
 LFS aggregate epi1 (/dev/dsk/epi1): 7293 K free out of total 63824 (2000 reserved)
 #
 # echo 'epib_withholding/W 0t0' | adb -kw /dev/ksyms /dev/mem
 physmem 2df3
 epib_withholding:               0x4e20          =       0x0
 # fts lsq .
 Fileset Name          Quota    Used  % Used   Aggregate
 test.emacs            40000   31373    78%    86% = 56531/65168 (LFS)
 # fts aggrinfo galileo epi1
 LFS aggregate epi1 (/dev/dsk/epi1): 8637 K free out of total 65168 (656 reserved)
 #
 # echo 'epib_withholding/W 0t20000' | adb -kw /dev/ksyms /dev/mem
 physmem 2df3
 epib_withholding:               0x0             =       0x4e20
 # fts aggrinfo galileo epi1
 LFS aggregate epi1 (/dev/dsk/epi1): 2717 K free out of total 59248 (6576 reserved)
 # fts lsq .
 Fileset Name          Quota    Used  % Used   Aggregate
 test.emacs            40000   31373    78%    95% = 56531/59248 (LFS)
 
 For comparison SunOS5 UFS shows 10% reserved:
 # fts aggrinfo apollo /usr
 Non-LFS aggregate /usr (/dev/dsk/c0t3d0s6): 16656 K free out of total 112313 (12470 reserved)
 
 While AIX JFS shows none:
 # fts aggrinfo saratoga /usr
 Non-LFS aggregate /usr (/dev/hd2): 2024 K free out of total 163840
 ota-Fri, 22 Oct 1993 10:16:18
 ota-Mon, 25 Oct 1993 16:31:20
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9449
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage - punt when mods to bitmap block map required
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 When repairing a short aggregate it is likely that the blockmap of the
 bitmap itself will point to disk blocks that are no longer present.
 These will be removed during an early stage of recovery.  However, since
 returning such bitmap to full consistency is somewhat involves we are
 not prepared to do this.  As a consequence we will get persistent
 (unrecoverable) errors.
 
 Instead of just reporting this and bailing out we lumber along and
 eventually get an unexpected error when trying to repair a damaged
 directory.  This crashes the salvager.
 ota-Fri, 15 Oct 1993 09:14:24
**Solution Text**
 ota:
 
 Delta: ota-db4501-detect-serious-bitmap-errors
 
 Detect when the bitmap has been modified during the scan pass, and punt
 any future bitmap checks and all repairs.
 ota-Fri, 15 Oct 1993 13:58:50
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9448
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage - uses NULL b in VerifyAnodeIBlocks
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 If the backing anode is damaged and the COW anode has indirect blocks
 the code in VerifyAnodeIBlocks will still reference through the NULL
 anode ptr.  On AIX this is harmless since the resulting value isn't used
 when b is NULL.  On the Sun, however, it generates a SEGV.
 ota-Tue, 12 Oct 1993 16:21:01
**Solution Text**
 ota:
 
 Delta: ota-db4473-check-NULL-backing-anode
 
 Only check backing anode block addresses if the backing anode is present.
 ota-Wed, 13 Oct 1993 10:42:29
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9447
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : mt - CompleteEC must not reference tP after unlock
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 In CompleteEC we are traversing the ecList and calling CompleteTran then
 TranUnlock on each one.  However, the looping structure references the
 next field of the just-unlocked tran.  This can be unsafe.
 ota-Thu, 16 Sep 1993 13:24:05
**Solution Text**
 ota:
 
 Delta: ota-db4338-safely-iterate-over-trans-in-CompleteEC
 
 Adding an explicit osi_Reschedule after the unlock caused test_anode
 running many_files_simultaneously.test to panic immediately.
 
 The fix is to remove each tran from the EC before processing it.
 ota-Thu, 16 Sep 1993 13:42:58
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9446
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : mt - initialize us_Complete lock safely
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 ota:
 
 The lock protecting the incomplete array is initialized in the handler
 thread.  If this thread doesn't get started promptly a request may be
 queued which will grab the lock before it is initialized.  This also
 means the completer thread could stomp in and reset the lock without
 warning.
 ota-Thu, 16 Sep 1993 13:09:42
 
 ota:
 
 Another explanation for this problem is that the osi_lock_pthread module
 was not thread safe during initialization.  Defect 4470 addresses this.
 ota-Tue, 12 Oct 1993 13:55:23
**Solution Text**
 Delta: ota-db4337-safely-init-us_io-lock r1.2
 
 ota:
 
 Move lock initialization into Init procedure that is called from us_open
 which, hopefully, is called during single threaded setup.
 ota-Thu, 16 Sep 1993 13:19:19
 
 ota:
 
 Alas, no!  The first revision did not have the desired effect because I
 was actually starting the us_Complete thread far earlier in the
 initialization process than I realized.  Thanks to Srivas and Blake for
 pointing this out.
 
 The fix is simply to move the thread create to the end of the
 initialization phase.  Also create an explicit us_Init function and call
 it before the thread create, too.
 ota-Tue, 12 Oct 1993 13:55:43
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9445
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : don't disable tkc log by default
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Solution Text**
 kazar-db4587-enable-tkc-log-by-default
 Mike Kazar
 Fri Oct 29 15:25:42 1993
 kazar-Fri, 29 Oct 1993 15:25:45
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9444
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : See description below
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/27/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 If the fileset or the aggregate run out of space the cache manager
 will be left with dcache chunks that are marked dirty but can't be
 written back. If this happens to enough files the cache will be full
 of such dirty chuncks and the cache manager will effectively deadlock.
 Dimitris Varotsis
 Wed Oct 27 12:54:45 1993
 dimitris-Wed, 27 Oct 1993 12:54:55
**Solution Text**
  kazar-db4573-fix-markbadscache-again
 Mike Kazar
 Fri Oct 29 15:24:32 1993
 kazar-Fri, 29 Oct 1993 15:24:35
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9443
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm page deadlock in uiomove
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Solution Text**
 kazar-db4478-fix-cm-uiomove-page-deadlock
 Mike Kazar
 Wed Oct 13 11:57:56 1993
 kazar-Wed, 13 Oct 1993 11:57:59
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9442
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : TKM takes forever to grant tokens
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/08/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 When there are lots of outstanding, non-conflicting tokens,
 still TKM takes forever to grant one.  With 500 tokens, for example,
 TKM took 3 seconds (!) on a Sparc 2 to grant a new token.
 Mike Kazar
 Fri Oct  8 17:37:24 1993
 kazar-Fri, 08 Oct 1993 17:37:29
**Solution Text**
  kazar-db4460-speed-up-token-compat-checks 1.3
 
 Mike Kazar
 Tue Oct 12 16:33:27 1993
 kazar-Tue, 12 Oct 1993 16:33:30
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9441
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : DFSEXPORT
Short Description             : bad trap in vol_Detach
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/20/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Took bad trap while running dfsexport -detach. Console:
 
 	BAD TRAP
 	dfsexport: Data Fault
 	kernel read fault
 	sync error reg80<INVALID>
 
 Stack trace:
 
 	die
 	trap
 	fault
 	vol_efsDetach
 	vol_Detach
 	vol_PurgeDesc
 	vol_PutDesc
 	ag_UnRegisterVolume
 	afscall_aggr
 	afs_syscall
 	syscall
 gait-Wed, 20 Oct 1993 15:08:25
**Solution Text**
 jdp-db4538-do-volume-detach-once
 Jeffrey Prem
 Mon Oct 25 10:02:31 1993
 jdp-Mon, 25 Oct 1993 10:02:34
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9440
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : No reservation when promoting inline files
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/28/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 When promoting a fragmented file, Episode fails to reserve the needed
 space.  Imagine a write of a 128 byte file in two 64 byte pieces.
 During the first call to efs_vmwrite, the call to segmap_getmapflt
 causes efs_getpage to be called which tries to reserve bytes 0 through
 64, but since all 64 bytes fit inline, no reservation is necessary.
 During the second call to efs_vmwrite, the affected page is already
 mapped in, and efs_getpage is never called.  Since all reservation is
 currently done in efs_getpage, we fail to call efs_reserve at this
 critical time where an inline file is being promoted to a fragmented
 (or blocked) file.  The result is that calls like stat(2) and
 VOL_GETSTATUS don't show the correct block usage until the space is
 actually allocated.
 
 To reproduce this bug create a 128-byte file, F.  Use dd to copy F to
 a new file, F1, in two 64 byte chunks.  Use "ls -ls" to view F1
 immediately.  The "blocks used" should be 0.
 
 # ls -ls F
    2 -rw-rw-rw-   1 root     wheel        128 Oct 28 09:58 F
 # dd bs=64 if=F of=F1; ls -ls F1
 2+0 records in
 2+0 records out
    0 -rw-rw-rw-   1 root     wheel        128 Oct 28 09:58 F1
 
 This bug causes the ZLC tests to fail, because they expect to see the
 fileset's quota usage change when creating and deleting files.
 
 Jeffrey Prem
 Thu Oct 28 11:14:14 1993
 jdp-Thu, 28 Oct 1993 11:14:24
**Solution Text**
 dimitris-db4579-more-reservation-problems
 
 Dimitris Varotsis
 Fri Oct 29 17:50:28 1993
 dimitris-Fri, 29 Oct 1993 17:50:31

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9439
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Make calls to q_Update to not check for quota errors
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Given the number of errors that have been found in the reservation
 code lately we will let IO proceed even we find at that point that
 this will put us over quota. We still do quota checks but only in 
 the reservation path, so if the reservation code is correct we should
 never run over quota.
 Dimitris Varotsis
 Wed Oct 13 11:16:18 1993
 dimitris-Wed, 13 Oct 1993 11:16:25

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9438
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Disk is full but VM has dirty data
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 In this case the aggregate is almost full and as we put data into it
 our space reservation does not appear to be accurate so we run out of 
 disk space but still hold dirty data in VM. This bug is different than
 4358.
 Dimitris Varotsis
 Wed Sep 29 15:03:09 1993
 dimitris-Wed, 29 Sep 1993 15:03:13
**Solution Text**
 blake-db4399-write-protect-unreserved-reads handles the portion of
 the fix concerned with read-ahead, by the simple expedient of disabling
 it.
 blake-Mon, 18 Oct 1993 16:50:03
 
 Entered as OT 9419
 
 andi-Wed, 10 Nov 1993 10:41:02
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9437
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : butc assertion failure.
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/07/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 I have a butc window there and I type bak dump command from other window.
 
 We have the message from butc window:
 
 Task 3: dump (cellset.thursday) Started
 Pass 1: Fileset fileset.16(0,,2649) successfully dumped
 Pass 1: Fileset fileset.15(0,,2646) successfully dumped
 Pass 1: Fileset fileset.14(0,,34) successfully dumped
 Pass 1: Fileset fileset.13(0,,31) successfully dumped
 End of pass 1: Filesets remaining = 0
 Task 3: dump (cellset.thursday) Finished
 Task 3: dump (cellset.thursday) successful : 4 filesets dumped
 (file/osi/osi_misc.c rev. 4.60 #296 assertion failed) osi_Free(28c064, 180) from 0: allocation size 720, from 0
 Abort
 
 From bak command we have:
 
 paradiso.transarc.com# bak dump cellset /cellset/thursday
 Starting dump of fileset family 'cellset' (dump level '/cellset/thursday')
 Total number of filesets : 4
 No parent dump cellset.cellset, looking for a higher level dump
 Doing level 0 dump due to missing higher level dumps
 Preparing to dump the following filesets:
         fileset.16 (id: 0,,2649; clone date: Wed Oct  6 18:57:00 1993)
         fileset.15 (id: 0,,2646; clone date: Wed Oct  6 18:57:00 1993)
         fileset.14 (id: 0,,34; clone date: Wed Oct  6 18:57:00 1993)
         fileset.13 (id: 0,,31; clone date: Wed Oct  6 18:57:00 1993)
 Starting dump
 bak: Dump ID of dump cellset.thursday : 3
 
 The command never ruturned.
 jess-Thu, 07 Oct 1993 10:14:25
**Solution Text**
    In the procedure FreeNode, two calls are made to osi_Free to free the
 linked lists of structures for dumps and restores.  FreeNode erroneously
 called osi_Free with a size of one structure instead of that structure's
 size times the number of elements in the linked list.
    The solution is to make the calls to osi_Free with the correct size,
 sizeof(struct <structure> * curPtr->arraySize).
 
    There were also two similar calls to osi_Free in procedure GCNodes but
 this procedure is obsolete and not used.  There use to be a thread started
 that would occasionally (every 60 minutes) check to see if there were any of
 these dump/restore nodes left around that needed to be cleaned up and free
 them.  This thread is no longer used and hence all references to GCnodes has
 been purged.
 
 CHANGE file/butc/list.c
 1.  In FreeNode corrected calls to osi_Free for dumps and restores.
 2.  Purged GCNodes related code.
 
 CHANGE file/butc/tcmain.c 
 Purged GCNodes related code.
 davecarr-Tue, 12 Oct 1993 14:56:04
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9436
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : fs tests have problem with diffs
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9435
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOS
Short Description             : bos test11 and test12 need to use remsh for hp
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Solution Text**
 Changed the tests to set a variable $RSH based on operating
 system to either rsh or remsh.  Tested this on both HP and RIOS
 platforms.
 comer-Wed, 27 Oct 1993 15:57:12

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9434
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : bosserver does not create directory with correct owner
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 The bosserver creates the appropriate directories under dcelocal if
 they do not already exist.  Currently, however, it does not set the
 owner of the directory appropriately.  So, subsequent calls to
 CheckDirs will return an error.
 comer-Wed, 20 Oct 1993 09:39:11
**Solution Text**
 I added a new function called GetRequiredOwner() that reads the proper
 owner from the permissions table.  The value returned by this is
 passed to chown when the directorry is created.
 
 I tested this on a dfs-103-3.33 build:
 
 	o kill bosserver and it's children
 	o mv /opt/dcelocal/bin /opt/dcelocal/bin.old
 	o start bosserver
 	o make sure /opt/dcelocal/bin is owned by proper owner
 		(bin on Solaris, root on AIX ref port)
 
 comer-Wed, 20 Oct 1993 10:31:25

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9433
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TKM
Short Description             : Need defenses against hosts holding and keeping too many tokens
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 The IBM product release has a bug wherein it gets new HERE tokens every few
 seconds and doesn't allow them to be revoked, that it apparently gets into
 sometimes when a fileset is moved.
 This bug is defense for the server against such a rogue client.
 cfe-Thu, 14 Oct 1993 16:31:06
**Solution Text**
 Delta: cfe-db4496-defend-server-against-many-tokens
 Backed by dfs-103 3.35
 cfe-Fri, 15 Oct 1993 12:34:14
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9432
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : CM grabs and keeps too many HERE tokens
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 The CM doesn't keep track of the token ID for its HERE token (the one
 that it wants to keep), so it has trouble knowing whether to let one be
 revoked.
 
 Also, the CM is keeping HERE tokens for filesets that it no longer cares
 about.
 cfe-Thu, 14 Oct 1993 13:27:52
**Solution Text**
 Delta: cfe-db4495-cm-let-here-tokens-be-revoked
 Backed by dfs-103 3.35
 cfe-Thu, 14 Oct 1993 16:24:06
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9431
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_RevoverSCacheToken calls cm_QueueAToken without completely specifying the token
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 From Mike Kazar:
 
 Near the very end of cm_RecoverSCacheToken, we call cm_QueueAToken
 (about 8 lines back from the end).  We pass in &tokenl as the 2nd 
 parameter, but while we've filled in the token ID, we need to fill 
 in the token.type and token.expirationTime fields, or else the token 
 will not get returned properly (it won't know what types to return, 
 and if it looks expired, it won't try to return it at all).
 Dimitris Varotsis
 Thu Oct 14 07:47:32 1993
 dimitris-Thu, 14 Oct 1993 07:47:37
**Solution Text**
 Delta: cburnett-ot9114-rcvrscp-init-4-tkn-rtn
 From OT 9114, clearly
  cfe-Fri, 15 Oct 1993 12:29:13
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9429
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : TSR did not recover the lock hold by a client.
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Three machines configured by 3.35 (M1, M2, and M3). M1 and M3 were running
 ITL tsr script to try lock (M1 held the lock and M3 was trying). After a reboot
 of M2 where the fileset the test file located exported, M1 still held the
 lock but M3 can get the lock. 
 
 jess-Wed, 13 Oct 1993 13:59:15
 
 ----------------------------------------------------------------------
 
 So far, this is correct.  If M2 crashes, M1 should discard its lock
 tokens and mark the file bad with ESTALE; M3 should be able to get the
 lock.
 
 Basically, lock tokens are treated specially.  As a long-lived token,
 for M1's CM to claim that the lock continues to be held throughout the
 crash, it would have to know things that M2's implementation can't yet
 tell it--designed and architected, but not implemented--so the upshot
 is that CMs lose lock tokens when a server crashes.  The test should
 be that M1 marks the file with ESTALE.  This is one of the things
 tested in Tu's hand tests.
 
 I'm marking this as RETURNED, for clarification as to whether Jess et al.
 still think there's a bug.
 
 cfe-Thu, 14 Oct 1993 09:03:45
 cfe-Thu, 14 Oct 1993 09:05:03
 
 -------------------------------------------------------------------------------
 
 Yes, Craig is right. M1 did get a ESTALE after M2 is rebooted. Then we
 have to change our test scenario accordingly. 
 
 Now let talk about a related problem that is the case of network partition.
 When disconnecting M2 for 105 seconds and then putting it back, M1 held
 the lock and everything was ok with M1 (No ESTALE returned) but M3 got the
 lock. It is ok to disconnect M2 for 30 seconds. Craig could you confirm if
 this is a problem or not. If it is I am going to reopen the defect.
 jess-Thu, 14 Oct 1993 11:59:39
 
 ---------------------------------------------------------------------------
 
 [cfe 14 Oct 1993]
 It should never be the case that M1 and M3 have conflicting locks and neither
 one returns ESTALE.  I'm not sure what the problem is.  To diagnose this,
 it would be most helpful to have dfstrace results (of the cmfx log) for all
 three machines immediately after the discrepancy is noted.  Thanks.
 
 cfe-Thu, 14 Oct 1993 13:01:25
 
 ---------------------------------------------------------------------------
 
 [cfe 15 Oct 1993]
 
 Some more diagnosis.  The server that has the token goes down and the client
 holding the lock (M1) doesn't do partition-TSR until after the lock-grabber
 (M3) does TSR and grabs the lock.  Thus, the server has legitimately marked
 M1 as down, granting the token.  What's bogus is that if the first call
 that M1 does is to release the lock, the file isn't marked with ESTALE;
 TSR happens but the file has no locks (by the time releaselockf tries to
 store the lock token back), so nothing is upset when the lock token cannot
 be renewed.  Unfortunately, the application cannot tell that the lock had
 been lost some time earlier; nothing marked the file with ESTALE.  Had TSR
 taken place some other time than while releasing the lock, this condition
 probably would not have happened.
 
 The CM had indeed marked the server as down.
 cfe-Fri, 15 Oct 1993 15:24:12
 
 ---------------------------------------------------------------------
 
 [cfe 10/18/93]
 The way to fix the problem is probably to check, when doing any lock work
 and particularly doing an unlock, that the CM can guarantee tokens--that
 the server isn't in TSR mode and there's been a successful RPC to the
 server within the server's HostLifetime.  If there hasn't been, then the
 CM should force an RPC (and concomitant possible TSR work) before proceeding
 with the lock.
 
 However, the window in which this problem seems to occur is pretty small,
 and is not as pressing a problem as it first appears.  That is, it seems to
 be a special case of TSR not working, not a general case.
 cfe-Mon, 18 Oct 1993 09:28:38
**Solution Text**
 Delta: cfe-db4486-ignore-tokens-on-old-hosts
 cfe-Thu, 21 Oct 1993 09:24:07
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9428
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : CM does not set its RPC timeout low for server probes
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 The code in the cache manager that probes servers that it thinks are down
 neglects to lower the RPC timeout in the case that it's making a connection
 to a file exporter.  The AFS_SetContext call from cm_ConnByHost is made
 with the normal RPC timeout rather than a shorter one.
 cfe-Wed, 13 Oct 1993 11:57:05
**Solution Text**
 Delta: cfe-db4484-short-timeout-on-checkdownservers
 Backed with dfs-103 3.35
 cfe-Wed, 20 Oct 1993 14:34:02
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9426
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : file append gets ESTALE
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 This is a problem introduced with the TSR protocol revs that attempt
 to handle the situation where an AFS_SetContext is called for the
 first time to a server from the secondary interface. In order for
 these changes to work AFS_SetContext calls had to be serialized. This
 has the side effect that it can cause an AFS_SetContext call in the
 primary interface to block an AFS_SetContext call in the secondary
 interface. Since calls on the primary interface are not protected from
 thread starvation this is bad news.
 Dimitris Varotsis
 Tue Oct 19 11:01:00 1993
 dimitris-Tue, 19 Oct 1993 11:01:06
**Solution Text**
 Delta: cfe-db4482-move-servermutex-to-connandreset 
 Backed by dfs-103 3.35
 cfe-Tue, 19 Oct 1993 15:40:48
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9425
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CMD
Short Description             : diskless DFSD initialization hangs on Sparc 1 box
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Diskless cache initialization fails (by hanging forever) on a Sparc 1 box.
 The dfsd process hangs in the kernel when it executes the CMOP_GO function.
 cfe-Tue, 12 Oct 1993 15:36:25
**Solution Text**
 Delta: cfe-db4472-fix-diskless-hang
 Backed by dfs-103 3.34
 cfe-Wed, 13 Oct 1993 09:38:46
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9424
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : XAGGR
Short Description             : Failed fileset destruction never gets cleaned up
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Srinivas was running ping-pong fileset moves and something went wrong;
 his NLSPATH/LANG variables weren't set, so it is difficult, and maybe
 impossible, to determine what the underlying problem was.  However,
 whatever happened when he moved a fileset from machine A to machine B
 left a struct volume on machine A that had the following properties:
 	- ref count of 1
 	- activeVnops count of 0
 	- no threads referring to the volume
 	- states of 0x3010059, meaning:
 		-- VOL_RW
 		-- VOL_OFFLINE
 		-- VOL_DELONSALVAGE
 		-- VOL_DEADMEAT
 		-- VOL_TYPE_RW
 		-- VOL_MOVE_SOURCE
 		-- VOL_ZAPME
 ``fts lshead'' didn't report the volume: it had no on-disk representation,
 but only a kernel memory structure.  It was in the volume registry--in fact,
 that was its only reference.
 
 The problem is that this kernel memory structure never goes away, and while
 it's around (until server reboot), it prevents the moved fileset from being
 moved back to the corrupted server.  Those attempts fail with an odd message
 from fts about being unable to check for the presence of the given fileset
 on the target server, since it's getting VOLERR_PERS_DELETED back as an error
 code (because of VOL_DEADMEAT above) whenever it wants to open the fileset.
 Detaching and reattaching the aggregate doesn't help, since there's no disk
 representation of the fileset and so it won't be removed.
 
 Here's the relevant section of FtLog:
 
 1993-Oct-08 09:39:42 Dumping 1:fs80 (0,,171)
 1993-Oct-08 09:39:55 Destroying 1:fs80 (0,,171)
 1993-Oct-08 09:43:09 trans 47a0c0 (Id=1576, 0,,171/1) is 187 seconds old (ref count 0)
 1993-Oct-08 09:43:09 (trans 47a0c0: desc=0, time=750087602, ctime=750087593, states=0x0, accs=0x1, acce=691089410)
 1993-Oct-08 09:43:39 trans 47a0c0 (Id=1576, 0,,171/1) is 217 seconds old (ref count 0)
 1993-Oct-08 09:43:39 (trans 47a0c0: desc=0, time=750087602, ctime=750087593, states=0x0, accs=0x1, acce=691089410)
 1993-Oct-08 09:44:09 trans 47a0c0 (Id=1576, 0,,171/1) is 247 seconds old (ref count 0)
 1993-Oct-08 09:44:09 (trans 47a0c0: desc=0, time=750087602, ctime=750087593, states=0x0, accs=0x1, acce=691089410)
 1993-Oct-08 09:44:39 trans 47a0c0 (Id=1576, 0,,171/1) is 277 seconds old (ref count 0)
 1993-Oct-08 09:44:39 (trans 47a0c0: desc=0, time=750087602, ctime=750087593, states=0x0, accs=0x1, acce=691089410)
 1993-Oct-08 09:45:09 trans 47a0c0 (Id=1576, 0,,171/1) is 307 seconds old (ref count 0)
 1993-Oct-08 09:45:09 (trans 47a0c0: desc=0, time=750087602, ctime=750087593, states=0x0, accs=0x1, acce=691089410)
 1993-Oct-08 09:45:09 trans 47a0c0 (Id=1576, 0,,171/1) is 307 seconds old (ref count 0): GCing
 1993-Oct-08 09:46:55 Failed to open 1:0,,171, code = 691089410 (cannot create socket (dfs / xvl))
 1993-Oct-08 09:47:03 Failed to open 1:0,,171, code = 691089410 (cannot create socket (dfs / xvl))
 
 It looks like the FTSERVER_DeleteVolume() call was pretty much successful,
 but that the transaction/open volume then just timed out and didn't remove
 the fileset from the registry.  That is, VOL_CLOSE() never got called.
 cfe-Fri, 08 Oct 1993 11:03:42
**Solution Text**
 Delta: cfe-db4457-cleanup-destroyed-volumes-too
 Backed by dfs-103 3.34
 cfe-Fri, 08 Oct 1993 17:13:15
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9423
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Erroneously setting MTIME to now on file restoration
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
    []	efs_vattr.c / vnva_SetAttr()
 
 	If vnva_SetAttr() is told to change both the size (.va_size != -1)
 	and MTIME (.va_mtime.tv_sec != -1, 0):  it FIRST sets the MTIME
 	to the specified value (.va_mtime) and THEN it sets the MTIME
 	to the current time (by virtue of having or'd EPIF_SFLAGS_MTIME
 	into markmask).  Is this intentional?  I would have expected that
 	either (a) epif_Mark() would be called before epif_SetStatus()
 	or (b) the EPIF_SFLAGS_MTIME flag would be cleared from markmask
 	by the code which decides that the MTIME is going to be explicitly
 	set.
 
 This doesn't look right to me either.  In other words, it seems as
 though a volume restore would always set every file's mtime to now.
 Am I wrong? (bwl)
 
 Right--looks like a bug. (cfe)
 cfe-Tue, 24 Aug 1993 14:05:23
**Solution Text**
 Delta: cfe-db4184-cannot-restore-mtime 
 Backed by dfs-103-3.34
 cfe-Mon, 11 Oct 1993 17:45:49
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9422
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : panic in efs_getlength
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/25/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 efs_getlength has recently been rewritten to make fairly strong assumptions
 about what's valid in the vnode.
 
 One problem is that it asserts that there is a non-null anode pointer.  This
 doesn't work in AIX because there can be dirty pages in VM corresponding to
 closed files, and we are closing the anode when we close the file.  This
 problem can be fixed simply by weakening the assertion, so that in AIX it
 asserts that EITHER the anode is non-null OR the VD_LENVALID bit is set.
 
 A more obscure problem is that phantomization will create vnodes in which
 there is neither a non-null anode nor a valid vd_len.  The "right" way to
 fix this is to ensure that these vnodes will never be passed to efs_getlength,
 i.e. the AIX strategy daemon must check for a busy fileset BEFORE calling
 efs_getlength.
 
 Solving the first problem is a high priority right now because it is preventing
 us from doing any non-trivial testing on RIOS.  So, we will solve it first,
 and then at leisure solve the second problem, which will take longer because
 it is noticeably more than a one-line change.
 bwl-Mon, 25 Oct 1993 16:57:34
 
 The above note doesn't tell the whole story.  efs_inactive used to obtain the
 length of the file and store it in vd_len, and set VD_LENVALID.  This enabled
 the AIX strategy daemon to call efs_getlength on closed vnodes without rousing
 them.  The point of this is that the strategy daemon calls efs_getlength in
 the course of sorting the buffers onto a list.  The interval between this and
 actually processing the I/O request may be long, and it would not be a good
 idea to hold the vnode "roused" during that time, nor would it be good for
 performance to rouse it twice, once during sorting and once for I/O.
 
 However, efs_inactive doesn't set vd_len any more.  This was removed as part
 of some earlier delta of which I was unaware.
 
 It should also be noted that the strategy daemon calls efs_getlength only to
 get the file system blocksize, and doesn't need the length.  Perhaps the
 smart thing to do is for efs_getlength to allow the length out-parameter to
 be a null pointer.  If it were null, the vnode's anode pointer would not have
 to be open.
 bwl-Tue, 26 Oct 1993 13:40:58
**Solution Text**
 Delta bwl-ot4563-panic-in-efs-getlength 1.3. 
 bwl-Wed, 03 Nov 1993 15:30:29

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9420
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : There is no synchronization between volops and rm due to the inactive daemon
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Before the inactive daemon the entire rm operation was completed
 between a vol_StartVnodeOp vol_EndVnodeOp clause so it was
 synchronized with the volops. Due to the introduction of the inactive
 daemon the VN_RELE call is executed asynchronously thus it can race
 with a volop. This bug causes the kernel to panic when it tries to do
 the epif_ChangeLink call and finds the anode refcount to be 2 instead
 of 1. (volops get the anode without going through the vnode layer)
 Dimitris Varotsis
 Thu Oct 14 07:18:04 1993
 dimitris-Thu, 14 Oct 1993 07:18:10
**Solution Text**
 blake-db4491-volops-must-wait-for-inactive-daemon.
 
 Add synchronization so that vnm_StopUse can wait for inactive
 processing to complete before returning.
 blake-Mon, 18 Oct 1993 18:08:38
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9419
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Disk is full but VM has dirty data
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 In this case the aggregate is almost full and as we put data into it
 our space reservation does not appear to be accurate so we run out of 
 disk space but still hold dirty data in VM. This bug is different than
 4358.
 Dimitris Varotsis
 Wed Sep 29 15:03:09 1993
 dimitris-Wed, 29 Sep 1993 15:03:13
**Solution Text**
 blake-db4399-write-protect-unreserved-reads handles the portion of
 the fix concerned with read-ahead, by the simple expedient of disabling
 it.
 blake-Mon, 18 Oct 1993 16:50:03
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9411
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : dfs.repfs_checklist step G
Reported Date                 : 11/9/93
Found in Baseline             : 1.0.3
Found Date                    : 11/9/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/file/Makefile
test/systest/file/repfs.data test/systest/file/dfs.repfs
Sensitivity                   : public

[11/9/93 public]

The dfs.repfs_checklist step G refers to running the system test dfs.lock
in a fileset with scheduled replication. This is (currently) too much
stress for replicated filesets. Another CHO test must be designed to
accomplish the scheduled replication verification without breaking the
"expected usage" parameters of replicated filesets (write infrequently,
read frequently). This CHO test tuned for replicated filesets should
force and verify use of the read-only replicas.

[12/1/93 public]
Deferring this work until DFS 1.1 release.

[1/27/94]
Wrote dfs.repfs to use in dfs.repfs_checklist.

[02/04/94 public]
Closed.



CR Number                     : 9410
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : config/dfs_config
Short Description             : dfs_config uncomments history comments
Reported Date                 : 11/9/93
Found in Baseline             : 1.0.3
Found Date                    : 11/9/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dfs_config,rc.dfs
Sensitivity                   : public

[11/9/93 public]
This problem first appeared around 1993/03/03, according to the history of
rc.dfs: 

# 	evision 1.1.4.3  1993/03/03  23:17:49  rsarbo
# 	insert space in key word in last comment (also unmentionable in
# 	this comment) that caused dce_config to uncomment that line
# 	in the history log

I suggest it be fixed in dfs_config this time by searching more carefully
to avoid the history comments.  In this way, we will avoid future instances
of this problem.  Lines 8, 16, 17 were uncommented when I configured a RIOS
fldb server/client.  'dfsd' and 'epidaemon' induced the problem.

08> # 	add support for restarting a dfsd with in-memory cache
:
16> # 	epidaemon is a kproc on RIOS and it's existence cannot be
17> # 	verified using ps.  Removed the check for epidaemon which

[11/9/93 public]
I should have fixed this the right way the first time...
Fixed in 1.0.3 using a double # prefix for lines that may
be uncommented in rc.dfs to distinguish them from history lines.

[12/17/93 public]
Closed.



CR Number                     : 9409
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkset
Short Description             : tkset_SetVolSync needs to check for valid tkset ptr
Reported Date                 : 11/9/93
Found in Baseline             : 1.0.2
Found Date                    : 11/9/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : tkset.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-o-ot9409-tkset_SetVolSync-check-valid-ptr
Transarc Herder               : jaffe@transarc.com

[11/9/93 public]
The tkset_SetVolSync() macro needs to check that the input tkset ptr
is non null before trying to modify fields of the structure.  The
px layer is written such that a null tkset pointer can be passed
in.  The result of course is usually a machine assertion when
this happens.
The tkset_SetVolSync should be changed as follows:
from:
#define tkset_SetVolSync(asetp, avolp, atype) \
    do { \
        (asetp)->volp = (avolp); \
        (asetp)->volSyncType = (atype); \
    } while(0);
to:
#define tkset_SetVolSync(asetp, avolp, atype) \
    do { \
        if (asetp) { \
            (asetp)->volp = (avolp); \
            (asetp)->volSyncType = (atype); \
        } \
    } while(0);
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/9/93 public]
Defect Closure Form
-------------------
Ran connectathon as regression.
Filled in Transarc Deltas with 
 `cburnett-o-ot9409-tkset_SetVolSync-check-valid-ptr' 
Changed Transarc Status from `open' to `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9388
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : ACL tests leave semi-deleted file
Reported Date                 : 11/5/93
Found in Baseline             : 1.0.3
Found Date                    : 11/5/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot4867-dhop-write-errors 1.3
Transarc Herder               : andi

[11/5/93 public]
After running the tests successfully, acl/scratch/syscalls/dir2/dir2_1/tdir1 
can't be deleted.
Added field Transarc Herder with value `andi' 
Added field Transarc Status with value `open'

[11/12/93 public]
Either revision 1.1 or 1.2 of this delta caused the problem, so it is
being fixed by revision 1.3.
Defect Closure Form
-------------------
--Verification procedure below--
See problem description above.
Associated information:
Tested on TA build:  
dfs-osf-1.9
Tested with backing build:  
dce1.0.3bl8
Changed Transarc Status from `open' to `export' 
Added field Transarc Deltas with value `bwl-o-ot4867-dhop-write-errors 1.3'

[12/7/93 public]
Imported into dfs-osf-1.12 and submitted to the OSF on 120793
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9387
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : /test/file/acl
Short Description             : acl tests don't run under HP /bin/posix/sh
Reported Date                 : 11/5/93
Found in Baseline             : 1.0.3
Found Date                    : 11/5/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi
Transarc Delta                : andi-o-ot9387-acl-tests-workaround-for-hp-shell

[11/5/93 public]
Sub-shells started by the acl tests are "restricted".
Added field Transarc Herder with value `andi' 
Added field Transarc Status with value `open'

[11/11/93 public]
Changed SHELL to PROG and exec $PROG in:
  test/file/acl/scripts/accheck/driver.sh
  test/file/acl/scripts/common/init.sh
Changed Transarc Status from `open' to `import' 
Added field Transarc Delta with value 
 `andi-o-ot9387-acl-tests-workaround-for-hp-shell'

[12/17/93 public]
Closed.



CR Number                     : 9379
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : xaggr
Short Description             : Zero-Link-Count files and replication
read-only replucas don't appear to get along very well.
Reported Date                 : 11/5/93
Found in Baseline             : 1.0.3
Found Date                    : 11/5/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot9379-zlc-detach-rep
Transarc Herder               : jaffe@transarc.com

[11/5/93 public]
Scenario:
  [] Assume a replicated fileset.
  [] Delete one of the files on the RW/master fileset.  Because
       it is replicated, it isn't deleted right away ... the vnode
       is held by the ZLC manager to give time for clients to
       register their use.
  [] Now release the fileset to the various replicas.
  [] This zero-link-count file on the replica server is now
       "grabbed" by the ZLC manager (courtesy of the volume
       set_attribute operation).
  [] Unfortunately, because the ro replica's .parentID field
       (fileset header) is non-zero, the ZLC manager will never
       actually delete or release the file ... it will hold the
       vnode reference forever.
The particular result of this that we observed is that this
ro-replica could not be detached ... because of the active vnode
references.

[11/11/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
(1) (configure a server, call it sv)
(2) (create 2 aggregates, call them rw and ro)
(3) (create a fileset on rw, call it rwf)
(4) (create a mountpoint for rwf, call it rwf)
(5) touch rwf/dummy_file
(6) fts setrepinfo rw -scheduled
(7) fts addsite rwf sv ro
(8) rm rwf/dummy_file
    It is important that the update that sends the ZLC file to the replica
    should be the FIRST update of the replica.  Thus, do step (8) ASAP after
    step (7).
(9) Wait until there has been an update.  This can be determined by
    examining the output of:
    fts statrep sv
(10) dfsexport -detach ro
     Unfortunately, dfsexport does not tell you if it fails.  Thus you must
     check that the detach went correctly by examining the output of:
     dfsexport
If the aggregate detached correctly, the bug is fixed.
Associated information:
Tested on TA build:  
dfs-osf-1.9
Tested with backing build:  
dce1.0.3bl8
Changed H/W Ref Platform from `hppa' to `all' 
Changed S/W Ref Platform from `hpux' to `all' 
Filled in Subcomponent Name with `xaggr' 
Changed Short Description from `Zero-Link-Count files and' to `Zero-Link-Count 
 files and replication' 
Added field Transarc Deltas with value `bwl-o-ot9379-zlc-detach-rep' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9342
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : dfs.read_write_all.main fixes needed
Reported Date                 : 11/3/93
Found in Baseline             : 1.0.3
Found Date                    : 11/3/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : dfs.read_write_all.main
Sensitivity                   : public

[11/3/93 public]

	- Look for check_RC.ksh in TROOT.
        - Check for datafile - exit if not found.
        - Print and check results of fts create, crmount,
        delmount and setquota commands.
        - Load check_RC function.
        - Output PASSED or FAILED in addition to 0 or 1
        exit.

[11/4/93 public]
Above fixes submitted via changes to dfs.read_write_all.main.

[11/11/93 public]
Closing.



CR Number                     : 9333
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 9241
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : Correct dfsd changes
Reported Date                 : 11/3/93
Found in Baseline             : 1.0.2
Found Date                    : 11/3/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : dfsd.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[11/3/93 public]
A few problems, such as placement of the new -namecachesize option in the
command's syntax and the text of its argument, need to be fixed on the
dfsd.8dfs manpage.  I'll fix them shortly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[11/4/93 public]
Completed small repairs to previous changes.  All work verified by Mike Comer.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 9316
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : config
Short Description             : lfs not detached in unconfig,remove
Reported Date                 : 11/2/93
Found in Baseline             : 1.0.3
Found Date                    : 11/2/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : dfs.clean,dfs.unconfig
Sensitivity                   : public

[11/2/93 public]
seems that dce_config unconfig and remove
still are'nt perfect...

I tried to unconfig and remove an FLDB server in
my cell and it complained that it could not
rm -rf /opt/dcelocal/var/dfs because the aggrs
dir was not empty, I then did a dfsexport and
found that the lfs aggr was not detached, but
all of the daemons had been stopped...

I also found that the serverentry and it's filesets
were still in the fldb of the root.dfs server
(I did perform and unconfig from there also)

once I manually performed an fts delfldbentry of
the filesets and then fts delserverentry of the
server then rebooted the server and manuallly
rm'd the var/dfs dir the remove seemed complete 
and I could config the server back into the cell.

[12/10/93 public]
Re-assign to Andy McKeen at OSF.  Defer to 1.1.

[04/25/94 public]

 ./config/dfs.clean
Added dfsexport -detach -all before bringing down deamons
 ./config/dfs.unconfig
Added code to cleanup filesets and server entry from fldb
if host is a ftserver

[09/27/94 public]
Closed.



CR Number                     : 9302
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 9316
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : config
Short Description             : unconfig doesn't remove serverentry
Reported Date                 : 11/1/93
Found in Baseline             : 1.0.3
Found Date                    : 11/1/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : src/config/dfs_unconfig
Sensitivity                   : public

[11/1/93 public]
removed an FLDB server from the cell, could not 
config it back into the cell 
performed manually the fix described in OT# 9120
but still seeing the following during fldb config

Host /.:/hosts/dfs_rios is now the sync site
        Enter the name of the system control machine:  dfs_rios
Could not create the server entry (676372512)
Error: FLDB: address already exists in another site (dfs / vls)

shouldn't a fts delserventry be performed somewhere? otherwise
how does the serverentry get removed from the FLDB?

[12/10/93 public]
Re-assign to Andy Mckeen.  Defer to 1.1.

[4/25/94 public]
This is the same problem that was addressed with the fix to 9316.  The fix
was submitted this morning.

[09/27/94 public]
Closed.



CR Number                     : 9293
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Several errno values (e.g.,
EDQUOT) are not being translated properly (from DFS-wire to OS-internal
encoding) for HPUX.
Reported Date                 : 10/29/93
Found in Baseline             : 1.0.3
Found Date                    : 10/29/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : osi/HPUX/osi_dfs2oserr.c, osi/HPUX/osi_os2dfserr.c
Sensitivity                   : public

[10/29/93 public]
No magic here: there are a number of errnos (EDQUOT, e.g.) which are not
being properly handled in osi/HPUX/osi_dfs2oserr.c and osi_os2dfserr.c.
This causes programs to fail with "unknown errno 128" instead of something
meaningful such as EDQUOT.

[12/17/93 public]
Closed.



CR Number                     : 9284
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 9650
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Episode recovery tests don't pass on HPUX.
Reported Date                 : 10/28/93
Found in Baseline             : 1.0.3
Found Date                    : 10/28/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/rcx/copyaggr.c
					     src/file/episode/rcx/runrcx
					     src/file/episode/sautils/asdb.c
Sensitivity                   : public
Transarc Deltas               : bwl-o-ot9284-hp-port-rcx-tests
Transarc Herder               : 
Transarc Status               : submit

[10/28/93 public]
Changes are needed in src/file/episode/rcx/runrcx,
src/file/episode/sautils/asdb.c, and in src/file/episode/rcx/README to make
these tests pass on HPUX.  The changes all have to do with the fact that
devices on HPUX are in, e.g. /dev/dsk/5s0 and /dev/rdsk/5s0.  Also, on
HPUX, the major device number is different between the block and raw
devices (they seem to be the same on AIX).  I will be submitting the fixes
shortly.

[12/17/93 public]
Closed.

[12/23/93 public]
Additional changes required:
- Use bdf instead of df to determine what's mounted.  Deal with idiosyncrasies
  of bdf, e.g. an output line may be split into two lines.
- Wait 30 seconds before calling bdf, so the syncer can update /etc/mnttab.
  The right thing to do would be to fix the code that calls mount; fixing
  runrcx instead is strictly a quick hack.
- copyaggr must pass the block device number, not the char device number, to
  the freeze and unfreeze syscalls.
Some further changes, to cope with partitions of different sizes, will be
described in another OT CR.
Filled in Subcomponent Name with `lfs' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[1/5/94 public]
Re-opening, to cope with the changes that were required to get rcx to actually
run on HP/UX.
Changed CR in Code, Doc, or Test? from `code' to `test' 
Filled in Inter-dependent CRs with `9650' 
Changed Interest List CC from `kinney@ch.hp.com' to `kinney@ch.hp.com, 
 kissel@ch.hp.com' 
Changed Status from `closed' to `defer' 
Changed Fix By Baseline from `1.0.3' to `1.1' 
Changed Fixed In Baseline from `1.0.3' to `' 
Changed Responsible Engr. from `kissel@ch.hp.com' to `bwl@transarc.com' 
Changed Resp. Engr's Company from `hp' to `tarc' 
Filled in Transarc Status with `open'

[1/5/94 public]
Defect Closure Form
-------------------
--Verification procedure below--
The verification procedure was to run various RCX tests.  I created a copy
of all_tests that ran basictest and bigsimple on 8K-1K and 4K-4K filesets.
I was able to get one successful run of this.  There were several unsuccessful
runs, due to a not-reliably-reproducible bug, described in another OT CR.
--Other explanation below--
To get the RCX tests to run on HP with Episode disks that are of different
sizes, it is also necessary to have the fix for OT CR 9650.
Associated information:
Tested on TA build:  
dfs-osf-1.12
Tested with backing build:  
dce1.0.3ab1
Filled in Transarc Deltas with `bwl-o-ot9284-hp-port-rcx-tests' 
Changed Transarc Status from `open' to `export'

[1/19/94 public]
Changed Status from `defer' to `fix' 
Filled in Fixed In Baseline with `1.0.3a' 
Changed Affected File from `src/file/episode/rcx/README' to 
 `file/episode/rcx/copyaggr.c' 
Changed Transarc Status from `export' to `submit'

[02/04/94 public]
Closed.



CR Number                     : 9275
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : ubik/flserver
Short Description             : able to create flserver without sync-site potential
Reported Date                 : 10/27/93
Found in Baseline             : 1.0.3
Found Date                    : 10/27/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot9275-ubik-servers-check-admin-lists
Transarc Herder               : 

[10/27/93 public]
Following the dfs.repfldb_checklist, took down the sync site
machine in my cell with a shutdown command to force the 2 remaining
sites to find quorum. They did eventually achieve quorum BUT
I cannot access dfs or the fldb:
 
root@dce8> cd /:
dfs: dce errors (code 676372523) from the fileset location server 130.105.202.28 in cell shades_cell
/bin/ksh: /:: bad directory
 
root@dce8> fts lsfldb
Could not access the FLDB for attributes
Error: FLDB: cannot create FLDB with read-only operation (dfs / vls)
 
The FlLog contains many lines like the following:
 
93-Oct-27 15:38:06 flserver: Initializing the FLDB header.
93-Oct-27 15:38:06 flserver: FLDB header initialization failed (1)!  Code 668147723: bad operation for this transaction type (dfs / ubk).
 
I did config the 2 remaining fldb servers by hand, that is, I upgraded
them from plain old file servers to fldb servers by:
	- adding hosts/<hostname>/dfs-server to subsys/dfs/dfs-fs-servers group
	- adding /.:/hosts/<hostname>/self to /.:/fs rpc group
	- bos creating the flserver entry
on each of the machines becoming fldb servers and:
	- rpccp show group -u /.:/fs
	- bos restart -server <hostname> -proc flserver
on the original fldb server.
 
The 3 fldb server cell was running fine before I began the repfldb checklist.
My only conjecture currently is that a fts crfldbentry command did not
complete before I shutdown the sync site and this somehow left things
in a bad state. This shouldn't be possible so perhaps something else is
wrong.

[10/27/93 public]
Looks like you killed off the initial flserver for good before your two
new ones had pulled over the existing database, leaving you with a completely
empty FLDB.  Did you wait for them to achieve quorum before zapping the
original flserver?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/27/93 public]
Yes - I had actually run dfs.read_write_all successfully for an hour
yesterday in the newly "upgraded to 3 flservers" cell. During that test,
filesets are created, mounted and written in by multiple test user principals.
I had done a fts lsfldb right before killing off the original fldb and
had shown 7 entries so I think things were okay - I'm just not sure I
did an lsfldb after the crfldbentry before I killed off the original fldb.
BUT - I do not have root.dfs replicated and root.dfs lives on the original
fldb server - is this significant? I can understand this blocking me from
access to /: but not from fts lsfldb.

[10/28/93 public]
Transarc will look at this bug after the 1.0.3a code drop.  We've got our
hands full right now running the functional tests.  I would appreciate gail
re-running this test on the new code drop once it gets there.  There have
been a number of fixes to ubik, which might account for this behavior.
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com'

[10/28/93 public]
Elliot - do you believe this problem will be easily reproducable? Are there
ubik OT #s you can refer me to which indicate what changes have been made
to ubik and why you believe they will solve this problem? I suspect that
we will lose valuable data if I don't gather it now. Let me know what you
think the problem is and what data, if any, I can obtain from the cell in
its current state to support our theories - thanks.

[10/28/93 public]
I've created the directory ~notuser/gmd/9275 which contains icl.flserver logs
from the 2 remaining flservers, corresponding udebug output from each machine
as well as bos status output. I've included a README with the 2 significant
points that:
1) sync site still thinks it's getting 4 votes from 3 servers - can't be -
only 2 servers running.
2) "Bosserver reports inappropriate access on server directories."
when bos status any server proc on either remaining server machine.

[10/28/93 public]
More data ...
root@dce13> ls -l fldb*
-rw-------   1 root     sys           16 Oct 26 15:27 fldb.DB0
-rw-------   1 root     sys           64 Oct 26 15:34 fldb.DBSYS1
root@dce13> rsh dce8 ls -l /opt/dcelocal/var/dfs/fldb*
dce8: dce8: cannot open
root@dce13> remsh dce8 ls -l /opt/dcelocal/var/dfs/fldb*
-rw-------   1 root     sys           16 Oct 26 15:29 /opt/dcelocal/var/dfs/fldb.DB0
-rw-------   1 root     sys           64 Oct 26 15:34 /opt/dcelocal/var/dfs/fldb.DBSYS1
root@dce13> remsh cobbler ls -l /opt/dcelocal/var/dfs/fldb*
-rw-------   1 root     bin       147520 Oct 26 15:07 /opt/dcelocal/var/dfs/fldb.DB0
-rw-------   1 root     bin           64 Oct 27 13:55 /opt/dcelocal/var/dfs/fldb.DBSYS1
that is, on cobbler, the original now disabled sync site, fldb.DBSYS1 is
newer than the corresponding file on either of the 2 remaining flserver
machines. By looking at the /usr/adm/syslog/daemon file on cobbler, the
original now disabled sync site, the shutdown -r now occurred around:
Oct 27 14:11:00 cobbler snmpd[5436]: NOTICE: stopsrc issued
Oct 27 14:11:00 cobbler snmpd[5436]: NOTICE: snmpd (5436) is terminating
Oct 27 14:11:00 cobbler snmpd[5436]: NOTICE: logging to /usr/tmp/snmpd.log is te
rminating
Oct 27 14:14:53 cobbler snmpd[5436]: NOTICE: logging started at level 0
Oct 27 14:14:54 cobbler snmpd[5436]: NOTICE: snmpd (5436) is starting
that is, a good 15 minutes after the update to fldb.DBSYS1. The database is
not large nor was there any activity in the cell that would explain what
the hold up was ... I'll save the fldb files in ~notuser/gmd/9275 as
well just in case they're useful.

[10/29/93 public]
The last note is a nice smoking gun.  Notice that the size of the file on 
the original sync site is 147520, and on the other sites its 16.  This is 
a clear indication that the other sites either never got updates for the
database, or blew away the database.  At the time that this was done, its
clear that the local machine and dce8 should not be able to achieve quorum
since they don't have a copy of the database.  This looks suspiciously like
some of the problems that we were having towards the end of 1.0.2a, where
the flservers needed to be restarted enmass after configuration.  Without
that restart, the flservers had confused ideas about what other flservers 
existed in the cell.  I believe that this has since been fixed (or at least
worked around), but I can't tell for sure.

[10/29/93 public]
  1) sync site still thinks it's getting 4 votes from 3 servers - can't be -
  only 2 servers running.
What does udebug say?
  2) "Bosserver reports inappropriate access on server directories."
  when bos status any server proc on either remaining server machine.
This is unrelated.  It is reporting that the permissions or ownership on 
some directory under /opt/dcelocal/ is wrong.
How close filling up the partition holding /opt were you on the
machine that became the sync site when you were running these tests?

[10/29/93 public]
Udebug and other output are in the same directory, ~notuser/gmd/9275. Udebug
says dce13 is now the sync site. I'm nowhere near filling the /opt/dcelocal
partition
root@dce13> ls -l /opt
lrwxrwxrwx   1 root     sys            7 Aug 31 10:05 /opt -> /u0/opt
root@dce13> bdf
Filesystem           kbytes    used   avail capacity Mounted on
/dev/dsk/c201d6s0    320406  273565   14800    95%   /
/dev/dsk/c201d5s0    319866   55807  232072    19%   /u0
I did not see this problem at the end of 102a - although I believe that it
could have existed then. The cirumstances under which I recall having to
restart flservers was when dce_config did not handle configuring multiple
flservers in a cell. As I stated above, I configured the 2nd and 3rd flservers
by hand and did the rpccp -u and bos restart commands by hand and successfully
achieved quorum between the three machines. Unfortunately, I did not save
the output of udebug and fts lsfldb commands prior to taking down the
original sync site - perhaps, as now, I achieved quorum when I shouldn't have.
I shouldn't have to restart flservers when the sync site goes down - and in
this case, even doing so hasn't helped. The permissions problem indicates
that I probably failed to do some acl_edit'ing that dfs_config does when
setting up fldb servers. I'll check this out and update the note here -
maybe it will help reconstruct the crime ...

[11/1/93 public]
There could be two reasons for this problem
1. When configuring additional ubik servers by hand, make sure you create the 
   admin file on the server's machine. The admin file should contain as members
   the subsys/dce/dfs-admin group among others. This could explain the failure
   in the db being copied over. The UBIK_SendFile RPC would fail to send the
   db over if the authorization check fails.
2. When configuring new servers it is better to configure them one at a time.
   The reason for this is, if two additional servers are configured together,
   and if these two and the original server are restarted, and if the original
   server fails to come up for any reason, the two new servers would obtain
   quorum, create a new(empty) db and mark it with a new version number. This
   could happen because RPCs to the original server fail. Now, even if the
   problem afflicting the original server is remedied, and the server brought
   back up, the db on the original server's machine(the one containing the
   goodies) would be overwritten with the empty one('cause the version number
   on the empty one is greater) and all will be lost forever. This could be
   attributed as a ubik bug, but I don't have any thoughts on how to fix this.
   Configuring new servers one at a time, making sure that all old servers
   come back up and the server group achieves quorum(use udebug) before 
   proceeding with adding additional servers should avoid this problem. 
   Probably this should be documented. udebug should also report authorization
   check failures, but its interface does not allow it right now.
Vijay
Filled in Interest List CC with `vijay@transarc.com'

[11/2/93 public]
Thank you Vijay - I believe the admin lists were the key. I never ended up with an
empty fldb but, after creating the admin lists and restarting the fldb servers, I 
achieved quorum AND was able to list the contents of the fldb. I wasn't able to do
extensive testing to see if the cell fully recovered (dfs.lock ran for 24 hours and
failed every iteration - unclear why - reconfiguring with dfs_config and retrying).
I've dropped the severity/priority and changed the short description - you shouldn't 
be able to start the flserver if the admin list will not allow it to be the sync site.

[11/10/93 public]
This is basically a problem with configuration.  Jeff K. and I looked
at the docs and they do not mention what you need to do with admin
lists.  For this particular problem, I'll make a change to
ubik_ServerInit() to check the appropriate admin list on the local
machine to make sure it has at least one entry.  If not, the server
will print a message and shut down.  While this doesn't fix the
problem, it adds some checks so that the server can catch it.  I'll
also open a doc defect on the admin list problem.  Do we have
concensus on this? 
Changed Interest List CC from `vijay@transarc.com' to `vijay@transarc.com, 
 jeff@transarc.com'

[11/10/93 public]
 
        "Do we have concensus on this?"
 
More importantly, do we have quorum...?

[11/16/93 public]
I did a little better than what I previously described.  Now, ubik
will check for each principal in the server group for existence in
particular server admin list.  If this check fails, a message will be
printed to stdout.
Defect Closure Form
-------------------
--Verification procedure below--
Tested yes and no cases of:
	server principal as principal in admin list
	server principal as member of group in admin list.
Filled in Transarc Deltas with `comer-ot9275-ubik-servers-check-admin-lists' 
Filled in Transarc Status with `export'

[12/3/93 public]
Adding comments from Dawn Stokes:
>  Are you taking into account the fact that the admin lists will
>  generally be provided via upclient/upserver which may or may not
>  be an flserver/bakserver system??  Currently our configuration
>  tools set up an initial admin list which contains the dfs-server
>  principal for the host and the subsys/dce/dfs-admin group.  But
>  normally, upclient/upserver will just replace this list with the
>  one on the upserver system so the ubik checking sort of gets 
>  invalidated.  I think its safe to assume that most cells will
>  run upserver/upclient to maintain these lists. However, there
>  is some lag time before upclient brings over the new list so
>  waiting for them could delay quorum.  Thoughts??
The scenario you suggest is certainly possible.  There is an
additional fix that I added that I forgot to mention.  If a probe
fails with an access check failure, a message is printed saying that
this has occurred and to what host.  This message should end up in the
BosLog.  So, there is really a check on both sides.  In the time
between when the admin list is dropped and when the list is fixed, the
flservers should continue to function as they did in Gail's desciption
above.
Changed Interest List CC from `vijay@transarc.com, jeff@transarc.com' to 
 `vijay@transarc.com, jeff@transarc.com, dstokes%austin.ibm.com@transarc.com'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9259
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test
Short Description             : admin_checklist needs corrections
Reported Date                 : 10/26/93
Found in Baseline             : 1.0.3
Found Date                    : 10/26/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/admin/file/tests/admin_checklist
Sensitivity                   : public

[10/26/93 public]

The file src/test/systest/admin/file/tests/admin_checklist needs some
corrections and enhancements in order to be more useful to the tester.  I
have provided a diff of the file I kept which contains my notes indicated
by ***** in the first 5 columns.  In some cases the correction follows, in
others I have just provided a suggestion for what is needed.  If you need
or want my copy of the file, please let me know.

marathon:/users/buhner/Dfs/RP -> diff admin_checklist.orig admin_checklist
112a113,114
> *****need statement of what expected configuration is - buhner
>
126c128,129
< /dev/rz1b       /u2             ufs     0,,13
---
> ***** must add the aggregate id number as well!!! - buhner
> /dev/rz1b       /u2             ufs   4    0,,13
164c167
< X> %newaggr -aggregate /dev/rz1e -blocksize 8192 -fragsize 1024
---
> *****need statement of what the initial configuration is - buhner
165a169,172
> *****note to add optional switches -overwrite and -verbose - buhner
>
> X> %newaggr -aggregate /dev/rz1e -blocksize 8192 -fragsize 1024
(-overwrite -verbose)
>
226a234,236
> ***** can't do the following unless the aggregates already exist on this
> ***** and another node.  Should let this be known before get to this
poing - buhner
>
267c277,278
< > %fts setquota -fileset epi.1
---
> ***** must include the -size switch and value for this command - buhner
> > %fts setquota -fileset epi.1 -size 10000
275a287,289
> ***** modify comment to more clearly state that the following set if
lines
> ***** are merely informational and are not meant to indicate that
anything
> ***** needs to be executed. - buhner
279a294,295
> The dce_config script exercises the above aspects of dfs cell
administration so there
> is no need to repeat them at this time.
281d296
< The dce_config script exercises these aspects of dfs cell administration.
367a383,386
>
> *****need statement of what the initial configuration is - buhner
>
>
374a394,397
> *****The above detach should be followed by a re-attach of the
> *****aggregate since there is another request to detach this
> *****device later on in this section - buhner
>
379a403,406
> *****Add note that the tester might want to perform the following test
when it
s the FLDB to be
> out of sync
> > %fts lsfldb -server valentine
> Check to make sure the fileset has been deleted
>
489c542
< (on dce12)
---
> (on dce12 only)
516a570,572
> *****NOTE: Given the previous sequence of events, it is not possible
> *****to perform this detach.  It was detached back in the beginning
>
822a895,897
>
> *****need statement of what the initial configuration is - buhner
>
848a924,925
> *****need statement of what the initial configuration is - buhner
>
920c997,999
< Firs we  must do a full backup from one LFS fileset to another.
---
> *****need statement of what the initial configuration is - buhner
>
> First we must do a full backup from one LFS fileset to another.
922c1001
< done.  Note that we're restoring  to a non-existant fileset, one
---
> done.  Note that we're restoring to a non-existant fileset, one

[10/27/93 public]
Thanks Cindy - I'm going to try to go through the admin_checklist
this week and next week so I'll be incorporating your input.

[12/22/93 public]
Added configuration information at the beginning of each section.
Changed machine names to be generic.
Fixed inconsistencies and errors in commands.
Added setup setup steps where needed.



CR Number                     : 9254
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : 64K/1K aggregate panics in elbr_WriteCheckpointBlock
Reported Date                 : 10/25/93
Found in Baseline             : 1.0.3
Found Date                    : 10/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : berman-o-ot9254-episode-with-nontraditional-blocksize
Transarc Herder               : 

[10/25/93 public]
Dfs.block_frag systest crahed a RIOS running dfs-osf-1.7.
Using an aggregate that was created with 64K/1K block/frag
causes episode/logbuf/recover.c to panic in elbr_WriteCheckpointBlock()
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/25/93 public]
Defect Closure Form
-------------------
--Regression test program below--
tests/systest/dfs.block_frag
Associated information:
Tested on TA build:  
dfs-osf-1.7
Tested with backing build:  
dce1.0.3bl4
Filled in Transarc Deltas with 
 `berman-o-ot9254-episode-with-nontraditional-blocksize' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9252
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : agfs
Short Description             : new console message when attaching aggr
Reported Date                 : 10/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot9252-hp-unwanted-debug-printf
Transarc Herder               : jaffe@transarc.com

[10/25/93 public]
There is a new (as of the fix for OT 8885) console message from agfs whenever
an aggregate is attached.  Is this really valuable/necessary, or is it just
an accident left over from debugging?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/17/93 public]
Re-assign to jaffe as discussed in tech call on 11/11

[11/18/93 public]
Bruce, please make the delta.
Changed Responsible Engr. from `jaffe@transarc.com' to `bwl@transarc.com'

[11/18/93 public]
Defect Closure Form
-------------------
--Other explanation below--
Watch for a console message during dfsexport.  If there isn't one, the
bug is fixed.
Associated information:
Tested on TA build:  
dfs-osf-1.11
Tested with backing build:  
dce1.0.3bl10
Filled in Transarc Deltas with `bwl-o-ot9252-hp-unwanted-debug-printf' 
Changed Transarc Status from `open' to `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9249
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs.lock
Short Description             : dfs.lock test setup undocumented
Reported Date                 : 10/25/93
Found in Baseline             : 1.0.3
Found Date                    : 10/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : dfs.lock, lock.data
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open

[10/25/93 public]
dfs.lock system test seems to require undocumented setup
procedures involving a variable STDEXC, BINDIR (README says TROOT).
Also unsupplied with the sources are:
a profile called profile.dcetest and a script called conf_util.sh.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'
[mhickey public]
The dfs.lock setup should be better documented, either in the books, the
script itself, or an associated README.  However, comments in both the
datafile and the script itself indicate the need for and use of the BINDIR
directory and the STDEXC variables.  In addition, the test is assumed to 
have been installed using the dcetest_config tool, in which case the
profile.dcetest file and the conf_util.sh script will be installed.  Use
of the dcetest_config tool is documented in the Porting and Testing Guide.

[11/3/93 public]
Changed area from code to test. I agree that there's no reason for dfs.lock
to differ so much from the other tests - will the following changes suffice?
	- replace BINDIR with TROOT
	- define STDEXC in datafile
	- document conf_util.sh dependency

[11/3/93 public]
The above changes will be just fine for resolving this problem.

[12/28/93 public]
Above changes made and submitted.

[01/17/94 public]
Submission fixed - closing.



CR Number                     : 9246
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dfs.wan_checklist
Short Description             : dfs.wan_checklist omissions
Reported Date                 : 10/22/93
Found in Baseline             : 1.0.3
Found Date                    : 10/22/93
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/file/dfs.wan_checklist
Sensitivity                   : public

[10/22/93 public]
dce_config used to start gdad by default.  Since it no longer does,
dfs.wan_checklist should include a directive to do so, probably via
dce_config.

[10/27/93 public]
I found that I needed to be dce_logged in to the cell_admin account that
corresponded to the cell in which I was working.  For example, if I used
filewnr to write a file in cell-A, from cell-B I needed to dce_login as
/.../cell-A/cell_admin in order to use filewnr to verify it.  I understand
that the right fix for this is to appropriately set the ACLs.  No mention
of ACLs is made in dfs.wan_checklist.

I also found that if hub doesn't run perfectly the first time, due for
example to the above problem, you will need to do a cm flush on the test
directory before restarting both spoke processes and rerunning hub.

[10/27/93 public]
Thanks Steve - I believe I had foreign ACLs set up when I ran the test
but failed to mention them in the checklist - good catch. Still not
sure of cm flush-hub connection but I'll look into it when I get to
this one.

[12/16/93 public]
Setting fixby field back to 1.0.3a in the hopes that I can update the
checklist as I run through it. Currently WAN testing is scheduled for
the second half of January.

[1/5/95 public]
Since I'll be doing the dfs.wan_checklist, I'll get this one.

[1/25/94]
Added note about installing and starting gdad daemon.
Added steps to setup the appropriate ACLs.

When going through the checklist, I also had trouble running the hub
and spoke tests, but flushing the cache was not neccessary.  Since I did
not see this problem, a comment about flushing the cache was not added.



CR Number                     : 9243
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dirwrite.sh
Short Description             : can't chown if not root
Reported Date                 : 10/22/93
Found in Baseline             : 1.0.3
Found Date                    : 10/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : README
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open

[10/22/93 public]
dfs.glue fail early on with chown in dirwrite.sh.  The README suggests:
NOTE 3: All the automated CHO tests SHOULD be run as test users whose
	UNIX and registry uids and gids match. The dfs.glue tests and
	dfs.read_write_all tests REQUIRE UNIX and registry uids and gids
	to match
But in UNIX chown can only be run by root.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[11/3/93 public]
Updated README - thanks.

[11/11/93 public]
Closing.



CR Number                     : 9241
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : add new option for dfsd
Reported Date                 : 10/22/93
Found in Baseline             : 1.0.3
Found Date                    : 10/22/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : dfsd.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/22/93 public]
We are adding a new option to the "dfsd" command in 1.0.3a, so
we'll need to document it.
The syntax of the new option is
dfsd -namecachesize <number of entries>
<number of entries> is required should be a number greater than zero.
The default value is 256.
Description:
This option allows one to configure the number
of entries allocated for DFS cache manager's name lookup cache.
The purpose of the name lookup cache is to store the results 
obtained from remote diretory lookup requests to DFS servers.
This allows subsequent lookup requests for that same file
or directory to be satisfied locally on the DFS client, rather 
than remotely on the DFS server.  Of course, since the 
cache entries are eventually recycled, the ability to satisfy
the request locally is dependent on the size of the cache. 
The default size of the caches is 256 entries.

[10/22/93 public]
I believe that information about this new option needs
to be added to the Administration Guide, Chapter 8,
"Configuring the Cache Manager" as well as to the
reference page on dfsd. It's not currently clear to
me which section in this chapter should contain the information,
so I'll consult with Diane Delgado to find out.

[10/22/93 public]
No information about the proposed new option needs to be included in the file
8_cachemgr_dfs.gpsml.  The only references to this functionality should be on
the dfsd.8dfs manpage.  Inclusion of this information in the DFS chapters of
the DCE Admin Guide would probably necessitate the addition of a whole new
section to discuss the name lookup cache.  Such information would do nothing
but obfuscate an already confusing chapter.  (Note that there are ample
precedents for not including this material in the DCE Admin Guide, as many
existing options of the overburdened dfsd command are not currently documented
in the guide.)
 
The priority of this change needs to be elevated; once the related code
enhancement is made, this is a required interface change.  The code and doc
need to match.
Changed Subcomponent Name from `admin_ref, admin_gd' to `admin_ref' 
Filled in Interest List CC with `jeff@transarc.com' 
Changed Severity from `C' to `B' 
Filled in Affected File with `dfsd.8dfs' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/22/93 public]
Corrected the less-than-articulate concluding paragraph of my previous comment.

[11/3/93 public]

Fixed dfsd.8dfs man page to include new option. Diane
Delgado verified. Will close this bug and open a new
CR (a 1.1 enhancement request) which suggests that the
Administration Guide cache manager chapter be improved
to include lookup cache info and to be less confusing.

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 9228
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/systest/file/filewnr.c
Short Description             : filewnr random read bug
Reported Date                 : 10/21/93
Found in Baseline             : 1.0.3
Found Date                    : 10/21/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/filewnr.c
Sensitivity                   : public

[10/21/93 public]
The read_file() routine in filewnr.c uses an extremely poorly designed
algorithm for choosing random blocks.  Basically, it consists of:

	1) Choose a random number out of all blocks in the file.

	2) If the block has been read, goto step 1.

	3) Read the block.

	4) Repeat until all blocks have been read.

This takes O(N log N) time, I believe, which is far too long for 320,000
blocks.

I have a fix, which is O(1) time.  The algorithm consists of:

	1) Pick a random number out of all blocks *not yet read*.

	2) Read the block, and remove it from the list of blocks not read.

	3) Repeat until all blocks have been read.

Here is a diff which implements this change:

904a905
>   long bleft;
914c915,916
<   /* set up for random access - create array of 0's and seed random #
generator */
---
>   /* set up for random access - create array of block numbers and */
>   /* seed random # generator */
915a918,920
>   for (i = 0; i < input->bnum; i++)
>     array[i] = i;
>   bleft = input->bnum;
922c927
<   for (i = 0; i < input->bnum; )
---
>   for (i = 0; i < input->bnum; i++)
925c930
<       r = RAND () % input->bnum;
---
>       r = RAND () % bleft;
927,928c932,935
<       /* if that block hasn't been read, read it, bump count (i) and mark
array */
<       if (!array[r])
---
>       /* Read block, replace entry with block number from last slot in */
>       /* range. */
>       if (lseek (fd, (array[r] * input->bsize) + input->offset,
>                SEEK_SET) == -1)
930,939c937
<         if (lseek (fd, (r * input->bsize) + input->offset,
<                    SEEK_SET) == -1)
<           {
<             LSEEK_ERROR;
<           }
<         else
<           read_pattern (input, fd, r);
<
<         i++;
<         array[r] = i;
---
>         LSEEK_ERROR;
940a939,943
>       else
>       read_pattern (input, fd, array[r]);
>
>       bleft--;
>       array[r] = array[bleft];
941a945,947
>
>   /* Destroy array */
>   free(array);

[10/22/93 public]
All is not as it seems on the surface.  While there is an algorithmic problem
here, there's a simple reason why it surfaced during the HP port:

From the code:
 *---* HP-UX does not have the random/srandom calls either.  To make this
 *---* portable, we will define RAND and SRAND to be rand and srand for
 *---* HPUX, and random and srandom for all others.

Checking the rand man page, we see:
 rand() uses a multiplicative, congruential, random-number
 generator with period 2^32 that returns successive pseudo-
 random numbers in the range from 0 to 2^15-1.

This is based on the RAND_MAX symbol defined in stdlib.h or thereabouts.

The range of random(), the orginal generator used on OSF/1 & rios, is 0
to 2^31 - 1.

So rand will never generate a number larger than about 32768, no matter how
you change the algorithm.  I haven't checked the suggested changes to see
how they circuvent the problem, but with the 32k limit, it's easy to see
why the test would take a long time for large files.

For blocks > 2^15-1, the algorithm is O(infinity), not O(n log n).

Regardless, the original algorithm is deficient, even for a wider ranging
random function, since there's no guarantee that ALL integers in the
range, 0 <= block count < RAND_MAX, will be generated in finite time.

[12/14/93 public]
Filewnr.c now contains a more efficient algorithm. This was submitted BEFORE
the 1.0.3 freeze under another CR - HP did their maxfile testing of their
port using this new filewnr.



CR Number                     : 9222
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : make the name lookup cache size configureable
Reported Date                 : 10/21/93
Found in Baseline             : 1.0.3
Found Date                    : 10/21/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : afsd.c cm.h cm_init.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[10/21/93 public]
This is a performance issue.
Currently the name look up cache size is fixed at 256 entries.
We would like to make this configureable so that clients can
increase their name look up cache sizes, thus reducing the
number of times a lookup operation must be performed at the server.
Here is what's needed to make this possible.
dfsd.c - modifications to accept a new parameter, namehashsize.
cm.h - add the namehashsize field to the cm_cacheparams strucutre
       so that dfsd can pass this information to the cm and the
       cm can use it during initialization.
cm_init.c - CacheInit function needs to be modified to remove the
      hard coded 256 value which gets passed to nh_init; it will
      use the namehashsize parameter, which will be added to its
      parameter list.  This implies that the call signature for
      this function will change.
      cm_DoInitCacheInit - modify to reflect the call signature change
      for CacheInit()
cm_dnamehash.c - no changes needed here; the initialization
  function already supports configureability.
cm_init.c - change the hard coded value of 256 which is passed to nh_init
  to be

[10/28/93 public]
fix submitted

[11/1/93 public]
We're going to import these changes into our code base.  There 
are a couple of changes that need to be made to the user interface
to make it consistent with the other options (and to make our
doc people happy).  Also bullet proof cm_init.c against getting
a zero name cache size if run with an old dfsd:
(Namecachesize argument should come before verbose or debug)
RCS file: /afs/tr/project/fs/dev/dce/src/file/afsd/RCS/afsd.c,v
retrieving revision 4.70
diff -r4.70 file/afsd/afsd.c
237,239c237,239
< #define VERBOSE                  14
< #define DEBUG                    15
< #define NUM_NAME_CACHE_ENTRIES   16
---
> #define NUM_NAME_CACHE_ENTRIES   14
> #define VERBOSE                  15
> #define DEBUG                    16
841c841
<           printf("dfsd: namehashentries must be greater than zero\n");
---
>           printf("dfsd: namecachesize must be greater than zero\n");
1304c1304
<                      "name lookup cache size", NUM_NAME_CACHE_ENTRIES);
---
>                      "number_of_name_cache_entries", NUM_NAME_CACHE_ENTRIES);
-------------------------------------------------------------------------------
Changes to file/cm/cm_init.c in open delta berman-o-inc-ot9222-cm-name-cache-size:
===================================================================
RCS file: /afs/tr/project/fs/dev/dce/src/file/cm/RCS/cm_init.c,v
retrieving revision 4.110
diff -r4.110 file/cm/cm_init.c
868a869,870
>     if (anamecachesize == 0)
>       anamecachesize = 256;
-------------------------------------------------------------------------------
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `'

[11/2/93 public]
Who is handling the update to the docs for this?  This note is
just to remind someone to make sure the 103 dfsd man page gets updated.
Changed Interest List CC from `hsiao' to `hsiao,demail1!carl' 
Added field Transarc Herder with value `'

[11/2/93 public]
The doc changes have already been made by the OSF
doc group.

[11/2/93 public]
Carl:  The doc defect corresponding to this change is 9241; I think Lisa is
responsible for it.  It has *not* been closed yet.  I am waiting for its
status to change so that I can verify the modifications and consider their
impact for our DCE release as well....

[12/17/93 public]
Closed.



CR Number                     : 9216
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : file/xaggr,flserver,pxd,dfsbind, 
					     rep,userInt/fts,bak,butc,fsprobe,
					     scout,bakserver
Short Description             : unresolved or undefined symbol
Reported Date                 : 10/21/93
Found in Baseline             : 1.0.3
Found Date                    : 10/21/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : xaggr/volDesc.c,
flserver/flprocs.c, pxd/pxd.c, dfsbind/bind_helper.c, dfsbind/dfsbind_test.c,
rep/rep_tkint.c, userInt/fts/volc_pioctl.c, bak/commands.c, butc/tcstatus.c,
fsprobe/fsprobe_test.c, scout/scout.c, bakserver/server.c;
/test/file/fx/scripts/stress
Sensitivity                   : public

[10/21/93 public]

The following error message:

0706-317 ERROR: Unresolved or undefined symbols detected:
                 Symbols in error (followed by references) are
                 dumped to the load map.
                 The -bloadmap:<filename> option will create a load map.
.dfs_GetJunctionName
*** Error code 8 (continuing)
`build_all' not remade because of errors.

occurred 12 times, in the following modules:
 
	file/xaggr
	file/flserver
	file/pxd
	file/dfsbind (twice)
	file /rep
	file/userInt/fts
	file/bak
	file/butc
	file/fsprobe
	file/scout
	file/bakserver

In the "test install" phase:

[ /test/file/fx/scripts/stress at 07:39 (AM) Thursday ]
....
make: don't know how to make fx_test (continuing)
`install_all' not remade because of errors.

[10/21/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 9213
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : tkc
Short Description             : Spurious text file busy from glue
Reported Date                 : 10/20/93
Found in Baseline             : 1.0.3
Found Date                    : 10/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : berman-o-ot9213-tkc-shared-tokens
Transarc Herder               : 

[10/20/93 public]
TKC grants a "share" token when creating a file in the
rios glue layer.  If subsequently you attempt to open the file
for write through the cache, TKC failes to revoke the share
token, and open fails with "Text file busy".  This happens
only sometimes in a directory with many files, while opening
and closing each file.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/22/93 public]
Defect Closure Form
-------------------
--Other explanation below--
The intermittent failure mode stemmed from the tkc_RecycleVcache()
function failing to clear the execOpen vcache field.  Thus when
recycling a vcache that was previously used for exec, the execOpen
counter was not zeroed.
Associated information:
Tested on TA build:  
dfs-osf-1.7
Tested with backing build:  
dce1.0.3bl4
Filled in Transarc Deltas with `berman-o-ot9213-tkc-shared-tokens' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9211
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : butc
Short Description             : Bogus error messages in butc
Reported Date                 : 10/20/93
Found in Baseline             : 1.0.2
Found Date                    : 10/20/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[10/20/93 public]
When butc writer over a tape, it deletes the old dumpid from
the database. It also prints out a bogus error message regardless
of success or failure of the deletedump operation.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[12/17/93 public]
Closed.



CR Number                     : 9208
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : bak fails when communicating with bakserver
Reported Date                 : 10/20/93
Found in Baseline             : 1.0.2
Found Date                    : 10/20/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bak/ubik_db_if.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[10/20/93 public]
The function bc_UnlockText() in bak has the following line : 
	if (code)
		return (code)
	} <--- End of function
If code is 0, the return falls through and garbage may be returned to the calling
function. This bug shows up on HP. 
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[11/1/93 public]
Sumitting one-line fix received from khale@transarc.com. This is needed
for completion of HP ref port testing.

Submission Log *****

  Submitted by brezak@ch.hp.com; User name: brezak
  Date: 11/01/93; Time: 15:24
  Number of files: 1; Defect number: 9208.
  Set name: brezak_rp; Sandbox: rp
  List of files and revisions:
./file/bak/ubik_db_if.c 1.1.9.1 1.1.7.2

  Detailed description:

[ ./file/bak/ubik_db_if.c ]
Remove test for error before returning in bc_UnlockText(). It always
should return a status even if it is 0.
[19:57:43  brezak]

End Log *****

[12/17/93 public]
Closed.



CR Number                     : 9205
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfsbind refuses to do anything
if any of a cell's FLDB servers is partially/incompletely configured
Reported Date                 : 10/20/93
Found in Baseline             : 1.0.3
Found Date                    : 10/20/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : ncsbuik
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/20/93 public]
 
(I had no idea of whom to put down as the "responsible engineer".  I picked
 Diane DelGado somewhat at random since ot insisted I put someone there.
 Please forward this to the correct individual.)
 
If the /.../cellname/fs junction (group) has a member (FLDB server) that's
not in the registry, dfsbind refuses to do much of anything.  This happened
to us as a result of a partial/incomplete configure (or unconfigure, I
don't know) ... and rendered our cell unusable until we removed the
offending server via "rpccp remove member ...".
 
Although the config/unconfig scripts should be extra cautious to not leave
things in this inconsistent state, it seems reasonable that dfsbind could
be more robust in this regard.

[10/20/93 public]
It's not just a dfsbind issue, it's an issue for any Ubik initialization,
including that in fts and the backup system.
 
However, it is a security check that's apparently failing.  The members
of the Ubik group really have to be in the associated security group, or
else there's a possible security violation--perhaps somebody has lied to
you about the contents of CDS, and the security group check is the only
way you have of noticing.
 
It could be that there's a special case of Ubik initialization that would
be OK trying to stumble along with only a partial set of the Ubik servers:
it would have to be a client-only initialization (because all Ubik servers
for an object have to have knowledge of the complete set of all Ubik servers
for that object); and it would have to be using Ubik calls that only read
the database, not update it, since they might not be able to contact the
legitimately-elected sync site if that site happens to be outside their subset.
 
The problem is, though, that even if this client initialization were to be
OK, the servers would have to re-initialize themselves pretty soon, since the
protocol for changing a Ubik configuration involves restarting all the
servers for the database provided by the collection of servers.  And, if the
Ubik configuration were screwed up, then those Ubik servers would be unable
to get started again.
 
Thus, it sounds like the configuration problem is more fundamental than any
simple thing that Ubik initialization could realistically do to cope with
the mis-configuration.
Filled in Interest List CC with `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/20/93 public]
Speaking here as a part time AFS administrator: the current behavior is
unacceptable... AFS today can deal with a spurious entry in the CellServDB
from clients (and on the DB servers, so long as you still have quorum,
counting the dead member).
What happened is that a system was removed from the cell without deleting
it from the CDS group /.:/fs .  I don't think it's reasonable for this to
be a death sentance for DFS in the cell.  (Isn't the whole point of Ubik
that so there isn't a single point of failure, even for updates?).  
The "underlying security check" which failed was that the principals in the
security database for that server (the "hosts/rat/*" principals) were
deleted; when dfsbind (presumably in ubik client agent code) tried to bind
to the server, it got an (unexpected) error code back indicating that the
server it was trying to bind to did not exist in the kerberos database.
I think Ubik should treat this case in the same way it treats an
unreachable server; it should scream loud & clear when it notices this, but
it shouldn't effectively terminate service for the cell when there are
sufficient resources for it to keep going.
May I suggest having some way of encoding a "unbindable server" in the
"struct ubik_client"?  Like having the client->conns[x][y] be NULL if you
couldn't bind to that replica/protocol pair, or have a separate state for
unbindable servers?

[10/20/93 public]
Just adding my 2 cents - I agree with Bill and Darryl that
if the cell has sufficient resources to continue functioning
that it should be allowed to do so. Is this not the point of
replicating servers anyway?  
So now the question is, is Bill's solution acceptable to 
everyone?  If so, I'll look into implementing it.

[10/21/93 public]
While I agree that this is a problem, I don't believe it is an A2
defect.  Adding robustness to ubik sounds more like an enhancement to
me.  I'm reluctant to slap a quick fix on it without understanding all
the ramifications.  Diane -- if you want to look into writing a design
for a future fix, we'd be happy to review it.
Changed Interest List CC from `cfe@transarc.com' to 
 `cfe@transarc.com,comer@transarc.com'

[10/21/93 public]

I'll write something up and be sure to send it
to all of you.

[3/8/94 public]

Here's what we have done to resolve this problem:

What we've tried to do is for the case where server A is misconfigured
(i.e, is in /.:/fs but not a member of dfs-fs-servers) but servers B
and C are properly configured, servers B and C will recognize each
other as valid ubik servers but will treat server A as an unknown,
possibly malicious server and reject any requests which many eminate
from it.  In this way we may still achieve quorum with 2/3 of the
servers functioning.

Ubik Client
----------

ubik_ClientInit will return a list of only those servers which 
   are considered to be properly configured.  ubik_ClientInit
   will print a warning message regarding any misconfigured servers
   that it finds.  
 
   This enables client programs such as fts, dfsbind, fxd to 
   take advantage of the new scheme without requring modifications
   to those programs.

Ubik Servers
-----------

    ubeacon_InitServerList  will filter out any hosts which are considered
        bad, and return a the list of properly configured hosts.  Note that
        in the case where at least one server is "bad", this does not affect
        the value of the ubik_quorum variable; we still compute the quorum
        needed based on the total number of servers in /.:/fs, not the
        total number of properly configured servers.  This routine will
        also print a warning message regarding any misconfigured servers
        that it finds.

    Also, the SUBIKVOTE, and SUBIKDISK routines will now check to see
    if this call is coming from a server that  is in the list of
    properly configured hosts, and if not, these routines  will return 
    UNOTMEMBER.

[3/8/94 public]

[3/10/94 public]

Fix submitted; we've also incorported Craig's suggestions, as well.

[09/27/94 public]
Closed.



CR Number                     : 9198
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : must invalidate some VM when truncating a file
Reported Date                 : 10/19/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[10/19/93 public]
When a memory mapped file is truncated, memory that was mapped to the "lost"
bits must be invalidated.  (Need to call mtrunc somewhere in some CM code.)
There is probably a similar bug in the HP port of Episode.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/11/93 public]
Re-assign to jaffe per 11/11 conference call

[12/17/93 public]
Closed.



CR Number                     : 9194
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dfs.block_frag
Short Description             : dfstab is not restored after test
Reported Date                 : 10/19/93
Found in Baseline             : 1.0.3
Found Date                    : 10/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/dfs.block_frag
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/19/93 public]
When the DFSTAB_EDITTED variable is FALSE, the dfstab is overwritten,
rather than modified.  It is not restored after the test completes, either.

[10/20/93 public]
Changed CR in Code, Doc, or Test? from `code' to `test' 
Filled in Affected File with `test/systest/file/dfs.block_frag' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/27/93 public]
Yup - typo caused dfstab to be overwritten. It was also leaving dfstab
as it was used during the test - better to restore original - fix
submitted.

[11/11/93 public]
Closing.



CR Number                     : 9192
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : Remove -verbose from repserver command
Reported Date                 : 10/19/93
Found in Baseline             : 1.0.2
Found Date                    : 10/19/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[10/19/93 public]
The -verbose option has been removed from the repserver command
because using the option does not provide any functionality (i.e.,
no messages are printed).
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/19/93 public]
The following file is affected:
	admin_ref/man8dfs/repserver.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified change in latest doc build and closed bug.



CR Number                     : 9191
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkc
Short Description             : tkc trace catalog not installed
Reported Date                 : 10/19/93
Found in Baseline             : 1.0.3
Found Date                    : 10/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/tkc/Makefile, file/Makefile
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : berman-ot9191-install-dfszkc.cat
Transarc Herder               : 

[10/19/93 public]
The tkc/Makefile is missing a rule for installing dfszkc.cat
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/19/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Build install_all and see dfszkc.cat installed in message 
catalog directory.
Associated information:
Tested on TA build:  dfs-osf-1.6
Tested with backing build:  dce1.0.3bl4
Changed Affected File from `file/tkc/Makefile' to `file/tkc/Makefile, 
 file/Makefile' 
Filled in Transarc Deltas with `berman-ot9191-install-dfszkc.cat' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9183
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : ftu_AggrLookupFset references non-allocated memory
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/08/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 ftu_AggrLookupFset assigns into non-allocated memory suffering a SEGV.
 
 rajesh-Fri, 08 Oct 1993 13:36:39
**Solution Text**
 rajesh-db4458-fix-SEGV-in-ftu_LookupAggrFset
 rajesh-Fri, 08 Oct 1993 15:39:23
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9182
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : SALVAGER
Short Description             : salvage -rec -salvage does not run salvage
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/30/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Rajesh, It looks to me like the -salvage option does not work if you specify 
 the -rec switch first,
 
 [stkbrk] time  salvage  /dev/dsk/c0t1d0s4 -salvage
 Salvaging /dev/dsk/c0t1d0s4
 Processed 3 vols 14 anodes 7 dirs 0 files 0 acls
 Done.  /dev/rdsk/c0t1d0s4 checks out as Episode aggregate.
 0.0u 0.0s 0:04 0% 0+0k 0+0io 0pf+0w
 [stkbrk] 
 [stkbrk] 
 [stkbrk] 
 [stkbrk] 
 [stkbrk] 
 [stkbrk] 
 [stkbrk] time  salvage  /dev/dsk/c0t1d0s4 -rec  -salvage
 Will run recovery on /dev/dsk/c0t1d0s4
 0.0u 0.0s 0:00 0% 0+0k 0+0io 0pf+0w
 [stkbrk] 
 bruce-Thu, 30 Sep 1993 15:09:51
 
 Specifying the -recover and -salvage switch only caused recovery to be
 executed. Also the salvager did not complain of if a directory was specified
 as the device early enough.
 rajesh-Fri, 01 Oct 1993 16:15:20
**Solution Text**
 Delta rajesh-db4408-episode-salvager-interface-robustness
 
 rajesh-Fri, 01 Oct 1993 16:15:24
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9181
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : memory leak when you change initial object and container acl
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/20/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 There are a couple of places in the code where memory leaks exist.
 The delta fixes these bugs. 
 
 The larger problem is that the for each acl that has non-simple
 entries, i.e. has a user, group etc entry, the incore version
 allocates slightly more than 3 times the amount of kernel memory needed.
 With dnlc caching and vnodes in the lru list, and in the presence
 of files with very large ACLs, this could and has caused kernel memory
 exhaustion. Bug 4360 addresses this issue.
 rajesh-Wed, 22 Sep 1993 13:07:41
 
 rajesh-Wed, 22 Sep 1993 13:19:03
**Solution Text**
 rajesh-db4348-episode-memory-leak-for-initial-dir-or-file-acl
 
 rajesh-Wed, 22 Sep 1993 13:07:43
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9180
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Directory entry has zero length name after recovery
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 This error is being encountered by Bob thru running low tests via the AFS
 translator. This is on IBM product release DFS. The data is in
 ~rajesh/epi/test/silo/rcx/rs_aix32/log/bob
 rajesh-Mon, 13 Sep 1993 20:13:54
**Solution Text**
 Delta rajesh-db4313-episode-check-object-name-length-on-creation.
 This add checks into Episode to prevent creation of objects with null
 names or names with strlen > 255.
 
 rajesh-Fri, 01 Oct 1993 09:44:54
 
 Updated to prevent creation of objects with strlen(name) > 256.rajesh-Tue, 05 Oct 1993 12:27:11
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9179
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : RCX tests and stat/lstat system call problems
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 These errors happen irregularly, while running RCX tests Solaris, in a
 mode where file operations on the source aggregate continue, while the
 copy is being checked for consistency. RCX tests lstat(2) each and
 every file/dir in each fileset on the copy aggregate in the process
 of determining consistency. Some of these lstats, return EIO and some
 others return a wrong link count!.
 
 Such a run was encountered on slack (sparc1+) on 3.29+. The aggregates
 are on ~rajesh/epi/test/rcx/log/coven/910.
 
 rajesh-Mon, 13 Sep 1993 12:23:15
 
 rajesh-Mon, 13 Sep 1993 20:15:45
 
 An update on the problem of EIO being returned. - 
 Some terms.
 	A - the original aggr that was frozen
 	B - a copy on which consistency checks are made.
 	C - another copy
 	D - another partition of similar specifications as A, B, C.
 On B, while running RCX consistency checks we, get EIO at an indeterminate
 point of time on say file F in volume V. After adding lots of ICL tracing
 progressively at different episode layers, I determined that EIO
 is being returned because epiv_OpenAnode encounters an anode with the
 first byte of the anode being zero instead of containing the magic
 #, type flags and status area size. 
 
 Pulling out the episode log from C, shows that there is a transaction
 to create the file on which the EIO is received in the log. There are 3 
 records in this transaction that modify the anode in question. 
 Examining the bits on B's disk seem to imply that this transaction was
 undone, i.e indicating some problem in recovery. Hence I copied C to D
 and ran recovery on D and the transaction is done. I repeated this several
 times thus indicating strongly that recovery is not at fault. 
 Doing lstats on B, results in the atimes being updated and the pass numbers
 of the log pages in B is in one case 18 whereas those in C have pass numbers
 14 (~rajesh/sb/rcx/slack/921/aggr.0921.aa.011), which seems to bear this out.
 Its possible that one of these updates screwed things up but unlikely. 
 To try to test this theory out, I dumped the current log in B but unfortunately
 that was not very helpful as it had only 1 active log page with 3 records
 which were for a different anode.
 
 At this stage I am parsing the log further to find other interesting
 information and will then compare some of that information with the
 state of B and try to determine which transactions did get done and
 which undone etc. 
 rajesh-Wed, 22 Sep 1993 19:05:50
**Solution Text**
 Salvager uses the raw device interface to the aggregate. When
 performing recovery or salvage, it opens the device with O_RDWR. It
 does not specify the O_SYNC flag, which could result in any writes
 being delayed for a while at the device driver level. Hence there is a
 race here, i.e. if you read the corresponding bytes off the disk
 before the write is done, you could get old data, depending on the
 device driver implementation. On RIOS we did not encounter this race
 but we do on Solaris.
 
 [ This does suggest some anamolies at the device driver level -
   The device driver manages to service a read request for a disk block
   before an older write request to the same block. 
 
   In the case of RCX tests, the block in question never got the right data
   and Ted pointed out, this implied that the write request was completely
   blown off on receiving the read request, before the write had been done. ]
 Delta: rajesh-db4309-fix-recovery
 rajesh-Wed, 29 Sep 1993 08:27:42
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9178
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Add checksumming to aggregate copies in RCX tests
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 To remove possibilites of problems in the "dd" of aggregates, I added
 checksum the source and destination aggregates and bail out if 
 checksums do not match.
 
 rajesh-Thu, 23 Sep 1993 15:33:45
**Solution Text**
 rajesh-db4280-checksum-aggregates
 rajesh-Thu, 23 Sep 1993 15:33:48
 
 This delta has been expanded tremendously in scope since its original
 inception. For the gory details check out the cml history (RCS log).
 
 rajesh-Wed, 29 Sep 1993 08:34:45
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9177
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Episode log has missing pages in the middle
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/03/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 RCX test generated an aggregate in
 ~rajesh/epi/test/rcx/log/luthien/723-724/aggr.0724.ab.004 on which
 salvager verifcation fails after recovery. The aggregate log
 has missing pages in the middle. 
 
 The log has 364 pages. The tail of the log is at page # 245 and the head of
 the log is at page # 166. (wrapped around log). The pass # on pages are
 as follows
 
 Page #s			Pass #
 
 245 - 363		 9
   0 -  27		10
  28 -  30		 4 !!!
  31 - 166               10.
 
 Hence the 3 pages 28 to 30 are bogus. 
 
 The # of in-transit pages count in some of the log pages are as follows.
 
 Page #			# in-transit pages
 
 363			3
 0			4
 ...
 27			4
 
 28			0  ]
 29			0  ] Missing pages
 30			0  ]
 
 31			4
 32			1
 33			2
 34			3
 35			0
 ...
 166			0
 rajesh-Fri, 03 Sep 1993 11:08:36
 
 rajesh-Fri, 03 Sep 1993 12:35:15
 The specific errors pointed out by the salvager are
 1. 
 In volume rcx.103 0,,103 (avl #7)
   in anode (#3) EOF @ (0,4288)
     index 3 is direct block 0x30a
       Block reference beyond end of file
     index 4 is direct block 0x6d9
       Block reference beyond end of file
     index 5 is direct block 0x6da
       Block reference beyond end of file
     index 6 is direct block 0x6db
       Block reference beyond end of file
     index 7 is direct block 0x6dc
       Block reference beyond end of file
     Anode allocated 16, calculated 6
     Anode visible 16, calculated 6
   Volume allocated/visible quota mismatched: usage 89/89, computed 79/79
 
 This is because a valid transaction in active portion of the log
 increases the size of the directory from 8K to 16K and fills in all
 relevant block numbers. However an invalid transaction in the
 log pages with pass# 4 changes the length from 10A0 to 10C0 causing
 salvager to complain about the blocks being end of file.
 
 2.
 In 5,3 at rcx.101:/rcx.test/
   Directory not okay because page 0, bad magic number in entry 149.
   in anode (#2)
 This is because a invalid transaction 140307 on page 30 (pass # 4)
 sets the entry to be free. And since the hashtable continues to point
 to it, the salvager complains.
 rajesh-Tue, 07 Sep 1993 16:14:58
**Solution Text**
 The delta 
 rajesh-db4269-check-pass-number-consistency-in-log-pages-before-recovery
 fixes recovery to bail out it encounters an active log page with
 a invalid pass number.
 
 This does not answer the question as to why some log pages did not make
 it out to disk. 
 rajesh-Tue, 07 Sep 1993 12:47:51
 
 Entered as OT 9013
 
 andi-Tue, 05 Oct 1993 10:50:24
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9176
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : crashes on NULL dev in epig_SayWhyNotLikeSuperBlock
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 ota:
 
 The simple_test on epi-blake fails when firstBlock is set to -1:
     ## Setting firstBlock (4 bytes at offset 65564) to 4294967295
     ##!! Salvager killed by signal 11
 [Mon Aug 30 15:41:17 1993]
 ota-Mon, 30 Aug 1993 15:41:24
**Solution Text**
 ota:
 
 Delta: ota-db4204-no-quota-checks-for-duplicate-block-resolution
 
 This problem was just an inverted if stmt which did the non-NULL dev stuff if
 dev was NULL instead of the other way around.  Fixed in the delta for 4204.
 [Mon Aug 30 16:16:33 1993]
 ota-Mon, 30 Aug 1993 16:16:38

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9175
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : ftserver core dump doing a fileset dump
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 When trying to do a fileset move, ftserver core dumps on the HP platform. 
 This seems to take place because of wrong use of pointers. 
 khale-Wed, 29 Sep 1993 11:43:00

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9174
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : sun putpage should discard pages from asyncError vnodes
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Solution Text**
 kazar-db4448-cm-ustrategy-should-discard-pages-on-asyncerror
 Mike Kazar
 Fri Oct  8 10:46:22 1993
 kazar-Fri, 08 Oct 1993 10:46:24
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9173
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : bullet-proof cm_readdir against 0 length records
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/07/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 kazar-db4446-bullet-proof-readdir-against-0-len-records 
 Mike Kazar
 Fri Oct  8 10:44:20 1993
 kazar-Fri, 08 Oct 1993 10:44:21
**Solution Text**
 kazar-db4446-bullet-proof-readdir-against-0-len-records 
 Mike Kazar
 Fri Oct  8 10:44:22 1993
 kazar-Fri, 08 Oct 1993 10:44:23
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9172
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : rename from afs to dfs deadlocks
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Rename from afs to dfs deadlocks because DFS makes up wrong fsid.
 Mike Kazar
 Thu Oct  7 10:58:58 1993
 kazar-Thu, 07 Oct 1993 10:59:06
**Solution Text**
 kazar-db4441-afs-to-dfs-rename-deadlocks 1.2
 Mike Kazar
 Fri Oct  8 10:43:24 1993
 kazar-Fri, 08 Oct 1993 10:43:26
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9171
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TKC
Short Description             : ZLC tests fail because of TKC optimistic grant change
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 The ZLC tests (src/test/file/zlc) reveal a regression introduced by the
 addition of optimistic grants to TKC.  The problem is that in 
 tkc_SelectTokens() (called from tkc_HandleClose()) we are looking for an exact
 match with TKN_OPEN_READ, but with optimistic grants, TKN_OPEN_READ may
 be granted as part of a token with other permissions.  The upshot is that
 we never return the TKN_OPEN_READ token to the ZLC manager, so that the
 vnode is not released.
 
 There is a secondary problem in that tkc_HandleClose and friends always returns
 tokens instead of checking to see if someone is actually waiting for them.
 jdp-Tue, 05 Oct 1993 12:15:03
**Solution Text**
 kazar-db4427-fix-tkc-open-confusion
 Mike Kazar
 Tue Oct  5 18:11:48 1993
 kazar-Tue, 05 Oct 1993 18:11:50
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9170
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : solaris glue code checks for EXDEV incorrectly
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Well, some UFS vnodes can be "spuriously" converted due to the
 way that Solaris recycles vnodes.  Our code to detect EXDEV
 is unprepared for this, and reports EXDEV whenever it finds
 an unconverted vnode.  Need to check volume instead.
 Mike Kazar
 Mon Oct  4 21:02:04 1993
 kazar-Mon, 04 Oct 1993 21:02:11
**Solution Text**
 kazar-db4424-watch-for-new-vops-sans-converted-flag
 Mike Kazar
 Tue Oct  5 18:11:09 1993
 kazar-Tue, 05 Oct 1993 18:11:12
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9169
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : hosts/machine/self (root) can't access DFS
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Transferring this defect from the TP database on behalf of
 Phil, Dan Hamel and DavePifer.
 
 Marked as "high" so we'll talk about it at today's 11:30 defect
 meeting.
 
 ### sanzi 9/30/93
 
 When logged on to a DFS client machine as root, you assume the machine's
 'hosts/<machine>/self' identity. This identity doesn't allow you to
 access any DFS directories except those that have any_other acl's.
 
 Wierdness:
 However, if you do a dce_login as 'hosts/<machine>/self', you can then
 see directories according to the acl's.
 
 This is particularly bad because one of the things that people will
 expect with DFS is that daemons running as root can have access
 to DFS (e.g. sendmail).
 The DFS directory of interest:
 colorado.transarc.com% ls -l hirsch
 drwxrwxrwx   2 root     root         256 Sep 21 11:26 hamel
 colorado.transarc.com%  acl_edit hirsch -l
 
 # SEC_ACL for hirsch:
 # Default cell = /.../ps.dce.transarc.com
 user_obj:rwxcid
 group_obj:r-x---
 other_obj:r-x---
 colorado.transarc.com% 
 Now, on an AIX client:
 
 # klist
 DCE Identity Information:
   Global Principal: /.../ps.dce.transarc.com/hosts/solitaire/self
   Cell:      007d6d42-c70d-1c98-8234-9e620a3a0000 /.../ps.dce.transarc.com
   Principal: 0000006f-afb9-2c9d-8a00-9e620a3a0000 hosts/solitaire/self
   Group:     0000000c-c70f-2c98-8201-9e620a3a0000 none
   Local Groups:
              0000000c-c70f-2c98-8201-9e620a3a0000 none
              0000006b-c78b-2c98-8a01-9e620a3a0000 subsys/dce/dts-servers
 
 Identity Info Expires: 93/09/23:23:29:56
 
 ##### Since you're authenticated as 'self', you'd expect access via
 ##### the other_obj acls
 
 # ls -l /:/usr/hirsch
 /:/usr/hirsch unreadable
 total 0
 # 
 
 # cd /:/usr/hirsch
 /:/usr/hirsch: Permission denied
 # 
 # dce_login hosts/solitaire/self solitaire
 # klist
 DCE Identity Information:
         Warning: Identity information is not certified
         Global Principal: /.../ps.dce.transarc.com/hosts/solitaire/self
         Cell:      007d6d42-c70d-1c98-8234-9e620a3a0000 /.../ps.dce.transarc.com
         Principal: 0000006f-afb9-2c9d-8a00-9e620a3a0000 hosts/solitaire/self
         Group:     0000000c-c70f-2c98-8201-9e620a3a0000 none
         Local Groups:
                 0000000c-c70f-2c98-8201-9e620a3a0000 none
                 0000006b-c78b-2c98-8a01-9e620a3a0000 subsys/dce/dts-servers
 
 Identity Info Expires: 93/09/24:00:14:44
 Account Expires:       never
 Passwd Expires:        never
 # 
 # 
 # cd /:/usr/hirsch
 # 
 
 ##### It works if we explicitly dce_login as 'self'!!!
 #############
 
 The same happens if the machines 'self' identity is explicitly on the
 acl list:
 
 # klist
 DCE Identity Information:
     Global Principal: /.../ps.dce.transarc.com/hosts/solitaire/self
     Cell:      007d6d42-c70d-1c98-8234-9e620a3a0000 /.../ps.dce.transarc.com
     Principal: 0000006f-afb9-2c9d-8a00-9e620a3a0000 hosts/solitaire/self
     Group:     0000000c-c70f-2c98-8201-9e620a3a0000 none
     Local Groups:
             0000000c-c70f-2c98-8201-9e620a3a0000 none
             0000006b-c78b-2c98-8a01-9e620a3a0000 subsys/dce/dts-servers
 
 Identity Info Expires: 93/09/23:23:29:56
 # 
 # 
 # cd /:/usr/hamel
 /:/usr/hamel: Permission denied
 # 
 # acl_edit /:/usr/hamel
 sec_acl_edit> get
 Granted permissions: r-x---
 sec_acl_edit> 
 
 colorado.transarc.com% acl_edit hamel -l
 
 # SEC_ACL for hamel:
 # Default cell = /.../ps.dce.transarc.com
 mask_obj:rwx-id
 user_obj:rwxcid
 user:hosts/solitaire/self:r-x---
 group_obj:rwx-id
 other_obj:rwx-id
 ################
 
 And you get the same results on a Solaris client:
 
 colorado.transarc.com% acl_edit hamel -l
 
 # SEC_ACL for hamel:
 # Default cell = /.../ps.dce.transarc.com
 mask_obj:rwx-id
 user_obj:rwxcid
 user:hosts/solitaire/self:r-x---
 user:hosts/colorado/self:r-x---
 group_obj:rwx-id
 other_obj:rwx-id
 colorado.transarc.com% 
 colorado.transarc.com# klist
 DCE Identity Information:
     Global Principal: /.../ps.dce.transarc.com/hosts/colorado/self
     Cell:      007d6d42-c70d-1c98-8234-9e620a3a0000 /.../ps.dce.transarc.com
     Principal: 0000007c-e3f6-2ca1-ad00-9e620a3a0000 hosts/colorado/self
     Group:     0000000c-c70f-2c98-8201-9e620a3a0000 none
     Local Groups:
            0000000c-c70f-2c98-8201-9e620a3a0000 none
            0000006b-c78b-2c98-8a01-9e620a3a0000 subsys/dce/dts-servers
 
 Identity Info Expires: 93/09/24:00:01:17
 Account Expires:       never
 Passwd Expires:        never
 
 colorado.transarc.com# 
 colorado.transarc.com# cd /:/usr/hamel
 /:/usr/hamel: Permission denied
 colorado.transarc.com# 
 colorado.transarc.com# 
 #### Note the the ACL manager returns the right results:
 
 colorado.transarc.com# acl_edit /:/usr/hamel
 sec_acl_edit> get
 Granted permissions: r-x---
 sec_acl_edit> 
 ##### Now explicitly dce_login as the same principal:
 
 colorado.transarc.com# dce_login hosts/colorado/self colorado
 colorado.transarc.com# klist
 DCE Identity Information:
     Warning: Identity information is not certified
     Global Principal: /.../ps.dce.transarc.com/hosts/colorado/self
     Cell:      007d6d42-c70d-1c98-8234-9e620a3a0000 /.../ps.dce.transarc.com
     Principal: 0000007c-e3f6-2ca1-ad00-9e620a3a0000 hosts/colorado/self
     Group:     0000000c-c70f-2c98-8201-9e620a3a0000 none
     Local Groups:
              0000000c-c70f-2c98-8201-9e620a3a0000 none
              0000006b-c78b-2c98-8a01-9e620a3a0000 subsys/dce/dts-servers
 
 Identity Info Expires: 93/09/24:01:54:47
 Account Expires:       never
 Passwd Expires:        never
 colorado.transarc.com# 
 colorado.transarc.com# 
 colorado.transarc.com# cd /:/usr/hamel
 colorado.transarc.com# 
 hamel-Thu, 23 Sep 1993 17:56:56
  sanzi-Thu, 30 Sep 1993 09:29:13
**Solution Text**
 kazar-db4406-allow-root-to-be-machine-self
 Mike Kazar
 Tue Oct  5 18:09:58 1993
 kazar-Tue, 05 Oct 1993 18:09:59
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9168
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : OSI
Short Description             : Potential security hole when AFS and DFS coexist
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/21/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Since the kernel PAG generated by DFS and AFS are indistinguishable and
 occupy the same slots in the unix credentials of a user, there is a
 possibility of PAG conflict (and thus a security breach) between two
 users---one AFS logged-in and another DFS logged-in.
 
 In spite of the fact that the DFS PAG counter starts at an initialized
 value of 1000 as opposed to the value of 0 used by the AFS PAG counter,
 there is still a finite probability of conflict.  Suppose that a user 
 having DFS credentials (say a PAG value X > 1000) requests for AFS tokens 
 (through 'klog' for instance) and obtains it.  A different user could
 have AFS logged in earlier or can AFS log-in later and obtainthe same PAG
 value of X, since the PAG spaces for the two file systems are distinct,
 thus resulting in a conflict and granting unauthorized access (AFS
 tokens) to one of the users.
 
 Since AFS may already be deployed at many of the sites where DFS is
 expected to go, this would pose a definite security hole that our
 technical staff has to contend with when questioned by the customers.
 
 What is described here is an accidental conflict that occurs with a
 non-suspecting non-malicious user.  But it is also not difficult to imagine
 a smart malicious user exploting this loophole to count up AFS PAGS at a
 sufficiently fast rate and catching up with a validated currently in-use 
 DFS PAG to gain unauthorized access to another user's credentials.
 
 Could we make the DFS and AFS PAGS distinguishable and have the OSI
 pag-to-group calls look for their own specific type of PAG in the user
 unix credentials?  Or can we make AFS and DFS share thier PAG counter
 somehow (may not be possible on all platforms)?
 nanda-Tue, 21 Sep 1993 14:10:37
 nanda-Tue, 21 Sep 1993 14:13:29
 
 Pervaze, 
 
 I've set the priority to high on this because I think we may need 
 to address this for this release.
 
 Rich
 sanzi-Wed, 22 Sep 1993 17:02:16
**Solution Text**
 kazar-db4350-separate-afs-dfs-pag-spaces
 Mike Kazar
 Wed Sep 29 08:43:29 1993
 kazar-Wed, 29 Sep 1993 08:43:31

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9167
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ITL
Short Description             : ITL did not export SA_INTERRUPT flag (was missing).
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/30/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 This delta fixes a missing flag problem for AIX platform and the sys v
 locking itl scripts on solaris.
 jess-Mon, 30 Aug 1993 18:08:49

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9166
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : DFSEXPORT
Short Description             : dfsexport should return distinct exit statuses
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 dfsexport should exit with distrinct statuses in each of the follwoing
 states:
 	everything exported without error
 	something needed to be salvaged
 	other kinds of errors
 
 This would allow the rc scripts to do "the right thing" without having
 to grep for words in dfsexport's output.  (Besides any issues of
 cleanliness, the grep method is sensitive to LANG and other nls
 state).
 nydick-Wed, 06 Oct 1993 11:17:25
**Solution Text**
 jdp-db4435-dfsexport-exit-with-distinct-code-for-recovery
 Jeffrey Prem
 Mon Oct 11 17:12:06 1993
 jdp-Mon, 11 Oct 1993 17:12:08
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9165
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Failure to drop cm_volumelock in cm_GetVolumeByFid
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/06/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 In the function cm_GetVolumeByFid(), if `quickSearch' is true, it is
 possible to return without dropping cm_volumelock.  I believe this bug
 is responsible for a CM hang that I saw on apollo this morning.
 Jeffrey Prem
 Wed Oct  6 15:56:42 1993
 jdp-Wed, 06 Oct 1993 15:56:44
**Solution Text**
 jdp-db4434-cm-remember-to-drop-volume-lock
 Jeffrey Prem
 Wed Oct  6 15:56:45 1993
 jdp-Wed, 06 Oct 1993 15:56:47
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9164
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : IBM product does not normalize vnode type
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/24/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Since dimitris-dump-vnode-device was never picked up for the IBM product,
 vnode types are not converted to the machine independent number space.  This
 causes a fundamental incompatiblity when restoring dumps taken from an IBM
 product machine to a 1.0.3 machine.  As a result, fileset moves don't work.
 The problem shows up whenever the fileset contains symbolic links, but other
 vnode types will most likely be affected as well.
 jdp-Fri, 24 Sep 1993 11:34:40
**Solution Text**
 jdp-db4374-normalize-vtypes-to-ibm-values
 Jeffrey Prem
 Thu Sep 30 10:29:01 1993
 jdp-Thu, 30 Sep 1993 10:29:04
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9163
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : Restore of inconsistent fset does not mark target inconsistent
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 It is currently possible to dump an inconsistent fileset; however,
 upon restoring the dump the target (of the restore) fileset is not
 marked as inconsistent.  There are at least two possibilities: don't
 allow dumps of inconsistent filesets, or mark the target as
 inconsistent even if the restore was otherwise successful.  The former
 approach is gratuitously limiting at best, but the latter is not
 supported by the current dump format.  Specifically, the volume states
 word is not part of the dump, but it may be possible to find an extra
 bit somewhere to store the status of the inconsistent bit.  I'll be
 investigating the latter approach.
 
 Jeffrey Prem
 Tue Aug 10 11:12:14 1993
 jdp-Tue, 10 Aug 1993 11:12:16
**Solution Text**
 jdp-db4071-keep-fsets-inconsistent-after-restore
 
 Jeffrey Prem
 Mon Oct 11 16:42:22 1993
 jdp-Mon, 11 Oct 1993 16:42:25
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9162
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : DFSEXPORT
Short Description             : Prints confusing error message for permission failure.
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 If you issue the dfsexport command, any you do not have root permisson, 
 you get the following error message.
 
 ftu_ListAggrs() failed. Cannot proceed
 
 A message indicating that the user does not have the required permission
 would be more helpful.
 fred-Tue, 13 Jul 1993 15:59:10
 
 ----------------------------------------------------------------------
 From DB 4048 (which was duped to this defect):
 
 The errors returned by dfsexport are not very illustrative
 
 Eg:
    If you dfsexport with out root access you get:
 [bruce] dfsexport
 		ftu_ListAggrs() failed. Cannot proceed
 
 Not the worlds most user fiendly output.
 
 ---------
 
 Eg:
    When dfsexport -all and there a already attached aggrs the error is 
 confusing:
 
 [bruce] dfsexport -all
 dfsexport: /dev/dsk/c0t0d0s5:/usr5 (id 901):  Already attached: not attached
 				                               ^^^^^^^^^^^^^
 								    |
 Tell the user that the aggr is already attached is good.  But the
 second message not attached is confusing. 
 bruce-Thu, 05 Aug 1993 10:47:18
 
 - created in lieu of TP bug 8487
 pakhtar-Fri, 06 Aug 1993 15:57:22
 
 ----------------------------------------------------------------------
 Jeffrey Prem
 Wed Aug 25 15:44:19 1993
 jdp-Wed, 25 Aug 1993 15:44:25
 
 Ok to fix this in first release - if time permits.
 pakhtar-Mon, 27 Sep 1993 13:26:59
**Solution Text**
 jdp-db3871-allow-aggr-reg-inquiry-as-non-root
 Jeffrey Prem
 Tue Oct  5 15:38:18 1993
 jdp-Tue, 05 Oct 1993 15:38:19
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9161
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : butc aborts when dumping 0 filesets
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
    If you try to dump a fileset family that has no members (i.e. the backup
 system can't find any filesets matching the description given in any of the
 fileset family entries), butc aborts because of an assertion failure in
 osi_Free.
 davecarr-Mon, 27 Sep 1993 13:54:54
**Solution Text**
    This is a special case of the problem reported in defect 4439 and is
 solved there. davecarr-Tue, 12 Oct 1993 11:59:58

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9160
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : XAGGR
Short Description             : Have UFS VOL_SETSTATUS not conflict with vnode ops
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/28/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Avoid deadlocks by allowing UFS to run vnode ops concurrently with
 VOL_SETSTATUS operations such as one might encounter doing syncserver or
 syncfldb subcommands of fts.
 
 This requires at least a sketch of how we'll avoid deadlocks once we
 implement Solaris UFS dump/restore operations.  That will be in the next
 bug report.
 cfe-Tue, 28 Sep 1993 14:56:12
 
 The other bug report is #4389.
 
 cfe-Tue, 28 Sep 1993 15:01:33
**Solution Text**
 Delta: cfe-db4388-fewer-blocking-ufsops
 Backed by dfs-103 3.33
 cfe-Mon, 04 Oct 1993 11:54:56
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9159
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : Protocol upgrades to set context better via secondary
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Document ~cfe/DFSdesigns/tsr-4.{ez,tex,ps} describes a collection of protocol
 changes designed to close windows in which the DFS CM and exporter may not
 synchronize their token states correctly.  In particular, communication
 initiated from a cache manager to a server by means of the server's secondary
 interface is subject to some uncertainty on the client end as to whether the
 server will be holding all its tokens.
 
 This bug is a follow-on to the fix for DB 4262.
 cfe-Tue, 14 Sep 1993 17:07:09
**Solution Text**
 Deltas:
 	cfe-db4322-protocol-rev 1.1
 	cfe-db4322-robustify-protocol-and-tsr 1.4
 	cfe-db4319-fix-refcount-init 1.2 through 1.9
 Backed by dfs-103 3.33.
 cfe-Wed, 06 Oct 1993 13:55:19
 
 Entered as OT 9158
 
 andi-Mon, 18 Oct 1993 14:27:57
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9158
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : Protocol upgrades to set context better via secondary
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Document ~cfe/DFSdesigns/tsr-4.{ez,tex,ps} describes a collection of protocol
 changes designed to close windows in which the DFS CM and exporter may not
 synchronize their token states correctly.  In particular, communication
 initiated from a cache manager to a server by means of the server's secondary
 interface is subject to some uncertainty on the client end as to whether the
 server will be holding all its tokens.
 
 This bug is a follow-on to the fix for DB 4262.
 cfe-Tue, 14 Sep 1993 17:07:09
**Solution Text**
 Deltas:
 	cfe-db4322-protocol-rev 1.1
 	cfe-db4322-robustify-protocol-and-tsr 1.4
 	cfe-db4319-fix-refcount-init 1.2 through 1.9
 Backed by dfs-103 3.33.
 cfe-Wed, 06 Oct 1993 13:55:19
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9157
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : VOLREG
Short Description             : need to initialize a lock before using it
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 vol_Attach() currently does VOL_HOLD followed by lock_Init of the volume's
 mutex lock.  VOL_HOLD will obtain and release the volume's mutex lock.
 Probably what should happen is that the VOL_HOLD should be replaced by an
 assignment of the reference count to 1 in this structure that's being
 initialized.
 
 It's not clear why this has not been a bigger problem.
 cfe-Tue, 14 Sep 1993 13:26:22
**Solution Text**
 Delta: cfe-db4319-fix-refcount-init 1.1
 Based on dfs-103 3.30
 cfe-Tue, 14 Sep 1993 16:01:41
 
 Beware: later revisions of this delta (through 1.9) were mistakenly co-opted
 to fix DB 4322.
 cfe-Wed, 06 Oct 1993 13:17:29
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9156
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : close NewTrans window so that t_accError will always be OK
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/25/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 The ftserver can erroneously return all-OK instead of an appropriate error
 code for FTSERVER_CreateTrans and FTSERVER_CreateVolume, which are the RPCs
 used to initiate sequences of fileset operations.  This occurs because there
 is a window between the time that an ftserver transaction is made visible to
 competing RPCs and the time that the appropriate competing-RPC error code is
 stored in the transaction structure.  Competing RPCs that happen to wish to
 treat the same fileset as the one that is being opened or created can get
 back ``success'' return codes from the ftserver, but no meaningful transaction
 identifier will have been returned from the ftserver, so in fact further
 fileset operations that use the same transaction identifier will not be
 successful, and will fail with the bizarre ``not a transaction'' error code.
 This problem was made particularly visible by an Episode bug that has since
 been repaired, wherein the kernel thread doing a fileset creation was hung,
 hanging the ftserver in its window.
  
 The window in the ftserver can easily be repaired by passing the appropriate
 error code into the ftserver_NewTrans() local call that allocates a transaction
 and makes it visible, so that the error code can be stored in the transaction
 before the transaction mutex is released.
 cfe-Tue, 14 Sep 1993 09:05:32
**Solution Text**
 Delta: cfe-db4194-close-ftserver-trans-window
 Backed by dfs-103 3.33
 cfe-Mon, 04 Oct 1993 17:39:40
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9155
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : fts syncfldb prints error message but succeeds.
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 If the execution of an fts syncfldb command requires that the 
 flserver create (I assume it is the same for delete) a fileset, i.e. the
 fileset exist on the aggregate but not in the fldb, the commands prints
 an error message (with the word ERROR) but succeeds.  The error message
 will be confusing to users, making them believe the command has failed.
 
 In this case no error message should be printed.  Perhaps an advisory message
 such as,
 
   Entry for fileset <name/id> being created in FLDB.
 
 would be more appropriate.  A similar message indicating deletion of
 an entry should be issued if a deletion is required.
 fred-Tue, 13 Jul 1993 15:53:52
**Solution Text**
 Delta: cfe-db3869-remove-syncfldb-non-error-message
 Backed by dfs-103 3.34.
 cfe-Fri, 08 Oct 1993 11:32:48
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9153
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : rename panics Episode
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/05/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 The following sequence of commands will cause Episode to panic
 with a recursive mutex enter:
 
 	mkdir a
 	mkdir a/a
 	mv a/a .
 
 The problem is evidently that the source directory is identical
 to the target file in this case.
 
 blake-Tue, 05 Oct 1993 16:35:01
**Solution Text**
 blake-db4432-missing-rename-check
 blake-Wed, 06 Oct 1993 16:50:44
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9152
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : statvfs call on aggregate mountpoint causes assert
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/01/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 Running the statvfs syscall on an aggregate mount point (ie: in
 /opt/dcelocal/var/dfs/aggrs) causes a "panic: assert".
 
 Stack trace:
 	panic
 	lock_ObtainRead
 	InitFreeSpace
 	epib_GetInfo
 	epig_GetAggregateStatus
 	ag_efsStat
 	agfs_statfs
 	cstatvfs
 	statvfs
 	syscall
 
 To repeat: run this program, giving an aggregate mount point as its
 argument.
 
 --------- vfsstat.c ---------
 /*
  * run statvfs on a filesystem.
  *
  * Copyright (C) 1993 Transarc Corporation - All rights reserved.
  *
  * HISTORY
  * 27-Jul-93  Daniel Nydick (nydick@transarc.com)
  *	Created.
  */
 #include <sys/types.h>
 #include <sys/statvfs.h>
 
 main(int argc, char **argv)
 {
     int exitval = 0;
     struct statvfs vsb;
 
     argc--; argv++;
 
     for (; argc > 0; argc--,argv++) {
 	if (statvfs(argv[0], &vsb)) {
 	    perror(argv[0]);
 	    exitval = 1;
 	    continue;
 	}
 	printf("%s: %s%s%s\n",argv[0],
 	       (vsb.f_flag&ST_RDONLY)?"ReadOnly ":"",
 	       (vsb.f_flag&ST_NOSUID)?"NoSuid ":"",
 	       (vsb.f_flag&ST_NOTRUNC)?"NoTrunc ":"");
 	printf("    %8d blocks, %8d avail, %8d free\n",
 	       vsb.f_blocks,vsb.f_bavail,vsb.f_bfree);
 	printf("    %8d files,  %8d avail, %8d free\n",
 	       vsb.f_files,vsb.f_favail,vsb.f_ffree);
 	printf("    %6s%s%s%s, block=%5d, frag=%5d\n",
 	       vsb.f_basetype,
 	       vsb.f_fstr[0]?"(":"",
 	       vsb.f_fstr[0]?vsb.f_fstr:"",
 	       vsb.f_fstr[0]?")":"",
 	       vsb.f_bsize,vsb.f_frsize);
     }
     exit(exitval);
 }
 nydick-Fri, 01 Oct 1993 10:05:25
**Solution Text**
 blake-db4414-get-global-lock-in-agfs_Statfs
 
 agfs was neglecting to pick up the global lock.  Also, it
 was getting blocks and fragments mixed up.
 blake-Tue, 05 Oct 1993 14:45:46
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9151
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Episode may panic when out of disk space
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/29/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 If Episode encounters an error (e.g., running out of disk space), while
 doing an asynchronous write via the slow path, it will unlock the pages
 associated with the I/O twice, causing an assertion failure in SunOS.
 blake-Wed, 29 Sep 1993 09:52:08
**Solution Text**
 blake-db4394-extra-unlock-on-error
 blake-Thu, 30 Sep 1993 17:09:48
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9150
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : another efs_inactive race
Reported Date                 : 10/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/21/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/18/93 public]
**Description Text**
 While running his vnode recycling test, Dimitris had a panic
 where efs_inactive dereferenced a null anode pointer.  This appears
 to be another race.  While efs_inactive closes the anode, another
 thread may do a VN_HOLD on the vnode; when this thread does the
 corresponding VN_RELE, efs_inactive will get called a second
 time on the same vnode.  But the first efs_inactive has already
 zeroed the anode pointer.
 
 We need to reorganize the code so that efs_inactive only closes
 the anode when no other thread is holding the vnode.
 blake-Tue, 21 Sep 1993 15:49:12
**Solution Text**
 blake-db4354-efs_inactive-race
 
 efs_inactive now holds the vnode mutex throughout most of its
 activities.  We drop it before calling VOP_PUTPAGE and reacquire
 afterwards, bailing out if the ref. count goes up in the interim.
 We also drop it when we are sure that the vnode's pages have all
 been cleaned or destroyed, so that no further VM activity is possible.
 By holding it the rest of the time, we rule out races of the type
 described above.  After closing the anode, we assert (while holding
 the mutex) that there are no other references to the vnode.
 
 It is still possible that efs_inactive will be called with a NULL
 anode pointer if vnvm_schedule fails to write out all the dirty pages,
 e.g., because of running out of disk space.  However, this problem
 will be addressed in the fix for 4359.
 blake-Thu, 30 Sep 1993 17:08:10
**Validation Text**

[10/18/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9142
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : for HPUX: xglue_vget() isn't
checking the vnode returned it by VFSX_VGET.
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.3
Found Date                    : 10/15/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : xvnode/HPUX/xvfs_osglue.c
Sensitivity                   : public

[10/15/93 public]
The following can/will crash a HPUX machine:

   [1] Process A: cd across NFS to some directory that is exported to
       DFS.

   [2] Process B: telnet over to the machine where the above directory
       resides and delete it.

   [3] Process A: issue a "/bin/ls" command.  This will crash the remote
       machine (where the directory physically resides).

The problem is that xglue_vget()  {in xvnopde/HPUX/xvfs_osglue.c} needs
to check both the return status (for 0) and returned vnode (for non-NULL)
before calling XVFS_CONVERT().

I've already fixed and verified this.  A request-to-submit will be
coming soon.

[10/20/93  public]
Fixed in the submit I did on 10/19/93.

[12/17/93 public]
Closed.



CR Number                     : 9140
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : PACs/credentials are being
improperly cached on a server.  See description below.
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.3
Found Date                    : 10/15/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : fshost/fshs_princ.c
Sensitivity                   : public

[10/15/93 public]

For lack of an obvious "responsible" engineer/company, I put my
name on this.  If someone else wants to own/fix this, feel free to...

Symptoms:
    [a] dce_login to some account and do stuff on a server (i.e., on a
        fileset at that server)

    [b] logout and change the primary group associated with that account
        (rgy_edit as cell_admin or whatever)

    [c] now dce_login again to that same account -- with the new/different
        group

    [d] although klist shows that you have the "correct" group, operations
        you perform on that server behave as if you were still in the
        old group (e.g.: touch a file and examine its group)

Why:
    The server is finding your "old" PAC/credentials (whatever the correct
    terminology is) and using them!

    The problem appears to be in fshost/fshs_princ.c :
    the fshs_FindPrincipal() routine, when walking the list of principals
    hanging off of a host structure, only compares the user and cell
    UUIDs in looking for a match.

    It seems as though it ought to do a complete comparison on all the PAC
    fields (user, group, group-list, foreign-group-list, etc.).

Within the next few days, I'm going to try the fix suggested above
(comparing all the PAC fields: cell/user/group/group-list/.etc.).  If
that "works", I'll send out a request-to-submit.

[11/9/93 public]

I've submitted the simple minded "compare all the UUIDs" fix.  Hopefully,
something more clever can be done in the future.

[12/17/93 public]
Closed.



CR Number                     : 9139
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Device files and fifos do not
work through DFS on HPUX (they hang/crash your node).
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.3
Found Date                    : 10/15/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : xvnode/HPUX/xvfs_osglue.c,
xvnode/HPUX/xvfs_os2vfs.c, xvnode/xvfs_vnode.c,
episode/vnops/efs_vnodeops.c, cm/cm_subr.c, osi/osi_ufs.c
Sensitivity                   : public

[10/15/93 public]
DFS/CM access to device files and fifos currently doesn't work under HPUX.
Symptoms: crashes, hangs, other strange things.
This problem showed up under the dfs.maxdir system test -- when it executes
its single "mkfifo" command.

I currently have a fix up and running and will mail out an appropriate
request-to-submit fairly soon.

[10/20/93 public]
Fixed (for DFS/CM access) in the submit I did on 10/19/93.]

[12/17/93 public]
Closed.



CR Number                     : 9137
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : dfstab parsing not robust
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.3
Found Date                    : 10/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/xaggr/dfstab.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot9137-dfsexport-robustify-parsing-of-dfstab
Transarc Herder               : jaffe@transarc.com

[10/15/93 public]
If a dfstab file is editted incorrectly, dfsexport still works
but the aggregate is inaccessible.
incorrect dfstab entry:
# cat /opt/dcelocal/var/dfs/dfstab
# blkdev aggname aggtype aggid 
/dev/dsk/c201d4s0       aggr1   lfs epi1        3
after dfsexporting, dfsexport reports:
# dfsexport
dfsexport: /dev/dsk/c201d4s0, lfs, 0, 0,,0
attempts to fts lsheader the aggregate fail with:
# fts lsheader -server shotz   
Error in lsheader: Requested access denied (dfs / dau)
The 0's in the dfsexport display are the only clue.

[10/15/93 public]
Unable to recover by detaching, editting and re-exporting. I can dfsexport
and lsaggr but I can neither create a new fileset nor see existing filesets
on the aggregate. Upping the priority and severity.

[10/15/93 public]
The way to recover is to reboot and then try this.  You've confused the
in memory dfstab file.  This problem is NOT B1 since you can only get there
by doing something unsupported.
Changed Priority from `1' to `2' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/15/93 public]
Hmm ... I don't agree that having to reboot is an acceptable workaround. There is
user error involved, any user-editted file is prone to that, but I don't see
why detaching, correcting and re-dfsexporting was not sufficient for clearing
up the in-memory dfstab's "confusion". I'd like to leave the priority at 1 unless
there's another workaround.

[10/15/93 public]
Perhaps I'm stating the obvious, but rather than focus on how
to recover, why not focus on putting some bullet proofing in 
dfsexport to prevent this situation from occurring again (much as
we did for dfsd and other daemons/utilities in 1.0.2).  There should
be some simple sanity checks dfsexport can perform on the dfstab
before it does anything with the contents.  I'd be surprised if
it took more than a day or so to implement those checks.  Elliot,
if you agree, can you do that for 1.0.3?

[11/4/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com'

[11/5/93 public]
Defect Closure Form
-------------------
Added some bullet-proofing to the dfstab parsing code.
--Verification procedure below--
Checked the following malformed dfstab lines:
	malformed fileset ID for ufs entry
	malformed aggrId for ufs and lfs entry
	ufs entries with only 4 fields
	entries with 1, 2, and 3 fields
Tested on TA build:  
	dce-103-3.36
Tested with backing build:  
	dce1.0.2ab4
Changed H/W Ref Platform from `hppa' to `all' 
Changed S/W Ref Platform from `hpux' to `all' 
Filled in Affected File with `file/xaggr/dfstab.c' 
Filled in Transarc Deltas with 
 `comer-ot9137-dfsexport-robustify-parsing-of-dfstab' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9132
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd,admin_ref
Short Description             : bosserver permission/owner checks are only defaults
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.2
Found Date                    : 10/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[10/15/93 public]
The permission and ownership checks made by the bosserver for certain
installation directories are only vendor-configurable defaults.  We need to
include a note to this extent in the documentation.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[10/15/93 public]
The fact that these protections are only defaults is now documented in the
following files:
 
	admin_gd/dfs/dfs/5_processes_dfs.gpsml
	admin_ref/man8dfs/bos_status.8dfs
	admin_ref/man8dfs/bosserver.8dfs
 
The changes were verified by Mike Comer.  This one can be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 9130
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test/file
Short Description             : clean up test/file
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.3
Found Date                    : 10/15/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[10/15/93 public]
There are a number of confusing things about the
file functional tests which need to be cleaned up.
To start with:
   backup:
     Tests in this directory are not the ones which were run
     for the 1.0.2a release.  In fact, the shell scripts in this
     directory don't even run.  The solution is to defunct the
     tests in this directory.  Transarc will by the end of 1.0.3a
     supply a file which enumerates the actual test commands
     and configurations for the tests which comprise the 
     backup hand-test suite. 
   recovery:
     These tests are useless, and infact, "recovery tests" actually
     refer to the rcx tests which live in file episode.  This
     directory should be defuncted and replaced with the rcx
     dirctory which currently lives in the source treee.
   salvage:
     Useless and will be defuncted.

[10/15/93 public]
please let us deal with this at Transarc.  At a minimum, we will deal 
with recovery, backup, and scavenge.  (Salvage is the tools that cleans
up aggregates). 
Changed Responsible Engr. from `delgado' to `jaffe@transarc.com' 
Changed Resp. Engr's Company from `osf' to `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/15/93 public]

Too late, man.  backup is defuncted.  We are leaving it to you
to submit the proprer backup testing procedure file.

recovery is also defuncted as well.  

There is no reason to keep these two around since no one
uses them, there are other tests which are used instead,
and their presence only serves to cause much confusion for everyone.

All of this was ok'd by Gail who is the dfs-drb authority for the
test area.


As for salvage, if these are in fact tools, then they don't belong
in the test tree at all, and should be put under src/file/episode.
Feel free to put them there.

[12/17/93 public]
Closed.



CR Number                     : 9129
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : fts rmsite works with detached aggregates
Reported Date                 : 10/15/93
Found in Baseline             : 1.0.2
Found Date                    : 10/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[10/15/93 public]
The fts rmsite command (as of code defect 8043) can remove a replication site
from a detached aggregate provided the user specifies the aggregate ID of the
detached aggregate with the -aggregate option.  This needs to be mentioned in
the documentation.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[10/15/93 public]
The instructions have been included in the appropriate places, as follows:
 
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_ref/man8dfs/fts_rmsite.8dfs
 
The changes were verified by the inimitable Dr. E; this one can be eradicated.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 9123
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : config/dfs_config
Short Description             : The REMOVE operation doesn't
completely remove an fldb server
Reported Date                 : 10/14/93
Found in Baseline             : 1.0.3
Found Date                    : 10/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dfs.unconfig
Sensitivity                   : public

[10/14/93 public]
 - 2 FLDB servers are already up and in a cell.
 - A third FLDB server (rat) was brought in to
   the cell. It crashed out of the
   config (I believe it was the bosserver that died) and 
   therefore never made it completely into a running
   FLDB server - but cdcsp and rpcd had been modified
 - An UNCONFIG of the third FLDB server was done on the security server.
 - It was noticed that ls /: in the permcell gave
   no information initially, then gave the following
   error:
     dfs: dce errors (336760839) from dfsbind helper
     stcode of # = Server not found in Kerberos database
 - The command that we needed to correct this problem, from
   test/systest/admin/file/tests/admin_checklist, was:
     rpccp show group /:
       to list all the groups and see that rat was still in there
     rpccp remove member  -m /.../permcell/hosts/rat/self /.../permcell/fs
       to actually remove rat from the list

We observed that dfs_config does cdscp and rpccp commands in the
dfs_ns_init routine and rpccp commands in the dfs_nsinit (the
latter does the add member).  There are no corresponding rpccp commands in
the dfs.rm file.  dfs.rm or dfs.unconfig should do the complementary
operations.  It seems like there may be other configurations that aren't
completely REMOVEd.

[10/14/93 public]
Ron Arbo is the man for DFS problems....

[10/25/93 public]
This is fixed.

[12/17/93 public]
Closed.



CR Number                     : 9119
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Document maximum possible number of replicas
Reported Date                 : 10/14/93
Found in Baseline             : 1.0.2
Found Date                    : 10/14/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[10/14/93 public]
The documentation needs to mention the maximum number of read-only replicas
that can be created of any fileset.  It also needs to discuss some factors,
such as site (server/aggregate) placement, that can affect how many replicas
are possible.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[10/14/93 public]
The maximum number of replicas of a given fileset is 16, with some caveats.
These facts were added to the following files:
  
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_ref/man8dfs/fts_addsite.8dfs
  
Dr. E verified the new information; this one can be nuked.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `See Description' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 9114
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_RecoverSCacheToken does not init tkn for return
Reported Date                 : 10/13/93
Found in Baseline             : 1.0.2
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : cm_tokens.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot9114-rcvrscp-init-4-tkn-rtn
Transarc Herder               : jaffe@transarc.com

[10/13/93 public]
cm_RecoverSCache() is not initializing the token structure (tokenl)
properly before passing it to cm_QueueAToken.  Namely
it is not setting the expiration time or the token types
fields.  This can cause the wrong rights to be returned
to the server.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/14/93 public]
Defect Closure Form
-------------------
Tested with dfstracing while moving a fileset which had open and
locked files on it.
Filled in Transarc Deltas with `cburnett-ot9114-rcvrscp-init-4-tkn-rtn' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9108
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Do not detect access beyond EOF of mapped file
Reported Date                 : 10/13/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot9108-AIX-map-beyond-EOF
Transarc Herder               : jaffe@transarc.com

[10/13/93 public]
In AIX, when an Episode file is mapped into memory (e.g. by the mmap
system call), we are not catching accesses beyond the last page of the file.
(The application should get a SIGBUS signal).  This is because we are not
checking for PFEOF.  It appears that IBM's release has this problem too.
Here's another problem which I am putting on the same OT report although it
is surely a different bug.  A store into a memory mapped to a hole in the
file generates a SIGSEGV signal.  I notice that IBM's release does not have
this problem.  I don't know what the problem is.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/14/93 public]
Filled in Interest List CC with `demail1!carl'

[10/26/93 public]
It is not possible to completely fix the problem of references beyond the
last page of the mmap'ed file.  The AIX VMM does not tell the file system
whether or not a read reference is to a mmap'ed file.  We only get this
information for write references.
The current set of mmap tests uses only read references to test this, so
I didn't test whether the beyond-last-page bug was fixed for write references.
However, the SIGSEGV bug is definitely fixed and the fix is adequately
tested by the mmap tests.  See defect closure form below.
Defect Closure Form
-------------------
--Regression test program below--
src/test/file/mmap/mmt.c
--Other explanation below--
If you get SIGSEGV while running this, the bug isn't fixed.
Associated information:
Tested on TA build:  
dfs-osf-1.7
Tested with backing build:  
dce1.0.3bl4
Filled in Transarc Deltas with `bwl-ot9108-AIX-map-beyond-EOF' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9106
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/admin/file/tests/admin_checklist
Short Description             : newaggr should be used with
the -overwrite option
Reported Date                 : 10/13/93
Found in Baseline             : 1.0.3
Found Date                    : 10/13/93
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/admin/file/tests/admin_checklist
Sensitivity                   : public

[10/13/93 public]
It's likely that whoever is reading this will need to use the newaggr
-overwrite option, so it should be mentioned.

[12/14/93 public]
This CR was missed due to non-"test" status. Updating and assigning
to Cindy who is doing a lot of clean up work on the admin_checklist.
Thanks Cindy!

[12/22/93 public]
Added the overwrite switch to the newaggr command.



CR Number                     : 9099
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Fix fms example
Reported Date                 : 10/13/93
Found in Baseline             : 1.0.2
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[10/13/93 public]
The figures listed in the example output of the fms command
are mathematically impossible according to the algorithm
used by the command
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/14/93 public]
The following files are affected:
	admin_gd/dfs/dfs/9_backup_dfs.gpsml
	admin_ref/man4dfs/FMSLog.4dfs
	admin_ref/man8dfs/fms.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and
closed this bug.



CR Number                     : 9098
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Mapping Episode files to memory causes panic
Reported Date                 : 10/13/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot9098-hp-episode-pageout-panic
Transarc Herder               : jaffe@transarc.com

[10/13/93 public]
When an Episode file (accessed locally) is mapped to memory via mmap, the
first pageout causes a panic.  The problem is that the kernel function
vfs_mapdbd has a switch statement on v_fstype which doesn't have a case
for VEFS, and the default case is to panic.
vfs_mapdbd is called from vfs_dbdcheck, which is called ultimately from
foreach_valid, which is called from vfs_pageout.
If it's not feasible to fix the kernel, it would be possible to fix this
wholly within Episode by writing our own versions of various functions:
efs_pageout, efs_vfdcheck, efs_mapdbd, etc.  Ugly but not terribly difficult.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/8/93 public]
Memory mapped files is not part of the exit criteria for 1.0.3a.

[11/23/93 public]
Several changes to get Episode memory mapping to work:
- write efs_pageout, efs_vfdcheck, and efs_mapdbd;
- dummy declaration of VEFS in efs_misc.c is for non-kernel only;
- must call mtrunc during file truncation;
- efs_pagein must catch references to pages beyond EOF and return SIGBUS;
- get rid of some superfluous definitions/declarations in efs_hpux_subr.c
  left over from previous work;
- (Transarc internal #4702) fix a VM deadlock by dropping the buffer lock
  in elbb_ModifySource around the call to copyin;
- (Transarc internal #4703) call uiomove instead of doing it "by hand" in
  epix_GetInline (and other places), because HP/UX has other segment types
  besides USER and SYS.
Defect Closure Form
-------------------
--Verification procedure below--
Run the mmap tests on locally mounted Episode.
Associated information:
Tested on TA build:  
dfs-osf-1.11
Tested with backing build:  
dce1.0.3bl10
Filled in Transarc Deltas with `bwl-o-ot9098-hp-episode-pageout-panic' 
Changed Transarc Status from `open' to `export'

[12/7/93 public]
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 9096
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : osi
Short Description             : panic when mapping DFS files to memory
Reported Date                 : 10/13/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[10/13/93 public]
When one maps DFS files to memory using mmap, a call to msync causes a panic.
The VASSERT string is:
  vm_valusema(&pfdat[pfn].pf_lock) <= 0, file: ../machine/hdl_fault.c, line 807
The stack trace is:
  panic
  assfail
  hdl_remap
  afs_strat_map
  osi_MapStrategy
  cm_strategy
  nux_strategy
  xglue_strategy
  asyncpageio
  vfs_pagiout
  nux_pageout
  xglue_pageout
  do_msync
  foreach_pregion
  msync
  syscall
  ...
My current guess is that the problem is that the problem is that afs_strat_map
is failing to save/restore the buffer's b_bcount field.  This would cause
pageiocleanup (which is called later) to fail to clean up correctly.  I don't
know how things work exactly, so I'm going to have to try this out.  My
guess is based on the fact that nfs_strat_map, on which this function appears
to have been modeled, does save/restore the b_bcount field.
Here's another, probably unrelated, difference.  nfs_strat_map holds the
filesys sema over the I/O and the calls to hld_remap and hdl_kunmap.
afs_strat_map only holds it over the I/O.  Is this OK?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/8/93 public]
Memory mapped files are not part of the exit criteria for 1.0.3a

[12/17/93 public]
Closed.



CR Number                     : 9089
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fset
Short Description             : Fix the search path of fset_test in lock00.itl
Reported Date                 : 10/12/93
Found in Baseline             : 1.0.2
Found Date                    : 10/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[10/12/93 public]
There is a minor problem in the test/fset/scripts/lock00.itl : If the 
user (root) does not have . in his seatch path then lock00.itl would not be
able to locate fset_test and the test will fail mysteriously.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/17/93 public]
Closed.



CR Number                     : 9088
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : in memory dfsd doesn't restart properly
Reported Date                 : 10/12/93
Found in Baseline             : 1.0.3
Found Date                    : 10/12/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dfs_config, src/config/rc.dfs
Sensitivity                   : public

[10/12/93 public]
The rc.dfs doesn't get updated with the arguments to dfsd when
configuring an in-memory dfs cache.  The result is that the
DFS may not come up properly on the first restart after a configure
of a in-memory DFS client.

[10/25/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 9085
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : crash in tkm_FidHash_TryWaitingTokens
Reported Date                 : 10/11/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/11/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot9085-token-on-stack
Transarc Herder               : jaffe@transarc.com

[10/11/93 public]
In CHO tests, we are getting crashes in tkm_FidHash_TryWaitingTokens due
to invalid entries on the tnksWaitingOnThisFid list of the fid hash cell.
Looking at the cell, we see that there is one entry on that list, and it
points to a stack address.  Presumably some other process declared a token
on its stack and put it on that list.  We think the culprit is 
tkm_FidHash_AsyncRevokeFid which declares a token on the stack and passes
it to tkm_Token_QueueRequest.  Later, some other process tries to use the
token, but the stack address isn't valid in another process.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/13/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
I don't know how to test this directly.  We found this in CHO tests, and
we tested it by running CHO tests.
Associated information:
Tested on TA build:  
dfs-osf 1.6
Tested with backing build:  
dfs-1.0.3bl4
Filled in Transarc Deltas with `bwl-ot9085-token-on-stack' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9084
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : udebug needs to be changed to match docs
Reported Date                 : 10/11/93
Found in Baseline             : 1.0.3
Found Date                    : 10/11/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ncsubik/udebug.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot9084-udebug-make-it-match-docs
Transarc Herder               : jaffe@transarc.com

[10/11/93 public]
Make a few minor changes to udebug so that it matches what we're 
saying about it in the documentation.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/11/93 public]
Defect Closure Form
-------------------
This command requires more work to get it to the point where it is 
understandable by a human.  The changes here are to make the u/i
more consistent with the other commands and to try to do a little
better than "server started at".  Some work has already been done for 
the AFS version that should be brought forward.
 [ncsubik] ./udebug help
 Usage: ./udebug  -rpcgroup <RPC_server_group> [-server <machine>]
 [-long] [-help]
 [ncsubik] ./udebug /.:/fs saratoga
 Host 192.55.207.146, his time is 0
 Vote: Last yes vote for 192.55.207.26 at -5 (sync site); Last vote started at -6
 Local db version is 750116014.2838
 I am not sync site
 Lowest host 192.55.207.26 at -5
 Sync host 192.55.207.26 at -5
 Sync site's db version is 750116014.2838
 0 locked pages, 0 of them for write
 This server has never been sync site
 [ncsubik] ./udebug /.:/fs vespers
 Host 192.55.207.26, his time is -1
 Vote: Last yes vote for 192.55.207.26 at -6 (sync site); Last vote started at -6
 Local db version is 750116014.2838
 I am sync site until 84 (4 servers)
 Recovery state 1f
 Sync site's db version is 750116014.2838
 0 locked pages, 0 of them for write
 This server last became sync site at -251884
 
 Server 192.55.207.36: (db 750116014.2838)
     last vote rcvd at -5, last beacon sent at -6, last vote was yes
     dbcurrent=1, up=1 beaconSince=1
 
 Server 192.55.207.146: (db 750116014.2838)
     last vote rcvd at -5, last beacon sent at -6, last vote was yes
     dbcurrent=1, up=1 beaconSince=1
 
 Server 192.55.207.94: (db 750116014.2838)
     last vote rcvd at -5, last beacon sent at -6, last vote was yes
     dbcurrent=1, up=1 beaconSince=1
 
Associated information:
Tested on TA build:  
	dfs-103-3.35
Tested with backing build:  
	dce1.0.2ab4
Filled in Affected File with `file/ncsubik/udebug.c' 
Filled in Transarc Deltas with `comer-ot9084-udebug-make-it-match-docs' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9066
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : dfsexport with no options requires no privileges
Reported Date                 : 10/7/93
Found in Baseline             : 1.0.2
Found Date                    : 10/7/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[10/7/93 public]
Issuing the dfsexport command with no options displays a list of the aggregates
and partitions that are currently exported from the machine.  No privileges are
required for this operation.  The documentaion needs to be corrected in this
regard.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[10/14/93 public]
The following files include the necessary information about using the dfsexport
command to list exported aggregates and partitions:
 
	admin_ref/man8dfs/dfsexport.8dfs
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
 
Craig verified the simple changes; this one can be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `See Description' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed
this bug.



CR Number                     : 9063
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cp -p does not preserve times in DFS
Reported Date                 : 10/7/93
Found in Baseline             : 1.0.2
Found Date                    : 10/7/93
Severity                      : B
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : cm_vnodeops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot9063-cp-p-does-not-preserve-times
Transarc Herder               : jaffe@transarc.com

[10/7/93 public]
    Here is an example:
    cd /tmp ; ls -l ifreqs.out
    -rw-r--r--   1 root     system        82 Aug 11 09:29 ifreqs.out
    cp -p ifreqs.out ifreqs.out-p; ls -l if*
    -rw-r--r--   1 root     system        82 Aug 11 09:29 ifreqs.out
    -rw-r--r--   1 root     system        82 Aug 11 09:29 ifreqs.out-p
    This shows that within the JFS the -p flag works fine. Now with DFS:
    cp -p ifreqs.out /:   ; cd /:  ; ls -l
    -rw-r--r--   1 root     system        82 Sep 22 14:47 ifreqs.out-p
    As you can see the actual date is put on the file and it seems that
    the -p flag is totally ignored. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/7/93 public]
Defect Closure Form
-------------------
cp -p now works.
Filled in Transarc Deltas with `cburnett-ot9063-cp-p-does-not-preserve-times' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 9050
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : No defined interface for representing file sizes > 2GB (32 bits) on 32 bit systems running DFS.
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.1
Found Date                    : 10/5/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1
Affected File(s)              : rfc
Sensitivity                   : public

[10/5/93 public]

This report addresses the problem of interaction  between systems
that support 64 bit file sizes (defined by off_t) and those that
define off_t as 32 bits.  Such questions arise as to what error 
is returned when a 64 bit client attempts to access a file location
beyond 32 bits on a 32 bit system?  Or a 32 bit client accesses a
4GB file on a 64 bit server.  Are the 'high' fields in the protocol
packets being looked at by all DFS implenmlementations?  No defined
method of handling these situations exists in DFS.

Perhaps the solution to this is material for the next sig.

[11/10/93 public]

This issue is slated for the 1.1 timeframe.  We are currently
working with DEC on this issue.  An RFC will appear soon - stay tooned.

[06/7/94 public]

We've published an RFC dealing with this issue.  This is
all that we can do.  Maybe the new world of 1.2 will actually
supply an implementation.



CR Number                     : 9043
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dfs.glue
Short Description             : dfs.glue fills "remote" disk with stale creds
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.3
Found Date                    : 10/5/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/dfs.glue
Sensitivity                   : public

[10/5/93 public]
We've been unable to complete the 48 hour dfs.glue test for the repfldb
checklist because the "remote" hosts keep filling up.  It turns out the
disk is being filled with stale cred files that are created each time a
dce_login is done.

[10/13/93 public]
add a kdestroy to follow every dce_login. Fixed in version 1.1.8.3.

[12/17/93 public]
Closed.



CR Number                     : 9040
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : efs_strategy breaks AIX B_DONE assumptions
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 In efs_strategy, we short-circuit any I/O that is scheduled after
 the evnode VD_NOIDENTITY bit is set, and as part of this, we call
 epia_NotifyBuf to do the appropriate post-I/O cleanup.  Before
 calling epia_NotifyBuf, we set the B_DONE bit in the buf's flags
 so that our cleanup code will think that the I/O is really finished.
 However, this is the wrong thing to do on AIX.  The AIX version
 of epia_NotifyBuf calls iodone, which insists that B_DONE not be
 set.  The statement setting B_DONE should have an #ifdef around it.
 blake-Thu, 16 Sep 1993 14:46:34
**Solution Text**
 blake-db4340-fix-AIX-B_DONE-bug
 blake-Thu, 16 Sep 1993 15:28:27
 
 Entered as OT 
 
 andi-Tue, 05 Oct 1993 11:12:48
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9039
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : typo in efs_strategy AIX code
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 I inadvertently removed a pair of parentheses from the AIX-specific
 code in efs_strategy; this makes Episode panic on AIX.
 
 blake-Thu, 16 Sep 1993 14:37:38
**Solution Text**
 blake-db4339-fix-efs_strategy-typo
 blake-Thu, 16 Sep 1993 15:25:01
 
 Entered as OT 
 
 andi-Tue, 05 Oct 1993 11:01:58
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9038
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : obsolete declarations in compat.h
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Someone has reintroduced declarations in <dcedfs/compat.h> for
 functions that no longer exist (and have not for some time).
 A side-effect of this is zillions of compiler warning messages
 while building.
 
 blake-Tue, 07 Sep 1993 12:38:47
**Solution Text**
 blake-db4275-remove-compat.h-cruft
 blake-Mon, 27 Sep 1993 15:41:11
 
 Entered as OT 
 
 andi-Tue, 05 Oct 1993 10:52:18
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9037
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : newaggr creates unusable aggregates
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Episode currently will not work (in fact it panics) if the
 file system block size is smaller than the system page size.
 It would be nice to fix this, but meanwhile, newaggr should
 not allow users to create aggregates with an unusable block
 size.
 blake-Fri, 13 Aug 1993 14:21:21
**Solution Text**
 Delta: ota-db4099-check-pagesize-in-newaggr r1.3
 
 ota:
 
 On VM integrated systems reject block sizes smaller than PAGESIZE.
 [Tue Aug 17 15:18:54 1993]
 
 ota:
 
 Rev 1.3 takes a more comprehensive approach to this.  It makes sure test_anode
 can continue to test all block sizes on all platforms and also clues the
 salvager in so that it can agree with newaggr and test_anode about what is
 legal and what it not.
 [Thu Aug 19 08:38:07 1993]
 ota-Thu, 19 Aug 1993 08:38:15
 
 Entered as OT 
 
 andi-Tue, 05 Oct 1993 10:19:30
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9036
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : ftutil restoreops.c uses osi_Free with zero bytes of length
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/21/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 Delta: cfe-db3936-restoreops-acl-strlens
 Tested on dfs-103 3.23.
 Allows restorations to happen without core-dumping the ftserver.
 cfe-Wed, 21 Jul 1993 16:31:20
 
 Entered as OT 
 
 andi-Mon, 04 Oct 1993 18:42:36

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9035
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : locally mounted episode doesn't write all data to files.
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Under certain load situations, episode loses data written to files.
 
 The following program is supposed to generate some number of out.a*
 and out.b* files.  They should all be the same legnth (700 bytes).  It
 also generates an out.all file whose length should be the sum of all
 the other files (28000 bytes).  The files contain records specifying
 the pid of their creator and a sequence number.  After running the
 program in a locally mounted episode directory some number of the out.
 files will be shorter than 700 bytes.  They are missing chunks of data
 (always from the beginning of the files?)
 nydick-Tue, 20 Jul 1993 16:33:19
 /*
  * test an episode problem
  *
  * Copyright (C) 1993 Transarc Corporation - All rights reserved.
  *
  * HISTORY
  */
 #include <fcntl.h>
 #include <unistd.h>
 #include <errno.h>
 
 quit(char *why)
 {
     perror(why);
     exit(1);
 }
 
 subproc(int id)
 {
     char buf[100];
     int i;
     int fd1, fd2;
 
     fd1 = open("out.all",O_WRONLY|O_APPEND|O_CREAT,0666);
     if (fd1 < 0) {
 	perror("testout");
 	exit(1);
     }
 
     for (i=0; i<100; i++) {
 	sprintf(buf,"p %06d r %02d\n",getpid(),i);
 	write(fd1,buf,strlen(buf));
 	sprintf(buf,"out.%c%d", (i&1)?'a':'b', getpid());
 	fd2 = open(buf,O_WRONLY|O_APPEND|O_CREAT,0666);
 	if (fd2 < 0) quit(buf);
 	sprintf(buf,"p %06d r %02d\n",getpid(),i);
 	if (write(fd2,buf,strlen(buf)) < 0) quit("write");
 	if (close(fd2)) quit("close");
     }
 
     close(fd1);
     exit(0);
 }
 
 main()
 {
     int i;
 
     for (i=0; i<20; i++)
       if (fork() == 0)
 	subproc(i);
 
     for (i=0; i<20; i++)
       waitpid(0);
     printf("done\n");
 }
 nydick-Tue, 20 Jul 1993 17:05:47
 
 Can one assume that this problem also exists with exported LFS?
 (In the unlikely event that this occurs only on stand-alone LFS, we should
 adjust this defect to reflect the fact that a fix isn't required for
 this release).
 
 pakhtar-Thu, 12 Aug 1993 17:44:59
 The report pretty clearly indicates that there is something seriously
 broken in Episode.  I'd be very surprised if the symptoms showed up
 only in the standalone case; moreover, attempting to characterize
 exactly when incorrect behavior will occur would most likely be a much
 harder task than just fixing the problem.
 
 blake-Thu, 12 Aug 1993 23:45:16
 
 Incidently, Blake's reasoning (that anything that shows up standalone
 is likely to bite us via DFS) is why I play with SA episode at
 all.  It seems clear that a problem which can be reproduced without
 all the additional DFS baggaage will be easier to debug.
 nydick-Fri, 13 Aug 1993 08:57:36
**Solution Text**
 Dan's program opened all its files with O_CREAT, so that they
 were opened in efs_create.  One of efs_create's other duties is
 to truncate files to zero length (in the case that O_TRUNC was
 set).  To decide whether to do this, it looks at the va_size
 member of its struct vattr parameter.  However, on SunOS,
 va_size is uninitialized unless the corresponding bit in va_mask
 is set.  Thus, efs_create was truncating files to zero length
 whenever an uninitialized variable happened to contain zero.
 
 Fixed by blake-db3928-bad-truncate-in-efs_create.blake-Mon, 16 Aug 1993 16:24:09
 
 Entered as OT 
 
 andi-Mon, 04 Oct 1993 18:41:21

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9034
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Need to prepare user buffer in vol_efs{Read,Append}dir
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 As part of the Solaris port, changes were made to vol_efs{Read,Write}
 to prepare the passed-in user buffers for use by the lower-level I/O
 system.  For example on Solaris, the buffer is mapped in to the kernel
 address space.  These same changes need to be made to the
 vol_efs{Read,Write}dir routines.
 
 Jeffrey Prem
 Wed Jul 14 18:38:23 1993
 jdp-Wed, 14 Jul 1993 18:38:25
 Jeffrey Prem
 Thu Aug  5 12:17:24 1993
 jdp-Thu, 05 Aug 1993 12:17:26
**Solution Text**
 jdp-db3888-prepare-user-buffer-in-vol-dir-ops 
 
 Jeffrey Prem
 Thu Aug  5 12:17:27 1993
 jdp-Thu, 05 Aug 1993 12:17:29
 
 Entered as OT 
 
 andi-Mon, 04 Oct 1993 18:28:11

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9033
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : need to release global lock and catch exceptions when draining pipes in FX
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 Delta: cfe-db3876-preempt-and-exceptions-in-fx-cleanup
 Built and tested on configuration dfs-103 3.20.
 cfe-Wed, 14 Jul 1993 10:04:11
 
 Entered as OT 
 
 andi-Mon, 04 Oct 1993 18:24:14

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9032
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : Can't move from aggr with 4k frags to one with 1k frags
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/24/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 You can't currently move a fileset from an aggregate with a 4k
 fragment size to one with a 1k fragment size.  The problem is that the
 constant `ftu_unlimitedQuota' (used to set the allocLimit) is not
 evenly divisible by 4k.  During the conversion from the external units
 (bytes) to the internal units (fragments), the result of the division
 by 4k is rounded up so that when the reverse conversion is done (to do
 a move, say) the resulting number of bytes is too big to fit in 32
 bits when converted to frags on the destination (1k fragsize).  This
 causes vol_efsSetStatus() to return EINVAL (failing the move).
 jdp-Fri, 24 Sep 1993 11:15:12
 
 This is not only a problem for 4k fragment sizes, the same problem
 exists for aggregates with a 2k fragment size, and thus, all fragment
 sizes that are a multiple of 2k are doomed.
 jdp-Fri, 24 Sep 1993 11:24:56
**Solution Text**
 There are two parts to the solution.  First, vol_efsSetStatus() should
 tolerate allocated and visible limits that are too big by setting the
 relevant status word to MAXINT.  Second, `ftu_unlimitedQuota' should be
 changed from 0x3fffffffc00 to 0x3ffffff0000, making it evenly divisible by
 64k, the largest possible LFS fragsize.
 jdp-Fri, 24 Sep 1993 11:19:25
 
 jdp-db4373-tolerate-big-quota-limit-values
 
 Jeffrey Prem
 Fri Sep 24 17:06:51 1993
 jdp-Fri, 24 Sep 1993 17:06:53
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9031
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : Bad error handling in fts
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 In volc_vprocs.c:CopyCloneVolume(), the cleanup code at the
 `copycloneBad' label is wrong.  When aborting or closing
 `cloneFromTid' and `*fromTranP', we should be using `fromconnp'
 instead of `toconnp'.   The result is that after a failure in
 CopyCloneVolume(), one or both of these filesets remains busy (until
 the 10 minute timeout).  If there is not a failure, the filesets are
 closed properly in the main path.
 
 This currently causes fts test #21 to fail under some circumstances.
 jdp-Fri, 24 Sep 1993 11:02:21
**Solution Text**
 jdp-db4372-fix-copy-clone-error-path
 Jeffrey Prem
 Fri Sep 24 14:20:52 1993
 jdp-Fri, 24 Sep 1993 14:20:55
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9030
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : volc interface prints messages that cannot be suppressed
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 VC_CreateVolume eventually (through vldb_CreateVldbEntry) calls
 vldb_EnumerateEntry, which calls printf unconditionally.  The printed
 material is not really useful for anything other than fts; so, if this is the
 interface that we are telling people to use to manipulate filesets, we need a
 way to suppress those messages for programs that don't want them.
 bab-Tue, 21 Sep 1993 18:24:27
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9029
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : bos test4 doesn't wait long enough for proc tp5 to die
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9028
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : AimIII panics on Episode
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/17/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 While running aimIII (and other stress tests) on Episode,
 I got a data fault panic in epib_EndTran.  efs_sync was attempting
 to update the times of a vnode that had already become inactive,
 and whose anode pointer was therefore NULL (cf. db4282).
 
 The problem is that vnm_SyncVnodes is unsafe.  It loops over all
 vnodes, skipping ones whose ref. count is zero or which are already
 locked.  No lock is held during this process, and when it finds a
 suitable candidate for syncing, it does not lock it or hold it.
 The vnode will be locked in vnm_Update_Anode and held in vnvm_schedule,
 but meanwhile, the state of the vnode may have changed.
 
 In the present case, it appears that efs_inactive has run while
 the vnm_UpdateAnode was in progress.  Since it happens that the
 call to epia_EndTran uses EVTOA(evp) in place of the supposedly
 equivalent local variable ap, we first notice this unpleasant
 fact when we try to end the transaction.
 
 The right fix is to lock and hold the vnode in vnm_SyncVnodes.
 This will require some shifting of responsibilities in the other
 callers of vnm_Update_Anode.
 
 blake-Fri, 17 Sep 1993 14:03:13
**Solution Text**
 blake-db4343-fix-sync-race
 
 Get the vnode mutex to prevent its changing from active to inactive
 while we are looking at it; also get the evnode lock without blocking.
 If we find that the vnode is active, increase its ref. count before
 starting to sync it so that there is no race with VOP_INACTIVE.
 blake-Tue, 21 Sep 1993 11:04:41
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9027
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : osi_Alloc recordkeeping code needs a mutex
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The recordkeeping code in osi_Alloc and osi_Free updates various
 shared data structures without a mutex to protect the critical
 sections.  This could result in corruption leading to panic; Ted
 has seen some test_anode failures that may be attributable to this.
 
 blake-Thu, 16 Sep 1993 14:50:47
**Solution Text**
 blake-db4341-add-osi_Alloc-mutex
 blake-Tue, 21 Sep 1993 11:00:45
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9026
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Serious typo in elbl_FullInfo
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 elbl_FullInfo has code
 
  if (tP) {
 	  tranOffset = tP->firstPass;   <<<<<<<<<<<<<<
 	  tranPass = tP->firstOffset;   <<<<<<<<<<<<<<
       } else {
 	  tranOffset = lmP->currentLogBlock;
 	  tranPass = lmP->passno;
       }
 
 The highlighed lines are incorrect!
 rajesh-Wed, 15 Sep 1993 18:19:26
**Solution Text**
 rajesh-db4333-typo-in-elbl_FullInfo
 rajesh-Tue, 21 Sep 1993 14:27:24
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9025
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : backup commands have erroneous -bakgroup option
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
    The dfs backup commands (bakserver, butc and bak) have a -bakgroup option
 that is used for testing purposes.  This is supposed to be ifdeffed out of
 the release version but this does not appear to be happening.
 
    Also, the bak command doesn't handle the -bakgroup option correctly.
 This probably needs to be opened as a separate defect.davecarr-Wed, 15 Sep 1993 14:25:52
**Solution Text**
 kazar-db4330-handle-cm-readdir-small-reads should fix some of the self-host
 cell build problems.  I'm not sure if this should go in this delta, but that's
 where I put it.
 
 Mike Kazar
 Tue Sep 21 10:28:05 1993
 kazar-Tue, 21 Sep 1993 10:28:08
 
 =============================================================================
 
 davecarr-db4330-backup-commands-have-erroneous--bakgroup-option properly
 handles the non-defining of the variable 'allow_multiple_bakservers' that
 controls the inclusion of the code necessary to implement multiple
 bakservers.  files changed: file/bak/main.c, file/bakserver/server.c and
 file/butc/tcmain.c.
 davecarr-Thu, 23 Sep 1993 10:31:28
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9023
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts gives errors when doing an rmsite of a release replicated fileset
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 When the fts command is invoked to do an addsite and then an rmsite on a release replicated
 fileset, then an error message is printed when the rmsite is done and no release has been
 done prior to this. This is because fts tries to delete the distinguished replica and
 fails. fts should allow for the error where the distinguished replica does not exist and 
 not print any error messages if that happens. 
 
 khale-Mon, 13 Sep 1993 12:01:51

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9022
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Link count errors develop - root can link/unlink dirs
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 ota:
 
 Details of this defect and some background can be found in OT 3579.
 
 I am proposing under this defect to conditionally control this
 misfeature under a global variable.  I will make the default setting
 "off" so that root may not link or unlink directories.
 
 We can revisit this decision later depending on customer requirements
 (outraged demands).  At present the salvager is not graceful about
 reporting errors of this type and this can lead to serious appearing
 errors that are very difficult to distiguish from genuine disk
 corruption.  We can probably get the salvager to do better and this
 should preceed enabling this capability by default.
 [Mon Sep 13 09:32:45 1993]
 ota-Mon, 13 Sep 1993 09:32:49
 
 Doesn't this change have implications for the documentation?
 
 Also, what about the "outraged demands"?  My impression from Blake's
 recollection of Sun's experience is that the default setting should be "on".
 Are you really proposing that the right way to handle this problem is to
 pass it on to Customer Support and to expect them to field complaints, for
 indefinitely many years into the future, by telling the complaining sys admin
 to fix the problem in adb/rrdb/lldb/... ?
 bwl-Tue, 14 Sep 1993 16:48:05
**Solution Text**
 ota:
 
 Delta: ota-db4305-prohibit-all-dir-link-ops
 
 Control whether root is allowed to link or unlink directories with a
 global variable.  Make the default setting of this variable be "OFF".
 Teach epistat to get and set this variable.
 
 Add efs_allowSuserDirLinkOps as vnops.allowSuserDirLinkOps (10.6).  Also
 make it setable.
 ota-Tue, 14 Sep 1993 15:31:57
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9021
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : See description below
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 hostutil_GetNameByInet() now expects a struct in_addr instead of a
 long.  This was causing the cm command ("cm whereis" in particular) to
 dump core.
 Jeffrey Prem
 Fri Sep 10 14:12:58 1993
 Bumped priority because this was keeping the ftcleanup tests from passing.
 Jeffrey Prem
 Fri Sep 10 14:15:45 1993
 jdp-Fri, 10 Sep 1993 14:15:49
**Solution Text**
 jdp-db4304-obey-signature-of-GetNameByInet
 
 Jeffrey Prem
 Fri Sep 10 14:36:25 1993
 jdp-Fri, 10 Sep 1993 14:36:37
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9020
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : vnsync - recycle should call LogForce only to complete tran
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 ota:
 
 See bboard transarc.afs.episode:
   10-Sep-93, "Results of efs_inactive review, et al, part 4"
 
 The LogForce done in Recycle and the TranForce in efs_inactive are both
 too heavy weight.  They insist that the transaction commit not just
 complete.  This could account for file deletion being much slower than
 one would expect.  The second parameter to this function is 1 but should
 be 2.
 [Fri Sep 10 12:15:23 1993]
 
 ota:
 
 ota-Fri, 10 Sep 1993 12:15:30
 ota-Fri, 01 Oct 1993 12:15:57
**Solution Text**
 ota:
 
 Delta: ota-db4301-gentler-logforce-on-recycle
 
 In Recycle and in efs_inactive (on vnodes with zero linkCount) we need
 to make sure the transid in the tlock is completed before we make the
 vnode available for another file.  Instead of forcing the transaction to
 commit only insist on completeness.  Also Recycle was calling epia_Fsync
 which seems entirely unnecessary.
 ota-Tue, 14 Sep 1993 10:55:12
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9019
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : Silly backup messages
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 FTS restore gave me this silly error messages
 [fs12]  fts restore fileset.12 testlab11 aggrf /:/fs8/fs12.dump -overw
 fts restore: The fileset fileset.12 (0,,37) already exists in the FLDB.
 fts restore: Overwriting the existing entry...
 STKN_TokenRevoke: still need token 747339198,,31104, type 0xc0c, on 0,,37.
 STKN_TokenRevoke: still need token 747339198,,31104, type 0xc0c, on 0,,37.
 STKN_TokenRevoke: still need token 747339198,,31104, type 0xc0c, on 0,,37.
 Restored fileset fileset.12 on testlab11 aggrf from /:/fs8/fs12.dump
 [fs12] bruce-Thu, 09 Sep 1993 14:03:46
**Solution Text**
 Delta: cfe-db4299-cut-verbosity-of-msgs
 cfe-Fri, 10 Sep 1993 14:26:28
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9018
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : repserver core dumps in bogus osi_free()
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 The repserver tries to free a variable, rp->numKAs in SREP_KeepFilesAlive()
 , which is not a pointer. This causes a core dump. 
 khale-Thu, 09 Sep 1993 13:46:02

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9017
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : dead code in anode/fid.c
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Function ultstr in episode/anode/fid.c is no longer called (presumably it
 used to be called from epid_PrintedFid).  Also, epid_PrintedFid itself is
 not called within the kernel.
 bwl-Thu, 09 Sep 1993 10:56:15
**Solution Text**
 Delta bwl-o-db4296-dead-fid-code.
 bwl-Fri, 10 Sep 1993 15:34:25

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9016
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : vnsync - add asserts
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 See bboard transarc.afs.episode:
   25-Aug-93, "Results of efs_inactive, et al review, part 1"
   26-Aug-93, "Results of efs_inactive, et al review, part 2"
    7-Sep-93, "Results of efs_inactive, et al, part 3"
 
 ota:
 
 ASSERTS:
 	a. NOIDENTITY: nothing but page invalidates.  No reads, writes, or
 	   calls to strategy.
 	b. In efs_strategy: Insist on both WFM and ALLOWSTRAT or neither, but
 	   see #4.
 	c. In FinishAlloc and Vinit assert that vnode (refcount == 1).
 	d. In efs_inactive: not WFM.
 	e. In efs_inactive: assert that the refCount upon exit is correct.
 	f. In vnm_Recycle(): assert that vp->v_count is 0
 
 Blake has tried (a) and found it wanting.  Perhaps he can add details.
 [Wed Sep  8 09:27:15 1993]
 ota-Wed, 08 Sep 1993 09:27:19
 
 ota:
 
 Status as of 3.30:
 
 Item "a" still needs study.
 Item "b" is done.
 Item "d" is done.
 Item "e" has been taken care of by not putting vnodes with high refCount
   on the LRU list.
 
 Also add:
 	g. Set vd_ap to NULL after closing the handle.  On Suns at
 	   least, this will drop system making an illegal reference its
 	   tracks.  A real assert would be quite expensive.
 ota-Mon, 13 Sep 1993 11:10:43
**Solution Text**
 ota:
 
 Delta: ota-db4282-add-asserts-for-vnsync
 
 Add trip wires for vnode synchronization problems.
 In and around vnode recycling insist that the vnode's refcount is correct.
 Clear the vnode's anode handle pointer upon close to forestall stale
   references (at least on the Sun which detects NULL ptr refs).
 ota-Tue, 14 Sep 1993 10:45:57
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9015
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Misleading salvager error messages for directory entries
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 When the verification phase of the salvager generates the error
   "Directory not okay because page 0, bad magic number in entry 149."
 it really means
   "Directory not okay because page 0, bad magic number in entry 
    WITH FIRST PIECE NUMBER 149."
 and not entry 149.
 
 We should clarify these messages.
 rajesh-Tue, 07 Sep 1993 15:39:29
**Solution Text**
 rajesh-db4277-clarify-salvager-directory-entry-error-messages
 rajesh-Wed, 08 Sep 1993 17:55:47
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9014
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : The error path in recovery is broken
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 rajesh-db4274-a-fix-in-error-path-of-episode-recovery
 rajesh-Tue, 07 Sep 1993 12:48:50

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9013
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Episode log has missing pages in the middle
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/03/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 RCX test generated an aggregate in
 ~rajesh/epi/test/rcx/log/luthien/723-724/aggr.0724.ab.004 on which
 salvager verifcation fails after recovery. The aggregate log
 has missing pages in the middle. 
 
 The log has 364 pages. The tail of the log is at page # 245 and the head of
 the log is at page # 166. (wrapped around log). The pass # on pages are
 as follows
 
 Page #s			Pass #
 
 245 - 363		 9
   0 -  27		10
  28 -  30		 4 !!!
  31 - 166               10.
 
 Hence the 3 pages 28 to 30 are bogus. 
 
 The # of in-transit pages count in some of the log pages are as follows.
 
 Page #			# in-transit pages
 
 363			3
 0			4
 ...
 27			4
 
 28			0  ]
 29			0  ] Missing pages
 30			0  ]
 
 31			4
 32			1
 33			2
 34			3
 35			0
 ...
 166			0
 rajesh-Fri, 03 Sep 1993 11:08:36
 
 rajesh-Fri, 03 Sep 1993 12:35:15
 The specific errors pointed out by the salvager are
 1. 
 In volume rcx.103 0,,103 (avl #7)
   in anode (#3) EOF @ (0,4288)
     index 3 is direct block 0x30a
       Block reference beyond end of file
     index 4 is direct block 0x6d9
       Block reference beyond end of file
     index 5 is direct block 0x6da
       Block reference beyond end of file
     index 6 is direct block 0x6db
       Block reference beyond end of file
     index 7 is direct block 0x6dc
       Block reference beyond end of file
     Anode allocated 16, calculated 6
     Anode visible 16, calculated 6
   Volume allocated/visible quota mismatched: usage 89/89, computed 79/79
 
 This is because a valid transaction in active portion of the log
 increases the size of the directory from 8K to 16K and fills in all
 relevant block numbers. However an invalid transaction in the
 log pages with pass# 4 changes the length from 10A0 to 10C0 causing
 salvager to complain about the blocks being end of file.
 
 2.
 In 5,3 at rcx.101:/rcx.test/
   Directory not okay because page 0, bad magic number in entry 149.
   in anode (#2)
 This is because a invalid transaction 140307 on page 30 (pass # 4)
 sets the entry to be free. And since the hashtable continues to point
 to it, the salvager complains.
 rajesh-Tue, 07 Sep 1993 16:14:58
**Solution Text**
 The delta 
 rajesh-db4269-check-pass-number-consistency-in-log-pages-before-recovery
 fixes recovery to bail out it encounters an active log page with
 a invalid pass number.
 
 This does not answer the question as to why some log pages did not make
 it out to disk. 
 rajesh-Tue, 07 Sep 1993 12:47:51
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9012
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_FindServer fails in token revocation after fileset move
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 Delta: cfe-db4262-findserver-after-fileset-move-problems
 Backed against dfs-103 3.30
 cfe-Thu, 16 Sep 1993 13:21:41
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9011
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : get rid of spurious TKM messages
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/02/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Get rid of spurious messages about being out of tokens, and about
 returning a token whose ID is unknown.
 Mike Kazar
 Thu Sep  2 17:32:48 1993
 kazar-Thu, 02 Sep 1993 17:32:50
**Solution Text**
 kazar-db4261-get-rid-of-tkm-advisory-messages
 Mike Kazar
 Thu Sep  2 19:39:43 1993
 kazar-Thu, 02 Sep 1993 19:39:45

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9010
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_QuickQueueTokens() references null pointer
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/02/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 In cm_QuickQueueTokens() if a token has a refCount of 0, a NULL
 serverp entry and is not found to be worthless (hasn't expired and
 token type is not 0) the kernel will panic.
 Dimitris Varotsis
 Thu Sep  2 10:49:46 1993
 dimitris-Thu, 02 Sep 1993 10:49:55
**Solution Text**
 kazar-db4258-fix-quickqueuetokens-null-server-reference
 Mike Kazar
 Thu Sep  2 19:39:09 1993
 kazar-Thu, 02 Sep 1993 19:39:11

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9009
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : ifdef out entire CATCH_ALL block
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/01/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 In ftu_FsetCloneWithKA(), the entire CATCH_ALL block should be ifdef'd
 out when compiling with DFS_BOMB turned off.
 
 Jeffrey Prem
 Wed Sep  1 16:31:05 1993
 jdp-Wed, 01 Sep 1993 16:31:08
**Solution Text**
 jdp-db4253-compile-catch-all-out
 Jeffrey Prem
 Wed Sep  1 16:38:32 1993
 jdp-Wed, 01 Sep 1993 16:38:34

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9008
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Do not give up trying to obtain buffer space
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/31/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Running the RCX tests overnight resulted in a panic when "attaching" an 
 aggregate thru "mount" because GetMoreBufferSpace was not able to grab
 buffer space after 10 tries.
 
 Analysing the situation with Ted, revealed many problems, some of which
 I attempt to summarize below.
 
 The attach code tries to read in the superblock trying various block
 sizes till it succeeds. To do the read, it needs to allocater a buffer 
 corresponding to the chosen blocksize. If this blocksize is different
 from that of the other attached aggregates on the system, then the
 buffer package "may" have to wait for a "bufferblock" (128K) to be
 relinquished by the other bufferpools, (if there is not sufficient 
 # of bufferblocks available). This is a very heavyweight operation
 for something very lightweight. And we should clean this up. I will open 
 another defect for this.
 
 There were other oddities and mistakes that we encountered while
 reviewing log package code including HandleLogFull. Ted will be summarising
 the main points of this in another existing defect. I will set this defect
 to point to that shortly. This is defect 4331.
 
 But since these fixes are relatively major changes, we want to work around
 it (at the cost of performance of course) by modifying GetMoreBufferSpace
 to not give up but to try more times.
 rajesh-Tue, 31 Aug 1993 11:26:06
 
 rajesh-Thu, 16 Sep 1993 17:47:09
**Solution Text**
 rajesh-db4245-do-not-give-up-trying-to-obtain-bufferspace 
 
 This has been tested on 3.29+ and 3.31+ thru AIMIII and RCX tests.
 rajesh-Thu, 16 Sep 1993 12:50:49

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9007
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : destroying volumes in repserver fails
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/31/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 Destroying of volumes in the repserver does not work. the repserver
 uses the new deplete interface to deplete volumes. However, it does not open the
 volumes with the proper flags set, hence the deleteion fails
 khale-Tue, 31 Aug 1993 10:33:02
**Solution Text**
 -------------------------------------------------------------------------------
 The solution is to make sure when opening a volume that the volume is also
 oepned for depletion.
 khale-Wed, 01 Sep 1993 14:40:48

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9006
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : FetchDCache sets IFEVERUSED with the wrong lock
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/31/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 kazar-db4242-use-dcachelock-setting-ifeverused-flag
 Mike Kazar
 Thu Sep  2 19:37:37 1993
 kazar-Thu, 02 Sep 1993 19:37:38

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9005
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_GetDownS can get lost when FlushSCache blocks.
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/31/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The cm_GetDownS function can block in cm_FlushSCache, and then the
 "up" variable is bad.  Lookup up later.
 
 Mike Kazar
 Tue Aug 31 09:12:38 1993
 kazar-Tue, 31 Aug 1993 09:12:41
**Solution Text**
 kazar-db4241-fix-obscure-getdowns-race
 Mike Kazar
 Thu Sep  2 19:37:04 1993
 kazar-Thu, 02 Sep 1993 19:37:05

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9004
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : remove use of dfsatab from TSR tests
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Remove use of obsolete dfsatab file from test setup.
 Fix a core-dump problem in the tsrtest.c program.
 Add SPARC notes to TSR-README.
 cfe-Mon, 30 Aug 1993 16:10:41
**Solution Text**
 Delta: cfe-db4236-remove-dfsatab-from-tests
 Backed by dfs-103 3.28.
 cfe-Mon, 30 Aug 1993 16:13:23
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9003
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : adjust TKM parameters
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Carl recommends the following:
 
 1.  Reduce token timeout to 2 hours from 4.
 
 2.  Increase period of GC from 2*N hours to N/3 hours, and don't
 adjust further dynamically (currently doubles! at times).
 Mike Kazar
 Sat Aug 28 10:22:12 1993
 kazar-Sat, 28 Aug 1993 10:22:14
**Solution Text**
 kazar-db4230-adjust-tkm-token-lifetime-and-gc-params
 Mike Kazar
 Thu Sep  2 19:36:33 1993
 kazar-Thu, 02 Sep 1993 19:36:35

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9001
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : tkm disobeys locking hierarchy during GC
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The token manager disobeys its own locking hierarchy for locking fid bucket
 locks when doing async GCs.  It *tries* to get around this problem by
 using lock_Obtain...NoBlock, but the functions called with the lock
 held all drop and release locks themselves w/o using the ...NoBlock
 option.  Furthermore, most are completely unprepared, and unpreparable
 for not re-obtaining the lock.
 
 This was observed with a 2 machine cell after about 10 mins of cache
 consistency tests and 2 machine append tests.
 
 The following stack trace was observed:
 
 tkm_GetToken > GetTokenLeaveOnFid > GetTokenNoVolCheck >
 FidHash_RevokeConflicts > MakeConflictList > InitSlicedToken >
 TokenFreeList_ObtainToken > GC_TryAsyncRevokes > FidHash_RevokeConflicts >
 SendRevocationsInParallel > tpq_Pardo > HostRevokeList_DoRevoke >
 FidHash_LookupValidate > FidHash_Lookup > lock_ObtainWrite
 Mike Kazar
 Sat Aug 28 10:19:30 1993
 kazar-Sat, 28 Aug 1993 10:19:32
**Solution Text**
 kazar-db4229-fix-tkm-gc-deadlock
 
 Mike Kazar
 Thu Sep  2 19:36:09 1993
 kazar-Thu, 02 Sep 1993 19:36:11

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 9000
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Test_anode end script command broken
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The "end" command does not end processing of the input script. This
 causes a assertion failure for one my test scripts later on.
 rajesh-Fri, 27 Aug 1993 12:53:02
**Solution Text**
 rajesh-db4222-fix-end-command-in-test_anode
 rajesh-Mon, 30 Aug 1993 08:45:06

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8999
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : TSR-move-locks cannot validate ctime/DV
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/27/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 TSR-move is being too conservative with validating the results of the
 AFS_GetToken call.  For a normal move, the cached CTIME and data version
 for a file isn't necessarily correct with respect to the file server; this
 is exposed by Fred's move-locks test.  If it's a normal move and the server
 is still in move-reestablish mode, then the CM can believe that the token it
 now holds is valid.
 
 As a related issue, the cm_RecoverSCacheToken code is now applied to all
 the scache entries in the system, whether they relate to the fileset being
 moved or to the server that's recovering or anything, unnecessarily flushing
 lots of cached data for the whole CM, possibly irretrievably.
 cfe-Fri, 27 Aug 1993 09:58:37
**Solution Text**
 Delta: cfe-db4220-tsr-move-too-conservative
 Backed by dfs-103 3.28
 cfe-Mon, 30 Aug 1993 16:24:47
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8997
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_tokens.c has a # of EndVolumeOp mismatches
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Two problems:
 
 1.  cm_EndVolumeOp logic is screwed up for basic gettoken.
 
 2.  No such management at all is done for cm_Get/HaveHereToken.
 Mike Kazar
 Wed Aug 25 19:55:21 1993
 kazar-Wed, 25 Aug 1993 19:55:22
**Solution Text**
 kazar-db4208-fix-gettoken-volume-op-counter-leaks
 Mike Kazar
 Thu Aug 26 17:50:43 1993
 kazar-Thu, 26 Aug 1993 17:50:59

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8996
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager crashes when allocating dup blocks
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 ota:
 
 It foolishly examines the quota of the volume in which the allocation is to
 occur to check for quota overflow.  The pointer it uses, however, was
 deallocated when the volume traversal finished, which was quite some time ago.
 This is also a fairly pointless check as I apparently noticed earlier when I
 inserted this comment in epiz_AllocResFrags:
 
 	/* TODO -- rexamine the logic of doing this.  Do we really care that
          *     much if the volume quota is exceeded here?  There are other
          *     places where the a fileset may end up over quota for various
          *     reasons.  This seems like a good candidate too. */
 [Wed Aug 25 16:39:52 1993]
 ota-Wed, 25 Aug 1993 16:39:56
**Solution Text**
 ota:
 
 Delta: ota-db4204-no-quota-checks-for-duplicate-block-resolution
 
 Remove check for fileset quota when allocating blocks and fragments for
 duplicate block resolution.  These quota values are no longer available at this
 point so we were getting a NULL pointer fault.  It was pretty dubious check to
 be making anyway.
 
 Also fixed if statement in epig_SayWhyNotLikeSuperBlock which was the cause of
 defect 4237.
 [Mon Aug 30 16:19:59 1993]
 ota-Mon, 30 Aug 1993 16:20:03

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8995
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Elimitate bogus volop/agop log messages
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Several volops return non-zero codes as part of their normal
 operation.  For example, the clone operations return ELOOP, VOL_SCAN
 and VOL_READDIR return VOL_ERR_EOF, etc.  These should not show up as
 failures in ICL logs or log files.  Finally, vol_efsProbe is always
 returning ESRCH for no good reason.
 
 Jeffrey Prem
 Wed Aug 25 16:12:49 1993
 jdp-Wed, 25 Aug 1993 16:13:00
**Solution Text**
 jdp-db4203-eliminate-spurious-ftu-log-mesgs 
 
 Jeffrey Prem
 Mon Aug 30 09:31:37 1993
 jdp-Mon, 30 Aug 1993 09:31:39

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8994
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Must close anode on delete failure in efsSomeClone()
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 In efsSomeClone(), if the call to epiv_DeleteAnode() fails, the anode
 `b' is never closed.
 
 Jeffrey Prem
 Wed Aug 25 15:54:52 1993
 jdp-Wed, 25 Aug 1993 15:54:56
**Solution Text**
 jdp-db4202-close-anode-after-failed-delete-in-efsSomeClone
 Jeffrey Prem
 Tue Sep  7 09:54:52 1993
 jdp-Tue, 07 Sep 1993 09:54:56

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8993
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : mmap tests faile after about 4 hours
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 mmap tests fail after about 4 hours.
 Does this have anything to do with token expiration?
 Only one data point so far.
 Mike Kazar
 Wed Aug 25 09:29:17 1993
 kazar-Wed, 25 Aug 1993 09:29:22
**Solution Text**
 kazar-db4192-fix-token-expiration-consistency-bug
 Mike Kazar
 Thu Aug 26 17:49:38 1993
 kazar-Thu, 26 Aug 1993 17:49:39

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8992
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : See description below
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/24/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 From the HP review.
 efs_volops.c, SetStatus(), is setting the volp->v_states field before
 verifying it for legality.
 
 vnva_FileConstruct() isn't clearing the UUID of the file that it creates.
 cfe-Tue, 24 Aug 1993 13:57:41
**Solution Text**
 Delta: cfe-db4182-status-validation-and-clear-uuid
 cfe-Thu, 26 Aug 1993 12:41:47
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8991
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : SwapIDs can be tough to recover if interrupted by a crash
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Splitting this work into two pieces.  The first piece ensures that there is
 at least one fileset with the correct ID after an identity swap.  The
 second piece allows more graceful cleanup.
 This defect (DB 4181) addresses the first, rough, cleanup.
 Another defect, DB 4214, addresses the graceful cleanup.
 cfe-Thu, 26 Aug 1993 12:24:05
**Solution Text**
 Delta: cfe-db4181-rough-fix-for-id-swap-race
 Backed by dfs-103 3.28
 cfe-Thu, 26 Aug 1993 12:38:24
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8990
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : DFSEXPORT
Short Description             : keep from generating dfsatab file in dfsexport
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 There's a doc submission (OT 8439, apparently) that removes mention of the
 dfsatab file.  The dfsexport command no longer reads the dfsatab file, but
 it does continue to write that file.  The file should be eliminated.
 cfe-Mon, 23 Aug 1993 12:53:15
**Solution Text**
 Delta: cfe-db4173-nuke-dfsatab-entirely
 Based on dfs-103 3.27
 cfe-Mon, 23 Aug 1993 15:45:42
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8989
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : In the ubik code, the error status carried by the pipe pull,
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 The error status set by the pipe push and pull routines was overwritten in
 the recovery daemon
 vijay-Fri, 27 Aug 1993 18:19:38
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-db4172-ubik-handle-preserve-pipe-error-status
 Build: dfs-103 3.27
 vijay-Fri, 27 Aug 1993 18:19:41

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8988
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : compilation warnings from calls to prototyped ubik routines
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-db4171-ubik-correct-warnings-on-dependent-modules
 Build: dfs-103 3.27
 
 Vijay Anand
 
 vijay-Mon, 23 Aug 1993 13:59:56

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8987
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : Failure to delete backup dooms subsequent delete attempts
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Fts does not clean up properly if it fails to delete a backup fileset.
 In VC_DeleteVolume(), we delete the R/W fileset first and then delete
 the backup fileset, if any.  If we fail to delete the backup fileset
 (after successfully deleting the R/W), the FLDB entry for the R/W is
 never invalidated.  This means that on subsequent delete attempts, the
 call to fts_GetToken() will fail with VOLERR_PERS_DELETED.  The only
 way out at this point (assuming that one is now able to delete the
 backup) is to zap the backup fileset and delete the FLDB entry.
 
 Two things should be done to fix this.  First, we should call
 vldb_RemoveVolume() on the R/W entry immediately after deleting the
 R/W.  Second, we should be tolerant of VOLERR_PERS_DELETED errors from
 fts_GetToken() and deleting a remaining backup fileset if it exists.
 
 Jeffrey Prem
 Mon Aug 23 11:14:33 1993
 jdp-Mon, 23 Aug 1993 11:14:46
**Solution Text**
 jdp-db4167-ignore-get-token-failure-during-delete 
 
 Jeffrey Prem
 Mon Aug 30 16:59:57 1993
 jdp-Mon, 30 Aug 1993 17:00:02

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8986
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : major synchronization error from ubik
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 I see USYNC errors come by when testing ubik under high load with utst_server
 and client. Obviously there is some problem with propagating transactions
 to other servers where a new transaction tramples on an existing one. More
 investigation needed.
 vijay-Mon, 23 Aug 1993 11:33:39
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-db4165-ubik-fix-major-synchronization-error
 Build: dfs-103 3.27
 
 Vijay Anand
 vijay-Fri, 27 Aug 1993 18:16:11

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8985
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_HaveTokens references random memory
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 In this function, the exactMatch reverification at the end of the
 function can use a bogus tlp pointer (the head of the tokenList list)
 if no token is found.  This might spuriously succeed.
 
 Mike Kazar
 Mon Aug 23 09:06:57 1993
 kazar-Mon, 23 Aug 1993 09:06:59
**Solution Text**
 kazar-db4164-havetokens-exactmatch-check-is-bogus
 Mike Kazar
 Wed Aug 25 09:52:03 1993
 kazar-Wed, 25 Aug 1993 09:52:05

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8984
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : See description below
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The functions cm_FindMatchingToken, cm_AddToken,
 cm_ReturnSingleOpenTokene and FindTokenByID need to check for deleted
 tokens and not glom on to them.
 Mike Kazar
 Mon Aug 23 09:04:02 1993
 kazar-Mon, 23 Aug 1993 09:04:05
**Solution Text**
 kazar-db4162-watch-more-carefully-for-deleted-and-async-tokens
 Mike Kazar
 Wed Aug 25 09:53:48 1993
 kazar-Wed, 25 Aug 1993 09:53:49

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8983
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : the dfs user space servers should send -verbose output to stderr
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 Some servers send -verbose output to stdout; they should actually be sending
 it to stderr. In the case of the flserver, -verbose doesn't really do more
 than a couple of messages on startup.
 vijay-Fri, 20 Aug 1993 13:00:04
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-db4155-flserver-verbose-to-stderr
 Build: dfs-103 3.27
 
 Vijay Anand
 vijay-Fri, 27 Aug 1993 18:14:28

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8982
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : epia_Write should not allocate indirect blocks on extending truncates
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 This defect is a child of DB3979.
 
 When epia_Write is called to extend the length of the file, it
 allocates all the indirect blocks necessary to that length. It however
 does not allocate the corresponding data blocks, as it realises that
 it would only be writing zeroes into it. 
 
 Hence epia_Write should make the same checks it makes to determine data
 blocks allocation for indirect block allocation.
 
 rajesh-Thu, 19 Aug 1993 11:21:15
**Solution Text**
 This delta fixes a # of problems found in this code.
 
 rajesh-db4143-correct-anode-layer-write
 
 Also DBA 4212 addresses some test issues related to the problems
 addressed by this defect.
  
 rajesh-Mon, 30 Aug 1993 08:47:59

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8981
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : EIO from reclone because of high anode refcount
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/18/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Given a typical fileset (dump in ~kazar/public/carl.dump.Z) and the
 following test case, Episode will eventually report EIO during the
 test's reclone phase.  The error is generated in efsSomeClone() where
 we're trying to delete a backing anode because the former COW anode
 in the R/W fileset has been rewritten.  epiv_DeleteAnode() is
 returning EPI_E_OPEN because the refcount on the backing anode is
 greater than 1.
 
 The test case follows.
 
 #!/bin/ksh
 
 integer i
 i=1
 
 integer processNum
 integer processCount
 fsname=dawnft2
 fpath=/:/$fsname
 aggrname=epi0
 servername=puzzle
 Msg()
 {
 	echo $*
 	echo $* >>runlog
 }
 
 rm -f runlog p.*
 processNum=1
 processCount=10
 
 while (( $i <= 100000  )) 
 do 
   Msg "${i}th loop"
 
   Msg "Cloning $fsname..."
   fts clone $fsname >> runlog 2>&1
   Msg "Clone finished."
 
   processNum=1
   while (( $processNum <=  $processCount ))
   do 
   {
       Msg "Process $processNum started"
 
       identifier=p.$processNum
 
       Msg "Process $processNum touch started..."
       find $fpath -exec \touch {} \; >> runlog 2>&1
       Msg "Process $processNum touch finished."
 
       if (( $processNum == $processCount ))
       then
          Msg "Process $processNum dumping ${fsname}.backup..."
          fts dump ${fsname}.backup -file /tmp/dump.out -time 0 >> runlog 2>&1
          Msg "Process $processNum dump finished."
       fi
          
       touch $identifier
   }&
   processNum=processNum+1
   done
  
   processNum=1
   while (( $processNum <= $processCount ))
   do 
      if [ -f p.$processNum ]
      then 
         Msg "Process $processNum finished."
         processNum=processNum+1
      fi
   done
 
   rm p.*
 
   Msg "fts restore started..."	
   fts restore -ftname $fsname -server $servername -aggregate $aggrname -o -file /tmp/dump.out >> runlog 2>&1
   Msg "fts restore finished."
 
   echo "Stop the tests here...."
   sleep 10
 
   i=i+1
 done
 Jeffrey Prem
 Thu Aug 19 09:23:20 1993
 [jdp-Thu, 19 Aug 1993 09:23:28]
 
 ota:
 
 The problem was that epia_GetInfo was read locking the anode then calling
 GetBackingAnode which would sometime assign into an anode field.  If two
 threads tried to do this at once one of the anode references would be lost.
 [Tue Aug 24 11:06:27 1993]
 ota-Tue, 24 Aug 1993 11:06:32
**Solution Text**
 ota:
 
 Delta: jdp-db4140-write-lock-when-calling-epix_GetBackingAnode r1.2
 
 Revision 1.1 fixes the most egregious instance of a set of cases where
 epix_GetBackingAnode() is called with the COW anode read locked.  Since
 epix_GetBackingAnode() sometimes writes into the COW anode, we really need to
 hold a write lock.  With this revision, the IBM test (see DB 4140) ran through
 23 iterations.  Previously, it would fail on the 3rd or 4th iteration.
 
 Revision 1.2 cleans up other locking problems related to this function.  It
 also adds an assert that the anode handle lock is held for write.
 [Tue Aug 24 11:06:33 1993]
 ota-Tue, 24 Aug 1993 11:06:36

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8980
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : TSR doesn't invalidate cache for expiring tokens
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/18/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The TSR code doesn't invalidate the caches for those tokens
 that vanish when TSR decides not to renew a token.  It needs to,
 otherwise after things reconnect, we can stumble over the old data
 in these caches.
 
 Mike Kazar
 Wed Aug 18 14:40:42 1993
 kazar-Wed, 18 Aug 1993 14:40:45
**Solution Text**
 kazar-db4135-invalidate-caches-in-tsr
 Mike Kazar
 Fri Aug 20 14:02:45 1993
 kazar-Fri, 20 Aug 1993 14:02:52

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8979
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : TSR is asking for STATUS/READ tokens by ID that it never held
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/17/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 TSR code sometimes asks (both at the server and the client) for tokens
 (STATUS_READ in particular) that it never held.  This is wrong, and can
 lead to TKM refusing to grant a token we should be able to get.
 At the server end, asking for tokens on the 2nd-ary interface can lead
 to deadlocks.
 Mike Kazar
 Tue Aug 17 15:08:26 1993
 kazar-Tue, 17 Aug 1993 15:08:29
**Solution Text**
  kazar-db4128-stop-adding-status-read-tokens-in-gettoken-requests
 Mike Kazar
 Fri Aug 20 14:01:57 1993
 kazar-Fri, 20 Aug 1993 14:01:59

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8978
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : expiration time unset in freely granted tokens
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/17/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The expiration time isn't being set in TKM when it creates a freely granted
 token.  Things work only because the we subtrace a small "grace period" from
 the expiration time, and because the field is unsigned.
 Mike Kazar
 Tue Aug 17 15:05:54 1993
 kazar-Tue, 17 Aug 1993 15:05:56
**Solution Text**
  kazar-db4127-set-exp-for-freely-granted-tokens 
 Mike Kazar
 Wed Aug 25 09:50:59 1993
 kazar-Wed, 25 Aug 1993 09:51:01

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8977
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : pioctl to set cache size should wait for space to become avail.
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/17/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The call to cm_CheckSize from the pioctl that changes the cache size should
 pass in a parameter indicating that it wants to wait for the space to
 become available, so that the command forces the size down immediately,
 instead of waiting until the bkg daemon does the work.
 Mike Kazar
 Tue Aug 17 15:01:35 1993
 kazar-Tue, 17 Aug 1993 15:01:37
**Solution Text**
 kazar-db4125-setcachesize-pioctl-should-wait-for-space
 Mike Kazar
 Tue Aug 17 16:37:18 1993
 kazar-Tue, 17 Aug 1993 16:37:19
 Mike Kazar
 Tue Aug 17 16:39:06 1993
 kazar-Tue, 17 Aug 1993 16:39:08

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8976
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_GetDLock can get in infinite loop on readonly files
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 When cm_GetDLock finds a chunk which is offline it calls cm_GetDOnLine
 to put it back online. cm_getDOnline gets a token for this chunk and
 then right before setting the online flag it wants to make sure that
 this token still has the TKN_STATUS_READ and TKN_DATA_READ flags set
 to avoid race conditions. The problem with readonly files is that the
 routine used to get the previously granted token (cm_GetTokenByID())
 will always return the first token in the vnode list since all tokens
 for readonly files have by convention the same id.
 
 If that first token does not have TKN_STATUS_READ and TKN_DATA_READ
 set cm_getDOnline will continiously fail and cm_GetDLock will loop
 forever.
 
 Dimitris Varotsis
 Mon Aug 16 17:42:15 1993
 dimitris-Mon, 16 Aug 1993 17:42:17
**Solution Text**
 Add a new call cm_FindMatchingToken() which gets passed in a vnode and
 a token and returns a token hanging off the vnode that is at least as
 'powerful' as the one passed in. Then change cm_GetDOnLine() to use
 this new call instead of cm_GetTokenByID().
 Dimitris Varotsis
 Mon Aug 16 17:42:18 1993
 dimitris-Mon, 16 Aug 1993 17:42:21
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8975
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : debug TSR 1 machine tests
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The TSR bomb point don't always do rational things.  Debug TSR and/or the
 tests until they work reliably.
 Mike Kazar
 Mon Aug 16 11:43:49 1993
 kazar-Mon, 16 Aug 1993 11:43:50
**Solution Text**
 kazar-db4110-fix-tsr-looping-problems
 Mike Kazar
 Tue Aug 17 16:35:56 1993
 kazar-Tue, 17 Aug 1993 16:35:57

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8974
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_AddToken gets confused by deleted tokens
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 cm_AddToken adds tokens out of order if it runs into a deleted
 token.  This can screw up sys 5 locking.
 Mike Kazar
 Mon Aug 16 11:42:55 1993
 kazar-Mon, 16 Aug 1993 11:42:59
**Solution Text**
 kazar-db4109-fix-addtoken-screwup
 Mike Kazar
 Tue Aug 17 16:35:33 1993
 kazar-Tue, 17 Aug 1993 16:35:36

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8973
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fix possible race in VFToEV
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 rajesh-db4096-fix-VFtoEV
 rajesh-Fri, 13 Aug 1993 18:19:30

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8972
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : missing lock around ScanStatus call
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 The cm_ScanStatus call in cm_StoreDCache should be pulled up outside
 of the loop so that it can be done before releasing the read lock.
 Mike Kazar
 Thu Aug 12 15:54:40 1993
 kazar-Thu, 12 Aug 1993 15:54:42
**Solution Text**
 kazar-db4095-do-scanstatus-under-readlock-in-storedcache
 Mike Kazar
 Mon Aug 23 09:09:43 1993
 kazar-Mon, 23 Aug 1993 09:09:44

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8971
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_MarkBadSCache can drop lock
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 cm_MarkBadSCache should do the ForceReturnTokens call right before
 clearing the MARKINGBAD flag, so that it doesn't drop the lock at the
 wrong time.
 
 Mike Kazar
 Thu Aug 12 15:51:44 1993
 kazar-Thu, 12 Aug 1993 15:51:45
**Solution Text**
 kazar-db4094-do-forcereturntoken-last-in-markbadscache
 Mike Kazar
 Mon Aug 16 13:14:07 1993
 kazar-Mon, 16 Aug 1993 13:14:09

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8970
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : places where asyncStatus checked against ESTALE are wrong
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Carl points out that most of the places where we check asyncStatus ==
 ESTALE should just be checking asyncStatus for a non-zero value.
 
 Mike Kazar
 Thu Aug 12 15:50:37 1993
 kazar-Thu, 12 Aug 1993 15:50:39
**Solution Text**
 kazar-db4093-allow-any-value-for-asyncstatus
 
 Mike Kazar
 Mon Aug 16 13:13:22 1993
 kazar-Mon, 16 Aug 1993 13:13:24

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8969
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : efs_getlength should not call epia_GetLength for invalid vnodes
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 rajesh-db4092-fix-efs_getlength-for-NOIDENTITY-vnodes
 rajesh-Wed, 08 Sep 1993 15:41:18

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8968
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : See description below
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 kazar-db4090-release-more-locks-in-tkc-revokedatatoken
 Mike Kazar
 Mon Aug 16 13:12:52 1993
 kazar-Mon, 16 Aug 1993 13:12:54

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8967
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : attach of bad aggregate causes panic
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Jess inadertently put a partition containing a UFS file system
 into his dfstab file and listed its type as "lfs".  When he did
 an dfsexport, the system panicked in elbb_ReadGeneral with an
 "Unrecoverable I/O error in meta-data".  elbb_ReadGeneral was
 called from GetAggrHdrFromDisk, which was trying to read the
 superblock.
 
 The problem appears to be that when it finds a bad superblock,
 GetAggrHdrFromDisk tries a series of alternate superblocks at
 successively larger disk addresses.  There is no check against
 reading past the end of the partition, so eventually the driver
 returns EIO, at which point elbb_ReadGeneral panics.
 blake-Tue, 10 Aug 1993 17:42:32
**Solution Text**
 ota:
 
 Delta: ota-db4080-punt-alternate-superblock-search
 
 The alternate superblock searching logic generates a kernel panic when
 it can not find a good superblock.  This can easily happen when
 attaching a disk device that does not contain an Episode aggregate.
 
 Because the alternate superblock handling code is largely disabled
 anyway, just check the first location and return failure immediately if
 it doesn't look good.
 [Tue Aug 17 14:19:08 1993]
 ota-Tue, 17 Aug 1993 14:19:11
**Validation Text**

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8966
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : unpredictable behavior by CM upon setuid
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 Here's a test program that basically tests the ability to detect changes in
 identity after a file is opened, and the effect on subsequent write and close.
 The test program is appended below. The reason for this defect is, the 
 behavior is unpredicatable. 
 
 On one test run, the file has 2K data in it.
 # /afs/tr/usr/vijay/tmp/test test 
 mode is 33216
 sleeping for 40seconds...done
 mode is 33216
 sleeping for 40seconds...done
 # ls -l fooo
 -rwx------   1 cell_admi12             0 Aug 10 15:56 fooo
 # ls -l test
 -rwx------   1 cell_admi12          2048 Aug 10 15:56 test
 
 On another run, the file is empty.
 # /afs/tr/usr/vijay/tmp/test test
 mode is 33216
 sleeping for 40seconds...done
 dfs: set auth binding failed (code 382312714), running unauthenticated.
 mode is 33216
 sleeping for 40seconds...done
 # ls -l test
 -rwx------   1 cell_admi12             0 Aug 10 15:42 test
 
 This clearly indicates a bug in the CM. I'm assigning this to Tu for further
 evaluation.
 
 /*----------- test program --------------*/
 #include <sys/types.h>
 #include <sys/stat.h>
 #include <fcntl.h>
 #include <errno.h>
 #include <stdio.h>
 main(argc, argv)
 int argc;
 char **argv;
 {
   int fd, i;
   char buf[1024];
   struct stat statBuf;
   if (argc != 2) {
     printf("Usage: %s <file name>\n", argv[0]);
     exit(1);
   }
   if ((fd = open(argv[1], O_RDWR|O_CREAT|O_SYNC, 
 		 S_IRWXU/*|S_IRWXG|S_IRWXO*/)) < 0) {
     perror("open");
     exit(1);
   }
   if (fstat(fd, &statBuf) < 0) {
     perror("fstat1");
     exit(1);
   }
   printf("mode is %d\n", statBuf.st_mode);
   for(i=0;i<1024;i++)
     buf[i]='a';
   if (write(fd, buf, 1024) <= 0) {
     perror("write");
     exit(1);
   }
   printf("sleeping for 40seconds...");
   fflush(stdout);  sleep(40);printf("done\n");fflush(stdout);
   if (setuid((uid_t)4730) < 0) {
     perror("setuid");
     exit(1);
   }
   if (setgid((gid_t)11) < 0) {
     perror("setuid");
     exit(1);
   }
   for(i=0;i<1024;i++)
     buf[i]='b';
   if (write(fd, buf, 1024) <= 0) {
     perror("write2");
     exit(1);
   }
   if (fstat(fd, &statBuf) < 0) {
     perror("fstat2");
     exit(1);
   }
   printf("mode is %d\n", statBuf.st_mode);
   printf("sleeping for 40seconds...");
   fflush(stdout);  sleep(40);printf("done\n");fflush(stdout);
   if (close(fd) < 0) {
     perror("close");
     exit(1);
   }
 }
 vijay-Tue, 10 Aug 1993 16:14:54
 [tu, Aug 10, 1993]
 
 The problem is that the CM picks up the existing rpc binding based on the 
 'pag' stored in the u area NOT on [uid, pag]. While this is a desired
 feature (i.e., allowing the user, after setuid with a new effective uid, 
 to use the same rpc binding as long as it uses the same 'pag'),
 it could create a problem when there is no such rpc binding. 
 
 Unfortunately when the CM creates a new rpc binding, it passes both
 the new effective uid and pag to the SEC server to do the authentication work
 and fails if this pair [uid, pag] is not registered. 
 
 The cm then let the new user use an unauthenticated binding thus and explains
 that the behavior is unpredicatable.
 Appreantly IBM is also seeing this and Mike K is handling this. Assign to
 him. 
 tu-Tue, 10 Aug 1993 17:44:23
 -------------------------------------------------------------------------------
 tu-Tue, 10 Aug 1993 17:50:59
**Solution Text**
 kazar-db4078-use-real-uid-in-authentication
 Mike Kazar
 Tue Aug 17 16:34:53 1993
 kazar-Tue, 17 Aug 1993 16:34:55

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8965
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Ignore previously uncloned anodes in vol_efsSomeClone
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 Imagine a fileset chain with three filesets, ft1, ft2, and ft3, where
 ft3 is the R/W and is backed by ft2 which is backed by ft1.  If we are
 interrupted while uncloning ft2 into ft3, we can be left in a
 situation where a copy-on-write anode (a1) in ft3 is backed by an
 anode in ft1 (because we already uncloned this particular anode).  On
 a subsequent attempt to finish the unclone of ft2 into ft3, we will
 come across a1 and notice that it is not backed by an anode in ft2 and
 return EINVAL.  This is the logic implemented by the following
 fragment from vol_efsSomeClone.
 
     if (ap) {
 	/* found an anode so {re,un}clone it */
 	code = epiv_OpenBackingAnode (ap, &volB, &bc);
 	afsl_MBZ (code);
 	/* For regular volume ops we require the following relationship to be
 	 * true.  If this volume pair is bogus we fail.  If the volumes are ok
 	 * but the index doesn't match we could have serious problems and so
 	 * panic. */
 ==>	if (volB != bvolh) {
 	    (void) epiv_CloseAnode (bc);
 	    bc = 0;
 	    (void) epiv_Close (buffer_nullTranId, volB);
 	    volB = 0;
 	    (void) epiv_CloseAnode(ap);
 	    ap = 0;
 	    code = EINVAL;
 	    goto out;
 	}
 
 Instead, we should not return an error as long as there is no anode at
 the same index in `bvolh'; otherwise, we will prevent users from
 restarting an interrupted unclone (and thus destroy).
 Jeffrey Prem
 Tue Aug 10 16:09:06 1993
 jdp-Tue, 10 Aug 1993 16:09:11
**Solution Text**
 jdp-db4077-ignore-uncloned-anodes-on-retry 
 
 Jeffrey Prem
 Mon Aug 23 16:33:20 1993
 jdp-Mon, 23 Aug 1993 16:33:21

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8964
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : add prototype for cm_MarkBadSCache, and fix missing parm
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 kazar-db4075-add-missing-arg-to-markbadscache
 Mike Kazar
 Mon Aug 16 13:12:23 1993
 kazar-Mon, 16 Aug 1993 13:12:25

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8963
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : in cm_ReleaseLockF, if token deleted, ntp is invalid on continue
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 kazar-db4074-fix-releaselockf-iteration-bug
 Mike Kazar
 Mon Aug 16 13:11:44 1993
 kazar-Mon, 16 Aug 1993 13:11:46

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8962
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FLSERVER
Short Description             : allow lookup entry VL_ calls to succeed without quorum
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: comer-db4073-lookup-entries-without-quorum
 Build: dfs-103 3.27
 
 Vijay Anand
 vijay-Thu, 19 Aug 1993 13:48:54

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8961
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Restore of a non-overlapping incremental dump SUCCEEDS !!
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/10/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 A restore of a non-overlapping incremental dump succeeds. Jeff and I
 looked at the code and vols_RestoreVolume does not make the check to
 test overlap if the VOL_DELONSALVAGE bit is set in the fileset state
 field. But this bit will always be set as the "open fileset for
 restore" call always sets this bit.
  
 rajesh-Tue, 10 Aug 1993 11:48:04
**Solution Text**
 rajesh-db4072-prevent-restore-of-non-overlapping-incr-dumps
 rajesh-Fri, 13 Aug 1993 18:18:50

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8960
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Use OSI_VN_RELE instead of VN_RELE
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 vnm_StopUse calls VN_RELE directly without dropping the premption
 mutex.  VN_RELE can call efs_inactive which will try to lock the 
 premption mutex that is already locked by the current thread and cause
 a panic due to recursive mutex_enter.
 
 rajesh-Mon, 09 Aug 1993 14:50:52
**Solution Text**
 rajesh-db4065-use-OSI_VN_RELE 1.1 
  
 rajesh-Fri, 13 Aug 1993 18:17:29

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8959
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : incorrect transaction nesting in Episode
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 After running for several hours under extremely heavy load, Episode
 hung with all threads waiting to start a new transaction.  Ted and
 I traced the problem to the fact that efs_create calls dnlc_enter
 while a transaction is still in progress.  dnlc_enter may in turn
 call efs_inactive, which will start a new transaction, violating
 the assumption of the logbuf code that the active transaction in
 efs_create will continue to make progress.  The fix is to reorder
 things so that the transaction finishes before we dnlc_enter.
 We should also survey the code to look for other instances of
 this problem.
 blake-Mon, 09 Aug 1993 12:54:52
**Solution Text**
 blake-db4064-fix-transaction-nesting.
 blake-Mon, 23 Aug 1993 13:49:11

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8958
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : panic prevention for dfsexport -detach
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 ota:
 
 There is an anode handle leak that appears to be caused by an error
 during reclone or dump/restore.  In any case the epiv_Close code panics
 if this condition is detected.  Ideally we should fix the anode handle
 problem but in the meanwhile having the system panic is too severe.
 
 We should just return some error code and give the sysadmin a change to
 cleanup or reboot when convenient.
 [Mon Aug  9 12:18:02 1993]
 ota-Mon, 09 Aug 1993 12:18:06

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8957
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fix ACL test shell scripts
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Solution Text**
 rajesh-db4062-fix-acl-test-script-errors
 rajesh-Mon, 09 Aug 1993 14:00:26

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8952
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : async grant before gettoken call completes not handled
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 kazar-db4060-fix-async-grant-race-condition
 Mike Kazar
 Thu Aug 12 12:01:03 1993
 kazar-Thu, 12 Aug 1993 12:01:05

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8951
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : get rid of spurious include file from cm_dcache.c
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 <dce/ker/pthread_exc.h> isn't required any more.
 Mike Kazar
 Mon Aug  9 09:20:31 1993
 kazar-Mon, 09 Aug 1993 09:20:41
**Solution Text**
 kazar-db4056-clean-extra-incl-file
 
 Mike Kazar
 Mon Aug 16 13:10:00 1993
 kazar-Mon, 16 Aug 1993 13:10:03

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8950
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Decouple deletion of files and empty volume in vol_efsDestroy
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 When cleaning up after a failed clone ftu_fset.c:DestroyFset() can
 also fail or be interrupted.  If it fails after calling
 ftutil_VolDestroyAux() but before unsetting the CLONEINPROG bit, the
 original fileset will be inaccessible (because of CLONEINPROG), but
 the clone fileset will be gone.  The problem is caused by the
 overloaded semantics of vol_efsDestroy().  Not only does it destroy
 each of the anodes in a fileset, but it also destroys the fileset
 itself.  There is no point at which the caller knows that the fileset
 is empty (and thus all previous backing anodes have reduced reference
 counts).  It is at just such a point that the CLONEINPROG bit should
 be turned off: after the fileset is depleted but before it is
 destroyed.  To accomplish this, a new volume operation will be
 invented.  VOL_DEPLETE() will be called repeatedly, returning ELOOP
 until it is finished.  When it is finished, the target fileset will be
 empty and can be passed to VOL_DESTROY(), which will destroy the empty
 fileset, remove it from the registry, etc.
 Jeffrey Prem
 Fri Aug  6 16:08:15 1993
 jdp-Fri, 06 Aug 1993 16:08:17
**Solution Text**
 jdp-db4049-add-VOL_DEPLETE 
 
 Jeffrey Prem
 Mon Aug 23 16:32:04 1993
 jdp-Mon, 23 Aug 1993 16:32:06

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8949
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : make the fx test readme clearer
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 There could be problems if the output of the test is redirected to a log 
 file in DFS because of the dce_login that the test script does. Also, if
 multiple instances of the test are run concurrently, the filesets within
 which the test is run should be different. This change should be made in the
 config file before running the script, else the multiple test runs may step
 on each other.vijay-Fri, 06 Aug 1993 10:23:11
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-sb4044-fx-test-enhancements
 Build: dfs-103 3.25
 
 Vijay Anand
 vijay-Thu, 19 Aug 1993 10:54:17

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8948
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Replace repserver printfs with ICL tracing
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 The repserver code itself currently has no ICL trace points, but instead 
 accepts a ``-verbose'' flag, which controls the state of a ``Debugging'' 
 variable, which causes lots of messages to be printed on stderr.  It would be 
 more useful to replace the Debugging code with ICL traces that could be 
 extracted as necessary.  Events that represent unusual errors should be logged 
 to RepLog. 
 khale-Thu, 05 Aug 1993 15:10:06

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8947
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : bossvr_ncs_procs.c is using AFS_VFSINCL_ENV.  Wish to flush
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/05/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 In bossvr_ncs_procs.c conditionals for AFS_VFSINCL_ENV
 AFS_NEXT20_ENV, and AFS_VFS40 surround includes of dirent.h and
 ndir.h.  All three are obsolete so we will flush them and 
 retain the includes which are relevant.
 bwl-Thu, 05 Aug 1993 14:51:33
**Solution Text**
 Delta berman-o-db4039-old-ifdefs-in-bosserver.
 bwl-Wed, 25 Aug 1993 16:43:16

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8946
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : OSI
Short Description             : Can no longer have empty def. of osi_RestorePreemption
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 With the advent of the bomb point macros, it is now common to see calls to
 osi_RestorePreemption in comma-separated sequences of expressions, e.g.
 
   BOMB_EXEC(...,
             (osi_RestorePreemption(0),
              ...
              ));
 
 Many C compilers do not like to see empty expressions in this construct.
 The definition of osi_RestorePreemption in osi/osi.h, where it expands to
 nothing, would not be accepted.  Likewise for definitions for some platforms
 in osi/<platform>/osi_port_mach.h.
 bwl-Wed, 04 Aug 1993 17:25:10
**Solution Text**
 Delta bwl-o-db4029-non-empty-restorepreemption.
 bwl-Wed, 25 Aug 1993 16:41:34

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8945
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : The cm could potentially enter an infinite loop ...
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/03/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 The cm could potentially enter an infinite loop when trying to return a token
 to the server while the server is in either TSR mode or in DOWN mode. 
 
 tu-Tue, 03 Aug 1993 19:13:23
 [tu Aug 9, 93]
 
 Export it. Remove the third argument of cm_QueueAToken. That is, 
 when trying tokens to server should not call cm_FlushQueuedServerTokens
 immediately.
 tu-Mon, 09 Aug 1993 11:33:44
**Solution Text**
 kazar-db4024-integrate-delta is also required to integrate
 this properly
 
 Mike Kazar
 Thu Aug 12 11:59:51 1993
 kazar-Thu, 12 Aug 1993 11:59:53

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8944
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : CM prints an odd collection of server-back-up messages, obscuring truth
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/03/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 Delta: cfe-db4022-stop-server-back-up-noise
 cfe-Wed, 04 Aug 1993 11:34:54
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8943
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : test_anode has preemption problems
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 Need global lock around threadsWithActiveTrans array used by
 RememberThreadTrans.
 [Tue Aug  3 14:22:33 1993]
 ota-Tue, 03 Aug 1993 14:22:36
**Solution Text**
 ota:
 
 Delta: ota-db4021-test_anode-preemption-problems
 [Tue Aug  3 15:23:43 1993]
 ota-Tue, 03 Aug 1993 15:23:46

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8942
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Waiting in epis_AdjustReservationUnits is unsafe
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/02/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The code in AdjustReservation that waits to withdraw reservation from
 the pool had a problem where it could lose a wakeup and so deadlock.
 
 Also there was a sign error in returning withdrawn quota when correcting
 for global reservation quantization.
 [Thu Aug  5 11:42:06 1993]
 ota-Thu, 05 Aug 1993 11:42:08
**Solution Text**
 Delta: ota-db4013-simplify-sleeping-in-adjust-reserve r1.3
 
 ota:
 
 Reorganize code that changes reservation pool size.
 
 Fix sign error.
 
 Change reservation functions to deal with int's instead of long's
 consistently and unsigned instead of signed when appropriate.
 [Thu Aug  5 11:42:09 1993]
 
 ota:
 
 [Thu Aug  5 15:35:06 1993]
 ota-Thu, 05 Aug 1993 15:35:09

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8941
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Do not bash buf.b_option on AIX
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/20/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 Assigning into b_bufsize in the AIX kernel is a very bad thing to do.
 This field is overlayed on b_options which is used by the LVM.  The
 actual need for this field by Episode is pretty weak, so the appropriate
 fix is to punt almost all references to it.  The user-space code does
 make some use of this flexibility to test alternate VM pages sizes.
 [Thu Aug  5 15:31:44 1993]
 ota-Thu, 05 Aug 1993 15:31:48
**Solution Text**
 Delta: ota-db4010-eliminate-b_bufsize-uses r1.3
 
 ota:
 
 Preserve the user-space capability since it is easy.  Make all
 references to b_bufsize via a macro (b_Bufsize) which is not an lvalue,
 so we cannot assign into it.  In user-space just reference the test_vm
 variable directly.
 
 To avoid depending on b_bufsize, pass length explicitly to
 vnvm_ReleaseUserBuffer
 [Thu Aug  5 15:32:08 1993]
 
 ota:
 
 Remove SHARED.
 [Thu Aug  5 16:38:21 1993]
 
 ota:
 
 Revision 1.3 of this delta remove the last reference in kernel code so I
 deem this delta safe to export.
 [Fri Aug  6 08:30:08 1993]
 ota-Fri, 06 Aug 1993 08:30:11

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8940
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Enhance efts to do incremental fileset dumps and restores
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 08/02/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 Efts currently does full dumps and restores. Enhance it for incremental dumps
 and restores. 
 
 Incremental dumps and restores from efts will be needed for the fileset
 stress tests.rajesh-Mon, 02 Aug 1993 10:07:46

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8939
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : Bogus code in bindingArraySorter
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/30/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 The pointers hdl1 and hdl2 are used before being initialized in
 bindingArraySorter.  Perhaps need &'s?  Why are we using bcopy's rather than
 just assignments?
 bwl-Fri, 30 Jul 1993 16:19:18
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-db4008-ncsubik-fix-regression
 Build: dfs-103 3.27
 
 Vijay Anand
 vijay-Fri, 27 Aug 1993 18:11:29

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8938
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : hashVal not initialized in lock_UpgradeSToWNoBlock
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/30/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 The HP compiler caught this error: hashVal is not initialized in function
 lock_UpgradeSToWNoBlock in osi/osi_lock_pthread.c.  Presumably this function
 is never in the current code base; otherwise this would be fatal.
 bwl-Fri, 30 Jul 1993 14:24:16
**Solution Text**
 Delta bwl-o-db4006-uninitialized-vbl.
 bwl-Wed, 25 Aug 1993 16:40:13

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8937
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm should track tokens on a per-server basis
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 TSR really requires tracking tokens on a per-server basis to work
 reliably.
 Mike Kazar
 Fri Jul 30 11:14:35 1993
 kazar-Fri, 30 Jul 1993 11:14:37
 
 Hmm. This was filed recently, and didn't have an owner. Could this be
 the last one filed without an assigned engineer?
 pakhtar-Tue, 03 Aug 1993 17:36:08
**Solution Text**
 kazar-db4004-add-per-server-token-tracking
 Mike Kazar
 Thu Aug 12 11:59:12 1993
 kazar-Thu, 12 Aug 1993 11:59:16

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8936
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : CloneID is being changed from the R/O ID in cloning or moving a fileset
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 Delta: cfe-db3998-leave-cloneid-alone-on-move
 cfe-Thu, 05 Aug 1993 14:51:18
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8935
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : OSI
Short Description             : osi_cv2string converts 0 to the null string, not 0.
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/29/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 Ever since blake-db3108-SunOS5.x-syscall-and-modload-code, zeroes wouldn't
 print in osi_cv2string.  This is fixed in cfe-db3997-print-zeroes-really,
 tested in dfs-103 3.24.
 cfe-Thu, 29 Jul 1993 13:42:54

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8934
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : fix asevent splbio for user-space and SUNOS5
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/29/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The user space synchronization using splbio was completely disabled.
 This is unacceptable with the preemptive threading model now in use.
 The SunOS5 kernel implementation seems to have several race conditions.
 [Thu Jul 29 11:17:22 1993]
 ota-Thu, 29 Jul 1993 11:17:27
**Solution Text**
 Delta: ota-db3995-add-user-space-splbio
 
 ota:
 
 Make the user space version work like the SunOS5 version in that it
 grabs an explicit lock when processing "interrupts".  In the process we
 need to make sure sleeping on events and on sets is correct and free of
 race conditions.  The old code was confusing normal process level
 locking of events and sets with the synchonization needed at interrupt
 level.
 
 Cleaning up this model and providing a generic user-space implementation
 required fairly extensive changes to the osi_intr and osi_buf families
 of include files.
 [Thu Jul 29 15:04:21 1993]
 
 ota:
 
 ota-Thu, 29 Jul 1993 15:04:24
 
 Added a global condition variable to implement user-space buffer waiting
 (the SunOS5 and AIX kernel schemes are both unworkable for different
 reasons).
 
 Blake and I reviewed the asevent code again and we made some further
 cleanups and simplifications.
 
 In particular, we removed SunOS5 Kernel specific #ifdefs from WaitEvent.
 This required some small modifications to the definition of
 osi_bio_wait.  These comments were carefully commented.
 [Wed Aug  4 16:07:28 1993]
 ota-Wed, 04 Aug 1993 16:07:33

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8933
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Following an ICL log does not work
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/27/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 On Solaris, dfs-103-3.24, following an ICL log, does not work. After
 displaying valid data for an indeterminate period of time from 1 min
 to 20 minutes, it enters a endless loop printing the same record out
 over and over again, as a record of zero length is encountered. At the
 point where the zero length record is encountered, there is big jump
 in the timestamp value, almost 28 seconds.
 
 There is some data in ~rajesh/keep/iclstuff2 wrt this problem.
 
 The problem does not seem to happen while dumping the logs.
 
 rajesh-Tue, 27 Jul 1993 16:56:49
**Solution Text**
 The follow code was missing a couple of parens causing it to place new
 data in the wrong position in the buffer.  Before, the code that converted
 the raw data into messages would get lost; now it doesn't.
 
 Verified the change on 1.0.3 3.25.
 comer-Tue, 10 Aug 1993 16:36:02

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8932
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : work-around for non-reentrant sscanf
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 On AIX sscanf is neither reentrant nor wrapped.
 [Tue Jul 27 15:31:51 1993]
 ota-Tue, 27 Jul 1993 15:31:54
**Solution Text**
 Delta: ota-db3975-wrap-.scanf r1.3
 
 ota:
 
 Provide wrappers that grab the global CMA mutex.
 [Tue Jul 27 15:31:55 1993]
 
 ota:
 
 ota-Tue, 27 Jul 1993 15:31:58
 
 Revs 1.2 & 1.3 make sure that the wrapper compiles on SunOS5 and make the
 test pgram work.  The latter verifies that these problems are AIX
 specific and do not affect scanf on Solaris.
 [Fri Aug 13 12:45:05 1993]
 ota-Fri, 13 Aug 1993 12:45:10

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8931
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : WaitForWrite must hold its own event reference
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The code in WaitForWrite remembers an asevent pointer in a local
 variable across an interval when the buffer lock is dropped.  This is
 only safe as long as the buffer lock is held since the the local
 variable is essentially sharing the buffer's reference to the event.
 When the buffer lock is dropped the buffer may be detached from the
 event and reattached to a different, new one.  In this case actually the
 new event was actually the "same" event structure and so the pointer
 comparison the code does was producing misleading results.
 [Tue Jul 27 08:08:16 1993]
 ota-Tue, 27 Jul 1993 08:08:21
**Solution Text**
 ota:
 
 Delta: ota-db3963-hold-event-in-WaitForWrite
 
 To protect the local reference we must hold the event and release it
 after reobtaining the buffer lock.  If the buffer's event pointer is no
 longer the same we must not process the post-IO buffer cleanup.
 [Tue Jul 27 08:16:11 1993]
 ota-Tue, 27 Jul 1993 08:16:29

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8930
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage should init BB buffer to make results reproducible
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 ota:
 
 Delta: ota-db3960-init-BB-buffer
 
 Check that the BB file's length is at least long enough to contain the
 frame header and the first entry's header.  Otherwise referencing these
 fields after reading in the file's contents would yield garbage.
 [Mon Jul 26 10:56:36 1993]
 ota-Mon, 26 Jul 1993 10:56:40

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8929
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Reorganize RCX test scripts
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 rajesh-db3959-reorganize-rcx-test-input-scripts
 rajesh-Mon, 30 Aug 1993 08:56:41

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8928
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Add assertion to epiv_GetIdent
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 Assert that the volume handle passed in is not null
 
 rajesh-Thu, 22 Jul 1993 16:15:37
**Solution Text**
 rajesh-db3946-assert-non-null-vol-handle-in-epiv_GetIdentrajesh-Sat, 07 Aug 1993 18:19:18

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8927
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : See description below
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ld.so: warning: /usr/5lib/libc.so.2.7 has older revision than expected 8
 -------------------------------------------------------------------------------
 
 change a call to srandom to srand48 or some such. 
 vijay-Thu, 22 Jul 1993 11:30:05
 -------------------------------------------------------------------------------
 
 fixed in
 
 Delta: vijay-db3943-bakserver-remove-some-bsdisms
 Build: dfs-103 3.23
 
 Vijay Anand
 Mon Jul 26 16:10:05 EDT 1993
 vijay-Mon, 26 Jul 1993 16:10:44

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8926
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fix panic in epiv_GetIdent on null volp
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/21/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 rajesh-Thu, 22 Jul 1993 16:17:54
**Solution Text**
 rajesh-db3935-use-a-flag-to-indicate-invalid-episode-vnode
 rajesh-Sat, 07 Aug 1993 18:20:05
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8925
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage crashes on short aggregate
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The aggregate's partition is shorter than the aggregate thinks it is.
 This means that some blocks are beyond the end of the device.  When
 doing repairs the salvager fixes these, but when verifying it needs to
 be more careful to skip such files.
 
 This particular problem shows up when verifying:
     ~ota/de/salvage-data/aggr.coven.0070993.coredumpbysalvage.bogusPartitionSize.Z
 In this case the bad block container is fragmented and the block
 containing the fragments is beyond the end of the device.
 VerifyAnodeFrags is at fault.
 [Tue Jul 20 16:26:54 1993]
 ota-Tue, 20 Jul 1993 16:27:00
**Solution Text**
 ota:
 
 Delta: ota-db3927-handle-short-aggregate
 
 Refuse to verify a bitmap that is longer than the aggregate can be.
 
 Return from VerifyAnodeFrags if the block fails its preliminary checks.
 This is necessary when not doing repairs so that epiz_MarkUsed doesn't
 panic.
 [Wed Jul 21 15:44:59 1993]
 ota-Wed, 21 Jul 1993 15:45:02

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8924
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : asevent pin/unpin code broken on AIX
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 The Episode asevent code uses the ASEVENT_CREATED bit to decide
 whether or not to pin I/O buffers before starting I/O and to unpin
 them afterwards.  This formerly allowed it to distinguish between
 the VM I/O path (where events were created dynamically) and the
 logbuf I/O path (where they were not).  Now, however, we dynamically
 create all events, breaking this dubious mechanism.  To fix this,
 we will set the ASEVENT_NOPIN bit as part of asevent_Create when
 appropriate.
 blake-Mon, 19 Jul 1993 18:18:23

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8923
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage needs bullet-proofing in log cksum versioning
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The log CRC needs to know what cksum version number to use when writing
 the restart block.  It used to obtain this from the old restart block,
 but if there wasn't one this could lead to using a garbage value.
 [Mon Jul 19 15:36:22 1993]
 ota-Mon, 19 Jul 1993 15:36:26
**Solution Text**
 ota:
 
 Delta: ota-db3895-use-safer-cksum-version-for-recovery
 
 The fix is the use the value encountered during recovery instead if
 recovery is run.
 
 Also several other cleanup changes:
   1. Only have one copy of write-restart-block code which is called from
      both user and kernel space.
   2. Move procedures needed by both log and recover module to recover.c
      since that module is lower than the log module in the call graph.
      Also change the global names to have appropriate prefix.  Ensure
      that private functions are declared "static".
   3. Replace recvr_ prefix with elbr_.
 [Mon Jul 19 15:36:27 1993]
 ota-Mon, 19 Jul 1993 15:36:30

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8922
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : Does incorrect error checking of DCE directories.
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 The bosserver checks a number of constraints on a set of DCE directories.
 One of these checks is that the /opt/dcelocal/bin directory be owned
 by root.  Well on Solaris this directory is owned by bin.  So there are 
 entries in the BosLog such as,
 
 Tue Jul 13 09:43:07 1993: Server directory access is not okay
 
 And when you issue a bos status against a Solaris machine you get this 
 message in addition to the regular output.
 
 Bosserver reports inappropriate access on server directories.
 
 fred-Wed, 14 Jul 1993 08:54:25
 ++++++++++++++++++++++++++++++
 The bosserver does permission checking according to the man page.
 I mentioned this discrepancy to Beth and she said she would check
 with Prasad to see what the story is.  I'm assigning this to Pervaze 
 in the mean time.
 comer-Tue, 27 Jul 1993 10:24:47
**Solution Text**
 I made a change to the code in the bosserver that checks the ownership
 of various directories to make it more vendor-configurable.  Instead
 of checking that the directory or file is owner by root, the user name
 of the required owner can now be specified.  For the solaris port, all
 directories are now checked for ownership by bin and admin .bos is
 checked for ownership by root.
 
 Defect Closure Form
 ===================
 I verified this change on a sparc and on a rios running 3.24.  The
 test was to start the bosserver, do a 'bos stat -long' and verify that
 "Bosserver reports inappropriate access on server directories." is not
 printed.  Now, change the owner of something like
 /opt/dcelocal/var/dfs, run the bos command again and verify that the
 message does get printed.
 comer-Tue, 10 Aug 1993 09:16:51

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8921
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Add thread scheduling control to test_anode
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 To isolate a bug that shows up intermittently with test_anode I needed
 to change the scheduling policy to FIFO.  The effectively restores the
 non-preemptive situation that test_anode previously enjoyed in 102 and
 before that running on LWP.
 [Thu Jul 15 08:04:39 1993]
 ota-Thu, 15 Jul 1993 08:04:43
**Solution Text**
 Delta: ota-db3875-thread-scheduling r1.4
 
 ota:
 
 Add test_anode options to set the scheduling policy and preemption
 quantum.  I have not fully tested all these features (and they will have
 no effect on Solaris) but setting the policy to FIFO makes the bug
 disappear.  This confirms at least to first order that the changes are
 having the desired effect.
 [Thu Jul 15 08:04:49 1993]
 
 ota:
 
 ota-Thu, 15 Jul 1993 08:04:52
 
 By rev 1.4 these changes also compile, but are disabled, on SunOS5.
 [Fri Aug 13 12:12:01 1993]
 ota-Fri, 13 Aug 1993 12:12:04

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8920
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage fails on bizarre directory
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The directory has a pgcount of 1 and the first block looks okay, but the
 anode length is 0xfc8eda41 and contains two blocks of zeros at extremely
 large offsets.
 
 The question "HOW?" comes to mind, but that isn't the point here, and
 anyway Dimitris denies everything.
 
 The aggregate is in ~ota/de/salvage-data/luthien.bad-dir.solaris.Z.
 [Thu Jul  8 15:55:10 1993]
**Solution Text**
 Delta: ota-db3843-repair-bizarre-dir
 
 ota:
 [Tue Jul 13 15:38:17 1993]
 
 ota:
 
 Handle container sizes >= 2^31.  However, if length implies a pgcount
 that will not fit in 16-bit pgcount field then reject it.
 
 Make sure toFile's length is set appropriately for pgcount.  This
 requires writing out the last page (pgno == pgcount-1).
 
 Salvage (and preserve) sparse directories with unallocated pages.
 
 Also make some efficiency improvments since this dir appears *very* big
 and so some things took a long time.
 [Tue Jul 13 16:10:11 1993]
 ota-Tue, 13 Jul 1993 16:10:16

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8919
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : New assertion in epia_Open causes salvager test failure
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 Inter-related CRs: 3561
 
 The delta ota-db3561-panic-in-epiv_CreateAnode added a new assertion to
 epia_Open to catch disk errors earlier but it causes the salvager tests
 to fail.
 [Thu Jul  8 13:26:50 1993]
**Solution Text**
 Delta: ota-db3841-fix-disk-error-assertion r1.2
 
 ota:
 
 Assert that disk index has expected value in KERNEL but in user space
 (for the salvager) return an error code instead.
 [Thu Jul  8 14:52:17 1993]
 
 ota:
 
 Rev 1.2 of this delta fixes a problem that caused the salvager to crash
 because the above check now catches a problem with copies verification
 that was leading to (unaddressed) resalvaging errors in the salt test.
 
 The fundamental problem is that the backing anode reference in an anode
 is by block/offset whereas the salvager tracks everything by index.  If
 these two are inconsistent with each other the salvager can get
 confused.  If the COW and backing anodes are encountered in the wrong
 order a resalvage is required.
 
 Save the block/offset pair used when opening a backing anode for copies
 verification.  During SalvageCopiesVerify check the block/offset pair to
 make sure they match that of the anode opened by index.  If they don't
 match or if the anode cannot be opened trigger resalvaging.
 [Tue Jul 13 15:52:02 1993]
 
 ota:
 
 ota-Tue, 13 Jul 1993 15:52:05
 [Fri Aug 13 10:31:26 1993]
 ota-Fri, 13 Aug 1993 10:31:28

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8918
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Bakserver core dumps in osi_free
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 Bakserver core dumps in osi_free in DbVerify.

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8917
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : osi_ThreadUnique broken on user space RIOS
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 The osi_ThreadUnique function on the RIOS in users space was defined to
 call getpid.  This resulted in all threads have the same id which
 caused trouble.
 
 There is also some confusion about the intended distinction between
 ThreadID and ThreadUnique.  I added the following comment in an attempt
 to clearify the issue:
 /* osi_ThreadID returns a convenient handle for the current thread.
  * osi_ThreadUnique returns an identifier that is unique for "all" time. */
 [Wed Jul  7 16:25:17 1993]
**Solution Text**
 Delta: ota-db3832-fix-thread-unique r1.2
 
 ota:
 
 Not yet tested on Solaris.
 [Wed Jul  7 16:25:21 1993]
 
 ota:
 
 Revision 1.2 fixes a compilation problem on security/dacl_lfs/testacl.
 [Fri Aug 13 10:28:36 1993]
 ota-Fri, 13 Aug 1993 10:28:38

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8916
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : User space VM no longer works in 103
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/02/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 ota:
 
 Regression testing of the user space testing against 103 reveals that
 the VM code has some bugs.  Principly, incorrect use of defines causes
 much critical code to disappear in user space.
 [Tue Jul  6 13:20:30 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3822-fix-user-space-VM
 
 Replace almost all occurrences of AFS_AIX31_VM (which is true only in
 the kernel) with AFS_AIX_ENV.  We really want the _VM symbol only when
 referencing KERNEL specific stuff like the xmem field.
 
 Also: flush OSF Logs.
 [Tue Jul  6 13:20:32 1993]

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8915
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : HandleLogFull can generate spurious console messages
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/29/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 The following message was printed to the console of elbereth (a Solaris
 machine) while running the file exporter stress tests.  Ted says it
 is harmless, but should be made an ICL statement rather then going to
 the console.
 
 HandleLogFull: wait is TRUE but NO action.
 
 ota:
 
 Rajesh and I had occasion to review the code associated with this problem.
 
 [. . .]
 ota-Wed, 01 Sep 1993 08:09:38
 
 ota:
 
 Our discoveries have been moved to defect 4331.
 ota-Wed, 15 Sep 1993 14:59:28
**Solution Text**
 ota:
 
 Delta: ota-db3798-elimiate-HandleLogFull-msgs
 
 Remove annoying and confusing printf.
 
 There are two interesting callers of HandleLogFull when no action is
 possible and the old code contained the printf.  In one case we now
 panic as it should never happen and we do not presently do anything but
 loop.  In the other case the caller will correctly handle the no action
 condition by waiting for an undo zero transaction to end.  We
 distinguish between the two by setting wait to one or two.  The wait==0
 case is fine.
 ota-Wed, 15 Sep 1993 14:59:37
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8914
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : ubik should convert runtime faults into error codes
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 The fault status attribute should be set in the .ACF file for ubik so that
 runtime faults show up as return codes instead of exceptions.
**Solution Text**
 ld.so: warning: /usr/5lib/libc.so.2.7 has older revision than expected 8
 -------------------------------------------------------------------------------
 
 Fixed in
 
 Delta: vijay-db3770-ubik-return-fault-status
 Build: dfs-103 3.23
 
 Vijay Anand
 Mon Jul 26 16:10:05 EDT 1993
 vijay-Mon, 26 Jul 1993 16:18:05

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8913
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Add tracing to efts
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/18/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Solution Text**
 rajesh-db3738-provide-tracing-support-in-efts
 rajesh-Fri, 13 Aug 1993 18:16:28

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8912
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : ubik unable to handle clock skews correctly
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 Clock skew between ubik servers causes the voting mechanism to go astray
 that it is unable to recover and achieve quorum once the clocks are back
 in line.
 -------------------------------------------------------------------------------
 
 This defect also includes other fixes
 
 1. clock skew (fwd & back) on both sync and non-sync sites
 2. concurrent beacons and probes using thread pool queues
 3. udebug not locking out a thread indefinitely at the server waiting for 
    some locks.
 4. auto-reconfiguration of ubik. This feature has been disabled for the release
 
 The document containing detailed design is in 
 /afs/tr/fs/dev/dfs/design/published/23.ubik_more_robustness.
 vijay-Fri, 27 Aug 1993 18:08:13
**Solution Text**
 -------------------------------------------------------------------------------
 
 Delta: vijay-db3670-ubik-handle-clock-skews
 Build: dfs-103 3.27
 
 Vijay Anand
 vijay-Fri, 27 Aug 1993 18:08:16

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8911
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : obscure error message when doing lsft on a non-ex vol type
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/26/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 I issued the following fts command, and got the error message shown.  I 
 believe the 'ix' should be 'id'.
 
 [cell_admin] fts lsft -fileset user.srinivas.backup
 No fileset of ix 2 (type 0x20000, backup) in FLDB entry for user.srinivas (0,,925)
 
 [cfe:]
 Nope, the 'ix' is an abbreviation for 'index', and it's the index within
 one of the arrays in the fldb entry.  The message could be clarified to say
 that there's no backup fileset in the FLDB entry, I suppose.
**Solution Text**
 Delta: cfe-db3605-clean-up-obscure-error-message
 cfe-Thu, 05 Aug 1993 14:49:00
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8910
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : Does not validate arguments for crmount command.
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 I entered the following command in the self-host cell, mistakenly
 placing the fileset name ahead of the directory where it was to be
 mounted.  There was no complaint from fts that the fileset was invalid.
 
 fts crm nobak.fred.aim3.dewitt /:/usr/fred/dfs/aim/dewitt_fs
**Solution Text**
 Delta: cfe-db3507-print-warning-for-any-flserver-complaint
 cfe-Thu, 05 Aug 1993 14:45:38
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8909
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : syscall test failed; mtime changed by a failed open call.
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 Jun. 14,93
 
 This was found in APR.. We still see it in DFS 103 3.17.
 
 --Jess
**Solution Text**
 kazar-db3483-dont-change-mtime-on-failed-open
 Mike Kazar
 Thu Sep  2 19:34:15 1993
 kazar-Thu, 02 Sep 1993 19:34:16

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8908
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Creating fileset with 0 quota leaves garbage
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/4/93 public]
**Description Text**
 If the maxquota field in the status structure passed to AG_VOLCREATE()
 is 0, the call fails with an EDQUOT error, but the new fileset is left
 in an inconsistent state on the aggregate.
 
 Jeffrey Prem
 Wed Apr 28 09:53:58 1993
 
 This is only an issue for stand alone LFS.
 Pervaze Akhtar
 Thu Sep  9 17:24:10 1993
 pakhtar-Thu, 09 Sep 1993 17:24:12
 
 Well, perhaps that's true as stated, but the related problem where an
 aggregate is nearly full is a problem in DFS as well.  (A fileset can be
 created on that aggregate, but there might not be room enough for its
 root directory, and Episode isn't cleaning up.)  Here are some remarks
 from the HP Episode code review, with Transarc developer comments at
 the end:
 
    []	efs_agops.c / ag_efsVolCreate
 
 	If the supplied quota (in the status) is TOO SMALL, terrible
 	things can happen.  Even though the call fails, the fileset is
 	created and left in a DELETE_ON_SALVAGE state.  It doesn't make
 	it into the (higher-layer) volreg database.  Although it "is
 	visible" via the enumerate fileset operation, you can't open
 	--- and therefore, delete --- it.  (Of course, if you detach and
 	then re-attach the aggregate, it does show up in the volreg
 	database.)  Because it's not in the volreg database, you can
 	mistakenly end up creating additional filesets with the same
 	name and ID if you're not careful (especially if you're
 	specifying IDs manually via the efts command).  Also, the
 	aggregate .a_nVolumes count gets all messed up.
 
 	Possible fixes that I've thought of:
 	  (a) In ag_efsVolCreate(): if the fileset WAS created (even if
 	      left in an DEL-ON-SALVAGE/INVALID/INCONSISTENT state, call
 	      vol_Attach() with it.  (Since this is the state it would
 	      be in if an attach/re-attach were done.)
 
 	  (b) In ag_efsVolCreate(): make sure the fileset is deleted if
 	      an error is returned.
 
 	  (c) ag_efsVolCreate() can fail if fileset by that name already
 	      exists
 
 	  (d) Although we could try to fix this above Episode
 	      ( {1} Call enumerate-fsets prior to the create ... if the
 	            name or ID already exist, reject it
 	        {2} Call enumerate-fsets if the create fails ... if the
 		    name or ID do exist, make sure it gets entered into the
 	            volreg database and what not )
 	      ... this seems awful).
 
 	  (e) ag_RegisterVolumes(): if volreg_Enter() returns EXIST on a
 	      fileset already in the database, don't bump the struct aggr
 	      ref count ... or bump the aggr.a_nVolumes field .
 
 	  (f) vol_Attach()  (vol_misc.c): has the same problem.  If its call
 	      to volreg_Enter() fails, then the hold on the aggr structure
 	      and the increment of aggr.a_nVolumes doesn't help matters!
 
 	My vote would be for doing (a), (e) and (f) ... or something
 	similar.
 
 If the problem is restricted to creating filesets with insufficient
 quota, then why not solve it at a higher level, by fiddling with the
 quota during the fileset creation, or by checking the quota
 first. (bwl)
 
 If there is a more general problem with recovery from incomplete
 ag/volume creation, I will hand the microphone to somebody who has
 worked on volume op recovery recently. (bwl)
 
 I'd do the suggested (a or b), (e), and (f).  (d) is a terrible idea,
 agreed.  Bruce's suggestion here won't work very well since you can't
 really tell how big a quota is necessary a priori. (cfe)
 cfe-Fri, 10 Sep 1993 15:43:45
**Solution Text**
 jdp-db3480-undo-failed-fileset-create
 Jeffrey Prem
 Wed Sep 22 17:27:30 1993
 jdp-Wed, 22 Sep 1993 17:27:33
**Validation Text**

[10/4/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[12/17/93 public]
Closed.



CR Number                     : 8905
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs.clean
Short Description             : login prompts unnecessary
Reported Date                 : 10/4/93
Found in Baseline             : 1.0.3
Found Date                    : 10/4/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dfs.clean
Sensitivity                   : public

[10/4/93 public]
Rick Kissel noticed that dfs.clean prompts for user id and password
even if the user is already logged into the sec_admin group.  Also,
it prompts even if there are no DFS daemons to kill and the login
is therefore unnecessary.

[10/4/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 8891
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : read_write_all porting changes
Reported Date                 : 10/1/93
Found in Baseline             : 1.0.3
Found Date                    : 10/1/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : README.RWALL, main, data, do.ksh
Sensitivity                   : public

[10/1/93 public]

main (dfs.read_write_all.main)
-	need to remove UNIX account setup routines - applied only to
	RIOS and would be cumbersome to maintain without adding to
	ease of use or coverage.
-	need to add verification routines for:
		- available space on aggregates
		- matching registry and local uids on all machines
		- existence and access of logging directories
-	need to add "wait" logic to verify success or failure of
	all users before exitting
-	RSH var for remsh vs. rsh
-	replace "ROOT" references with "BASE" where appropriate
	to avoid confusion with unix user root and root filesystem
-	ping_command var for syntax variation
-	replace "home directory" references with "home fileset" references
	since they can now differ
-	correct fts setquota syntax
-	need to check for POPULATE_USER_ENV var
-	need to remove unnecessary acl_edits
-	standardize on print (somethings are echo some not)
-	fix and enable delete routines

data (dfs.read.write.data)
-	minor reordering, recommenting, renaming and removing to
	support changes in main

README.RWALL
-	give 2 invoke syntax options for whether mount points are in LFS or native
-	move default certificate lifetime comment closer to invoke syntax
-	move root setup to add_root_as_admin script
-	remove warnings about not cleaning up
	
do.ksh
-	use KSH SECONDS rather than complex TIMER
-	output PASSED or FAILED and exit with 0 or 1
-	add -r to rm to handle subdirectories	
-	use $username variable

[10/4/93 public]

Tested, submitted and built - closing.



CR Number                     : 8889
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : DFS docs need Transarc Copyright
Reported Date                 : 10/1/93
Found in Baseline             : 1.0.2
Found Date                    : 10/1/93
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Detailed Description
Sensitivity                   : public

[10/1/93 public]

Many of the DFS documentation files should, but do not, have Transarc
Copyright notices.  The following copyright notice must be added to the
files listed below


..."Copyright (C) 1989, 1991, 1992, 1993 Transarc Corporation


app_ref/man3dfs:
--------
BOSSVR.3dfs
BOSSVR_CreateBnode.3dfs
BOSSVR_DeleteBnode.3dfs
BOSSVR_DeleteKey.3dfs
BOSSVR_EnumerateInstance.3dfs
BOSSVR_Exec.3dfs
BOSSVR_GetCellName.3dfs
BOSSVR_GetDates.3dfs
BOSSVR_GetInstanceInfo.3dfs
BOSSVR_GetInstanceParm.3dfs
BOSSVR_GetLog.3dfs
BOSSVR_GetRestartTime.3dfs
BOSSVR_GetStatus.3dfs
BOSSVR_ListKeys.3dfs
BOSSVR_Prune.3dfs
BOSSVR_ReBossvr.3dfs
BOSSVR_Restart.3dfs
BOSSVR_SetNoAuthFlag.3dfs
BOSSVR_SetRestartTime.3dfs
BOSSVR_SetStatus.3dfs
BOSSVR_SetTStatus.3dfs
BOSSVR_ShutdownAll.3dfs
BOSSVR_StartupAll.3dfs
BOSSVR_WaitAll.3dfs
FTSERVER_AggregateInfo.3dfs
FTSERVER_Clone.3dfs
FTSERVER_DeleteVolume.3dfs
FTSERVER_Dump.3dfs
FTSERVER_Forward.3dfs
FTSERVER_GetOneVolStatus.3dfs
FTSERVER_GetStatus.3dfs
FTSERVER_ListAggregates.3dfs
FTSERVER_ListVolumes.3dfs
FTSERVER_Monitor.3dfs
FTSERVER_ReClone.3dfs
FTSERVER_Restore.3dfs
FTSERVER_SetFlags.3dfs
FTSERVER_SetStatus.3dfs
VC.3dfs
VC_BackupVolume.3dfs
VC_CreateVolume.3dfs
VC_DeleteVolume.3dfs
VC_ListVolumes.3dfs
VC_MoveVolume.3dfs
VC_RenameVolume.3dfs
VC_SetQuota.3dfs
VC_SyncServer.3dfs
VC_SyncVldb.3dfs
VC_VolserStatus.3dfs
VC_VolumeStatus.3dfs
VC_VolumeZap.3dfs
VL_ChangeAddress.3dfs
VL_GenerateSites.3dfs
VL_GetNewVolumeId.3dfs
intro.3dfs

app_ref/man5dfs:
--------
dump.5dfs


admin_ref/man4dfs:
--------
BakLog.4dfs
BosConfig.4dfs
BosLog.4dfs
CacheInfo.4dfs
CacheItems.4dfs
FMSLog.4dfs
FilesetItems.4dfs
FlLog.4dfs
FtLog.4dfs
NoAuth.4dfs
RepLog.4dfs
TE.4dfs
TL.4dfs
TapeConfig.4dfs
UpLog.4dfs
Vn.4dfs
admin.bak.4dfs
admin.bos.4dfs
admin.fl.4dfs
admin.ft.4dfs
admin.up.4dfs
dfstab.4dfs
intro.4dfs

admin_ref/man8dfs:
--------
bak.8dfs
bak_adddump.8dfs
bak_addftentry.8dfs
bak_addftfamily.8dfs
bak_addhost.8dfs
bak_apropos.8dfs
bak_deletedump.8dfs
bak_dump.8dfs
bak_dumpinfo.8dfs
bak_ftinfo.8dfs
bak_help.8dfs
bak_labeltape.8dfs
bak_lsdumps.8dfs
bak_lsftfamilies.8dfs
bak_lshosts.8dfs
bak_readlabel.8dfs
bak_restoredb.8dfs
bak_restoredisk.8dfs
bak_restoreft.8dfs
bak_rmdump.8dfs
bak_rmftentry.8dfs
bak_rmftfamily.8dfs
bak_rmhost.8dfs
bak_savedb.8dfs
bak_scantape.8dfs
bak_setexp.8dfs
bak_status.8dfs
bak_verifydb.8dfs
bakserver.8dfs
bos.8dfs
bos_addadmin.8dfs
bos_addkey.8dfs
bos_create.8dfs
bos_delete.8dfs
bos_gckeys.8dfs
bos_genkey.8dfs
bos_getdates.8dfs
bos_getlog.8dfs
bos_getrestart.8dfs
bos_install.8dfs
bos_lskeys.8dfs
bos_prune.8dfs
bos_restart.8dfs
bos_rmadmin.8dfs
bos_rmkey.8dfs
bos_setauth.8dfs
bos_setrestart.8dfs
bos_shutdown.8dfs
bos_start.8dfs
bos_startup.8dfs
bos_status.8dfs
bos_stop.8dfs
bos_uninstall.8dfs
bosserver.8dfs
butc.8dfs
cm.8dfs
cm_checkfilesets.8dfs
cm_flush.8dfs
cm_flushfileset.8dfs
cm_getcachesize.8dfs
cm_getdevok.8dfs
cm_getsetuid.8dfs
cm_lscellinfo.8dfs
cm_lsstores.8dfs
cm_resetstores.8dfs
cm_setcachesize.8dfs
cm_setdevok.8dfs
cm_setsetuid.8dfs
dfsbind.8dfs
dfsd.8dfs
dfsexport.8dfs
flserver.8dfs
fms.8dfs
fts.8dfs
fts_addsite.8dfs
fts_aggrinfo.8dfs
fts_clone.8dfs
fts_clonesys.8dfs
fts_create.8dfs
fts_crfldbentry.8dfs
fts_crmount.8dfs
fts_crserverentry.8dfs
fts_delete.8dfs
fts_delfldbentry.8dfs
fts_delmount.8dfs
fts_delserverentry.8dfs
fts_dump.8dfs
fts_edserverentry.8dfs
fts_lock.8dfs
fts_lsaggr.8dfs
fts_lsfldb.8dfs
fts_lsft.8dfs
fts_lsheader.8dfs
fts_lsmount.8dfs
fts_lsreplicas.8dfs
fts_lsserverentry.8dfs
fts_move.8dfs
fts_release.8dfs
fts_rename.8dfs
fts_restore.8dfs
fts_rmsite.8dfs
fts_setquota.8dfs
fts_setrepinfo.8dfs
fts_statftserver.8dfs
fts_statrepserver.8dfs
fts_syncfldb.8dfs
fts_syncserv.8dfs
fts_unlock.8dfs
fts_unlockfldb.8dfs
fts_update.8dfs
fts_zap.8dfs
ftserver.8dfs
fxd.8dfs
growaggr.8dfs
intro.8dfs
newaggr.8dfs
repserver.8dfs
salvage.8dfs
scout.8dfs
upclient.8dfs
upserver.8dfs

Of the 28 DFS chapters and PostScript figures, the following 9 files (all
of which are figures) do *not* include Transarc copyright notices:

App Dev Guide:
--------------
figures/1_fig_cache_manager.dfs

Admin Guide:
------------
figures/1_figure.ps
figures/1_figure2.ps
figures/3_figure1_acl_inherit.ps
figures/6_figure1_disk_part.ps
figures/6_figure2_fts_types.ps

User's Guide:
-------------
u1_figure2_quotas.ps
u1_figure3_cmd.ps
u3_figure1.ps

[10/1/93 public]

deleted dfsatab.4dfs from the above list.  dfsatab is no longer used within
DFS per Keith Uher and the file had already been defuncted.

[10/8/93]

This is done. Verified by inspection.

[11/11/93 public]

Inspected files and closed this CR.



CR Number                     : 8887
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dfs.lock
Short Description             : dfs.lock doesn't run on HPUX
Reported Date                 : 10/1/93
Found in Baseline             : 1.0.3
Found Date                    : 10/1/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/dfs.lock
Sensitivity                   : public

[10/1/93 public]
Dfs.lock just needs some minor porting changes.

[10/6/93 public]
Modify command invokation to accommodate HPUX.  Commands are rsh
(remsh), ps, awk, and echo.  Only rsh (remsh) requires a test for
HPUX.
Fixed in version 1.1.8.1.

[12/17/93 public]
Closed.



CR Number                     : 8885
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Several minor
improvements/tweeks to the HPUX port of Episode and DFS.
Reported Date                 : 10/1/93
Found in Baseline             : 1.0.3
Found Date                    : 10/1/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : numerous
Sensitivity                   : public

[10/1/93 public]
I've created this OT to cover the minor tweeks/improvements that I want to
make for the HPUX port of Episode and DFS.  There was a generic "Episode
doesn't build for HPUX" OT that I've used in the past, but I can't remember
its number now.

Examples of the kinds of things I have in mind:
   Reporting correct fs-type fields (in vfs, stat/statfs structs) for the CM,
   Episode and AGFS.

   Make minor changes to get mounted Episode filesets and AGFS aggregates
   /etc/mnttab so that the df command will see them.

   There's a potential bug (NEVER seen, however) in the HPUX implementation
   of the ufs_setattr() glue code.

   Remove the "Bad dfs call" messages that keep arising from one of the the
   unimplemented DFS calls.

   Transarc found a bug in the HPUX implementation of the nux_create()
   glue.

[11/15/93 public]
Just thought I'd close this out since HP is "done" with 1.0.3 porting
changes.

[12/17/93 public]
Closed.



CR Number                     : 8884
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : README and README.RWALL not being installed
Reported Date                 : 10/1/93
Found in Baseline             : 1.0.3
Found Date                    : 10/1/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/Makefile
Sensitivity                   : public

[10/1/93 public]

README and README.RWALL not being installed - need to update Makefile.

[10/4/93 public]

Makefile updated to install README, README.RWALL and add_root_as_admin.ksh.

[11/11/93 public]
Closed - makefile submission and subsequent builds have succeeded.



CR Number                     : 8865
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : assertion failure in BufRead
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Found by Jason Gait.  Blake's separation of AIX code from SunOS code in
 strategy.c inadvertently caused a statement that reinitializes firstBp to get
 left out in the code that handles empty blocks in BufRead.  (At least I think
 that's the problem.)
 bwl-Wed, 28 Jul 1993 12:30:25
**Solution Text**
 Delta bwl-s-db3987-assertion-failure-in-bufread.
 bwl-Wed, 25 Aug 1993 16:33:02
 
 Entered as OT 
 
 andi-Tue, 28 Sep 1993 15:58:21

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8864
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CONFIG
Short Description             : agfs code releases recursively help kernel lock
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/05/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 Entered as OT 
 
 andi-Tue, 28 Sep 1993 14:18:07

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8863
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Goofiness in the tracing is hindering ability to interpret results
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 Delta: cfe-db4005-fix-goofy-traces
 Tested in dfs-103 3.24
 cfe-Fri, 30 Jul 1993 14:46:37

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8862
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : CM cannot handle multiple threads simultaneously discovering that a fileset has moved
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 Delta: cfe-db4001-manage-vol-server-db-better
 Tested against dfs-103 3.24.
 Eliminates spurious ENODEV failures due to servers returning persistent
 fileset errors, even though those servers aren't in the cm_volume structure
 any more.
 cfe-Fri, 30 Jul 1993 13:34:32

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8861
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Modify epimount to be able to mount filesets readonly
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 This is important as it will be needed for testing incremental fileset
 dumps and restores.
 
 rajesh-Thu, 29 Jul 1993 12:19:30

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8860
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Quota usage of a DFS/Episode fileset does not drop on file removal
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/29/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 On dfs-103-3.23+ on Solaris, the quota usage of a DFS/Episode fileset
 does not drop on removal of a file created by redirection from cat(1).
 
 When SAFS_RemoveFile calls px_TryRemove, the reference count on the
 vnode representing the file is 1. On return from px_TryRemove, the
 ref count is 2. SAFS_RemoveFile then drops the reference to the vnode
 it obtained at the beginning dropping the ref count to 1.
 Since the ref count never when to zero, efs_inactive was not called
 which would account for the fact that the fileset quota did not drop.
 
 Reading the code of px_TryRemove and zlc_TryRemove indicates that 
 zlc_TryRemove obtains a reference to the vnode, adds it to the ZLC
 list and tries to obtain an open-for-delete token on the file. If
 successful, it removes the vnode from the ZLC list and drops the
 reference it obtained, else it lets the vnode remain on the ZLC list.
 After the call to px_TryRemove, examining the zlc_removeQueue does show
 the vnode to be on the ZLC list.
 Log of operations that reveal this problem:
 
 # pwd
 /.../coven.dce.transarc.com/fs
 # fts lsq .
 Fileset Name          Quota    Used  % Used   Aggregate
 root.dfs              95000     706     0%     1% = 1739/97192 (LFS)
 # cat /etc/passwd > foofile
 # !f
 fts lsq .
 Fileset Name          Quota    Used  % Used   Aggregate
 root.dfs              95000     722     0%     1% = 1755/97192 (LFS)
 # rm foofile
 # ls foofile
 foofile: No such file or directory
 # !f
 fts lsq .
 Fileset Name          Quota    Used  % Used   Aggregate
 root.dfs              95000     722     0%     1% = 1755/97192 (LFS)
 
 				^^^
 rajesh-Thu, 29 Jul 1993 09:13:50
**Solution Text**
 kazar-db3991-fix-glue-close-semantics
 
 Mike Kazar
 Fri Jul 30 13:19:59 1993
 kazar-Fri, 30 Jul 1993 13:20:00

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8859
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : tkm initializes WVT tokens incorrectly
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The fix to TKM to create WVT tokens earlier screwed up their initialization,
 breaking all WVT token conflicts.
 Fix up initialization.
 Mike Kazar
 Wed Jul 28 10:59:42 1993
 kazar-Wed, 28 Jul 1993 10:59:52
**Solution Text**
  kazar-db3984-fix-wvt-token-initialization
 Mike Kazar
 Wed Jul 28 11:58:05 1993
 kazar-Wed, 28 Jul 1993 11:58:08

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8858
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fset test clone01.itl tickles a vnode refcount leak
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 jdp-db3980-do-pageio_done-in-BufRead 
 
 Jeffrey Prem
 Thu Aug  5 12:18:10 1993
 jdp-Thu, 05 Aug 1993 12:18:11

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8857
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Get Episode's tracing dfszXX.cat files installed correctly.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 Delta: cfe-db3976-install-dfszXX.cat-files
 Tested in dfs-103 3.23
 cfe-Tue, 27 Jul 1993 15:52:43

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8856
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_GetDCache loses cm_dcachelock in getdslot
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 cm_GetDownD is dropping the cm_dcachelock, and cm_GetDCache isn't
 written with that assumption.  Need to reverify that the chunk
 still isn't in the cache after getting the dcache entry.
 Mike Kazar
 Mon Jul 26 16:59:40 1993
 kazar-Mon, 26 Jul 1993 16:59:42
**Solution Text**
 kazar-db3967-getdcache-must-reverify-chunk-identity
 
 Mike Kazar
 Fri Jul 30 10:35:08 1993
 kazar-Fri, 30 Jul 1993 10:35:09

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8855
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : tkset_Create needs starvation avoidance code
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 BPG has a test case where several threads are pounding on the same dir
 at the same time, and all are getting TKM_RELOCK back from tkset.
 Need to serialize things after a while.
 Mike Kazar
 Mon Jul 26 16:58:15 1993
 kazar-Mon, 26 Jul 1993 16:58:17

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8854
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : vnm_MapCommonError is broken
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Some recent changes to vnm_MapCommonError() cause it to turn `code' to
 mush if it is one of {EINVAL, EIO, ENXIO, EFAULT}.  The routine sets
 `*codeP' to `newCode', but `newCode' is uninitialized if one of the
 values in the set above was passed in.
 Jeffrey Prem
 Mon Jul 26 14:08:59 1993
 jdp-Mon, 26 Jul 1993 14:09:03
**Solution Text**
 jdp-db3962-fix-vnm_MapCommonError 
 
 Jeffrey Prem
 Mon Jul 26 17:11:44 1993
 jdp-Mon, 26 Jul 1993 17:11:45

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8853
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : zap unused back-ptr from tokenlist
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Clean this up for Carl, since the bug is already partly fixed.
 Mike Kazar
 Mon Jul 26 09:42:42 1993
 kazar-Mon, 26 Jul 1993 09:42:44
**Solution Text**
 kazar-db3958-zap-unused-back-ptr-from-tokenlist
 
 Mike Kazar
 Mon Jul 26 10:41:09 1993
 kazar-Mon, 26 Jul 1993 10:41:18

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8852
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : fix client-side RPC ugliness and BOMB_EXEC bugs
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/26/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 There are two relatively minor problems introduced with the latest
 code base.  First, I stupidly added some TRY/CATCH stuff that Carl points out
 is completely unnecessary.  Secondly, I did some BOMB_EXEC stuff incorrectly,
 doing work without properly holding the global lock.
 Mike Kazar
 Mon Jul 26 08:57:02 1993
 kazar-Mon, 26 Jul 1993 08:57:04
**Solution Text**
 Fixed in
 kazar-db3957-fix-bomb-exec-and-try-problems.
 
 Mike Kazar
 Mon Jul 26 10:38:47 1993
 kazar-Mon, 26 Jul 1993 10:38:50

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8851
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : typo in AIX BufRead
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The AIX version of BufRead has a typo, which causes us to use the
 wrong buf structure later in AwaitIO.
 
 blake-Fri, 23 Jul 1993 15:45:37

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8850
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : File ending with hole can panic Episode
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 If a file ends with a hole (i.e., an unallocated logical block)
 and you try to read past the end of file, Episode may panic in
 BufRead.
 blake-Fri, 23 Jul 1993 11:28:29

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8849
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : buf structs should be pinned on AIX
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Episode dynamically allocates buf structures for I/O; on AIX, these
 structures should be pinned.
 
 blake-Thu, 22 Jul 1993 10:15:57
**Solution Text**
 Actually, I was pinning them all along. 
 blake-Thu, 22 Jul 1993 12:02:05

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8848
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : buffer shutdown must cleanup events
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 It turns out that after doing I/O with an event the caller MUST use
 either WaitEvent or StatEvent to clean up the event.  Principly this
 means clearing the INTRANSIT bit.  The specialized code in elbb_Shutdown
 does not do this.
 [Thu Jul 22 08:19:17 1993]
 ota-Thu, 22 Jul 1993 08:19:20
**Solution Text**
 ota:
 
 Delta: ota-db3939-finish-events-in-shutdown
 
 Also document this requirement in asevent.c.
 [Thu Jul 22 13:26:00 1993]
 ota-Thu, 22 Jul 1993 13:26:03

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8847
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : tkc deadlocks on page invalidation
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 TKC can't call page_lock while holding the vcache lock, since tkc
 also has to grab the vcache lock when returning from xglue_getpage, after
 obtaining a locked page.
 Mike Kazar
 Wed Jul 21 15:44:29 1993
 kazar-Wed, 21 Jul 1993 15:44:31
**Solution Text**
 Fixed by kazar-db3937-tkc-should-lock-pages-sans-vcache-lock-in-revoke.
 
 Mike Kazar
 Mon Jul 26 09:35:31 1993
 kazar-Mon, 26 Jul 1993 09:35:36

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8846
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : asevent synchronization problems
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Ted has pointed out a number of race conditions and misplaced locks
 in the asevent code.  Moreover, the current locking mechanism is
 hard to understand and not well matched to the structure of the
 code.  We are also paying in added complexity for unused features
 that we are unlikely ever to need.
 
 blake-Tue, 20 Jul 1993 22:32:34
**Solution Text**
 Delta: blake-db3929-asevent-locking-problems
 
 ota:
 
 [Thu Jul 29 11:11:27 1993]
 ota-Thu, 29 Jul 1993 11:11:29
 
 Additional work in this general area is addressed by db 3995.
 []
 ota-Thu, 29 Jul 1993 11:19:04

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8845
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : asevent_MakeBuf should have length parameter
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The asevent_MakeBuf interface takes a buffer address, but not its
 length, and as a result, b_bufsize is not getting initialized.
 blake-Tue, 20 Jul 1993 12:21:09

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8844
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : epiinit returns random exit status
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The epiinit program falls off the end of main() without either
 calling exit() or returning a value; its exit status is therefore
 garbage.
 blake-Fri, 16 Jul 1993 18:04:31
**Solution Text**
 Fixed in blake-db3903-fix-epiinit-exit-status.
  
 blake-Wed, 21 Jul 1993 18:54:28

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8843
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : wrap try/catch_all/endtry macros around cm pipe-based calls
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Add exception wrappers to CM pipe-based calls since otherwise an
 exception might be propagated through our code.
 Mike Kazar
 Fri Jul 16 11:19:43 1993
 kazar-Fri, 16 Jul 1993 11:19:46
**Solution Text**
 Well, this one's fixed, but it will also be undone shortly, since it is also
 completely unnecessary, as Carl has pointed out.
 Mike Kazar
 Mon Jul 26 09:25:57 1993
 kazar-Mon, 26 Jul 1993 09:25:59

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8842
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Token management for unexported volumes
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Doing token mgt for unexported volumes is bad, since we can't do mgt
 for CM volumes without causing deadlocks.
 Mike Kazar
 Thu Jul 15 10:35:20 1993
 kazar-Thu, 15 Jul 1993 10:35:23

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8841
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : KERNEL
Short Description             : tpq function GCQueue() leaks memory
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 Added the necessary code to take the entry that was being thrown away
 and put it on the ree list instead.  This code is probably never 
 executed so the change was verified by inspection.
 comer-Thu, 15 Jul 1993 11:09:09

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8840
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak savedb fails when missing tape label
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
    When using a blank tape (i.e. one without a backup label), the savedb
 command fails, as it should, but it fails later than it should.  The tape
 should be labeled NULL or bak_db_dump.1 for the savedb command to work but
 it incorrectly accepts a tape with no label on it.  The pertinent section of
 a butc script is included below to show the problem:
 
    ******* OPERATOR ATTENTION *******
    Device :  /dev/rmt1
    Please insert a writeable tape bak_db_dump.1 for the database dump
    Hit return when done
   
    file_tm: end of tape (dfs / btm) Reading tape label
    Tape accepted - now dumping database      <<<tape should not be accepted!>>>
    file_tm: Invalid argument writing file mark
    file_tm: Invalid argument Failed closing tape file
    Database dump encountered errors
 davecarr-Wed, 14 Jul 1993 11:23:01
 davecarr-Thu, 15 Jul 1993 10:09:06
**Solution Text**
 -------------------------------------------------------------------------------
 
 Both the savedb problem and the assertion failure problem are fixed in
 
 Delta: vijay-db3885-bak-savedb-fails-when-missing-tape-label
 Build: dfs-carl 1.13
 Backed: dce1.0.2ab4
 
 Vijay Anand
 Thu Jul 15 16:19:25 EDT 1993
 vijay-Thu, 15 Jul 1993 16:20:18

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8839
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : On giving a bogus mount point to epiunmount, it does not return error
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Main() does not propogate any errors from epi_unmount().
 
 rajesh-Wed, 14 Jul 1993 10:31:59

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8838
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : adjust certain initial allocs downward
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Adjust incremental allocations for GetDSlot (to 10) and fshs_AllocHost (to 64).
 Mike Kazar
 Tue Jul 13 21:26:26 1993
 kazar-Tue, 13 Jul 1993 21:26:28

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8837
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : fix tkm core leaks
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 TKM has some core leaks, at least partially attributable to a few structures
 that simply don't get freed (a byte range descr and a blocking op desr).
 Free them.
 Mike Kazar
 Tue Jul 13 21:22:51 1993
 kazar-Tue, 13 Jul 1993 21:22:52

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.

[11/20/95 public]

Received diffs from Transarc.

Date: Mon, 20 Nov 1995 09:47:08 -0500 (EST)
To: biyani+@transarc.com, axg@osf.org
Subject: diff for OT8837
Message-Id: <Akg9HgCSMUQlRixU5U@transarc.com>

Checked-out CML/delta/kazar-db3878-fix-tkm-core-leaks, revision 1.1 
Delta: kazar-db3878-fix-tkm-core-leaks
Change: file/tkm/tkm_fidHash.c from 4.97 to 4.99
*** file/tkm/tkm_fidHash.c
--- 4.99	1993/07/14 20:36:43
***************
*** 783,788 ****
--- 783,789 ----
  					      &(sliceSet), placeP) != TKM_ERROR_NOMEM) {
  		conflArray[*realConflCount].flags |= AFS_REVOKE_COL_A_VALID;
  	      }
+ 	      osi_Free((char *)slicedRange.lo, sizeof(tkm_byterange_t));
  	    }
  	    
  	    if (slicedRange.hi != (tkm_byterange_p)NULL) {
***************
*** 791,796 ****
--- 792,798 ----
  					      &(sliceSet), placeP) != TKM_ERROR_NOMEM) {
  		conflArray[*realConflCount].flags |= AFS_REVOKE_COL_B_VALID;
  	      }
+ 	      osi_Free((char *)slicedRange.hi, sizeof(tkm_byterange_t));
  	    }
  
  	    /* now we're back, have things locked and our tokens are still
***************
*** 1603,1609 ****
    int			holdingFidBucketLock = 0;
  int errorCond = 0;
  
- tkm_blockingOp_data_t *	rpcCellP = (tkm_blockingOp_data_t *)osi_Alloc(sizeof(tkm_blockingOp_data_t));
  static char		routineName[] = "tkm_FidHash_TryWaitingTokens";
  
    extern tkm_longTermStats_t *	tkm_ltStatsP;
--- 1605,1610 ----
***************
*** 1616,1628 ****
  
    dmprintf(tkm_debug_flag, TKM_DEBUG_BIT_FIDHASH, ("\n%s entered", routineName));
    
-   if (rpcCellP) {
-     bzero((char *)rpcCellP, sizeof(tkm_blockingOp_data_t));
-   }
-   else {
-     osi_dp("%s (%d): unable to allocate RPC cell\n", routineName, mypid);
-   }
- 
    obtainFidAxisLock = (*fidBucketLockPP == TKM_LOCK_NULL);
    if (obtainFidAxisLock) {
      /* first we have to find out where the lock is, in order to lock it */
--- 1617,1622 ----
***************
*** 1929,1942 ****
  	  }
  #endif /* defined(KERNEL) && defined(TKM_LOCK_DEBUG) */
  
- 	  if (rpcCellP) {
- 	    bzero((char *)rpcCellP, sizeof(tkm_blockingOp_data_t));
- 	    if (! tkm_OutRPC_ReAdd(rpcCellP, tkm_rpc_async_grant, __FILE__, __LINE__,
- 				   (caddr_t)tokenToActivateP, (caddr_t)savedHostP)) {
- 	      osi_dp("%s (%d): unable to add pending RPC cell\n", routineName, mypid);
- 	    }
- 	  }
- 	  
  	  /* die a horrible death if we're trying to grant a dummy token */
  	  osi_assert(savedHostP != (struct hs_host *)NULL);
  
--- 1923,1928 ----
***************
*** 1955,1966 ****
  	  ENDTRY;
  #endif /* defined(TKM_CATCH_EXCEPTIONS) */
  	  
- 	  if (rpcCellP) {
- 	    if (! tkm_BlockingOps_Remove(rpcCellP) ) {
- 	      osi_dp("%s (%d): unable to find pending RPC cell\n", routineName, mypid);
- 	    }
- 	  }
- 	  
  	  /* regrab the lock we dropped */
  	  tkm_Lock_Obtain(fidBucketLockForActivationP, TKM_WRITE_LOCK, __FILE__, __LINE__);
  
--- 1941,1946 ----
***************
*** 2077,2084 ****
      tkm_lock_DropLockAndReleaseWaiters(*fidBucketLockPP, TKM_WRITE_LOCK);
      *fidBucketLockPP = TKM_LOCK_NULL;
    }
- 
-   if (rpcCellP) osi_Free(rpcCellP, sizeof(tkm_blockingOp_data_t));
  
    dmprintf(tkm_debug_flag, TKM_DEBUG_BIT_FIDHASH,
  	   ("\n%s exiting, returning %d", routineName, rtnVal));
--- 2057,2062 ----
Delta: kazar-db3878-fix-tkm-core-leaks
Change: file/tkm/tkm_getToken.c from 4.82 to 4.84
*** file/tkm/tkm_getToken.c
--- 4.84	1993/07/14 20:36:52
***************
*** 991,1013 ****
  	 */
  	tkm_Token_SetupPendingRequest(newTokenP, fidHashCellP);
  
- 	/* first, make sure this is the only lock we are holding */
- 	if (!(sleepCellP = (tkm_blockingOp_data_t *)osi_Alloc(sizeof(tkm_blockingOp_data_t)))) {
- 	  osi_dp("%s (%d): unable to allocate sleep cell\n",
- 		 routineName, mypid);
- 	}
- 	else {
- 	  bzero((char *)sleepCellP, sizeof(tkm_blockingOp_data_t));
- 	  if (!tkm_PendingSleep_ReAdd(sleepCellP, __FILE__, __LINE__,
- 				      (caddr_t) conflInactiveP, (caddr_t) *fidBucketLockPP,
- 				      (caddr_t) newTokenP, (caddr_t) conflInactiveP)) {
- 	    osi_dp("%s (%d): unable to add sleep cell to pending list\n",
- 		   routineName, mypid);
- 	  }
- 	}
- 	
- 	tkm_BlockingOps_PrintList(__FILE__, __LINE__);
- 
  #if defined(KERNEL) && defined(TKM_LOCK_DEBUG)
  	if (tkm_FidHash_CheckLocks(__FILE__, __LINE__,
  				   *fidBucketLockPP, (tkm_lock_data *)NULL) > 1) {
--- 991,996 ----
***************
*** 1025,1037 ****
  	conflInactiveP->sleepers++;
  	tkm_Osi_SleepW((caddr_t)conflInactiveP, *fidBucketLockPP, __FILE__, __LINE__);
  	/* we hold the lock, again, here */
- 
- 	if (sleepCellP) {
- 	  if (!tkm_BlockingOps_Remove(sleepCellP)) {
- 	    osi_dp("%s (%d): unable to remove sleep cell from pending list\n",
- 		   routineName, mypid);
- 	  }
- 	}
        }
        else {
  	/* resolution of these tokens may take longer, so we are not willing to sleep */
--- 1008,1013 ----
Delta: kazar-db3878-fix-tkm-core-leaks
Change: file/tkm/tkm_hostRevokeList.c from 1.19 to 1.20
*** file/tkm/tkm_hostRevokeList.c
--- 1.20	1993/07/14 20:36:56
***************
*** 218,225 ****
    tkm_fidHash_t *		fidHashCellP = (tkm_fidHash_t *)NULL;
    tkm_lock_data *	fidBucketLockP = TKM_LOCK_NULL;
    int				lockSearchIndex;
-   tkm_blockingOp_data_t *	rpcCellP =
-     (tkm_blockingOp_data_t *)osi_Alloc(sizeof(tkm_blockingOp_data_t));
    int				errorCond;
    long				mypid;
  #if defined(AFS_DEBUG)
--- 218,223 ----
***************
*** 237,249 ****
    mypid = TKM_LOCK_NOPID;
  #endif /* KERNEL */
  
-   if (rpcCellP) {
-     bzero((char *)rpcCellP, sizeof(tkm_blockingOp_data_t));
-   }
-   else {
-     osi_dp("%s (%d): unable to allocate RPC cell\n", routineName, mypid);
-   }
-   
  #if defined(AFS_DEBUG)
    if (tkm_debug_flag & TKM_DEBUG_MASK(TKM_DEBUG_BIT_HOST)) {
      osi_dp ("\n%s about to revoke: host: %x, number revocations: %d\n",
--- 235,240 ----
***************
*** 291,304 ****
    }
  #endif /* defined(KERNEL) && defined(TKM_LOCK_DEBUG) */
    
-   if (rpcCellP) {
-     bzero((char *)rpcCellP, sizeof(tkm_blockingOp_data_t));
-     if (! tkm_OutRPC_ReAdd(rpcCellP, tkm_rpc_token_revoke, __FILE__, __LINE__,
- 			   (caddr_t)myNewTokenP, (caddr_t)myHostP)) {
-       osi_dp("%s (%d): unable to add pending RPC cell\n", routineName, mypid);
-     }
-   }
-   
  #ifdef notdef
    if (HS_ISHOSTDOWN(myHostP)) {
      osi_dp("%s: about to make RPC to down host: 0x%x\n",
--- 282,287 ----
***************
*** 320,331 ****
    ENDTRY;
  #endif /* defined(TKM_CATCH_EXCEPTIONS) */
    
-   if (rpcCellP) {
-     if (! tkm_BlockingOps_Remove(rpcCellP) ) {
-       osi_dp("%s (%d): unable to find pending RPC cell\n", routineName, mypid);
-     }
-   }
-   
  #if defined(AFS_DEBUG)
    if (tkm_debug_flag & TKM_DEBUG_MASK(TKM_DEBUG_BIT_HOST)) {
      osi_dp ("\n%s revoke returned: host: %x, code: %d\n",
--- 303,308 ----
***************
*** 429,436 ****
    }
  
    tkm_lock_DropLockAndReleaseWaiters(fidBucketLockP, TKM_WRITE_LOCK);
- 
-   if (rpcCellP) osi_Free(rpcCellP, sizeof(tkm_blockingOp_data_t));
  
    ((tkm_hostRevokeData_t *)argP)->tkmCodeFromRevocation = rtnVal;
  
--- 406,411 ----



CR Number                     : 8836
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : fix tkc core leaks
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 TKC has some core leaks, at least partially attributable to the fact
 that when it gets a new token, it doesn't return the previous tokens
 that are now completely subsumed by the newest token.
 Mike Kazar
 Tue Jul 13 21:21:46 1993
 kazar-Tue, 13 Jul 1993 21:21:48

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.

[11/20/95 public]

Received diffs from Transarc for this CR.

Date: Mon, 20 Nov 1995 09:45:37 -0500 (EST)
To: biyani+@transarc.com, axg@osf.org
Subject: diff for OT8836
Message-Id: <0kg9GFeSMUQl1ihk4r@transarc.com>

Checked-out CML/delta/kazar-db3877-discard-spurious-tokens-on-release, revision 
 1.2 
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc.c from 4.39 to 4.40
*** file/tkc/tkc.c
--- 4.40	1993/07/14 18:39:58
***************
*** 367,372 ****
--- 367,378 ----
      register struct tkc_vcache *vcp;
  {
      /* just a hint, so we don't care if we're missing some locking */
+     if (vcp->tokenAddCounter != vcp->tokenScanValue) {
+ 	/* prune non-lock tokens; they'll be deleted by DoDelToken */
+ 	tkc_PruneDuplicateTokens(vcp);
+     }
+ 
+     /* still a hint */
      if (vcp->vstates & TKC_DELTOKENS) {
  	tkc_DoDelToken(vcp);	/* called with no locks held */
      }
***************
*** 689,694 ****
--- 695,701 ----
      register struct tkc_tokenList *tlp;
      static int tcounter = 1;
      int async;
+     int flags;
      struct tkm_raceLocal localRaceState;
  
      async = block & 2;	/* bit 2 means async */
***************
*** 708,713 ****
--- 715,721 ----
  	 */
  	token.type.high = 0; token.type.low = tokenType;
  	tlp = tkc_AddToken(vcp, &token); /* do early to catch quick revokes */
+ 	/* above sets TKC_LLOCK if this is a requested lock token */
  	tlp->reqID = tcounter;
  	tkm_StartRacingCall(&tkc_race, &vcp->fid, &localRaceState);
  	lock_ReleaseWrite(&vcp->lock);
***************
*** 724,730 ****
  	icl_Trace2(tkc_iclSetp, TKC_TRACE_GETTOKENSTART,
  		   ICL_TYPE_POINTER, (long) vcp,
  		   ICL_TYPE_LONG, token.type.low);
! 	code = TKM_GETTOKEN(vcp, block||async, (&token), tcounter, rblockerp);
  	icl_Trace1(tkc_iclSetp, TKC_TRACE_GETTOKENEND, ICL_TYPE_LONG, code);
  
  	/* return call to pool */
--- 732,745 ----
  	icl_Trace2(tkc_iclSetp, TKC_TRACE_GETTOKENSTART,
  		   ICL_TYPE_POINTER, (long) vcp,
  		   ICL_TYPE_LONG, token.type.low);
! 	flags = TKM_FLAGS_NOEXPIRE;	/* our tokens never expire */
! 	/* if not a lock token, do optimistic grant */
! 	if (!(tlp->states & TKC_LLOCK))
! 	    flags |= TKM_FLAGS_OPTIMISTICGRANT;
! 	/* if we want to wait, use queuerequest flag */
! 	if (block || async) flags |= TKM_FLAGS_QUEUEREQUEST;
! 	code = tkm_GetToken(&vcp->fid, flags, &token, &tkc_Host,
! 			    tcounter,rblockerp);
  	icl_Trace1(tkc_iclSetp, TKC_TRACE_GETTOKENEND, ICL_TYPE_LONG, code);
  
  	/* return call to pool */
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc.h from 4.24 to 4.25
*** file/tkc/tkc.h
--- 4.25	1993/07/14 18:40:00
***************
*** 310,315 ****
--- 310,319 ----
  				 * token revoke.
  				 */
      short locks;		/* boolean non-zero if have any lock tokens */
+     short tokenAddCounter;	/* bumped on each token add */
+     short tokenScanValue;	/* value of tokenAddCounter when last
+ 				 * token duplicate scan done.
+ 				 */
      unsigned char gstates;	/* state bits controlled by rclock */
      unsigned char vstates;	/* state bits controlled by vcache lock */
  } tkc_vcache_t;
***************
*** 348,353 ****
--- 352,358 ----
  #define TKC_LWAITING	0x4	/* someone is waiting for a token grant */
  #define TKC_LSELECTED	0x8	/* selected for returning (stop using) */
  #define TKC_LDELETED	0x10	/* entry should be deleted when refcount 0 */
+ #define TKC_LLOCK	0x20	/* represents a token that is a lock */
  
  struct tkc_stats {
      u_long	spaceAlloced;
***************
*** 377,387 ****
  /*
   * Interface calls to TKM (we just call tkm_GeToken & tkm_ReturnToken)
   */
- #define TKM_GETTOKEN(vcp, f, tok, id, rlockerp)	\
-   tkm_GetToken(&(vcp)->fid, \
- 	       TKM_FLAGS_NOEXPIRE|((f) ? TKM_FLAGS_QUEUEREQUEST:0), \
- 	       (tok), &tkc_Host, id, rlockerp)
- 
  #define	TKM_RETTOKEN(fidP, tok, f)	\
    tkm_ReturnToken((fidP), (tkm_tokenID_t *) &(tok)->tokenID, \
  		  (tkm_tokenSet_t *) &(tok)->type, (f))
--- 382,387 ----
***************
*** 431,436 ****
--- 431,438 ----
  extern int tkc_FlushEntry(struct tkc_vcache *);
  extern void tkc_FlushVnode(struct vnode *);
  extern struct tkc_vcache *tkc_GetVcache(struct vnode *);
+ 
+ extern int tkc_PruneDuplicateTokens(struct tkc_vcache *);
  
  /*
   * Symbols exported by tkc to other kernel modules.
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc_cache.c from 4.19 to 4.21
*** file/tkc/tkc_cache.c
--- 4.21	1993/07/14 18:46:15
***************
*** 209,214 ****
--- 209,215 ----
  	QAdd(&tkc_vclruq, &vcp->lruq);
  	tkc_stats.extras++;
      }
+     vcp->tokenAddCounter = vcp->tokenScanValue = 0;
      return (vcp);
  }
  
***************
*** 410,416 ****
  	 * ever was granted and we still have some token types
  	 * remaining.
  	 */
! 	if ((tlp->states & TKC_LHELD)
  	    && (tlp->token.type.high || tlp->token.type.low)
  	    && !tkm_IsGrantFree(tlp->token.tokenID)) {
  	    TKM_RETTOKEN(&vcp->fid, (&tlp->token), TKM_FLAGS_PENDINGRESPONSE);
--- 411,417 ----
  	 * ever was granted and we still have some token types
  	 * remaining.
  	 */
! 	if ((tlp->states & (TKC_LHELD|TKC_LDELETED) == TKC_LHELD)
  	    && (tlp->token.type.high || tlp->token.type.low)
  	    && !tkm_IsGrantFree(tlp->token.tokenID)) {
  	    TKM_RETTOKEN(&vcp->fid, (&tlp->token), TKM_FLAGS_PENDINGRESPONSE);
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc_hostops.c from 4.18 to 4.22
*** file/tkc/tkc_hostops.c
--- 4.22	1993/07/14 18:46:17
***************
*** 457,463 ****
  		/* Remove revoked token bits */
  		tlp->token.type.low &= ~tokenType->low;
  		vcp->heldTokens = tkc_OrAllTokens(&vcp->tokenList);
! 		if (!tlp->token.type.low)
  		    tkc_DelToken(tlp);
  	    }
  	    lock_ReleaseWrite(&vcp->lock);
--- 457,463 ----
  		/* Remove revoked token bits */
  		tlp->token.type.low &= ~tokenType->low;
  		vcp->heldTokens = tkc_OrAllTokens(&vcp->tokenList);
! 		if (tlp->token.type.low == 0 && tlp->token.type.high == 0)
  		    tkc_DelToken(tlp);
  	    }
  	    lock_ReleaseWrite(&vcp->lock);
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc_locks.c from 4.6 to 4.7
*** file/tkc/tkc_locks.c
--- 4.7	1993/07/14 18:46:18
***************
*** 107,116 ****
      for (qp = vcp->tokenList.next; qp != &vcp->tokenList; qp = tqp) {
  	tqp = QNext(qp);
  	tlp = (tkc_tokenList_t *) qp;
! 	if (tlp->token.type.low & (TKN_LOCK_READ|TKN_LOCK_WRITE))
! 	    newLocks = 1;
  	if ((tlp->procid == (unsigned long) osi_ThreadUnique())
! 	    && (tlp->token.type.low & RANGE_TKN)
  	    && !(tlp->states & TKC_LDELETED)) {
  
  	    lmin = 0; lmax = -1; rtMin = 0; rtMax = -1;
--- 107,115 ----
      for (qp = vcp->tokenList.next; qp != &vcp->tokenList; qp = tqp) {
  	tqp = QNext(qp);
  	tlp = (tkc_tokenList_t *) qp;
! 	if (tlp->states & TKC_LLOCK) newLocks = 1;
  	if ((tlp->procid == (unsigned long) osi_ThreadUnique())
! 	    && (tlp->states & TKC_LLOCK)
  	    && !(tlp->states & TKC_LDELETED)) {
  
  	    lmin = 0; lmax = -1; rtMin = 0; rtMax = -1;
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc_revoke.c from 4.19 to 4.20
*** file/tkc/tkc_revoke.c
--- 4.20	1993/07/14 18:46:19
***************
*** 242,247 ****
--- 242,253 ----
       struct tkc_tokenList *tlp;
  
  {
+     /* if this is an optimistically granted lock token, we can return
+      * it, since we don't care about these; we will also have an explicit
+      * lock token if we have a real lock that we need to cover.
+      */
+     if (!(tlp->states & TKC_LLOCK)) return 0;
+     
      /* 
       * Don't let lock tokens being used be revoked, but do return this 
       * locker's info.
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc_tokens.c from 4.9 to 4.10
*** file/tkc/tkc_tokens.c
--- 4.10	1993/07/14 18:46:20
***************
*** 100,115 ****
      bzero((caddr_t)tlp, sizeof(*tlp));
      bcopy((caddr_t)tokenp, (caddr_t)&tlp->token, sizeof(struct tkm_token));
      QAdd(&vcp->tokenList, &tlp->vchain);
      /* set tokens as held if we already have them, not before */
      tlp->backp = vcp;
      tlp->procid = (unsigned long) osi_ThreadUnique();	/* For file locking */
      tkc_stats.tentries++;
      tlp->states = TKC_LQUEUED;
!     if (tokenp->type.low & (TKN_LOCK_READ|TKN_LOCK_WRITE))
  	vcp->locks = 1;	/* mark that we've got active lock tokens */
      return tlp;
  }
  
  
  /*
   * Finish granting of a token when it comes in; vcp must be write locked
--- 100,195 ----
      bzero((caddr_t)tlp, sizeof(*tlp));
      bcopy((caddr_t)tokenp, (caddr_t)&tlp->token, sizeof(struct tkm_token));
      QAdd(&vcp->tokenList, &tlp->vchain);
+     vcp->tokenAddCounter++;	/* so we know that there are more tokens
+ 				 * in the list.
+ 				 */
      /* set tokens as held if we already have them, not before */
      tlp->backp = vcp;
      tlp->procid = (unsigned long) osi_ThreadUnique();	/* For file locking */
      tkc_stats.tentries++;
      tlp->states = TKC_LQUEUED;
!     /* record if this is a requested lock token.  Do it based on
!      * *requested* type, in case we support optimistic grants someday.
!      */
!     if (tokenp->type.low & (TKN_LOCK_READ|TKN_LOCK_WRITE)) {
  	vcp->locks = 1;	/* mark that we've got active lock tokens */
+ 	tlp->states |= TKC_LLOCK;
+     }
      return tlp;
  }
  
+ /* look through the list of tokens hanging off of a vcache, and
+  * remove tokens that are covered by other tokens.
+  * Don't do this for lock tokens (they represent actually held
+  * locks, as well as tokens, so returning them spuriously will
+  * confuse the sys 5 lock code).
+  *
+  * Set the tokenScanValue to tokenAddCounter when done; this means
+  * that as of the "time" when this vcache had an add counter
+  * of a particular value, we know that there were no duplicate tokens.
+  * This saves us the trouble of calling this function unless new tokens
+  * were added.
+  *
+  * No locks should be held when calling this function.  The vcache
+  * entry should be held.
+  */
+ tkc_PruneDuplicateTokens(vcp)
+   register struct tkc_vcache *vcp;
+ {
+     register struct tkc_tokenList *tlp;
+     long held;	/* tokens already seen */
+     int didAny;	/* did any token returns */
+     struct afsToken ttoken;	/* temp for returning token */
+ 
+     /* look at all tokens, starting with most recently added tokens,
+      * and looking back in time, and if we have a token that is covered
+      * by the preceding tokens, copy out enough info to return it,
+      * and then return it w/o holding any locks.
+      * Don't do this processing for lock tokens, since we have a 1 to 1
+      * mapping between actually held locks and lock tokens.
+      */
+     lock_ObtainWrite(&vcp->lock);
+     do {
+ 	held = 0;	/* none seen yet */
+ 	didAny = 0;	/* none done yet */
+ 	for(tlp = (struct tkc_tokenList *) QNext(&vcp->tokenList);
+ 	    tlp !=(struct tkc_tokenList *) &vcp->tokenList;
+ 	    tlp = (struct tkc_tokenList *) QNext(&tlp->vchain)) {
+ 
+ 	    /* if this token already deleted, or if it is a lock
+ 	     * token, or it hasn't been granted yet, then skip it.
+ 	     */
+ 	    if (tlp->states & (TKC_LDELETED|TKC_LLOCK)
+ 		|| !(tlp->states & TKC_LHELD)) continue;
+ 
+ 	    /* check to see if we can return this token */
+ 	    if ((~held & tlp->token.type.low) == 0) {
+ 		/* no bits in token are missing from "held" bits */
+ 		/* save token before unlock */
+ 		bcopy((caddr_t)&tlp->token, (caddr_t)&ttoken, sizeof (ttoken));
+ 		tkc_DelToken(tlp);	/* not done until vcache released */
+ 		lock_ReleaseWrite(&vcp->lock);
+ 		TKM_RETTOKEN(&vcp->fid, &ttoken, TKM_FLAGS_PENDINGRESPONSE);
+ 		lock_ObtainWrite(&vcp->lock);
+ 		didAny = 1;
+ 		/* can't proceed, since tlp may have been destroyed
+ 		 * while vcache was unlocked.  So, retry from the top
+ 		 */
+ 		break;
+ 	    }
+ 
+ 	    /* otherwise, we're keeping this token, so add it to our
+ 	     * set of known token types.
+ 	     */
+ 	    held |= tlp->token.type.low;
+ 	}
+     } while (didAny);
+     /* we made it through without blocking to return any tokens, so
+      * it is safe to set the tokenScanValue field.
+      */
+     vcp->tokenScanValue = vcp->tokenAddCounter;
+     lock_ReleaseWrite(&vcp->lock);
+ }
  
  /*
   * Finish granting of a token when it comes in; vcp must be write locked
Delta: kazar-db3877-discard-spurious-tokens-on-release
Change: file/tkc/tkc_cache.c from 4.21 to 4.22
*** file/tkc/tkc_cache.c
--- 4.22	1993/07/15 20:24:13
***************
*** 411,417 ****
  	 * ever was granted and we still have some token types
  	 * remaining.
  	 */
! 	if ((tlp->states & (TKC_LHELD|TKC_LDELETED) == TKC_LHELD)
  	    && (tlp->token.type.high || tlp->token.type.low)
  	    && !tkm_IsGrantFree(tlp->token.tokenID)) {
  	    TKM_RETTOKEN(&vcp->fid, (&tlp->token), TKM_FLAGS_PENDINGRESPONSE);
--- 411,417 ----
  	 * ever was granted and we still have some token types
  	 * remaining.
  	 */
! 	if (((tlp->states & (TKC_LHELD|TKC_LDELETED)) == TKC_LHELD)
  	    && (tlp->token.type.high || tlp->token.type.low)
  	    && !tkm_IsGrantFree(tlp->token.tokenID)) {
  	    TKM_RETTOKEN(&vcp->fid, (&tlp->token), TKM_FLAGS_PENDINGRESPONSE);



CR Number                     : 8835
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Clean up icl_DumpUserToFile and icl_Dump
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8834
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : Dont log ELOOP returns as failures
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The ftutil library logs all volop calls that return a non-zero code.
 Unfortunately, this includes all of the ELOOP return codes from the
 clone ops.  We should not be logging these as failures.
 
 Jeffrey Prem
 Tue Jul 13 16:01:25 1993
 jdp-Tue, 13 Jul 1993 16:01:29
**Solution Text**
 jdp-db3872-dont-log-ELOOP 
 
 Jeffrey Prem
 Wed Jul 21 15:23:33 1993
 jdp-Wed, 21 Jul 1993 15:23:36

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8833
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : ftu_LookUpAggrByName calls free on an invalid pointer causing SEGV
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 If you run efts lsft multiple times on Solaris multiple times, a
 random invocation core dumps with SEGV from inside a malloc call. On
 showing the core dump stack to Blake, Blake mentioned its very likely
 to be a memory corruption problem where someone frees much more that 
 it had allocated and the problem showing up in a later malloc.
 
 So convert efts (including ftutil) to use osi equivalents of malloc
 and free as they have memory tracing.
 [Tue Jul 13 12:55:23 1993]
 
 -------------------------------------------------------------------------------
 rajesh:
 
 rajesh-Tue, 13 Jul 1993 12:55:24
 
 The bug is in ftu_LookupAggrByName which has the following code:
 
  for (i = 0, aggrP = aggrs; i < numAggrs; i++, aggrs++)
 	if (strncmp(aggrName, aggrP->name, sizeof aggrP->name) == 0) {
 	    if (aggrIdP)
 		*aggrIdP = aggrP->Id;
 	    code = 0;
 	    break;
 	}
 
     if (aggrs)
 	free(aggrs);
 
 Notice it increments aggrs in update portion of for loop loop each time
 and then calls free on it. This also causes the function not to do its job
 as it always uses the first entry in the list in each iteration of the loop.
 The fix is to increment aggrP instead.
 [Wed Jul 14 09:24:46 1993]
 
 -------------------------------------------------------------------------------
 rajesh:
 
 rajesh-Wed, 14 Jul 1993 09:24:49
 
 **** The corresponding delta name is rajesh-db3867-fix-ftu_LookupAggrByName
 **** and NOT rajesh-db3867-use-osi-versions-of-malloc-and-free which belongs
 **** to db3868 and was misnamed.
 [Wed Jul 14 10:02:44 1993]
 rajesh-Wed, 14 Jul 1993 10:02:46

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8832
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Enable logging in vols_DumpVolume
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Uncomment the exception logging at the end of vols_DumpVolume().
 
 Jeffrey Prem
 Tue Jul 13 17:07:16 1993
 jdp-Tue, 13 Jul 1993 17:07:19
**Solution Text**
 jdp-db3865-log-exceptions-while-dumping 
 
 Jeffrey Prem
 Tue Jul 13 17:07:19 1993
 jdp-Tue, 13 Jul 1993 17:07:21

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8831
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : Restore preemption around call to rpc_server_register_auth_info
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 In px_invokeserver(), preemption should be restored before (and turned
 off after) the call to rpc_server_register_auth_info().
 
 Jeffrey Prem
 Mon Jul 12 17:53:08 1993
 jdp-Mon, 12 Jul 1993 17:55:18
**Solution Text**
 jdp-db3863-fx-restore-preemption-before-rpc-call 
 
 Jeffrey Prem
 Thu Aug  5 12:16:30 1993
 jdp-Thu, 05 Aug 1993 12:16:35

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8830
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : XCODE_RECURSIVE check missing from vol_StartVnodeOp
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Probably this code was accidentally removed as part of handling
 a merge conflict.
 Mike Kazar
 Mon Jul 12 09:47:07 1993
 kazar-Mon, 12 Jul 1993 09:47:13

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8829
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm call reservation scheme completely unworkable
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The call reservation scheme in use by the CM is completely unmaintainable,
 as well as broken right now.  Apparently it doesn't hurt at the loads we're
 running at now, but things should be able to deadlock.
 
 Change to an advisory mechanism where if things are reserved incorrectly, we
 simply allocate a few extra packet buffers, as discussed with Dimitris.
 Mike Kazar
 Fri Jul  9 19:48:48 1993
 kazar-Fri, 09 Jul 1993 19:48:51
**Solution Text**
 Fixed by  
 kazar-db3857-use-advisory-call-reservations
 
 Mike Kazar
 Mon Jul 19 11:49:58 1993
 kazar-Mon, 19 Jul 1993 11:50:07

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8828
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : bullet-proof cm against symlinks with nulls.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Sometimes the CM finds a symlink having a null in it, and this confuses the
 code that figures out how much to free using strlen.  Bullet-proof the code
 by either ensuring that we turn the nulls into something else when we
 read them, or by tracking the length to free directly.
 Mike Kazar
 Fri Jul  9 18:30:13 1993
 kazar-Fri, 09 Jul 1993 18:30:16
**Solution Text**
 Fixed by  
 kazar-db3854-bullet-proof-cm-against-bad-symlinks.
 
 Mike Kazar
 Mon Jul 19 11:55:08 1993
 kazar-Mon, 19 Jul 1993 11:55:10

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8827
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : FXD
Short Description             : fxd has bogus errno (EBADF) inheritted by child process.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 JULY 7, 1993
 
 Function mainproc() close all filedescriptors without any error check.
 After forking a child process the error (EBADF) was passed the child
 process and causes the bogus message:
 
 fxd: failed to create a multi-thread pool for FX server
 
 when starting the fxd.
 
 --Jessjess-Sun, 11 Jul 1993 11:42:34
 
 While the close is where the error comes from, the problem is that 
 the return code from afs_syscall is not being looked at.
 comer-Mon, 12 Jul 1993 09:18:47

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8826
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : bakserver core dumps on being asked to verify database
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 Bakserver core dumps when the bakdbverify command is given

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8825
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : fix tkc processgrant vcache refcount leak
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 TKC's ProcessGrant function iterates through the list of all vcache
 entries in a bogus manner, sometimes leaving vcache entries bumped.
 In addition, the code to determine whether to return
 HS_ERR_JUSTKIDDING looks bogus, and could decide spuriously to return
 this code, losing track of a token granted during an async grant call.
 
 Mike Kazar
 Tue Jul  6 14:37:14 1993
**Solution Text**
 kazar-db3833-fix-tkc-processgrant-vcache-refcount-leak fixes it.
 
 Mike Kazar
 Mon Jul 19 13:09:48 1993
 kazar-Mon, 19 Jul 1993 13:09:49

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8824
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : tkm fails to wakeup everyone it is supposed to in pending mode
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The TKM fails to wakeup threads sleeping on tokens that are in RPC mode
 when the RPC completes.  This is definitely true for the async grant case,
 and lots of other cases look suspect, too.
 
 Mike Kazar
 Tue Jul  6 12:04:01 1993
**Solution Text**
 kazar-db3782-fix-tkc-release-panic fixes it.
 
 Mike Kazar
 Mon Jul 19 13:16:38 1993
 kazar-Mon, 19 Jul 1993 13:16:39

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8823
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_ReserveBlocks can truncate file being fetched
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 cm_ReserveBlocks can truncate a file while its being fetched.  This is
 true even though the dcache is originally stabilized, since we unlock the
 dcp several times before resorting to truncation.
 
 The fix is to stabilize the dcache entry while we're in cm_ReserveBlocks, before
 we truncate the dcp.
 
 Mike Kazar
 Tue Jul  6 10:22:06 1993
**Solution Text**
 Fixed by  
 kazar-db3829-guard-reserveblocks-truncate-against-fetching-flag
 
 Mike Kazar
 Mon Jul 19 13:06:22 1993
 kazar-Mon, 19 Jul 1993 13:06:25

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8822
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : avoid duplicate truncates at the server
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 kazar-db3825-avoid-extra-truncates fixes it.
 
 Mike Kazar
 Mon Jul 19 13:10:32 1993
 kazar-Mon, 19 Jul 1993 13:10:34

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8821
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : fshost can scramble error code when > 1 call made for revoking
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The file server can lose an earlier HS_ERR_PARTIAL return code if more
 than one RPC is made to revoke a set of tokens, and the first retursn
 HS_ERR_PARTIAL and the 2nd returns 0.
 
 Mike Kazar
 Sun Jul  4 14:29:40 1993
**Solution Text**
 kazar-db3824-fix-spurious-token-success-branch-in-fshost fixes it.
 
 Mike Kazar
 Mon Jul 19 13:18:08 1993
 kazar-Mon, 19 Jul 1993 13:18:10

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8820
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : starting a butc with a tcid when one is already running is BAD
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 07/01/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 Starting a second butc on the same tcid when one is already running and 
 possibly doing some useful work is disruptive to the previously started butc.
 The right behavior would be for the second invocation to fail when there is
 a butc running on the same tcid.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed in
 Delta:  vijay-db3815-butc-notice-existing-coordinator 1.1
 Build:  dfs-carl 1.13
 Backed: dce1.0.2ab4
 
 Vijay Anand
 Fri Jul 09 13:48:04 1993
 vijay-Fri, 09 Jul 1993 13:53:48

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8819
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Bullet-proof vol_efsScan against INCON filesets
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 Inter-related CRs: 3810
 
 It'll be a cold day in hell before we get time to deal with DB3810.  So
 in the meanwhile to avoid panics when attempting to dump a damaged
 fileset, we bullet-proof the epif_GetStatus call in vol_efsScan.  This
 volop is called before every attempt to dump or restore a file.  If
 epif_GetStaus fails, we assume some ACL problem exists and we return an
 error.  This should abort the ftserver operation which should
 effectively prohibit dumping, restoring (and as a consequence moving)
 filesets that are INCONSISTENT due to aborted destroy/clone/reclone
 operations.
 [Wed Jun 30 15:03:12 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3811-bullet-proof-vol_efsScan
 
 This fix isn't easy to test, but since it is basically just removing an
 MBZ it should be pretty safe.
 [Fri Jul  2 11:20:15 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8818
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Clear EOF return code in ftu_AggrEnumerateFsets
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 In ftu_AggrEnumerateFsets(), clear code if it is equal to VOL_ERR_EOF.
 
 Jeffrey Prem
 Wed Jun 30 12:29:37 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8817
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : KERNEL
Short Description             : zlc can lose track of number of files in list
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 2 places where items can be removed from the ZLC list: by the manager
 and by a call to zlc_CleanVolume().  Only in the former case is the
 size of the list decremented.  The change will be to handle the vnodes
 on the list for the CleanVolume case but to defer the actual removal
 to the manager.
**Solution Text**
 Made a general clean-up of the ZLC code.  Fixed:
 
 	-  A potential panic point in zlc_CleanVolume()
 	-  Cleanup of CLean_Volume() entry deletion
 	-  Complete checks of vp != 0 before VN_RELE(vp)
 	-  Cleanup of unnecessary unlock/lock pairs.
 
 Verified on dfs-103-3.19 build on a rios by hand-testing:
 
 	1) check quota (Q1), create big file, check quota (Q2), open file,  
 	delete file, check quota (same as Q2), close file, check quota
 	(same as Q1).
 
 	2) do the same except move the fieset before closing the file.
 	This still doesn't return the storage but it does remove the
 	entry from the ZLC list and releases the vnode.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8816
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : fts runtest needs to export EPI_AGGRNAME_3.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 JUNE 29, 93
 
 The aggrname 3  needs to be exported and intiated by a non-null value
 when just one server found.
 
 --Jess

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8815
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : fts release fails when no readonly exists in the RW site
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 fts test19 fails in test E10 because fts release fails with error "no such
 fileset" when the first release is done. In this case no RO exists in the
 RW site, and DeleteVolume routine in fts fails, even though it shouldn't.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed in
 
 Delta: vijay-db3796-fts-release-fails-with-no-such-fileset
 Build: dfs-103 3.19
 
 Vijay Anand
 Fri Jul 16 16:05:39 EDT 1993
 vijay-Fri, 16 Jul 1993 16:22:58

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8814
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : ACL test id selection mechanism needs generalization
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/28/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 rajesh-Thu, 22 Jul 1993 13:54:28
**Solution Text**
 rajesh-Thu, 22 Jul 1993 13:54:31
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8813
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : fts syncfldb dumps core when looking at empty aggregates
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Dimitris Varotsis
 Fri Jun 25 10:24:01 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8812
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : epiunmount does not return errno
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 Epiunmount does a statfs on the filesystem to be unmounted. If statfs
 fails, instead of returning errno it returns the statfs return value (-1), 
 incorrectly.
 [Fri Jun 25 09:51:50 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8811
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : tkc_Release assert failure on ref count
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 TKC can grab a vcache entry out of the LRU queue, when it is already
 being recycled.  Use vnode hash table to do tkc_FindByFid instead,
 since recycled vcache entries are removed from this table when we don't
 want them to be found.
 Mike Kazar
 Thu Jun 24 17:28:06 1993
**Solution Text**
 kazar-db3782-fix-tkc-release-panic fixes it.
 
 Mike Kazar
 Mon Jul 19 13:16:03 1993
 kazar-Mon, 19 Jul 1993 13:16:04

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8810
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : SECURITY
Short Description             : Make a pass through security code closing core leaks
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/24/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8809
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : cm test5 expects 777 on /:
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 cm test5 tries to touch the file /:/temp.  This assumes the person 
 running the test has write permission on that directory.  This is not
 always the case.  Either we need to change the test, or update the README
 file to indicate this is a requirement.
**Solution Text**
 Changed the README file to indicate that the /: directory needs write
 permission for the person running the tests, since test5 creates a file
 there.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8808
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UPDATE
Short Description             : clean up RPC i/f in update w.r.t. exceptions
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 Bullet-proffed upserver/upclient with respect to exceptions from the
 RPC.  The model is to use TRY/FINALLY/ENDTRY constructs in the
 functions that call the pipe functions on the server.  If an exception
 is raised, the lock is released and the exception is propogated to the
 next context.  Eventually, this will reach the TRY in the server stub
 itself.  The exception will be propogated as a fault to the client,
 where it will be turned into an status code (since fault_status is set
 in the ACF file).
 
 To verify the change, I ran the uptest successfully.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8807
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : The bakserver_test program core dumps on lists.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The GetDumps function of the bakserver_test program is freeing memory
 that is not yet copied.
**Solution Text**
 Fixed the itl_bakCharList.c file to correctly treat charListT types as
 static structures.  Currently the stings they hold are limited to 1024
 characters.
 Fixed the itlBak_CreateList function to correctly allocate space for 
 the list it is copying.
 Changed the macros in itl_bakListType.c to use unique names for local
 variables.  Made the same change to the ITLU_ASSIGN_VARIABLE macro 
 in itl_utils.h.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8806
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TKC
Short Description             : tkc vnode refs cause FS to run out of vnodes
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 If the tkc cache size (256) is larger than the local file system's vnode
 cache size (Episode default is 128), the local file system can run out of
 vnodes just because of vnode references from the tkc.  To see this,
 configure a DFS system with the default vnode cache size for Episode, and
 create a directory in Episode with more than 128 entries.  Then locally
 mount the directory and, via the local path, do 'ls -l' or 'grep foo *'.
 It will be unable to open some of the files.
**Solution Text**
 Delta bwl-o-db3754-vnode-table-hash-size.
 
 In the long run, the best solution is for the tkc hash table to be organized
 by fid's, rather than by vnode pointers, so that it does not have to hold
 vnode pointers.  The short-term fix is to simply raise the default size of
 the vnode table.  We have also put the definition of that default in one
 place (efs_opcode.h).
 bwl-Thu, 15 Jul 1993 09:59:22

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8805
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Allow EXDEV when opening adjacent filesets during delete
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 A failed move can leave "move-temp" filesets with llfwdId's that point
 to a RW fileset on another aggregate.  This causes fts_zap to fail
 with EXDEV when trying to delete the temp filesets.  We already allow
 ENODEV errors when trying to open an adjacent fileset in
 preparation for a delete.  We should also allow EXDEV.
 
 Jeffrey Prem
 Tue Jun 22 10:41:02 1993
 
 Jeffrey Prem
 Tue Jun 22 10:44:02 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8804
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : dfstrace dumps core when setting specific set states
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 I ran through the commands that take set names and log names.  They
 now work.  The ones thats were failing before were show set, clear set, 
 clear log, and set set.  The problem exists on the rios but apparently
 doesn't cause a problem.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8803
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : A clone of a backing fileset should fail
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/22/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 In dfs-103-3.18, an efts clone of a backing fileset succeeds. The ftutil layer
 should prevent clones of backing fileset in user space and the episode
 vnode layer should have some checks to guard against this.
 [Tue Jun 22 09:17:48 1993]
 
 -------------------------------------------------------------------------------
 rajesh:
 
 The episode vnode layer (actually meant volops layer) should not make this
 check i.e. This check should be restricted to user space ftutil layer only 
 as per discussion with Ted and Jeff.
 [Tue Jun 22 10:52:05 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8802
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Assertion failure in efsSomeClone
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/21/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 In an error path, a anode is not closed, and later fails an assertion that
 the anode be closed.
 [Mon Jun 21 14:59:07 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8801
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : The resetstores pioctl has no return statement.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/21/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The cm_PResetStores functon does not have explicit return statement.  It
 should be returning 0.  On Solaris this causes this command to return
 an error (-1) with errno set to -28, which appears to be garbage.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8800
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : reduce CM stack space usage
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 We're getting stack overflows in the CM due to having some large
 locals on the stack.  This is causing watchdog resets on Solaris, and
 some undebuggable panics on the worthless OSF/1.
 Mike Kazar
 Sat Jun 19 13:17:05 1993
**Solution Text**
 kazar-db3740-reduce-stack-space-usage fixes some things in this area.
 Mike Kazar
 Mon Jul 19 13:12:53 1993
 kazar-Mon, 19 Jul 1993 13:12:57

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8799
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Fix small possibility of getting incorrect info from vol_efsGetStatus
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/18/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8798
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Incorect usage of EQUAL_DEV in vol_efsGetStatus
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/18/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 The if statement in the loop below in vol_efsGetStatus
 
 	for (aemp = &start_efsMount; aemp < &end_efsMount; ++aemp) {
 	    if (aemp->volHandle) {
 		/* Check if volume is already mounted */
 		if (EQUAL_DEV (aemp->m_devvp, dev))
 		    continue;
 		if (hsame(aemp->volId, volp->v_volId)) {
 		    AssertInvariant (aemp->volHandle == volh);
 		    break;
 		}
 	    }
 	}
 
 needs to be negated as follows :
 
 	if ( ! EQUAL_DEV (aemp->m_devvp, dev))
 	    continue;
 [Fri Jun 18 08:43:29 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8797
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Improve error tracing in episode vnops layer
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/17/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 The function vnm_MapCommonError should be a macro instead of a
 function so that the so that the FILE and LINE data at which the
 32 bit error is generated is not lost.
 [Thu Jun 17 19:30:19 1993]
 
 -------------------------------------------------------------------------------
 cfe:
 
 Just a comment--FILE and LINE information could be passed in as arguments
 to the function.  It's a pretty sizeable function, as I recall, to feel good
 about freely sprinkling through the code if it were instead a macro.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8796
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : loop in vnm_FindVnode
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/17/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Mike Comer observed saratoga looping in vnm_FindVnode (11 June 1993), and
 he and Ted determined that the problem was due to a hash chain being messed up.
 
 I have found a way for hash chains to be messed up, as follows:
   - A vnode is recycled.  The function that does this, PartialAlloc, removes
     the vnode from its hash chain (by calling unvhash, which calls QRemove),
     and removes the vnode from the LRU list.
   - The caller of PartialAlloc is vnm_FindVnode.  Upon return from
     PartialAlloc, vnm_FindVnode redoes the hash search for the vnode it is
     trying to create, and discovers that, while PartialAlloc was sleeping,
     some other process created the vnode.  In other words, it lost a race.
     So it discards the vnode created by PartialAlloc.  It does so by calling
     AbortAlloc.
   - AbortAlloc puts the vnode back into the LRU list, but it does NOT put the
     vnode back in the hash table.
   - Some time later, PartialAlloc is called again, and again recycles the
     vnode.  It calls unvhash, which calls QRemove.  But since the vnode is
     not in the hash table, this is inappropriate.  In fact, a glance at the
     implementation of QRemove will show that this is not only inappropriate,
     but will mess up at least one hash chain.  That is, QRemove cannot be
     called twice on the same hash queue entry (it is not idempotent).
 
 Of course, it will not be easy to reproduce this bug, nor to test whether it
 has been fixed, nor even to confirm that it is the cause of the loop that
 Mike found.  But this analysis seems plausible.
**Solution Text**
 Delta bwl-db3730-loop-in-vnm-findvnode.
 bwl-Thu, 15 Jul 1993 09:51:31

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8795
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : allow chunks to get swapped out with clean pages extant
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/17/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The CM currently requires no pages in VM when it recycles a chunk.
 This hurts when running with large VM and small caches.  We really
 only need to pin chunks when we have dirty pages, and conveniently
 enough, we can on Solaris  even track which pages are dirty/writable.
 So, allow chunks to float away if we only have clean pages hanging
 off w/o clearing those pages out of VM.
 
 Mike Kazar
 Thu Jun 17 11:24:32 1993
**Solution Text**
 kazar-db3723-disengage-ro-vm-pages-from-dcaches fixes it.
 
 Mike Kazar
 Mon Jul 19 13:12:15 1993
 kazar-Mon, 19 Jul 1993 13:12:17

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8794
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Restrict explicit change in realm ownership of file/dir to DFS admin
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 Bob found that he could change the realm of an ACL directly without
 being DFS admin. Since change of owner of file is limited to DFS admin,
 direct change of realm should also be, as it is effectively a change
 in ownership.
 
 Need to try this with acl_edit. In any case, Episode should enforce this.
 [Wed Jun 16 17:40:20 1993]
 -------------------------------------------------------------------------------
 cfe:
 
 This seems like a sizeable security hole.  Is there a simple fix we can
 ship to IBM ASAP?

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8793
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Improve error handling in vol_efsBulkSetStatus()
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 At the top of vol_efsBulkSetStatus() there is a loop to stash away the
 current status structures of the filesets that we're about to change.
 If an error occurs during this phase, we should bail out immediately
 instead of falling through to the more complex error-handling code
 below.
 
 Jeffrey Prem
 Wed Jun 16 14:13:30 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8792
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : The CM panicked during a fts move
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/16/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The cm panicked during a cascade failure mode while a fileset is moved
 from a source server to a target server.  
 
 When moving a fileset, the CM encountered an rpc timeout and believed the
 server is down. When the CM detects that the server comes back and performs
 its TSR work. It then notices the fileset is actually moved to a new 
 location during the time that the CM is isolated. The CM has to TSR-move
 first as part of the TSR-crash procedure. 
 
 The CM then panicked during the TSR-move. In addition, sometimes, the CM 
 uses an un-itialized cred structure.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8791
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Use PID, not thread, in struct vol_calldesc
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The volume descriptor code contained in xvolume/vol_desc.c wants to
 record the process ID of the process who is opening a volume.  If a
 different process attempts to use the call descriptor, it is supposed
 to fail with EBADF.  Unfortunately, we are using osi_ThreadUnique() to
 fill in cdp->proc.  This doesn't cause problems under AIX, but under
 Solaris, where we have real kernel threads, it isn't quite what we
 want.  For example, one thread in the ftserver does the open, but
 other threads will most likely respond to other RPCs.  This is most
 evident when attempting an fts_move on Solaris.
 
 Jeffrey Prem
 Tue Jun 15 18:27:29 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8790
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : The bakserver test program fails to read a charListT.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 When you try to define a fs_bakCharList type it produces an error message
 about an invalid data type.
**Solution Text**
 Fix the read function to send the correct structure to the New function.
 Also, change the read function to prompt for the data first, then the length.
 If you enter 0 for the length the program will use the length of the data
 to set the length field.
 Change the GetField function to handle accessing fs_bakCharList entries
 by byte or as a whole string.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8789
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : VN_RELE() should be OSI_VN_RELE() in vol_efsCreate()
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 In vol_efsCreate() where we construct a new directory, there is a call
 to VN_RELE().  This should be OSI_VN_RELE(), which on Solaris turns
 wraps the call to VN_RELE() with a osi_RestorePreemption(0),
 (void)osi_PreemptionOff() pair.  This causes a panic when doing
 a filset restore in efs_inactive() when we try to turn off preemption
 again.
 Jeffrey Prem
 Tue Jun 15 09:46:53 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8788
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FXD
Short Description             : make fxd run under self credentials
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/14/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8787
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : bosserver may run procs after shutdown
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 An RPC may be waiting for the bosserver mutex while the bosserver is
 restarting.  In the process of restarting, the bosserver will free
 then destroy the mutex.  The RPC may run with the destroyed mutex.
 This change makes the bosserver continue to hold the mutex until it
 execs or exits, blocking the RPC from being serviced.
 
 This fix was tested by running through the restart code and making
 sure that there are no problems in restarting.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8786
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Add ICL tracing to strategy path
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/11/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 Add ICL tracing to find out why strategy is sometimes taking a long
 time, for instance, during a vnm_StopUse.
 [Tue Jun 15 14:53:09 1993]
 
 ota:
 
 I plan to use this delta to do some cleanup work affecting all three
 Episode ICL deltas:
     [this one] ota-db3687-add-icl-tracing-for-strategy 1.1 (not exported)
     ota-db3652-add-icl-tracing-to-logbuf 1.2
     cfe-db3554-more-ftserver-yield 1.4 
 
 Also I depend on:
     comer-db3642-icl-add-flags-to-log-and-set-creation
 [Mon Jun 21 16:36:50 1993]
 
 ota:
 
 Also delta cfe-db3554-sync-before-stopuse affects tracing so make sure
 it is includes too.
 [Tue Jun 22 10:37:42 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3687-add-icl-tracing-for-strategy
 [Tue Jun 15 14:53:11 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8785
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Implanting bomb points in DFS kernel code
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/11/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8784
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Cleanup committed transactions more aggressively
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 Analysis of ICL tracing of the WaitForTran paths indicates that we are
 waiting longer than necessary and not calling GCTran to reclaim
 committed transactions aggressively enough.
 
 This code is still prone to serious thundering herd problems, but fixing
 that will be more involved.
 [Thu Jun 10 08:37:45 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3661-gc-trans-more-often
 [Thu Jun 10 08:37:51 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8783
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Begin adding ICL tracing to logbuf
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 Inter-related CRs: 3554
 
 Add ICL tracing to the logbuf layer following Craig's approach to adding
 ICL tracing to the vnops layer in delta cfe-db3554-more-ftserver-yield.
 
 Temporarily tracing is enabled only in the kernel.
 [Tue Jun  8 11:31:49 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3652-add-icl-tracing-to-logbuf
 [Tue Jun  8 11:31:53 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8782
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : BOS
Short Description             : Tagged corefile with incorrect name.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 A shell script running via bos cron, had a failure that resulted in
 a core file.  (This shell script just runs the fts clonesys command and
 checks the result.)  However the core file was named bakserver.core
 instead of clone_users.core.
 
 The entry for the bakserver appears immediately after the entry for 
 clone users.  A status output follows.
 
 Instance clone_users, (type is cron) currently running normally.
     Auxiliary status is: run next at Tue Jun  8 03:00:00 1993.
     Process last started at Mon Jun  7 03:01:35 1993 (3 proc starts)
     Last exit at Mon Jun  7 03:23:30 1993
     Last error exit at Mon Jun  7 03:23:30 1993, by exiting with code 1
     Parameter 1 is '/bin/sh /opt/etc/scripts/clone_users.sh'
     Parameter 2 is '03:00'
 
 Instance bakserver, (type is simple) temporarily enabled, has core file, currently running normally.
     Process last started at Mon Jun  7 10:33:29 1993 (3 proc starts)
     Last exit at Mon Jun  7 08:11:32 1993
     Parameter 1 is '/opt/dcelocal/bin/bakserver'
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8781
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : translate_et should handle standard errno values
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 Try translating the standard errno values if the call to
 dce_error_inq_text fails and the error value is less than 256.
 [Mon Jun  7 09:34:13 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3649-translate_et-print-errnos-too
 [Mon Jun  7 09:34:16 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8780
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : StatusNodesInit routine generates rpc runtime assertions
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 The StatusNodesInit routine does not serialize access to data structures
 holding binding handles and may interfere with calls to rpc_binding_copy
 and other such routines by not following the locking strategy. This should
 be fixed asap.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed in 
 delta: vijay-db3647-bak-bad-status-nodes-initialization
 build: dfs-carl 1.11
 backed: dce1.0.2ab2
 
 Vijay Anand
 Sun Jun 06 15:24:35 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8779
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : sets and logs in ICL sometimes require flags at creation time
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/04/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8778
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak should issue message when using local cache.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/04/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 When the bak command fails to contact the bakserver for commands regarding
 text information, which it caches, it should print a message to let the
 user know that the information being displayed is from the local cache, and
 that currently bak cannot contact the bakserver.  This would affect 
 commands like lsftfamilies and lsdump.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed in 
 delta: vijay-db3641-bak-inform-when-using-local-cache-info 1.1
 build: dfs-carl 1.10
 backed: dce1.0.2a-0427
 
 Vijay Anand
 Fri Jun 04 17:46:36 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8777
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Return aggrId in vol_efsGetStatus
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Some merge somewhere went awry and a line of code in
 vol_efsGetStatus() got merged out.  This line caused the aggregate ID
 to be returned as part of the volume's dynamic status.  The change was
 originaly made in revision 4.296 of efs_volops.c which was part of
 delta jdp-db3112-ftserver-ftutil-integration, r1.4.
 
 Jeffrey Prem
 Thu Jun  3 17:41:13 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8776
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Need to push VV into backing fset on reclone
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 At the end of a successful reclone loop, vol_efsSomeClone() should be
 pushing the volume version number from the front fileset into the
 backing fileset before turning off the inconsistent bit.  This was
 previously done in user-space via a call to VOL_SETSTATUS.  This isn't
 quite good enough, since the inconsistent bit has already been turned
 off.
 
 Although its not as important as the VV, the unique field should also be
 pushed back as a result of a reclone.
 
 Jeffrey Prem
 Thu Jun  3 16:24:47 1993
 
 This may be part of the cause of OT 8038, but Ted's already filled in
 a delta for that.
 Jeffrey Prem
 Thu Jun  3 16:50:36 1993
**Solution Text**
 jdp-db3637-push-vv-on-reclone
 
 Jeffrey Prem
 Thu Jun  3 19:00:08 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8775
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Elements of buffer reservation waiters FIFO are stack variables
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/02/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 If the buffer reservation function Reserve cannot reserve the requested number
 of buffers, it adds an entry into the buffer reservation waiters FIFO and
 puts the caller to sleep on the address of this entry. But the entry
 that is used is an automatic local variable ON THE STACK and NOT ALLOCATED
 ON THE HEAP.
 
 Further, Ted pointed out that CheckReservation, that wakes up processes
 waiting for buffers, is also setting the "need" field in the FIFO element
 to zero. But the FIFO element is a variable on the stack of the sleeping 
 process and hence CheckReservation invoked by another process
 ends up modifying some arbitrary location on its own stack. The "need" field
 in the variable on the stack of the sleeper never gets set to zero and
 hence the process waiting for buffer reservation will go right back to sleep.
 effectively - looping forever.
 [Wed Jun  2 16:43:45 1993]
 
 -------------------------------------------------------------------------------
 rajesh:
 
 The buffer reservation needs revisiting and some reorganisation. DB 3638
 has been created to address that issue.
 [Thu Jun  3 17:26:34 1993]
**Solution Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 Change Reserve to allocate the FIFO element, so that its on the heap
 and not on the stack. Also, to localize changes, set the "need" field
 to zero and remove element from FIFO in Reserve itself and not
 CheckReservation.
 [Wed Jun  2 16:43:51 1993]
 
 rajesh-db3629-fix-buffer-reservation
 rajesh-Mon, 16 Aug 1993 15:42:02

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8774
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : bad call to rpc_ns_binding_lookup_done
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/02/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 In one of the error paths in ubik_get_bindings routine, a call to 
 rpc_ns_binding_lookup_done was made when it actually should have
 been rpc_ns_binding_import_done. This was causing a core dump when
 a member of the ubik server group in CDS does not have a binding
 associated with it. 
**Solution Text**
 -------------------------------------------------------------------------------
 
 Just replaced the call to rpc_ns_binding_lookup_done with rpc_ns_binding_import_done.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8773
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : small improvements in consistency checks for fts restore
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 When restoring a fileset, if the fileset already exists in the FLDB and in the
 server and is on-line, and if the command line parameters to fts restore
 specify a different aggregate or server, we should warn the user about an 
 existing on-line fileset with the same name and id, and fail the restore 
 process. This would avoid the case where the restore process tries to create
 a fileset on a different aggregate on the same server and fails with EXDEV,
 or succeeds in creating a fileset on a different server, but leaves the 
 original fileset lying around. In both cases, a suitable warning should be
 issued and the operation should fail.
 
 Such a change affects the backup system also.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed in
 
 Delta: vijay-db3796-fts-release-fails-with-no-such-fileset
 Build: dfs-103 3.19
 
 Vijay Anand
 Fri Jul 16 16:05:39 EDT 1993
 vijay-Fri, 16 Jul 1993 16:20:24
 -------------------------------------------------------------------------------
 
 The above entry is wrong.
 
 Fixed in
 Delta: vijay-db3618-fts-improved-fts-restore-consistency-checks
 Build: dfs-carl 1.13
 Backed: dce1.0.2ab4
 
 Vijay Anand
 Fri Jul 16 16:36:20 EDT 1993
 vijay-Fri, 16 Jul 1993 16:36:57

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8772
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : epidump loops on zero dir entry length
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/28/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 ota:
 
 Delta: ota-db3613-handle-zero-length-dirent
 
 Use a minimum of 1 for directory entry lengths when calculating the
 offset of the next entry.
 [Fri May 28 09:04:44 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8771
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : dirRevokes needs to be incremented by token rev. race code
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/18/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 kazar-db3610-bump-dirrevokes-when-token-race-occurs fixes the problem.
 
 Mike Kazar
 Mon Jul 19 12:12:30 1993
 kazar-Mon, 19 Jul 1993 12:12:32

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8770
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : revoke procedures should never return 0, should return HS_ERR_PARTIAL
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/18/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Solution Text**
 kazar-db3595-fix-hs-err-partial-returns fixes the problem.
 
 Mike Kazar
 Mon Jul 19 12:09:21 1993
 kazar-Mon, 19 Jul 1993 12:09:24

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8769
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : syncfldb and not syncserv should correct the FLDB for non-existing filesets
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The code that modifies the FLDB entry for a fileset when the fileset
 is not found in a site indicated in the FLDB belongs in syncfldb, and
 not syncserv.
 
 Dimitris Varotsis
 Thu May 20 14:02:00 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8768
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : fts syncfldb should not be deleting stray backup volumes
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/18/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Syncfldb should only be affecting the data in the FLDB and not be
 deleting backup volumes that thinks that they shouldn't be where they
 are. If this functionality belongs anywhere its in syncserv.
 Dimitris Varotsis
 Tue May 18 15:59:08 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8767
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : text update necessary when adding and deleting entries
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/18/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 When adding a text entry such as a fileset family, if the text is out of date,
 the addition fails. Instead, it should try to update the text, and if the
 text version is still not in sync with the database, the addition should fail.
 Such updates are necessary at all places where text addition or deletion 
 takes place, along with the lookups where they are already being done.
**Solution Text**
 ld.so: warning: /usr/5lib/libc.so.2.7 has older revision than expected 8
 -------------------------------------------------------------------------------
 
 Just call bc_Update{Volumeset,DumpSchedule,Host}Text if there exists a version
 mismatch when creating or deleting text.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8766
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : have bosserver maintain dfs-server key
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/17/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8765
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : one instance of bad locking in bakserver
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 The bak deletedump command could lead to deadlock in bakserver because the 
 RPC acquires a shared lock on the memory db and then proceeds to obtain a 
 write lock. This is actually inconsistent with the locking scheme in the
 bakserver, and is different from all other cases in the bakserver where 
 proper locking is observed. Just correct this inconsistency.
 
 This case is in procs.c and ends up in the following stack trace
 
 **************** Thread 20 ********************
 
 #0  0xd03e8b88 in cma__dispatch ()
 #1  0xd03e86a0 in cma__block ()
 #2  0xd03c45d4 in pthread_cond_wait ()
 #3  0x1004fa64 in afslk_ObtainWrite ()
 #4  0x1008686c in CheckInit ()
 #5  0x1004718c in InitRPC ()
 #6  0x10048d94 in deleteDump ()
 #7  0x10049f24 in BUDB_DeleteDump ()
 #8  0x1002dab8 in op2_ssr ()
 #9  0xd054049c in rpc__dg_execute_call ()
 #10 0xd04f6a00 in cthread_call_executor ()
 #11 0xd04049a8 in cma__thread_base ()
 #12 0xd03edfec in cma__restore_thread_ctx ()
 #13 0x20068f2c in ?? () from (unknown load module)
 #14 0x140004 in scale60 ()
 Stack appears to loop back on itself.  Punting...
**Solution Text**
 ld.so: warning: /usr/5lib/libc.so.2.7 has older revision than expected 8
 -------------------------------------------------------------------------------
 
 The problem in the BUDB_DeleteDump RPC was, two transactions were being
 initiated, one nested within the other. The first one was a read trans, 
 followed by a write trans to delete the dump entry. The fix is to just open
 a write trans, and then acquire the lock before deleting the dump entry.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8764
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Episode gets error from epit_Free in epiv_CreateAnode
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Inter-related CRs: DB3841
 
 ota:
 
 IBM is seeing panics under high stress in epiv_CreateAnode called from:
     epif_Create
     vnva_FileCreate
 ...
 [Fri May 14 13:56:17 1993]
 ...
 More info in ~ota/junk/bad-vt-db3561.bug.
 [Mon May 17 11:43:37 1993]
 ...
 [Mon May 17 13:49:29 1993]
 
 ota:
 
 Epidump output shows that the correct anodes and their copies in the
 wrong place differ only in their atimes.  The ones in the correct
 locations are newer that the misplaced ones.  Further every other anode
 in the block containing the misplaced anodes is newer.
 
 This suggests that at some point the contents of block 5816 was written
 to block 5832 instead.  Because of caching, the error could have gone
 undetected until the block was discarded from the cache and subsequently
 reread.  If this cache reload happened on the call to epit_Allocate it
 would explain the panic.
 
 What is not explained is how/why a block was written to the wrong place.
 [Mon May 17 14:18:29 1993]
 
 ota:
 
 A far more plausible suggestion is that the contents of 5832 somehow did
 not get initialized and just happen to contain an old copy of 5816.  A
 more detailed scenario goes like this:
  1 Create a new file
  2 Volume table needs to be extended so call NewPage
  3 Call epia_Write to exend VT container (this does not make any
    modifications but just allocates a disk block and relabels a buffer
    to point to this block.
  4 Call epic_FormatAnodeBlock to build the block header and zero all the
    anode flag words.
  5 Bump number of pages and nFree.
  6 Return to epit_Allocate which searches for a free anode.
 
 Note that steps 3, 4, and 6 all separately call the buffer cache to gets
 the buffer holding the anode block.  So there is plenty of opportunity
 to things to go wrong at the lower levels.  The VT (and the volume) is
 held locked thoughout, however, so anode layer, contention is presumably
 ruled out.  Perhaps this is why multiple aggregates are necessary.
 
 Still I don't exactly have a solid theory yet.
 [Wed May 19 14:34:31 1993]
 
 ota:
 
 Here are the crash summaries from the ~ota/junk/bad-vt-db3561.bug file
 mentioned above.
 
 < Crash#1 -- 0514 Summary >
 Create-style crash
 Salvage index errors
 No crash dump
 Before:
     2d70000  2bf27f4f b7afc1db 00000005 b4102f6c
     2d70010  00000000 00000000 000005c0 00000001
     ...
     2D710C8(47648968.) index: 5d1(1489.), unique: f518(5352.)
 
     2d90000  2bf27f4f b7afc1db 00000005 b4002f6c
     2d90010  00000000 00000000 00000840 00000030
     2D910C8(47780040.) index: 5d1(1489.), unique: f518(5352.)
 After: <same>
 Incore: unknown
 
 < Crash#2 -- data.0524 Summary >
 Open-style panic
 fid is 0x539,,0x2b96; epia-style index is 0xa5a(2650.).
 Disk block of 0x85a is 0x16c4
 Disk block of 0xa5a is 0x16d4
 Before:
     2d88000  2c00dfa0 b7afc1db 00000005 b4102f6c
     2d88010  00000000 00000000 00000840 00000003
 	     ...
     2d899a4  b4102f6c 00000000 00000000 0000085a
 
     2da8000  2c00e1ec b7afc1db 00000005 b4102f6c
     2da8010  00000000 00000000 00000a40 00000001
 	     ...
     2da99a4  b4102f6c 00000000 00000000 00000a5a
 After: <same>
 Incore:
     fileC->disk: block is 0x16d4; index is 0x85a
 
 < Crash#3 -- data.0525 Summary >
 Open-style panic
 Fid is 0x6e3,,0x2517; index is 0xdc4(3524.)
 Disk block of 0xac4 is 0x16bf
 Disk block of 0xdc4 is 0x16cf
 Before:
     2d7e000  2c022f45 b7afc1db 00000005 b4102f6c
     2d7e010  00000000 00000000 00000ac0 00000003
 
     2d9e000  2c022f45 b7afc1db 00000005 b4102f6c
     2d9e010  00000000 00000000 00000ac0 00000003
 	     ...
     2d9e3fc  b4102f6c 00000000 00000000 00000ac4
 After:
     2d7e000  2c022f45 b7afc1db 00000005 b4102f6c
     2d7e010  00000000 00000000 00000ac0 00000003
 
     2d9e000  2c022f45 b7afc1db 00000005 b4112f6c
     2d9e010  00000000 00000000 00000dc0 00000004
 	     ...
     2d9e3fc  b4102f6c 00000000 00000000 00000dc4
 Incore:
     fileC->disk: index is 0xac4; unique is 0x17ad
 < Crash#4 -- data.0526 Summary >
 Create-style panic
 Buf is for block 0x16b1 has no free anodes
 No salvage errors
 Disk block of 0x59c is 0x16a1
 Disk block of 0x85c is 0x16b1
 Before:
     2d42000  2c026e8a b7afc1db 00000005 b4102f6c
     2d42010  00000000 00000000 00000580 00000003
 	     ...
     2d43e90  00000000 00000000 00000000 0000059f
 	     ...
     2d62000  2c026e8a b7afc1db 00000005 b4102f6c
     2d62010  00000000 00000000 00000580 00000003
 	     ...
     2d63e90  00000000 00000000 00000000 0000059f
 After:
     2d42000  2c026e8a b7afc1db 00000005 b4102f6c
     2d42010  00000000 00000000 00000580 00000003
 	     ...
     2d43e90  00000000 00000000 00000000 0000059f
 	     ...
     2d62000  2c026e8a b7afc1db 00000005 b4002f6c
     2d62010  00000000 00000000 00000840 00000028
 	     ...
     2d63e90  b4102f6c 00000000 00000000 0000085f
 Incore: buffer cache for blk 0x16b1
     05c36000: 2c026e8a b7afc1db 00000005 b4102f6c
     05c36010: 00000000 00000000 00000580 00000003
 
     05c37e90: b4102f6c 00000000 00000000 0000085f
 < Crash#5 -- data.0527 >
 Create-style panic?
 No index related salvage error before recovery
 No salvage errors after recovery
 No crash dump.
 
 < Crash#6 -- data.0527a NO summary >
 
 < Crash#7 -- data.0527b Summary >
 Open-style panic (different lv, same disk)
 Fid is 0x544,0x17af epia-index=0xA85(2693.)
 Salvage reports many errors including index-related problems in the
   expected block.
 Disk block of 0x7c5(1989.) is 0xeb5
 Disk block of 0xa85(2693.) is 0xec5
 Before:
     1d6a000  2c03e141 b7afc1db 00000005 b4112f6c
     1d6a010  00000000 00000000 000007c0 00000004
 	     ...
     1d6a4f8  b4102f6c 00000000 00000000 000007c5
 	     ...
     1d8a000  2c03e141 b7afc1db 00000005 b4112f6c
     1d8a010  00000000 00000000 000007c0 00000004
 	     ...
     1d8a4f8  b4102f6c 00000000 00000000 000007c5
 After:
     1d6a000  2c03e141 b7afc1db 00000005 b4112f6c
     1d6a010  00000000 00000000 000007c0 00000004
 	     ...
     1d6a4f8  b4102f6c 00000000 00000000 000007c5
 	     ...
     1d8a000  2c03e141 b7afc1db 00000005 b4112f6c
     1d8a010  00000000 00000000 000007c0 00000004
 	     ...
     1d8a4f8  b4102f6c 00000000 00000000 000007c5
 Incore:
     fileC->disk index: 0x7c5
     buffer for blocks 0xec5 and 0xeb5 not in cache
 
 CONCLUSIONS:
 
 After swithing controller and disk hardware the problem disappeared.
 This is currently being attributed to an undiagnosed hardware fault.
 [Tue Jun  1 09:56:43 1993]
 
 ota:
 
 [Thu Jul  8 13:29:57 1993]
**Solution Text**
 Delta: ota-db3561-panic-in-epiv_CreateAnode r1.3
 
 [Fri May 14 14:13:32 1993]
 [Thu May 20 11:03:10 1993]
 [Fri May 28 16:37:43 1993]
 
 ota:
 
 On its technical merits this defect is REJECTED.
 
 This delta is being exported, however, because it contains useful
 assertions and new test code.  The assertions, particularly, should make
 future detection of disk problems faster and easier.
 [Tue Jun  1 09:56:46 1993]
 
 ota:
 
 One of these new assertions caused a salvager test to fail.
 
 The delta ota-db3841-fix-disk-error-assertion makes this assertion
 KERNEL only.
 [Thu Jul  8 13:30:01 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8763
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : icl picks the wrong file name to dump to
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 If the server with an ICL log is not exporting an RPC interface, the
 dump code intuits the name from the first log it finds.  This is a bad
 choice since it is likely that an application will set up it's log
 before the libraries it calls do the same.  This means that it should
 be using the last log in the list since the later logs are pushed on
 to the front.  Use that one, instead.
 Michael Comer
 Wed May 12 13:20:42 1993
**Solution Text**
 Use the last name in the list (the first created) instead of 
 the first.
 
 Michael Comer
 Wed May 12 15:09:48 1993
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8762
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Provide bomb point capability for the kernel
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Continue the bomb point implementation, providing support for the
 kernel in this round.
 
 Jeffrey Prem
 Mon May 10 09:49:56 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8761
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : bosserver may mistakenly rename core file
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 If the bosserver starts up with a core file in the adm directory, it
 may later think that this core file is from one of its managed
 processes and proceed to name it incorrectly.  Chances are this is a
 core file from bosserver, itself.
 Michael Comer
 Mon May 10 09:54:53 1993
**Solution Text**
 Rename core to core.bosserver on startup.
 Michael Comer
 Mon May 10 09:55:05 1993
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8760
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : make fts lsfldb print out owning group properly
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 fts lsserver prints the proper server entry owner but fts lsfldb
 doesn't.  Make it do so.
 Michael Comer
 Sun May  9 17:32:41 1993
**Solution Text**
 There was already code in place to do the translation from uuid to
 owning group.  This code was being used by lsserver but not by lsfldb.
 Changed it.
 Michael Comer
 Mon May 10 09:15:39 1993
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8759
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : error when cache device is full is misleading
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The error returned by the cache manager when the disk that the cache is
 stored on is full, but the cache maximum has not been reached yet, is
 'no space left on device' without any other messages sent to the user
 or the console. This is misleading in particular if the user was
 performing a write operation at the time since it appears as if there
 was no space in the remote disk. A more appropriate behavior would be
 to post a message in the console indicating that the device used for
 caching is full.
 
 Dimitris Varotsis
 Fri May  7 10:41:14 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8758
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : fts create reports the wrong error message for non exported aggregates
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/07/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The error message for fts create when given a non exported aggregate
 should be similar if not identical to the one returned when the
 aggregate specified is non existent. Currently the error returned is
 'no such fileset (dfs / fts)'
 Dimitris Varotsis
 Fri May  7 10:32:06 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8757
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Episode spuriously prints lots of console messages
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Interest List CC: tu
 
 While moveing a fileset between two servers, the episode file system
 spuriously printed out the following messages on the console. 
 
 -tu
 
 Portion of the messages is given below:
 
 (file/episode/anode/block_alloc.c rev. 4.110 #2700) under-reserved allocated blocks
 (file/episode/anode/volume.c rev. 4.79 #493) volume reservation 0/0 is too small 8/0
 (file/episode/anode/strategy.c rev. 4.67 #1325) under-reserved allocated blocks
 
 . . .
 
 []
 
 ota:
 
 These are the msgs I plan to remove:
 (file/episode/anode/anode.c rev. 4.100 #1201) under-reserved visible blocks
 (file/episode/anode/anode.c rev. 4.100 #1207) under-reserved allocated blocks
 (file/episode/anode/block_alloc.c rev. 4.110 #2700) under-reserved allocated blocks
 (file/episode/anode/block_alloc.c rev. 4.110 #3058) under-reserved allocated frags
 (file/episode/anode/block_alloc.c rev. 4.110 #3223) Too few blocks/frags X/Y reserved to unreserve X/Y
 (file/episode/anode/strategy.c rev. 4.67 #1318) under-reserved visible blocks
 (file/episode/anode/strategy.c rev. 4.67 #1325) under-reserved allocated blocks
 (file/episode/anode/strategy.c rev. 4.67 #1390) under-reserved visible blocks
 (file/episode/anode/strategy.c rev. 4.67 #1394) under-reserved allocated blocks
 (file/episode/anode/volume.c rev. 4.88 #500) volume reservation X/Y is too small X/Y
 (file/episode/anode/volume.c rev. 4.88 #577) volume reservation X/Y is too small X/Y
 [Wed May 12 16:06:07 1993]
 
 ota:
 
 I am also adding traces (ALWAYS enabled) which will be trigged we are
 returning a volume or aggregate full error when space is supposed to
 have been reserved.  This should detect only the cases where the above
 warning would cause trouble.  The form this trouble would take is dirty
 pages being left in VM; Episode would be unable write them out until the
 full condition was alleviated.
 [Thu May 13 12:29:24 1993]
 
 ota:
 
 Also fixed these traces which I didn't notice earlier but showed up when
 running the SDET benchmark:
 (file/episode/anode/anode.c rev. 4.100 #1172) under-reserved allocated frags
 (file/episode/anode/anode.c rev. 4.100 #1176) under-reserved visible frags
 [Wed May 19 08:02:43 1993]
**Solution Text**
 Delta: ota-db3526-punt-obnoxious-reservation-msgs r1.2
 
 [Wed May 12 16:06:14 1993]
 
 ota:
 
 [Wed May 19 08:02:50 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8756
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : syncfldb prints incomplete fileset information
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 When syncfldb creates fldb entries, the entry information it prints may be
 incomplete as shown below
 ...
 -- Processing entry 1 of total 1 --
 Could not fetch FLDB entry (fs=0,,25, type=0)
 Error: FLDB: no such entry (dfs / vls)
 ------- Creating a new FLDB entry for  ------- 
 Created the FLDB entry for fileset 
         readWrite   ID 0,,25  valid
 number of sites: 0    number of addresses: 1
    server           flags     aggr   siteAge principal      owner             
 dfswitch.austin.ibm RW       /jfs3   0:00:00                <nil>             
 ...
 
 In particular, the principal and owner information may be missing. The reason
 for this is, when fts creates the fldb entry, the principal and owner info
 is filled in by the flserver. fts should get the updated fldb entry before
 printing it. If fts fails to get this info, it should then fall back to 
 printing the entry that is just created.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8755
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : efts needs to set fileset types and flags for testing
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 In order to reproduce 'abnormal' situations caused by interrupted
 operations it would be nice if efts could set some of the flags of
 a fileset.
 Dimitris Varotsis
 Wed May  5 15:48:44 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8754
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvaging very deep directories does not work
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 05/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ota:
 
 An aggregate with a pathname 224 entries deep causes the salvager to
 crash apparently in lwpdummy.  Probably this is just due to stack
 overflow corruption.
 [Tue May  4 16:38:26 1993]
**Solution Text**
 ota:
 
 Delta:  ota-db3513-do-not-hold-dirbs-on-recurse
 
 The old tree walk held the dir buffers during the recusion step.  This
 means that a deep tree would exhaust the dir buffer space and crash.
 
 Keep the entry about each dir on the stack (fid) or heap (name) and
 release the dirb before recursing.
 
 Also, reduce dir buffers to 16 (from 100 == 819.2Kbytes!).
 [Fri May  7 12:34:26 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8753
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak dumpinfo starts list at 2, not 1.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 After doing a full dump of the user filesets in the self host cell, I got
 the following information from the dumpinfo command.
 
 bak> dumpinfo -id 736175468
 Dump: id 736175468, level 0, filesets 183, created: Fri Apr 30 09:11:08 1993
 
 Tape: name all_users.full.1
 nFilesets 183, created 04/30/93 09:12
 
  Pos     Clone time    Nbytes Fileset
    2 04/30/93 07:54      1424 user.khale.backup
    3 04/30/93 07:46     67208 user.cell_admin.backup
    4 04/30/93 08:04       484 user.srinivas.backup
    5 03/08/93 09:34 117610251 user.mason.dfs.obj.backup
 What seems strange is that the first fileset is at postion 2, not 1.  Perhpas
 there is some type of header that is at position 1, but that means we
 are exposing our tape layout to the user.  It would seem preferable to
 start the position number at 1 (or 0 for the C hard-liners).  So for this
 report the code would need to adjust the internal value prior to printing.
 The same goes for any commands that require this position number.  They
 will need to translate the user value, e.g. 1, to the internal value, e.g.
 2.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Position 1 is occupied by the tape label, but, as Fred mentions, there is no
 reason to make this visible. This might lead to confusion about the missing
 element in position 1. The fix that I implemented is to adjust the positions
 to begin at 1 only for printing purposes. No change has been made internally
 and the first fragment is still at position 2. There were three places where
 positions are printed and all these had to adjusted suitably.
 
 1. dumpinfo
 2. dumpinfo -verbose
 3. restore{ft,disk} -noaction
 
 This fix is in
 
 Delta:  vijay-db3495-bak-fset-pos-begins-at-1
 Build:  dfs-carl 1.13
 Backed: dce1.0.2ab4
 
 Vijay Anand
 Fri Jul 09 14:08:31 1993
 vijay-Fri, 09 Jul 1993 14:16:55

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8752
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : efts create does not return error for malformed fileset ids
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 -------------------------------------------------------------------------------
 
 Currently efts_CreateFt ignores the return value of ftu_ParseFsetId. It should
 not. It calls down into the kernel to create a fileset with a malformed 
 fileset id, finally getting a EPI_E_BADARGS in epiv_Create  which gets mapped
 to EIO.
 [Thu Apr 29 12:25:26 1993]
**Solution Text**
 -------------------------------------------------------------------------------
 rajesh:
 
 Modify efts_CreateFt to return any error from ftu_ParseFsetId.
 [Thu Apr 29 12:37:24 1993]
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8751
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm setcache responses wrong information when cache size is large enough.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 ARP. 28, 93
 
 "cm setc command return wrong message when the cache size is greater
 than 2^31.
 
 --Jess
**Solution Text**
 Initialize the max cache size by right value.
 
 --Jess

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8750
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : stat should report reserved+used, not just used, blocks
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Statfs has been fixed to report reserved blocks as if they were used (rather
 than free), but stat has not been fixed.  So if you write a large file under
 AIX or Solaris Episode and do ls -s's on it, you won't see the allocation grow
 until the file is sync'ed or fsync'ed.
 
 While fixing this, we should also fix stat to get the visible, not the
 allocated, blocks, so that after doing a clone, you don't get "Total 0" from
 ls -l on the RW fileset.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8749
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : SECURITY
Short Description             : add nominal tracing to the security packages
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8748
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : the ICL trace tables should have dummy first entry
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 The first entry in the .et file should be a dummy entry. This is not so for
 ubik and flserver. Fix this.
**Solution Text**
 -------------------------------------------------------------------------------
 
 Fixed. For solution see above.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8747
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ITL
Short Description             : The common hyper type needs to export a comparison function.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The ITL common hyper type needs to epxort a comparison function for 
 other ITL types to invoke.  This is also true of the other common data
 types.  They are in the common directory because they are intended to be
 used by other ITL data types, so they should support interfaces that make
 it eaasy for those data types to perform operations on these common types.
**Solution Text**
 I only fixed the hyper data type.  As the functions for the other types
 are needed we can open defects and fix them.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8746
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : add nominal ICL tracing to bosserver
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Take the dmprintf() statements and generate equivalent ICL trace
 lines.  Then, add the supporting icl calls.  This will give us a
 nominal amount of tracing in the code.
 Michael Comer
 Wed Apr 21 16:21:44 1993
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8745
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : File exporter does not appear to garbage collect principals.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The 'conn' field of the scout program displays the number of current 
 users connected to a file exporter.  These field should increase and
 decrease as time goes by.  However, it always appears to increaase and
 never decreases.  It would appear that principals are never garbage collected
 by the file exporter.  They are supposed to be garbage collected after
 2 hours of inactivity.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8744
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOS
Short Description             : Does not require time parameter for cron job entry.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 I entered a bos create command for a cron job.  I entered all arguments
 but forgot the parameter indicating when the job was to run.  No error
 message was printed.  I then did a bos status and got the following.
 
 Instance bos_status, (type is cron) currently running normally.
     Auxiliary status is: run next at Sat Apr 17 00:00:00 1993.
     Parameter 1 is '/bin/sh /opt/etc/scripts/bos_status.sh'
 
 Notice the time is 12:00 midnight. 
 
 The man page I have (which is 1.0.1) does not say anything about this
 default value.
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8743
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : reset bindings on receiving rpc_s_wrong_boot_time
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 The binding handles used by ubik could get stale if the server is rebooted.
 They may return one of a number of errors. So far, we handle 
 rpc_s_comm_failure. Another one to handle is rpc_s_wrong_boot_time. Just call
 rpc_binding_reset on all bindings to that server upon receiving such errors.
**Solution Text**
 -------------------------------------------------------------------------------
 
 A general solution to reset binding handles upon any rpc error has been done
 to handle all such cases.
 
 Delta:  vijay-db3391-ubik-reset-binding-handles 1.2
 Build:  dfs-carl 1.11
 Backed: dce1.0.2ab2
 
 Vijay Anand
 Fri Jul 09 13:48:04 1993
 vijay-Fri, 09 Jul 1993 13:48:34

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8742
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Need to establish vld_lastIndex when beginning whole-volume loop
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The initialization code at the beginning of efsSomeClone(),
 vol_efsClone(), and vol_efsDestroy needs to initialize the
 `vld_lastIndex' member of the voldata structure.  The current code
 only initializes this member in vol_efsCreate() and vol_efsOpen().
 This causes problems when any two of the three routines mentioned
 above is called without closing the volume and reopening it in
 between because vld_lastIndex may have a bogus value.  The most
 obvious case is when a clone fails part-way through and we then
 attempt to destroy it.  Since the clone was opened right after it was
 created, vld_lastIndex is 0, dooming to failure the attempt to
 immediately destroy it.
 
 Jeffrey Prem
 Fri Apr  9 16:26:35 1993
**Solution Text**
 Entered as OT 7902

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8741
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : better handling of butm errors
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 When an error in the butm module such as a media error happens, lots of 
 messages are printed in the butc console, and the fileset is marked as
 successfully dumped (in the logs). Both of these are wrong. 
 
 Also, there is a big core leak in the dump when such errors happen. Fix 
 all these and more.
**Solution Text**
 ld.so: warning: /usr/5lib/libc.so.2.7 has older revision than expected 8
 -------------------------------------------------------------------------------
 
 butm no longer prints messages, but returns error codes instead.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8740
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Remove diff dirs from two machines cause an EINVAL.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/07/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Apr. 7, 1993
 
 Remove dirs from both clients in a two machine cell may result that
 one of the client (the one far away from the server) returns EINVAL.
 
 --Jess
**Solution Text**
 APR. 7, 93
 
 The bug is fixed. In fact, the problem is that NameFidp in AFS_RemoveDir
 function is a pointer to the input parameter and is used as a local
 variable. It was cahnged when retry.
 
 --Jess
 
 APR. 9, 93
 
 The delta also fixes the same problem in AFS_RemoveFile function. It has been
 tested by "remove.file.dist.scr".
 
 -- Jess
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8739
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : SCOUT
Short Description             : scout has many problems
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/07/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The original synopsis of this defect was that scout did not display
 "Server" over the server column.  After looking at the code, I noticed
 a bunch of bugs and decided to fix them under this delta.  Here are
 the bugs:
 
   o  Scout didn't reset binding handle after communications loss.
      This could mean that scout might never recover from a host going
      down and coming back up.  It now resets the binding.
 
   o  Scout stopped getting info on hosts after a failure.  If one host
      went down, scout would stop collecting statistics on all the
      hosts that follow it in the list.  It now continues.
 
   o  Scout was applying the basename prefix to hostnames that began
      with '/'.  It no longer does this.
 
   o  Scout was only displaying the first part of a name.  If the name
      was a DCE host name, you didn't get much past the cell name.  It
      no longer does this.
 
   o  Scout didn't print "Server" over the servers.  A doc defect has
      been entered for this, also.  It now does this.
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8738
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : fts server doesn't properly handle remote owners of server entries
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/01/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 When displaying the owner of a server, fts only binds to the local
 cell, so it won't properly translate a uuid from a remote cell.
 Michael Comer
 Thu Apr  1 14:59:02 1993
**Solution Text**
 It's actually a little more complex than that.  The owning group of
 the entry should be a group in that cell.  This group, as specified by
 the -cell option, should be used to resolve the owning group uuid.
 Also, since the owning group option can take a fully-qualified
 principal name, the cell name part of the principal name has to be
 checked against that specified by the -cell option.
 Michael Comer
 Thu Apr  8 19:04:46 1993
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8737
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Provide bomb point utilities
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 This defect # will be used to add testing utilities for error
 injection ("bomb points").
 Jeffrey Prem
 Tue Mar 30 18:38:58 1993
**Solution Text**
 Entered as OT 7893

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8736
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Elbr_GetCleanLogInfo should return the log page header always
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/26/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 [ rajesh 3/25/93 ]
 
 Test_anode test recovery.2.test causes a seg. violation in SafeWRB
 when trying to bcopy the first log record into the passed in buffer.
 This is because the pointer to the buffer is invalid. It became invalid
 because the checksum version passed to SafeWRB is an invalid value 217
 which causes a bogus length to be returned by GetChecksumLength.
 The pointer to the buffer to hold the data is incremented by this
 value causing it to become invalid.
 -------------------------------------------------------------------------------
 [ rajesh 3/25/93 ]
 
 This is because the log page header structure passed from elbl_Init to
 elbr_GetCleanLogInfo is not filled in, if the log is not a clean log.
 We always need elbr_GetCleanLogInfo to fill in the log page header, asn
 elbr_Recover, called by elbl_Init if its not a clean log, needs the
 checksum value from the log page header.
**Solution Text**
 -------------------------------------------------------------------------------
 [rajesh 3/25/93 ]
 Modify elbr_GetCleanLogInfo to fill in the log page header always.
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8735
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : id permbits are irrelevant for dummy initial object acls
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/24/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 [rajesh 3/24/93]
 
 Interested CC: gmd@osf.org, bwl, comer
 
 When a directory does not have an explicit initial object acl and a permission
 category has the write unix mode bit specified, the initial object acl
 also acquires the insert and delete perm bits.
 
 # ls -ld .
 drwxrwxrwx   2 root     transarc     256 Mar 24 15:51 .
 # acl_edit . -io -l
 
 # Initial SEC_ACL for objects created under: .:
 # Default cell = /.../sherwood
 user_obj:rwxcid 		<<<
 group_obj:r-x---
 other_obj:r-x---
 
 [ group_obj and other_obj do not end up getting w bit as the umask is 022]
 
 The id bits are irrelevant for objects.
**Solution Text**
 -------------------------------------------------------------------------------
 [ rajesh 3/24/93 ]
 
 When efsx_getacl conjures up dummy acls, it calls dacl_InitEpiAcl with
 the mode bits for the file and expects an acl back. An argument to
 dacl_InitEpiAcl specifies whether the acl is being created for a dir
 or a file so that the write mode bit can be interpolated to a
 combination of write,insert,delete permission bits in the acl.
 Efsx_getacl specifies that the dummy acl is being created for a
 directory even for a dummy initial object acl.
 
 Changing efsx_getacl so that the "dir" flag is not specified for 
 dummy initial object acls.
 
 -------------------------------------------------------------------------------
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8734
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Improper locking while deactivating a logmap
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh 3/23/93
 
 DeallocLogMap removes the logmap from the active list and puts it into
 the free list with out holding the global log lock.
 
 CC: ota
**Solution Text**
 -------------------------------------------------------------------------------
 Modified Elbl_Close not to release the global log lock before calling
 DeallocLogMap.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8733
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Memory leak in shutting down a log
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh 3/23/93
 
 When the last reference to an aggregate is dropped and epig_CloseAggregate
 is called, the code never frees the array holding the disk addresses 
 of the log blocks. DeallocLogMap should free up this array.
 
 CC: ota
 
 -------------------------------------------------------------------------------
**Solution Text**
 -------------------------------------------------------------------------------
 Modify DeallocLogMap to free the log block disk address array.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8732
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : elbr_GetCleanInfo should release memory allocated for log block
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 rajesh 3/22/93
 
 Elbr_GetCleanLogInfo should unpin and release the memory it allocates to hold the first log block before returning.
 
 CC: ota
**Solution Text**
 -------------------------------------------------------------------------------
 [ rajesh 3/30/93 ]
 
 Unpin the block sized memory and free it just before returning.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8731
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : Default fileset allocLimit set too low
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/18/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Throughout the code, there are places where a fileset's `allocLimit' is
 set to 0x40000000 bytes (1 gigabyte).  This is an artifact of the
 Episode quota management changes.  Since there is currently no
 administrative interface to change a fileset's `allocLimit', this
 artificially limits the maximum size of a fileset to 1 gigabyte.
 
 To address this problem, I plan to introduce a new constant (a hyper),
 `vol_unlimitedQuota'.  It will be initialized to 0x3ff,,0xfffffc00
 (4.4 terabytes).  This value is calculated to yield the largest
 possible limit on an Episode fileset with a fragsize of 1K (the
 smallest possible fragsize).  If in the future, we decide to fully
 support the notion of an `unlimited' quota value, we can change the
 value of `vol_unlimitedQuota' (maybe to 0xffffffff,0xffffffff) without
 changing its uses.
 
 The new `vol_unlimitedQuota' constant will be used in all of the
 places where `allocLimit' is initialized.
 
 Jeffrey Prem
 Thu Mar 18 11:18:03 1993
 Jeffrey Prem
 Mon Mar 29 11:13:13 1993
 
 Jeffrey Prem
 Mon Mar 29 11:13:46 1993
 
 Testing cc: mail facility - Pervaze

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8730
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : date specific retores do not work
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 In restoreft, date specific retores tend to restore everything upto the last
 incremental to date. It should use the date supplied and not restore any
 incrementals/fulls after this date.
 
 The last clone time is not computed correctly during dump. The warning that
 nothing changed in a fileset appears even if there was a change to the fileset
 being dumped.
 
 The clone time of a fileset on tape is 0. It should be the actual clone time
 of the fileset.
**Solution Text**
 -------------------------------------------------------------------------------
 
 After extensive testing, it seemed that the first part of this
 report is not correct. All tests with a combination of 
 full and incremental dumps and a combination of read write and
 read only clones seem to indicate that date specific restores work. 
 
 The clone time on the fileset written to tape is indeed 0. This has been
 fixed. 
 
 The clone time is now computed correctly, by using the suggestion. 
 The warning  works for incremental dumps, but not always for full
 dumps. Implementing the fix for full dumps would require a big performance
 hit.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8729
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : restore does not say done in butc window
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 bak restoreft and restoredisk would now print a brief completion message in
 the butc window upon completion of the restoration. A more detailed message
 including the count of filesets that were successfully restored and the count
 of those that failed would be more useful information, but this would need
 quite a bit of code reorganization. Such a change would be done under a 
 separate defect.
 
 The change is incorporated into
 vijay-db3294-butc-restore-does-not-say-done
 
 Vijay Anand
 Mon Jul  5 15:51:27 EDT 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8728
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : scantape cannot handle malformed tapes
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 scantape prints spurious errors when supplied a tape not written by the DFS
 backup system. It should be able to handle this gracefully.
 -------------------------------------------------------------------------------
**Solution Text**
 -------------------------------------------------------------------------------
 The solution is for scantape to print out better error messages
 when it encounters  problems with malformed tapes.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8727
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak labeltape core dumps for long tape names.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 The following command causes a core dump
 bak> labeltape -tape "abcdefghijklmnopqrstuvwxyzaaaaab1234567890123456789012345678a123456789012345678901234567890123456"
 Segmentation fault (core dumped)
 
 The problem here is, the statusS structure has the taskName array that is of
 size 64, but is trying to hold a much larger tape name field. A good idea would
 be to dynamically allocate taskName after computing the required size.
 -------------------------------------------------------------------------------
**Solution Text**
 -------------------------------------------------------------------------------
 The solution is to truncate the tape name when copying into the taskname if its too large.
 Also, there is code in both bak and butc that doesnt check the tapename size before
 storing it or sending it across. In all these cases, checks should be made to make sure
 that array boundaries arent overrun.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8726
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : no way to kill a task in butc
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 If bak starts a task in butc, and say, the admin quit from bak. He restarts
 bak and wants to kill the task running on butc, He can't do it short of
 restarting butc. This is not a good idea. There should be some way of 
 controlling the status nodes in butc, even after bak has lost its own status
 nodes when it restarted. A good way for bak would be to do a status on each
 of its host/tcid entries and updates it status node list. Of course, this 
 should be done only for the interactive bak and would induce more delay in
 bak startup time. Something to think about.
**Solution Text**
 -------------------------------------------------------------------------------
 
 The fix is to have interactive bak poll the butcs for status when it comes up and
 to update its internal tables with this status. Then, the normal bak status thread
 will function as desired and it should be possible to abort a job in
 butc. 
 -------------------------------------------------------------------------------

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8725
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak savedb has error message problems
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 no error message from savedb if no butc listening on a TCID.
 
 savedb does not warn the user in the butc window that the wrong tape was
 loaded, but simply keeps prompting for bak_db_dump.1. 
 
 savedb has no check for > 1023 tcids on command line. In general all command
 should check for tcid < 0 and > 1023.
 
 restoredb has many of the same problems
 
 bak status also does not check for > 1023 tcid.
 -------------------------------------------------------------------------------
**Solution Text**
 -------------------------------------------------------------------------------
 Some of these problems are dups of fixed problems, like the
 problem about tcid checking. 
 
 The otheres are all fairly easy to fix.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8724
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : bak dumpinfo has problems with command line switches
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 bak dumpinfo has some problems with the command line switches. If the -id 
 switch is not on, does the -verbose make sense? Should it be verbose on all
 dumps listed? It doesn't do this right now. It coolly ignores the -verbose
 switch. In my opinion, -verbose should list verbose info on all dumps listed
 by dumpinfo. This way it can be combined with -id (to list verbose info on one
 dump) or with -ndumps (to list verbose info on as many dumps as the argument
 for -ndumps). Think about this.
 -------------------------------------------------------------------------------
 Another thing that could fixed along with the above defect is error messages
 from dumpinfo. dumpinfo does not produce an error message if dump ID entered
 is not in the database, nor does it produce error messages if dumpID is invalid
 
 -------------------------------------------------------------------------------
 dumpinfo has a similar problem as deletedump in that it takes only one dumpID
 as an argument. It should take an unlimited number.
 
 -------------------------------------------------------------------------------
 dumpinfo prints an error message when -ndumps is 0. It should just list nothing
**Solution Text**
 -------------------------------------------------------------------------------
 
 1. dumpinfo now warns when -verbose is used without using the -id switch
 2. Invalid values for -id and -ndumps are caught with appropriate messages
 
 Fixed in
 Delta: vijay-db3278-bak-dumpinfo-cmd-line-fixes
 Build: dfs-carl 1.13
 Backed: dce1.0.2ab4
 
 Vijay Anand
 Fri Jul 16 17:58:24 EDT 1993
 vijay-Fri, 16 Jul 1993 17:58:40

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8723
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Update the acl test README file
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Add comment about making the directory containing the ACL tests writable by the
 principal/user running the tests.
**Solution Text**
 Comment added
 
 Imported into 3.7 rajesh-db3272-update-acl-test-README.tmp 1.1

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8722
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : Have butc give number of volues dumped and failed
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 When backup completes with you get the message for no failed volumes
 and failed volumes respectively:
 	Job 1: dump: Dumpid 7654321 finished
 	Job 1: dump: Dumpid 7654321 finished, 4 of 75 volumes failed
 
 What is wanted is the number of volumes dumped and number of volumes
 failed for every dump.
 
 -------------------------------------------------------------------------------
 When butc completes it prints (and logs) the following message:
 	Task 1001: dump morin.fulldump (DumpID 7654321): Finished
 
 Also want butc to print (and log) the number of volumes dumped and
 number of volumes failed in this dump.
**Solution Text**
 -------------------------------------------------------------------------------
 Backup will implement the following solution for no failed volumes and
 failed volumes respectively:
 	Job 1: dump: Dumpid 7654321 finished. 75 volumes dumped
 	Job 1: dump: Dumpid 7654321 finished. 71 volumes dumped, 4 failed
 
 	- John Morin.
 	  March 10, 1993
 
 -------------------------------------------------------------------------------
 butc will also print (and log) the following message for no failed
 volumes and failed volumes respectively:
 	Task 1001: dump morin.fulldump (DumpID 7654321): Finished. 75 volumes dumped
 	Task 1001: dump morin.fulldump (DumpID 7654321): Finished. 71 volumes dumped, 4 failed
 
 	- John Morin.
 	  March 10, 1993
 -------------------------------------------------------------------------------
 
 Imported into 3.7 srinivas-db3250-fix-compile-problems 1.1

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8721
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : Scan N file marks instead of one at a time on restores.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/10/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 A performance enhancement. during restores a tape is scanned one file
 mark at a time.  Change this so that it scans the n file marks all at
 once.
**Solution Text**
 -------------------------------------------------------------------------------
 1.  Changes so when forwarding over file marks, it forwards over all
     of them in one call instead of forwarding over them one at a time.
 2.  Also will make IOMGR_Poll calls before and after the following
     calls: rewind, forwardSpace, and backwardSpace.  This is because
     these calls can take a long time causing whatever connections to
     time out (so lets try to minimize it).
 
 	- John Morin.
 	  March 10, 1993
 
 Imported into 3.6 srinivas-db3249-fix-compile-problems 1.1

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8720
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : Dump status summary message missing from Tape Log
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/09/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 This defect is a duplicate of 3241 filed against AFS.
 
 The AFS 3.1 backup system used to have a log message, summarizing the
 outcome of a dump, something like:
 
 Dump (foo): Task <x>: Successful (n volumes dumped) or
 
 Dump (foo): Task <x>: Completed (n volumes failed, m volumes succeeded)
 
 This message used to go into the log file. The backup system still prints
 as summary in the users session (printed by the status watcher when a dump
 terminates), but no such summary is recorded in the log.
 
 Some customers used this log message for determining the state of backups and
 would like to see it put back in the log. This summary was inadvertently
 omitted when the new dump code was written.
**Solution Text**
 -------------------------------------------------------------------------------
 This fix is pretty simple to implement : esentially butc should write 
 to console and the tape log when dumps are finished.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8719
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : improve filesetfamily expansion
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The terminology is that of the AFS backup system. This is a dup of 3239
 filed against AFS.
 
 A customer suggested change to the backup system follows. It is a performance
 enhancement.
 
 For volume regular expressions, where no "expansion" is required, i.e.
 all the attributes are fully specified, i.e
 	- volume name
 	- server
 	- partition
 
 The system should not do a complete vldb enumeration in the cases above.
 Under the correct circumstances, this will greatly reduce the time required 
 for task startup.
 Pervaze Akhtar
 Tue Mar  9 15:08:01 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8718
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Invoke fileset descriptor gc in vol_open only if no px.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 [ rajesh 3/8/93 ]
 Currently vol_open calls the fileset garbage collection procedure each 
 time its called, even if px is running on the machine. Thanks to Craig, 
 for mentioning that this is too aggresive.  Change the code
 to gc fileset desc only if px is not running.
**Solution Text**
 -------------------------------------------------------------------------------
 [ rajesh 3/8/93 ]
 
 Call vol_GCDesc in  vol_open up from only if px is not running. 
 The value of zlc_recoveryState variable is checked to see if 
 px is running or not.
**Validation Text**

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8717
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Transactions should be permanent
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/19/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 wam 2/19/93
 
 This isn't really a defect.  Rather it is an open question about the
 strength of the underlying system semantics.  I'm using this DB # as a
 holder for trying to commit all transactions on end.  This provides
 incredibly strong semantics - much stronger than are really required.
 What is the impact on performance?
 
 Oddly enough, when I implemented this, it didn't make the zero-th
 order tests any slower.  Obviously it will require additional work to
 really figure this one out.
**Solution Text**
 wam 2/19/93
 
 I placed the code in question under control of the AFS_FORCE_METADATA
 #ifdef.  So, next time someone asks us if they can have this, we can
 say "yep".
 Imported into 3.6  mason-db3189-make-trans-permanent 1.1

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8716
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : should write data more aggressively (compile-time option)
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 wam 2/19/93
 
 Kazar believes that Episode really should try to write data more
 aggressively to disk.  To this end (and to see what the impact on
 performance really is) I'm going to make the buffer package always
 schedule dirty data buffers (not meta-data).  This is the choice the
 BSD guys make and it could actually be more efficient, not less,
 because these buffers are probably not going to be rewritten soon
 anyway.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8715
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : elbl_WriteWithBuffer should break-up calls to WriteLocked
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 mason 02/19/93
 
 There is some concern that because WriteLocked will call itself
 recursively, this will lead to large stacks.  This is really only an
 issue for the smaller blocks sizes (where a directory page or ACL
 would be much larger than a log block.)
 
 The only path where this is true is elbl_WriteWithBuffer.  It should
 iterate over the call to WriteLocked passing no more than the
 blocksize in data - this limits the recursion level to no greater than
 three and almost always no deeper than two.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8714
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : WriteLocked miscomputes available space on page
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 mason 02/19/93
 
 WriteLocked (log.c) computes the available space on the page
 incorrectly as it assumes there is always both old and new data.  This
 computation needs to be done correctly.  While this "works" it is
 grossly inefficient and could potentially lead to stack overflows.

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8713
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : backup - error recovery etc.
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Three easy problems:
 
 1) contact lost flag isn't getting cleared - should be easy to fix.
 2) Task nos. aren't matched on bak and butc.
 3) Spurious cryptic error messages when trying to abort a dump.
 
 -----------
 
 bak> jobs
 Job 1: Dump (all.full), 11985 Kbytes, fileset user.mason.dfs.obj [butc contact l
 ost]
 
 ** Contact lost flag doesn't seem to be getting cleared.
 
 ------------------------------------
 On butc
 Task 6: dump (all.full) Started
 
 on bak
 Job 1: Dump (all.full), 44282 Kbytes, fileset user.mason.dfs.obj [butc contact l
 ost]
 bak> status 2
 Task 0: Dump : 44282 Kbytes transferred, fileset user.mason.dfs.obj
 
 Task no. mismatch??
 
 ------------------------------------
 dewitt - when trying to abort dump
 
 Can't get 0x404 token on 0,,543, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 
 AFS_SetContext(saratoga.transarc.com) fails: status 16c9a016 (dce / rpc) (unknow
 n)
 Can't get 0x404 token on 0,,58, server saratoga.transarc.com: status 29313007 (d
 fs / xvl) (unknown)
 Pervaze Akhtar
 Wed Feb  3 17:03:54 1993
**Solution Text**
 -------------------------------------------------------------------------------
 
 1) contact lost flag isn't getting cleared - should be easy to fix.
 
 	Already fixed in a previous delta, can't find its name.
 
 2) Task nos. aren't matched on bak and butc.
 
 	This is fixed now with 
 	Delta: vijay-db3140-bak-task-number-mismatch
 	Build: dfs-carl 1.13
 	Backed: dce1.0.2ab4
 
 3) Spurious cryptic error messages when trying to abort a dump.
 
 	Can't reproduce this either. Could have been fixed earlier.
 
 Vijay Anand
 Fri Jul 09 14:08:31 1993
 vijay-Fri, 09 Jul 1993 14:12:08

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8712
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : Butc operation errors - various
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Some of these may not be reproducible, so for the moment I'm grouping
 the errors.
 
 1) having to hit return twice when reading a tape label. 
 	Only saw this once.
 
 2) Spurious clone time error messages. Need to eliminate some of these
 	checks. This really should be configurable.
 
 3) Any blocking tape operation causes a failures for subsequent requests.
 	Note that this is most true of 8mm drives.
 
 4) Error messages during dump. Clean up the error reporting.
 	(I hope users can avoid knowing how rpc_server_listen works in order
 	to interpret failure modes :-) )
 Dewitt:
 readlabel
 
 ******* OPERATOR ATTENTION *******
 Device :  /dev/rmt2
 Put in the tape whose label is to be read
 Hit return when done
 ******* OPERATOR ATTENTION *******
 Device :  /dev/rmt2
 Put in the tape whose label is to be read
 Hit return when done
 
 Thanks, now proceeding with tape label reading operation.
 **********************************
 file_tm: I/O error Reading tape label
 Could not read tape label; status 20944006 (dfs / btm) (unknown)
 
 ** Had to hit return twice. Bug???
 
 ------------------------------------------
 
 Lots of spurious messages relating to clone time - when dumping the first
 time.
 
 ------------------------------------------
 bak> labeltape all.full -tcid 2
 bak> readlabel 2
 bak: Can't get version number (dfs / bak) Butc version check failed
 Due to blocking by labeltape.
 
 ------------------------------------------
 
 bak> readlabel 2
 
 bak: communications failure (dce / rpc) ; Failed to read tape label
 
 Is this the latest version?
 
 ------------------------------------------
 bak> labeltape all.full.1 2
 bak: Can't get version number (dfs / bak) Butc version check failed
 
 This messages is really unhelpful. Plus, is the placement of the tcid in
 the parameter list right? (Should echo back the port used?)
 
 ------------------------------------------
 Initial Errors during dump
 Returned from rpc_server_listen: status 16c9a022 (dce / rpc) (unknown)
 Can't get 0x404 token on 0,,561, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Can't get 0x404 token on 0,,558, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Can't get 0x404 token on 0,,555, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Can't get 0x404 token on 0,,552, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Can't get 0x404 token on 0,,549, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Can't get 0x404 token on 0,,546, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Can't get 0x404 token on 0,,543, server dewitt.transarc.com: status 29313002 (df
 s / xvl) (unknown)
 Pervaze Akhtar
 Wed Feb  3 17:00:51 1993
**Solution Text**
 -------------------------------------------------------------------------------
 
 1) having to hit return twice when reading a tape label. 
 	Only saw this once.
 
 	Couldn't reproduce this. Ignoring...
 
 2) Spurious clone time error messages. Need to eliminate some of these
 	checks. This really should be configurable.
 
 	This has been fixed already in 
 	khale-sb3296-date-specific-restores-dont-work
 
 3) Any blocking tape operation causes a failures for subsequent requests.
 	Note that this is most true of 8mm drives.
 
 	This is a major piece of work and will be deferred. Directly related
 	to process blocking capability of ioctl. May not be problem on solaris.
 
 4) Error messages during dump. Clean up the error reporting.
 	(I hope users can avoid knowing how rpc_server_listen works in order
 	to interpret failure modes :-) )
 
 	This is the only gripe that I fixed in response to this bug report. The
 	fix is in
 
 	Delta:  vijay-db3139-fts-unnecessary-listening
 	Build:  dfs-carl 1.13
 	Backed: dce1.0.2ab4
 
 Vijay Anand
 Fri Jul 09 14:08:31 1993
 vijay-Fri, 09 Jul 1993 14:08:57

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8711
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : backup - startup errors and error reporting
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 On saratoga
 bak: access to database denied (dfs / bdb)  : can't establish instance Id
 bak: access to database denied (dfs / bdb) ; Couldn't initialize VLDB library
 bak: bak Initialize failed, exiting
 
 Note the reference to the VLDB
 (authenticated as pakhtar)
 
 On saratoga
 bak: Unable to understand dump schedule file
 bak: [bc_ParseVolumeSet] Can't get required info on host ''
 bak>
 Three problems:
 1) can't establish instance Id - this can be fixed by retrying the locks
 	This may in fact be fixed already by Vijay.
 
 2) The reference to the VLDB should be changed. In fact, the error 
 report isn't all that helpful. All user output of VLDB or vldb should be
 changed at some convenient time.
 
 3) The "Unable to understand dump schedule file" occurred when no
 dump schedule existed. Shouldn't be complaining. This may be hard to 
 reproduce.. might want to read the code.
 Pervaze Akhtar
 Wed Feb  3 16:53:24 1993
**Solution Text**
 -------------------------------------------------------------------------------
 
 There has been quite a few changes to the bak code since this defect was
 opened. The VLDB references have all been removed. The error message printed
 when RPCs made by bak to the bakserver fail because the client is not 
 authenticated is succint. There has been some code changes to the fileset 
 family manipulation, and so, the error message from bc_ParseVolumeSet could
 not be reproduced. This defect was probably fixes earlier.
 
 The latest fix is in
 vijay-db3138-bak-better-error-messages
 
 Vijay Anand
 Mon Jul  5 16:06:15 EDT 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8710
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : butc startup error - poor error reporting
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Fri Jan 15 11:47:53 EST 1993
 
 bak>  dumpinfo
 dumpInfo: Unknown code DFS:bdb 15 (543920143) Can't get dump information
 
 1) Why does it say Unknown code? Message catalog dirs not setup?
 
 Accidentally pasted the following into bak:
 
 fydb        check the integrity of bak database
 bak>  dumpinfo
 dumpInfo: Unknown code DFS:bdb 15 (543920143) Can't get dump information
 bak>
 
 Result was:
 bak>
 Too many arguments
 bak> Exception: Invalid memory address (dce / thd)
 Abort process
 
 ** error reproducible, provided the sequence is:
 	authenticated as cell_admin
 	no message catalog path setup
 	don't know why dumpinfo failing
 	may be the command package bug Dave C has fixed.
 
 bak
 dumpinfo - fails as described above
 fydb ..... 
 
 On saratoga:
 fts lsfldb|more
 
 Typing q to more after the first page, produces
 
 Exception: Broken pipe (dce / thd)
 Abort process (core dumped)
 
 Doesn't seem quite correct.
 ---
 
 Tue Jan 19 13:41:15 EST 1993
 On saratoga: (running against b6 core components. This is prior to upgrading
 to 2.3 and b11)
 
 Machine wouldn't bring up DFS. The cause was traced down to CDS not starting
 correctly, therefore all subsequent services failed to start properly.
 
 Procedure, after trial and error:
 
 create /nodce
 reboot the machine
 remove the file /opt/dcelocal/etc/cdscache.shmid (thanks to Vijay)
 rc.dce
 rc.dfs
 
 Now the latter gives:
 # sh rc.dfs
         Starting epidaemon
         running dfsexport...
 dfsexport: /dev/epi0:epi0 (id 11):  Already attached: not attached
 dfsexport: /dev/epi1:epi1 (id 12):  Already attached: not attached
         running bosserver: be patient...
         running dfsbind: be patient...
         running fxd...
 
 Fri Jan 29 10:34:30 EST 1993
 On dewitt:
 # ./butc 2
 Tape Coordinator:  TCID 2   Debug level 0   Cell /.../mandos.dce.transarc.com
 butc: /.../mandos.dce.transarc.com/hosts/dewitt or dewitt.transarc.com not in da
 tabase
 1) Can't it determine what the cause of the error is? Surely it can.
 2) I know dewitt is in the database. It just isn't in the database as port 2.
 
 The error message should be sufficiently clear so that one doesn't have
 to guess as to solutions.
 
 Pervaze Akhtar
 Wed Feb  3 16:47:44 1993
**Solution Text**
 -------------------------------------------------------------------------------
 
 All the above gripes have been fixed earlier, although I can't quite get names
 Two things however
 1. If message catalogs are not setup or if the LANG and NLSPATH variables are
    not set, error messages could turn out to be cryptic. This is not a bug in
    bak however.
 2. Shell piping does not work with the libdce.a supplied by the OSF. This again
    is not a bug in fts, but with the SIGPIPE handling by the libdce.a
 
 I'm closing this defect cause I can't reproduce this.
 
 Vijay Anand
 Fri Jul 09 14:21:06 1993
 vijay-Fri, 09 Jul 1993 14:21:51

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8709
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : Need to integrate ftserver with ftutil
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 01/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 The ftutil library currently implements a large subset of the fileset
 operations that are also implemented in the ftserver.  The difference
 is that the ftserver is callable only through an RPC interface, while
 the ftutil library can be linked into any program that needs to do
 fileset operations.  Efts and the fset test interpreter are examples
 of local programs that do fileset operations.  The usefulness of these
 programs should be clear.  If we are ever to have a credible plan for
 standalone Episode, we need a way for an administrator to manipulate
 filesets, and efts could form the core for a standalone administrative
 suite.  Possibly more importantly, we need to have the ability to test
 the fileset operations efficiently, without the setup cost and
 complications associated with testing through the fts/ftserver
 interface.  This will become clear with the use of branch coverage
 tools to direct testing effort.  It may be difficult or impossible to
 test certain scenarios through the fts command-line interface because
 the number, combinations, and ordering of fileset operations is
 "fixed" by the fts/ftserver implementation.  Finally, the ftutil
 library provides a consistent, more refined interface to the otherwise
 idiosyncratic AGOP an VOLOP interface.
 
 Assuming we've made a case for having the ftutil library in the first
 place, it should then be clear that it is counter-productive to
 maintain two copies of identical or at least very similar code.  This
 is especially true since the code in question is relatively complex
 and is the recipient of much maintenance.  The repserver also contains
 code that is very similar to that currently implemented by both the
 ftutil library and the ftserver.  It could also be retrofitted, at
 least partially, to use the ftutil library.  The overhaul of the
 repserver to use ftutil is not part of this proposal, I merely point
 out that it will be productive to move in the direction of more
 consolidation of these very similar implementations.
 
 The work to consolidate ftserver and ftutil will be done in three
 phases.  First, ftutil has to be brought up to date with recent
 changes made to the ftserver.  The divergence hasn't been extremely
 large, but there are some important changes that have been made to the
 ftserver but not to ftutil; these will have to be addressed.  Second,
 the ftutil library has to be expanded to include the file-level
 fileset operations that are used to do fileset dumps and restores.
 Besides the dump/restore file operations, this work will also involve
 implementing the actual dump and restore operations in the ftutil
 layer.  Finally, the ftserver will be retrofitted to call the ftutil
 library instead of implementing the fileset operations directly.  I
 don't believe there are any dependencies on other work except that
 this will require that there are only a small amount of concurrent
 changes being made to ftserver and ftutil.
**Solution Text**
 jdp-db3112-ftserver-ftutil-integration 
 
 Jeffrey Prem
 Thu Aug  5 12:13:10 1993
 jdp-Thu, 05 Aug 1993 12:13:13

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8708
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : cm_StoreDCache clears the DC_DWRITING field incorrectly
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/21/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 cm_dcache.c - cm_StoreDCache clears the DC_DWRITING flag even if
 scp->writers > 0, which means that it could clear the flag even if
 more people still have the file open for writing.  The rules for this
 flag, which should be documented, are apparently that the flag goes on
 during a fetch or a write, and stays on until the last of either a
 fetch or the last user closes the file and it gets stored back to the
 server.  The idea is that this flag should be set whenever people are
 modifying the cache entry in such a way that it no longer represents
 the version number stored within its dcache entry
 
 [mason 3/5/93]
  I doubt this bug is still around.  I'll leave it for Mike & Tu to
 decide, however.
 [tu 7/21/93]
 
 This is actually indeed fixed long time ago. The fix is in 103 already.
 Cancel this.tu-Wed, 21 Jul 1993 14:22:51
**Solution Text**
 Actually, I was wrong: the bug was stiill there.  It is now fixed by
 
 kazar-db3984-fix-wvt-token-initialization
 
 Mike Kazar
 Wed Jul 28 11:55:48 1993
 kazar-Wed, 28 Jul 1993 11:55:52

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8707
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Cache volume header block address
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 01/24/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Inter-dependent CRs: 3500
 
 Save block addr of volume header in volume handle.  Similarly, save
 block address of bitmap pages in allocation handle.
 
 benefit = 5
 cost = 5 days
 []
 
 ota:
 
 This defect will address only the volume header.  Defect 3500 considers
 the bitmap portion of this work.
 [Mon May  3 11:54:55 1993]
**Solution Text**
 ota:
 
 Delta: ota-db2364-cache-vol-header-block-addr
 
 Also fix big_vt.test and add recover script for it.
 [Mon May  3 15:16:14 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8706
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : File status VV
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 01/24/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Interest List CC: tu, pakhtar
 Inter-dependent CRs: OT5004
 
 ota:
 
 Move volume version number to file status and allow vnops layer to
 manage it.  Aside from various other advantages, the vnops layer only
 needs to update it once per vnode operation.
 
 benefit = 20 cost = 10 days
 []
 [Fri Apr  9 09:11:47 1993]
 
 ota:
 
 Also fix OT 5004 which observes that the salvager checks the VV of
 inconsistent filesets which is shouldn't.
 [Wed Apr 28 15:37:19 1993]
**Solution Text**
 Delta: ota-db2355-file-status-VV -r1.2
 
 Date: Wed, 25 Mar 1992 17:23:42 -0500 (EST)
 From: Shu-Tsui_Tu@transarc.com
 To: DFS_Bugs_Mailing-List@transarc.com
 Subject: most of the bugs have been tracked by Open Track now ..
 Cc: Pervaze_Akhtar@transarc.com
 
 Please remove the following bugs from Transarc's DFS-Bugs list.
 Most of these bugs have been tracked by OSF's Open Track and there is
 no reason for Transarc's DFS_Bug database to continue the tracking work.
 
 2530,
 2531,
 2354,
 2355                  are ALL  tracked  by the same OT#2358
 []
 
 ota:
 
 It seems that TU must have mis-typed the DB number since this defect
 should not be closed as it was not moved to OT nor was it fixed.
 [Fri Apr  9 09:12:08 1993]
 
 ota:
 
 Revision 1.2 finishes this work.  OT 5004 also updated.
 [Wed May  5 09:29:12 1993]
 
 ota:
 
 OT8038 reports a salvager problem regarding ACLs and FTS_Restore
 operations that is fixed by this delta.
 [Tue May 25 09:27:05 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8705
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : VOLUME MTIME
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 01/24/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
 Update the volume modification time less often.  Decide how often we
 really need to update it.  Since this is only a long (not a hyper, or
 struct timeval) it can't be necessary more than once a second.  Perhaps
 we can live with every 10 seconds or more?
 
 Benefit = 20
 Cost = 2 days
 []
 
 ota:
 
 Craig confrms that this field is not used for semantic purposes so it is
 only being used for advisory purposes.  This suggests that updating it
 no more often that 5 seconds would be acceptable (somehow this seems
 better that 10sec).
 [Wed Apr 28 15:21:41 1993]
**Solution Text**
 ota:
 
 Delta: ota-db2352-relax-on-vol-mtime
 
 Set the volume's modTime and accessTime to the current time only when
 setting the volume's status.  On the VV and Unique bumping paths (which
 are the time critical ones) only update the times if they are 5 seconds
 or more old.
 [Wed Apr 28 15:21:44 1993]

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8704
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : BAK ADDFTENTRY ALLOWS DUPLICATE ENTRIES TO BE ADDED
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 11/25/91
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[9/28/93 public]
**Description Text**
     <Note by pehkonen (J. Pehkonen), 91/11/21 16:34:21, action: create>
 
 On DFS 6.1, doing : 
 bak addftentry -family simpsons -server taco -aggregate .* -fileset .*
 twice adds 2 entries to the DB.  It should not allow the second entry
 to be added.
 
 [mason 3/5/93] 
 DB cleanup.  I cannot confirm that this is gone, although I'm betting
 it is.  If so, Vijay can mark it "cancelled".
**Solution Text**
 -------------------------------------------------------------------------------
 
 Yes, this is fixed by Abhijit. The delta name for this fix is
 khale-sb2179-addftentry-allows-duplicate-entries
 
 Vijay Anand
 Mon Jul 05 15:40:01 1993

[9/28/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[12/17/93 public]
Closed.



CR Number                     : 8701
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : add performance counters to CM
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.3
Found Date                    : 9/28/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-o-ot8701-cm-add-performance-counters
Transarc Herder               : jaffe@transarc.com

[9/28/93 public]
Add performance counters to the cache manager.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/8/93 public]
Defect Closure Form
-------------------
Checked out counters with a tool that will read them out of the kernel
and a simple cache consistency test.  They look like they're doing the
right thing (it's hard to tell by looking).
Associated information:
Tested on TA build:  
	dfs-103-3.33/dce1.0.2ab4
Filled in Transarc Deltas with `comer-o-ot8701-cm-add-performance-counters' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8700
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts test 21 fails
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.3
Found Date                    : 9/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : tu-o-ot9321-fts-test21-test-fails
Transarc Herder               : andi
Transarc Status               : export

[9/28/93 public]
The configuration was a 2 machine HP cell.  The test log is
in ~notuser/rsarbo/fts.runtests.log.

[11/8/93 public]
Elliot says he'll fix this at Transarc.

[11/8/93 public]
Tu has already been working on a fix for this. 
Changed Responsible Engr. from `jaffe@transarc.com' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/8/93 public]
At the time I came across this defect I did not realize this ot has been
opened. I actually opened another one, 9321; I will cancel it by marking it
as a dup. 
                                                                               
There are several probelms with this test21. I suspected the one uncovered
by Ron is the first one listed below. 
                                                                               
Attached is the comment I made to delta tu-o-ot9321-fts-test21-test-fails:
                                                                               
DELT tu-o-ot9321-fts-test21-test-fails
There are two problems with the current test21 of fts test suite.
                                                                               
The test scenario is to move a large fileset to a target aggregate (which has
little space left) and expects to fail. However when it comes to filling
the target aggregate (to the situation where there is little disk space left
in the aggregatej), the test is to copy "/unix" from the local
machine to the target aggregate. Unfortunately, there is no such file by
the name of "/unix" on the HP platforms and the test fails silently and
continue to proceed.
The other problem occurred on rios where there is a "/unix".
After filling the the target aggregate to the point where the test gets
a "disk full" erorr (which is good), the test then removes the last three
copies of /unix it just copied in the target aggregate. The intent is to
make a little room large enough for the cloning but not enough for the
source fileset to be moved into.  However, depending on the size of
/unix and size of the source fileset to be moved, if the three removals of
/unix makes room large enough for the source fileset, then the move would
succeed and fail the test.
CHANGE test/file/fts/test21 1.10 1.11
Create a one MBytes file on the fly and use it to fill the aggregate and
removes the last copy if experiencing a disk full error. This ensures
that the space left in aggregate is between one and two Mbytes.
Filled in Transarc Deltas with `tu-o-ot9321-fts-test21-test-fails'

[11/8/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Run the fts-long test, both directions, in a two machines (rios, hp) cell
and passed.

[12/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `some' 
Filled in Transarc Herder with `andi' 
Filled in Transarc Status with `export'

[2/4/94 public]
Closed.



CR Number                     : 8699
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : cryptic error msg when not root
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.3
Found Date                    : 9/28/93
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3871-allow-aggr-reg-inquiry-as-non-root
Transarc Herder               : 

[9/28/93 public]
If you're not root and you try to run dfsexport (ie. to see what's
exported), you get the following cryptic message:
% dfsexport
ftu_ListAggrs() failed. Cannot proceed
I had thought this would work more similarly to "mount", where non-root
users could see what was currently mounted ... but even if that was never
intended, it'd be nice if the error message was more obvious about having
to be root.

[9/28/93 public]
Filled in Interest List CC with `jdp@transarc.com' 
Changed Responsible Engr. from `jaffe' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/6/93 public]
Defect Closure Form
-------------------
From defect:
Before booting the new kernel (with the less restrictive aggr ops), I
ensured a reasonable messages was printed if we fail to list the
attached aggregates.  After installing the less restrictive aggr ops,
I ensured that a non-root user could list the attached aggregates.
Jeffrey Prem
Tue Oct  5 15:38:21 1993
jdp-Tue, 05 Oct 1993 15:38:25
Associated information:
Tested on TA build:  
	dfs-103-3.33
Changed H/W Ref Platform from `hppa' to `all' 
Changed S/W Ref Platform from `hpux' to `all' 
Filled in Reported by Company's Ref. Number with `3871' 
Filled in Transarc Deltas with `jdp-db3871-allow-aggr-reg-inquiry-as-non-root' 
Filled in Transarc Status with `export'

[12/7/93 public]
imported into dfs-osf-1.12 and submitted to the OSF on 120793
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 8698
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : poor message when unable to detach native
Reported Date                 : 9/28/93
Found in Baseline             : 1.0.3
Found Date                    : 9/28/93
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : unknown
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : 

[9/28/93 public]
Operator error of doing a:
	fts delfldbentry -fileset dce13_tmp
BEFORE detaching native filesystem aggregate results in:
root@dce13> dfsexport /dev/dsk/c201d6s0 -detach -verbose
dfsexport: Revoking tokens for filesets on aggregate 3...
STKN_TokenRevoke: still need pending token of type 0xd0c for 0,,4.
getFSConn(dce13.osf.org): can't find principal (FLDB client not initialized (dfs / fts))
[release the token on 0,,4 (dce13.osf.org): FLDB client not initialized (dfs / fts)]
dfsexport: Failed to detach /dev/dsk/c201d6s0:/ (Unknown code  255 (65535))
Using the -force switch has no effect.
I can't recover by doing a crfldbentry can I? I won't get the same fsid ...
Logging as a B 2 since this looks like it will require a reboot and it was
really easy to do - it either shouldn't be allowed OR there should be a way
to recover.

[9/28/93 public]
Filled in Interest List CC with `cfe@transarc.com' 
Changed Responsible Engr. from `jaffe' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[11/9/93 public]
I believe the 255 error code is the same problem that was fixed by 
comer-o-ot8392-ufsops-map-neg-one-into-EBUSY-on-detach.  That is, the
filset is still busy and can't be detached.  It's true that the "FLDB
client not initialized" error code will keep the detach from
succeeding, but the -force option should override this (if the fileset
were not busy).  Printing a message and requiring the administrator
to use the -force option is probably the right thing to do.  A better
message could be printed but I don't think that kind of clean-up is in
the scope of 1.0.3a.

[11/9/93 public]
It was just the ``fts delfldbentry'' that did this?  I would think that you'd
need to do an ``fts delserverentry'' to get dfsexport to fail in this way,
and that re-creating the server entry with the correct principal would allow
``dfsexport -detach'' to do its job.

[11/17/93 public]
Gail, as promised over the e-mail last time, I have re-run the same test 
scenario described above on our most recent  build, dfs-osf-1.11. 
 
The problem disappeared! It works just fine. Attached is the log of my running
on the recent build. In fact, I did experience the problem with the 
"dfsexport -detach" running on one of our previous builds. The problem was 
in xaggr as I debugged into the kernel. Unfortunately, that build was a 
short lived one and I could not get a chance to figure it out where the problem
was. I actually repeated the same test scenario on our last two builds, they
both work ok. I suspect that Blake's delta could accidently fix it. 
 
Please verify it later and cancel it appropriately. 
*****************************************************************************
# fts lsfldb
epid0s0
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
bogus.transarc.com  RW       epid0s0 0:00:00 hosts/bogus    <nil>
 
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
larry.transarc.com  RW       /usr/dfs 0:00:00 hosts/larry    <nil>
----------------------
Total FLDB entries that were successfully enumerated: 2 (0 failed; 0 wrong aggr
type)
# dfsexport
dfsexport: /dev/dsk/c201d0s0, lfs, 30, 0,,0
dfsexport: /dev/dsk/c201d1s0, lfs, 40, 0,,0
# fts delfldbentry -fileset epid0s0 -verbose
Deleted FLDB entry for fileset epid0s0 (0,,4)
# dfsexport
dfsexport: /dev/dsk/c201d0s0, lfs, 30, 0,,0
dfsexport: /dev/dsk/c201d1s0, lfs, 40, 0,,0
# dfsexport -detach /dev/dsk/c201d0s0 -verbose
dfsexport: Revoking tokens for filesets on aggregate 30...
# dfsexport
dfsexport: /dev/dsk/c201d1s0, lfs, 40, 0,,0
# dfsexport -all
dfsexport: /dev/dsk/c201d1s0:epid1s0 (id 40):  Already attached: not attached
# dfsexport
dfsexport: /dev/dsk/c201d1s0, lfs, 40, 0,,0
dfsexport: /dev/dsk/c201d0s0, lfs, 30, 0,,0
#
# fts lsfldb
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
larry.transarc.com  RW       /usr/dfs 0:00:00 hosts/larry    <nil>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed; 0 wrong aggr
type)
Changed Interest List CC from `cfe@transarc.com' to `cfe@transarc.com, 
 comer@transarc.com' 
Filled in Transarc Herder with `jaffe@transarc.com'

[11/24/93 public]
Not surprisingly, I am still able to reproduce part of this problem on the
"pre-Transarc drop" code base here at OSF. The "getFSConn/FLDB client" error
is not reproducable - perhaps this is related to the admin.fl admin lists
that were missing from my config around that time? I'm not positive this
was the case (missing admin.fls) but it's my only guess currently.
 
The key factor for reproducing the "unknown code" when unable to detach is:
 
	- busy native filesystem dfsexported (must be write activity on
	local path) 
 
Note that using the -force switch produces STKN_TokenRevoke message
Write activity via the dfspath to the native filesystem will produce the
STKN_TokenRevoke message BUT the forced dfsexport will succeed.

[11/29/93 public]
(re-added clobbered gmd text)
 
I'm not clear on what the remaining problem is.
The version that OSF will get won't have the STKN_TokenRevoke message.
The ``unknown code'' message should go away with the drop, as well.
If this problem isn't closed with the drop, though, a new bug should be
opened and its sev/pri should be revisited, particularly if the problem
is a confusing error message.

[12/16/93 public]
Not in a position to verify yet but you are correct - I did not reset the
severity/priority correctly - doing so now.

[12/17/93 public]
Closed.



CR Number                     : 8693
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : incorrect check for errno in newaggr
Reported Date                 : 9/27/93
Found in Baseline             : 1.0.3
Found Date                    : 9/27/93
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/edbasic.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[9/27/93 public]
A misleading error message appears during the execution of the newaggr
command.  It looks something like this:
read() of /dev/rdsk/c201d4s0, 2147483520 for 128: No such device or address
This happens within a function which guesses a device's capacity by doing
a sequence of lseeks and reads.  Thus an error return is (eventually)
expected.  HPUX can return ENXIO for this sort of invalid read.  This
should not be considered an error condition.

[9/27/93 public]
I assume the problem is
with the thread package's redefinition of "read" so I am reassigning it
accordingly.  If it is really the case that read(2) can return ENXIO for
a request to read beyond the end of a partition in HP/UX, I would be surprised.
Certainly the man page does not mention this possibility.
Changed Component Name from `dfs' to `thr' 
Changed Subcomponent Name from `episode' to `' 
Filled in Interest List CC with `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/18/93 public]
It's not a threads bug -- the test programs give the same errno whether
or not you used a multithreaded test program.  The situation is
that HPUX has some odd errnos:
	(1) A read or write on a non 512-byte boundary on a device
	special file returns ENXIO.
	(2) A read or write past the end of file on device special files
	returns ENOSPC.
I'm modifying src/file/anode/edbasic.c to check for ENXIO when AFS_HPUX_ENV
is defined.

[11/12/93 public]
fixed->closed.

[12/17/93 public]
As far as I can tell the previous change of status from closed to fix
is because Transarc didn't accept the exact literal text of my change.
The difference is inconsequential, so I'm changing status to closed.

[12/17/93 public]
Transarc's change of Dec 7 was due to a clerical error.  A list of OT's
which we knew to have been fixed in our code base was compiled, and we
marked them all "fix".  Presumably those that had already been closed in
1.0.3 should have been weeded out, but this one escaped.  If you find any
others for which Transarc has taken some mysterious action, feel free to
ask us about it.  Thanks.



CR Number                     : 8690
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 8524
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Functional test files
installed in wrong place
Reported Date                 : 9/24/93
Found in Baseline             : 1.0.3
Found Date                    : 9/24/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/file/util/Makefile
Sensitivity                   : public

[9/24/93 public]

The following files are installed in "install/<mach>/dcetest/dce1.0"
instead of "install/<mach>/dcetest/dce1.0/test/...".  This is most
likely a problem with IDIR in the Makefiles:

file/util/fs_FunctionsScript
file/util/fs_GenerateFile
file/util/fs_SetSysVar
file/util/dce_AddUserPrincipal
file/util/gen_FunctionsScript
file/util/CreateLocalAccount
file/util/DeleteLocalAccount
file/util/icldump

[9/28/93 public]
Corrected file/util/Makefile submitted - status changed to fix.

[12/17/93 public]
Closed.



CR Number                     : 8685
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_config
Short Description             : DFS doesn't restart properly
Reported Date                 : 9/24/93
Found in Baseline             : 1.0.3
Found Date                    : 9/24/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dfs.clean
Sensitivity                   : public

[9/24/93 public]
The use of "bos stop" in dfs.clean should be replaced with
"bos shutdown" so that the daemons will come up on reboot
automatically (i.e. the Run/Not Run flag shouldn't be touched
in the BosConfig file).

[9/24/93 public]
Fixed.  Also added dcelogin call since the bos command
was failing before anyway.

[12/17/93 public]
Closed.



CR Number                     : 8679
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/filewnr
Short Description             : filewnr -C okay_message
broken; "filewnr -C okay_message -w waitfilename" broken for HPUX
Reported Date                 : 9/24/93
Found in Baseline             : 1.0.3
Found Date                    : 9/24/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/filewnr.c
Sensitivity                   : public

[9/24/93 public]
Specifying the filewnr -C okay_message option causes to fail when it waits
on the waitfilename, regardless of its contents.

[9/24/93 public]
Got me!  If you look a the output from running 'filewnr -C <message>'
(though I can't figure out why you would want to do this), a line is
printed that says :

  darkman_5027_WNR: must specify -w waitfile with -C <message>

and then a usage message is printed.  The program then proceeds to fail.
The fix is to put an exit after the usage message print.  The diff is 
below.

   diff filewnr.c  /project/dce/build/dce1.0.3/src/test/systest/file/filewnr.c
1181d1180
<       do_exit (BAD_OPTIONS);

[10/1/93 public]
I've incorporate the above do_exit change, as well as removing the O_NDELAY
mode for open_file call.  Built and tested on HPUX.

[12/17/93 public]
Closed.



CR Number                     : 8671
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8670
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : parentless unkillable processes created during
exectution of rc.dfs
Reported Date                 : 9/23/93
Found in Baseline             : 1.0.3
Found Date                    : 9/23/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : file/osi/HPUX/osi_port_mach.h,
tpq/tpq_init.c, rpc/kruntime/HP800/pthread_sys.c
Sensitivity                   : public

[9/23/93 public]

During exection of rc.dfs (specifically kload) the following processes are created:

sh -cQ cp /hp-ux /opt/dcelocal/ext/ALL.out
cp /hp-ux /opt/dcelocal/ext/ALL.out

Probablly these are created from kload via the system(3) function, with the second 
a child of the first.  However ps -ef shows both these processes to be 
parentless, they are unkillable, and they persist until a system reboot.

This might be a bug in HP process management.

[10/20/93 public]
These are actually kernel threads kicked off by DFS: their "names" were
never properly set and so they were showing up with the name of the last
thing to run in their process slot.  This has been fixed.

[12/17/93 public]
Closed.



CR Number                     : 8670
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : intermittent panic during rc.dce if 
the UFS partition for /opt is DFS exported
Reported Date                 : 9/23/93
Found in Baseline             : 1.0.3
Found Date                    : 9/23/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : cm/cm_init.c , px/px_init.c ,
libafs/HPUX/px_config.c , libafs/HPUX/cm_config.c
Sensitivity                   : public

[9/23/93 public]

The setup: /opt is a symbolic link to /u0/opt.  /u0 appears in the dfstab
as an exported UFS partition.  We are rebooting a machine which was 
previously successful with this configuration.  

During the execution
of rc.dfs, the system panics during the dfsexport of a different
LFS aggregate.  The panic occurs in the CM directory name cache code,
and it is a consequence of the fact that the nh_cache hash table
was never initialized.  This was determined from the core dump
in which the cacheinit_flag is zero.  The panic occurs in
nh_delete_dvp when it tries to dereference a pointer from the
non-initialized hash table.  

There appears to be a race condition in initializing the hash
table, since this problem is intermittent.

[10/1/93 public]

Various indirect vectors that the OS uses to call into DFS via (to release
resources) are now established at DFS startup time ... as opposed to
DFS "load" time.

[12/17/93 public]
Closed.



CR Number                     : 8666
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfstrace
Short Description             : Underscores missing from -cdsentry option
Reported Date                 : 9/23/93
Found in Baseline             : 1.0.3
Found Date                    : 9/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : See description
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-o-ot8666-make-dfstrace-match-documentation
Transarc Herder               : 

[9/23/93 public]
The following dfstrace commands are missing underscores
in the -cdsentry option of their usage messages:
	dfstrace clear
	dfstrace dump
	dfstrace lslog
	dfstrace lsset
	dfstrace setlog
	dfstrace setset
Example: [-cdsentry <server entry in CDS>]
Should be: [-cdsentry <server_entry_in_CDS>]
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/1/93 public]
Changed Interest List CC from `comer@transarc.com' to `kdu@transarc.com' 
Changed Responsible Engr. from `kdu@transarc.com' to `comer@transarc.com'

[10/6/93 public]
Defect Closure Form
-------------------
Fixed underscore problem and made dormant a state on its own.  Also,
cleaned up some output: print strings not errnos.
 [icl] ./dfstrace lss -help
 Usage: ./dfstrace lsset  [-set <set_name>...] [-cdsentry <server_entry_in_CDS>] [-help]
 [icl] ./dfstrace lss -cds /.../mandos.dce.transarc.com/hosts/saratoga/bosserver
 Available sets:
 bosserver: active
 dfsauth: dormant
 dacl: inactive
Associated information:
Tested on TA build:  
	dfs-103-3.33
Tested with backing build:  
	dce1.0.2ab4
Changed Defect or Enhancement? from `enh' to `def' 
Changed Found in Baseline from `1.0.2' to `1.0.3' 
Filled in Transarc Deltas with 
 `comer-o-ot8666-make-dfstrace-match-documentation' 
Changed Transarc Status from `open' to `export' 
Added field Transarc Herder with value `'

[12/17/93 public]
Closed.



CR Number                     : 8648
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/dfs.lock
Short Description             : The -p <passes> option really
does <passes + 1> passes.
Reported Date                 : 9/21/93
Found in Baseline             : 1.0.3
Found Date                    : 9/21/93
Severity                      : E
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : dfs.lock
Sensitivity                   : public

[9/21/93 public]
I specified "-p 1" and the test completed iteration 0 and continued into
iteration 1.  See here:
$ ./dfs.lock_cl -d -p 1
:
   - Running for 1 passes.
:
COMPLETED dfs.lock ITERATION 0 AT Tue Sep 21 17:23:12 EDT 1993


BEGINNING dfs.lock ITERATION 1 AT Tue Sep 21 17:23:12 EDT 1993
----
The offending code is at the end of the main loop where it compares
$ITERATION to $PASSES.  It should instead compare $PASSES to $ITERATION +
1.

[12/17/93 public]
Might as well get it in this release.

[12/28/93 public]
Script now increments ITERATION before comparison.

[01/17/94 public]
Submission fixed - closing.



CR Number                     : 8645
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file
Short Description             : dfs.lock specifies "-P" option
in usage info - should be "-p"
Reported Date                 : 9/21/93
Found in Baseline             : 1.0.3
Found Date                    : 9/21/93
Severity                      : E
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/systest/file/dfs.lock
Sensitivity                   : public

[9/21/93 public]
Just a simple typo, the one liner says it all.

[12/17/93 public]
Might as well - along with other dfs.lock bugs for this release.

[12/28/93 public]
Typo fixed.

[01/17/94 public]
Submission fixed - closing.



CR Number                     : 8634
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref, users_gdref
Short Description             : Change anonymous to nobody
Reported Date                 : 9/20/93
Found in Baseline             : 1.0.2
Found Date                    : 9/20/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[9/20/93 public]
The user anonymous should be the user nobody. This change
has already been made in the admin_gd, it must be made
in the admin_ref and the users_gdref.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/23/93 public]
The following files are affected:
	admin_ref/man4dfs/NoAuth.4dfs
	admin_ref/man8dfs/bos.8dfs
	admin_ref/man8dfs/bos_addadmin.8dfs
	admin_ref/man8dfs/bos_addkey.8dfs
	admin_ref/man8dfs/bos_create.8dfs
	admin_ref/man8dfs/bos_delete.8dfs
	admin_ref/man8dfs/bos_gckeys.8dfs
	admin_ref/man8dfs/bos_genkey.8dfs
	admin_ref/man8dfs/bos_getdates.8dfs
	admin_ref/man8dfs/bos_getlog.8dfs
	admin_ref/man8dfs/bos_getrestart.8dfs
	admin_ref/man8dfs/bos_install.8dfs
	admin_ref/man8dfs/bos_lsadmin.8dfs
	admin_ref/man8dfs/bos_lscell.8dfs
	admin_ref/man8dfs/bos_lskeys.8dfs
	admin_ref/man8dfs/bos_prune.8dfs
	admin_ref/man8dfs/bos_restart.8dfs
	admin_ref/man8dfs/bos_rmadmin.8dfs
	admin_ref/man8dfs/bos_rmkey.8dfs
	admin_ref/man8dfs/bos_setauth.8dfs
	admin_ref/man8dfs/bos_setrestart.8dfs
	admin_ref/man8dfs/bos_shutdown.8dfs
	admin_ref/man8dfs/bos_start.8dfs
	admin_ref/man8dfs/bos_startup.8dfs
	admin_ref/man8dfs/bos_status.8ds
	admin_ref/man8dfs/bos_stop.8dfs
	admin_ref/man8dfs/bos_uninstall.8dfs
	admin_ref/man8dfs/fts.8dfs
	admin_ref/man8dfs/fts_addsite.8dfs
	admin_ref/man8dfs/fts_aggrinfo.8dfs
	admin_ref/man8dfs/fts_clone.8dfs
	admin_ref/man8dfs/fts_clonesys.8dfs
	admin_ref/man8dfs/fts_create.8dfs
	admin_ref/man8dfs/fts_crfldbentry.8dfs
	admin_ref/man8dfs/fts_crserverentry.8dfs
	admin_ref/man8dfs/fts_delete.8dfs
	admin_ref/man8dfs/fts_delfldbentry.8dfs
	admin_ref/man8dfs/fts_delserverentry.8dfs
	admin_ref/man8dfs/fts_dump.8dfs
	admin_ref/man8dfs/fts_edserverentry.8dfs
	admin_ref/man8dfs/fts_lock.8dfs
	admin_ref/man8dfs/fts_lsaggr.8dfs
	admin_ref/man8dfs/fts_lsfldb.8dfs
	admin_ref/man8dfs/fts_lsft.8dfs
	admin_ref/man8dfs/fts_lsheader.8dfs
	admin_ref/man8dfs/fts_lsreplicas.8dfs
	admin_ref/man8dfs/fts_lsserverentry.8dfs
	admin_ref/man8dfs/fts_move.8dfs
	admin_ref/man8dfs/fts_release.8dfs
	admin_ref/man8dfs/fts_rename.8dfs
	admin_ref/man8dfs/fts_restore.8dfs
	admin_ref/man8dfs/fts_rmsite.8dfs
	admin_ref/man8dfs/fts_setquota.8dfs
	admin_ref/man8dfs/fts_setrepinfo.8dfs
	admin_ref/man8dfs/fts_statftserver.8dfs
	admin_ref/man8dfs/fts_statrepserver.8dfs
	admin_ref/man8dfs/fts_syncfldb.8dfs
	admin_ref/man8dfs/fts_syncserv.8dfs
	admin_ref/man8dfs/fts_unlock.8dfs
	admin_ref/man8dfs/fts_unlockfldb.8dfs
	admin_ref/man8dfs/fts_update.8dfs
	admin_ref/man8dfs/fts_zap.8dfs
	users_gdref/man1dfs/fts_lsquota.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 8633
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : The bos create command
Reported Date                 : 9/20/93
Found in Baseline             : 1.0.2
Found Date                    : 9/20/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed
Transarc Herder               : 

[9/20/93 public]
The following information needs to be added to the documentation of
the bos create command:
	o A cron process created with the comand runs
	  under the local identity root and the DCE identity
	  self.
	o If the time associated with a cron process is omitted,
	  it defaults to 00:00 on the indicated day.
	o The command will not create an entry in BosConfig
	  if the specified executable file does not exist.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/20/93 public]
A couple of minor points:
 o  The bosserver will be "unauthenticated" as far as DFS is concerned. 
 o  The "bos create" command will return an error message if the program
    specified to the create does not exist:
	bos: failed to create new server instance foo of type 'simple' (specified executable not found (dfs / bbs))
Added field Transarc Herder with value `'

[9/23/93 public]
The following file is affected:
	admin_ref/man8dfs/bos_create.8df
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified changes in latest doc build and closed
this bug.



CR Number                     : 8623
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dfs.glue
Short Description             : Inadequate configuration 
					     documentation
Reported Date                 : 9/17/93
Found in Baseline             : 1.0.3
Found Date                    : 9/17/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/README, dfs.glue
Sensitivity                   : public

[9/17/93 public]
Here is a list of the undocumented configuration requirements for
dfs.glue:
- UNIX and DCE login uid and gid must be identical.
- required executables and their pathnames relative to the test
  directory are:
  - systest/file/filewnr
  - systest/file/dirread
  - systest/file/dirwrite.sh
  - file/cache_mgr/spoke
  - file/cache_mgr/hub
- UFS_PATH and/or TROOT must be chosen to be valid from all hosts.
  Either an NFS pathname, e.g. "/net/hostname/foo", or a duplicate of
  the test tree must be used.

[9/30/93 public]

Notes on the following were added to the README:
- UNIX and DCE login uid and gid must be identical.
- TROOT must be chosen to be valid from all hosts.

List of:
- required executables and their pathnames relative to the test
were added to dfs.glue header comment.

[10/1/93 public]

Verified made it through nightly build - closing.



CR Number                     : 8621
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : Client's cache FS does not unmount clean on shutdown
Reported Date                 : 9/17/93
Found in Baseline             : 1.0.2
Found Date                    : 9/17/93
Severity                      : B
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : cm_shutdown.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8621-cachefs-allow-clean-unmount
Transarc Herder               : jaffe@transarc.com

[9/17/93 public]
The DFS client (cache manager) keeps references on the CacheItems
file in the cache filesystem which keeps the cache filesystem
from being unmounted cleanly on system shutdown.  When the
DFS client is quiesed via unmount /... on shutdown, the DFS client
should release it's references to files in the client's cache
filesystem so that it will unmount cleanly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/9/93 public]
Filled in Transarc Deltas with `cburnett-ot8621-cachefs-allow-clean-unmount' 
Changed Transarc Status from `open' to `import'

[12/17/93 public]
Closed.



CR Number                     : 8617
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : AuxFlushActiveSCache can leak CM vnodes
Reported Date                 : 9/16/93
Found in Baseline             : 1.0.2
Found Date                    : 9/16/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8617-plug-cm-periodic-storback-scp-leaks
Transarc Herder               : jaffe@transarc.com

[9/16/93 public]
There are bugs in the AuxFlushActiveSCaches() procedure which can
result in CM scache entries being leaked.  This can occurr if
the file is in TSR or the file is a RO file.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/9/93 public]
Filled in Transarc Deltas with 
 `cburnett-ot8617-plug-cm-periodic-storback-scp-leaks' 
Changed Transarc Status from `open' to `import'

[12/17/93 public]
Closed.

[11/20/95 public]

Received diffs from Transarc.

Date: Mon, 20 Nov 1995 09:48:02 -0500 (EST)
To: biyani+@transarc.com, axg@osf.org
Subject: diff for OT8617
Message-Id: <skg9IWWSMUQlRj4U4K@transarc.com>

Checked-out CML/delta/cburnett-ot8617-plug-cm-periodic-storback-scp-leaks, 
 revision 1.2 
Delta: cburnett-ot8617-plug-cm-periodic-storback-scp-leaks
Change: file/cm/cm_daemons.c from 4.183 to 4.184
*** file/cm/cm_daemons.c
--- 4.184	1993/09/16 23:49:35
***************
*** 922,928 ****
      if (scp->states & SC_RO) {
          /* In general, we should not be here in the first place */
          /* osi_assert(serverp); */
!         return;
      }
      
      /* 
--- 922,928 ----
      if (scp->states & SC_RO) {
          /* In general, we should not be here in the first place */
          /* osi_assert(serverp); */
!         goto out;
      }
      
      /* 
***************
*** 932,956 ****
          lock_ObtainRead(&volp->lock);
  	if (volp->states & VL_TSRCRASHMOVE) {
  	    lock_ReleaseRead(&volp->lock);
! 	    return;
  	}
  	lock_ReleaseRead(&volp->lock);
!     } else {
!         /* should NOT be here, really */
!         return;
      }
      if (scp->modChunks > 0 || (scp->m.ModFlags & CM_MODTRUNCPOS)) {
  	cm_SyncDCache(scp, (osi_cred_t *) 0);
  	cm_ConsiderGivingUp(scp);
      }
!     if (scp->m.ModFlags & CM_MODMASK) {
  	cm_SyncSCache(scp, 0, 0);
      }
  
      /* release this entry (held by tpq_ParSet caller) */
!     lock_ObtainWrite(&cm_scachelock);
!     CM_RELE(scp);
!     lock_ReleaseWrite(&cm_scachelock);
  }
  
  /*
--- 932,955 ----
          lock_ObtainRead(&volp->lock);
  	if (volp->states & VL_TSRCRASHMOVE) {
  	    lock_ReleaseRead(&volp->lock);
! 	    goto out;
  	}
  	lock_ReleaseRead(&volp->lock);
!     } else { /* should NOT be here, really */
!         goto out;
      }
+ 
      if (scp->modChunks > 0 || (scp->m.ModFlags & CM_MODTRUNCPOS)) {
  	cm_SyncDCache(scp, (osi_cred_t *) 0);
  	cm_ConsiderGivingUp(scp);
      }
!     if (cm_NeedStatusStore(scp)) {
  	cm_SyncSCache(scp, 0, 0);
      }
  
+ out:
      /* release this entry (held by tpq_ParSet caller) */
!     cm_PutSCache(scp);
  }
  
  /*
***************
*** 975,981 ****
      opCount = 0;
      for (i = 0; i < SC_HASHSIZE; i++) {
  	for (scp = cm_shashTable[i]; scp; scp = scp->next) {
! 	    if (scp->modChunks > 0 || cm_NeedStatusStore(scp)) {
  	         /* Only used as a hint only -- no read lock first */
  	         CM_HOLD(scp);	/* will be released later */
  		 (void) tpq_AddParSet(parsetp, AuxFlushActiveScaches,
--- 974,981 ----
      opCount = 0;
      for (i = 0; i < SC_HASHSIZE; i++) {
  	for (scp = cm_shashTable[i]; scp; scp = scp->next) {
! 	    if (((scp->states & SC_RO) == 0) && 
! 		scp->modChunks > 0 || cm_NeedStatusStore(scp)) {
  	         /* Only used as a hint only -- no read lock first */
  	         CM_HOLD(scp);	/* will be released later */
  		 (void) tpq_AddParSet(parsetp, AuxFlushActiveScaches,
Delta: cburnett-ot8617-plug-cm-periodic-storback-scp-leaks
Change: file/cm/cm_scache.c from 4.278 to 4.283
*** file/cm/cm_scache.c
--- 4.283	1993/09/16 23:49:41
***************
*** 967,977 ****
  cm_NeedStatusStore(scp)
    register struct cm_scache *scp;
  {
!     /* cm_NeedStatusStore MUST NOT be more aggressive than what cm_ScanStatus can consume. */
!     /* Else, the loop in RevokeStatusToken cannot make sense. */
       if ((scp->m.ModFlags & (CM_MODTRUNCPOS | CM_MODLENGTH | CM_MODMOD
  			      | CM_MODOWNER | CM_MODGROUP | CM_MODMODE)) ||
! 	((scp->m.ModFlags & CM_MODACCESS)
  	 && scp->m.AccessTime.sec > scp->m.serverAccessTime + CM_ACCESSSKEW))
  	return 1;
      else
--- 967,978 ----
  cm_NeedStatusStore(scp)
    register struct cm_scache *scp;
  {
!     /* cm_NeedStatusStore MUST NOT be more aggressive than what cm_ScanStatus 
!      * can consume. Else, the loop in RevokeStatusToken cannot make sense.
!      */
       if ((scp->m.ModFlags & (CM_MODTRUNCPOS | CM_MODLENGTH | CM_MODMOD
  			      | CM_MODOWNER | CM_MODGROUP | CM_MODMODE)) ||
! 	(((scp->states & SC_RO) == 0) && (scp->m.ModFlags & CM_MODACCESS)
  	 && scp->m.AccessTime.sec > scp->m.serverAccessTime + CM_ACCESSSKEW))
  	return 1;
      else
Delta: cburnett-ot8617-plug-cm-periodic-storback-scp-leaks
Change: file/cm/cm_vnodeops.c from 4.506 to 4.516
*** file/cm/cm_vnodeops.c
--- 4.516	1993/09/17 14:17:42
***************
*** 2209,2219 ****
  			   ICL_TYPE_LONG, tscp->fid.Unique);
  		tmpscp = tscp;			/* remember for later */
  		tscp = cm_GetSCache(tscp->mountRoot, &rreq);
  		if (!tscp) {
  		    code = ENOENT;
  		    goto done;
  		}
- 		cm_PutSCache(tmpscp);		/* we're done with it */
  		/* 
  		 * Always reevaluate the .. ptr for the mount point to
  		 * point back to the appropriate place.
--- 2209,2219 ----
  			   ICL_TYPE_LONG, tscp->fid.Unique);
  		tmpscp = tscp;			/* remember for later */
  		tscp = cm_GetSCache(tscp->mountRoot, &rreq);
+ 		cm_PutSCache(tmpscp);		/* we're done with it */
  		if (!tscp) {
  		    code = ENOENT;
  		    goto done;
  		}
  		/* 
  		 * Always reevaluate the .. ptr for the mount point to
  		 * point back to the appropriate place.



CR Number                     : 8616
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Foreign group cannot own server entry
Reported Date                 : 9/16/93
Found in Baseline             : 1.0.2
Found Date                    : 9/16/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[9/16/93 public]
The documentation for fts crserverentry and fts edserverentry states that
a group from a foreign cell can own a server entry in the local cell.
In fact, this type of ownership is not allowed.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/17/93 public]
The following files are affected:
	admin_gd/dfs/dfs/2_issues_dfs.gpsml
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_ref/man8dfs/fts_crserverentry.8dfs
	admin_ref/man8dfs/fts_edserverentry.8dfs
	
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 8611
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dfs.glue
Short Description             : doesn't run properly on HP-UX
Reported Date                 : 9/15/93
Found in Baseline             : 1.0.3
Found Date                    : 9/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/dfs.glue
Sensitivity                   : public

[9/15/93 public]
Remote tests fail, and test never terminates.

[9/24/93 public]
Fixed in version 1.1.7.2.

[9/24/93 public]
Defect changed to open once again.  Fix was not propogated to ODE2.3
backing build so this hasn't been confirmed in a nightly build.  Sorry, our
mistake.

[10/1/93 public]
Built and tested for HPUX.  Fixed in version 1.1.11.2.

[12/17/93 public]
Closed.



CR Number                     : 8608
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Change args to -server options on *serverentry cmds
Reported Date                 : 9/15/93
Found in Baseline             : 1.0.2
Found Date                    : 9/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-db4325-fix-syntax-to-match-doc
Transarc Herder               : jaffe@transarc.com

[9/15/93 public]
The arguments to the -server options of the following commands need to be
changed to match the documentation:
 
   fts crserverentry
   fts delserverentry
   fts edserverentry
 
The arguments for the -server options of these commands are currently
 
   -server {<machine> | <address>}
 
They need to be changed to the following to match the documentation and the
rest of the fts commands:
 
   -server <machine>
 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'
Filled in Reported by Company's Ref. Number with `db 4325'

[9/29/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Issuing the ``fts help crserverentry'', ``fts help edserverentry'', and ``fts
help delserverentry'' demonstrates that the usage/syntax description lines
have been changed as requested.
--Other explanation below--
The delta has been reviewed by Jeff Prem.
Associated information:
Tested on TA build:  dfs-103-3.33
Tested with backing build:  dce-solaris-09.014
Filled in Transarc Deltas with `cfe-db4325-fix-syntax-to-match-doc' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8607
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Backing up non-LFS filesets
Reported Date                 : 9/15/93
Found in Baseline             : 1.0.2
Found Date                    : 9/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[9/15/93 public]
The fts clonesys command cannot be used to back up non-LFS filesets.
The command needs to be updated to show the output that will
result if a user attempts to back up a non-LFS fileset.
Also, the commands to return values must be documented: 0 indicates
that all LFS filesets were successfully backed up, 1 indicates
that one or more LFS fileset backups failed.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/23/93 public]
The following files are affected:
	admin_ref/man8dfs/fts_clone.8dfs
	admin_ref/man8dfs/fts_clonesys.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 8593
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : ubik test
Short Description             : READ_ME incorrect
Reported Date                 : 9/14/93
Found in Baseline             : 1.0.3
Found Date                    : 9/14/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot8593-utst-add-new-test-driver
Transarc Herder               : 

[9/14/93 public]
The READ_ME in the test/file/ubik directory is incomplete and
insufficient. It needs to be updated.

[9/14/93 public]
Diane Delgado claims that Dawn stokes had/has a document which 
describes how to run these tests.  Does anyone have a copy?
Changed Interest List CC from `kulkarni, kissel, kinney' to `kulkarni, kissel, 
 kinney, delgado' 
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/13/93 public]
Defect Closure Form
-------------------
I wrote a new test driver for utst_{client,server}.  The new driver
will run all the commands and will configure and de-configure (from
any machine).  Here's the new syntax:
 [ubik] ./utst_driver -help
        ./utst_driver -help
        ./utst_driver -info (show detailed help information)
        ./utst_driver -init host ...
        ./utst_driver -shutdown
        ./utst_driver -status [host ...]
        ./utst_driver -verify (verify setup of configuration)
        ./utst_driver -procedure (print by-hand procedure)
        ./utst_driver [-trunc | -inc | -qget | -get | -mget ]
I updated the READ_ME file to describe the new driver and I included
another informational file that describes configuring the tests by
hand. 
Associated information:
Tested on TA build:  
	dfs-103-3.35
Tested with backing build:  
	dce1.0.2ab4
Filled in Transarc Deltas with `comer-ot8593-utst-add-new-test-driver' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 8592
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/dirread.c
Short Description             : minor porting changes,
e.g. "#define TRUE true", etc.
Reported Date                 : 9/14/93
Found in Baseline             : 1.0.3
Found Date                    : 9/14/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/dirread.c
Sensitivity                   : public

[9/14/93 public]
I need to make some minor HP-UX specific porting changes to dirread.c.

[9/17/93 public]
Fixed in version 1.1.6.2.

[12/17/93 public]
Closed.



CR Number                     : 8590
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd
Short Description             : Configuring a Fileset Database machine
Reported Date                 : 9/14/93
Found in Baseline             : 1.0.2
Found Date                    : 9/14/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[9/14/93 public]
The documentation that discusses configuration of a Fileset Database
machine fails to mention the need to create a server entry
in the FLDB using the fts crserverentry command.  The documentation
will be updated to include this information.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/16/93 public]
The following file is affected:
	admin_gd/dfs/dfs/2_issues_dfs.gpsml
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified change in latest doc build and closed
this bug.



CR Number                     : 8579
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : The upclient program needs
additional porting changes for HPUX.
Reported Date                 : 9/9/93
Found in Baseline             : 1.0.3
Found Date                    : 9/9/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : update/client.c
Sensitivity                   : public

[9/9/93 public]

The upclient program needs add'l HPUX porting changes.  There are several
routines which don't explicitly return values -- we were seeing random
return status codes returned.  Also, the utime() call for HPUX takes a
different argument than that used by utimes() on other platforms.

[09/13/93 public]

Seems to be ok now.

[12/17/93 public]
Closed.



CR Number                     : 8578
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/file/filewnr.c
Short Description             : filewnr.c won't build for hpux.
Reported Date                 : 9/9/93
Found in Baseline             : 1.0.3
Found Date                    : 9/9/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/systest/file/filewnr.c
Sensitivity                   : public

[9/9/93 public]
Filewnr.c won't build under HPUX.  Minor changes include the addition of a
couple headers, and a couple of #define's.

[9/17/93 public]
Fixed in version 1.1.10.2.

[12/17/93 public]
Closed.



CR Number                     : 8564
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : failure to catch quota/space errors for mapped files
Reported Date                 : 9/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 9/7/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : not_available
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : open

[9/7/93 public]
When efs_pagein gets a request for a writeable page, it must check for a hole
in the file.  If there is a hole in the file, it should make sure the storage
can be allocated.  (If it cannot, it will generally be due to shortage of
quota or of disk space.)
The result will be that if the user stores into a hole in a mapped file, and
there is not enough quota or space to allocate, he will get a SIGBUS right
then.  It's no good to wait until the call to vfs_pageout forces the storage
to be allocated; by then the user process may be long gone.
In Episode, we do not have a function that allocates the storage without
doing the I/O, but we can reserve it.  The following code fragment from the
AIX VM code may be helpful (a similar fragment can be found in the Solaris VM
code):
        offset = dbtob (bp->b_blkno);
        resid = bp->b_bcount;
        efs_getlength (vp, &length, 0, 0);
        if (offset + resid > length) resid = length - offset;
        epia_InitUio (&uio, &iov, offset, resid, bp->b_un.b_addr);
        code = efs_reserve (vp, &uio);
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[3/24/94 public]
Bruce's fix for this was submitted to 1.1a under 9851.  Fix.

[09/27/94 public]
Closed.



CR Number                     : 8550
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : xvolume
Short Description             : allocate spare error codes for license-failure
Reported Date                 : 9/2/93
Found in Baseline             : 1.0.3
Found Date                    : 9/2/93
Severity                      : D
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-ot8550-allocate-license-failure-codes
Transarc Herder               : jaffe@transarc.com

[9/2/93 public]
There are several VOLERR_xxx codes that are currently spares.  One persistent
and one transient code should be allocated to the licensing-failure condition.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/2/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Code compiles.  The new VOLERR_xxxx codes are installed in
.../usr/include/dcedfs/vol_errs.h as appropriate.
Tested on TA build:  dfs-103 3.29
Filled in Transarc Deltas with `cfe-ot8550-allocate-license-failure-codes' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8549
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bakserver
Short Description             : after executing dfs.rm,
bakserver failure attempting to find /opt/dcelocal/var/dfs/backup directory
fills up disk with creds files
Reported Date                 : 9/1/93
Found in Baseline             : 1.0.3
Found Date                    : 9/1/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : bakserver/server.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[9/1/93 public]
Installation of dce/dfs creates the directory
/opt/dcelocal/var/dfs/backup.  If, at any time after the initial configuration
of the node, a decision is made to execute dfs.rm by removing the
configuration through dce_config or by hand executing dfs.rm, this
directory is deleted and never gets re-created.  Therefore when the
bosserver goes to start up the bakserver, the bakserver will die because it
can not find this directory (happens in the call from ubik_ServerInit in
the bakserver main).  The bosserver and bakserver continue this dance of
starting and stopping and all the while create more and more creds files to
the point of eventually filling up the disk.
 
I believe the bakserver needs to be made responsible for checking to see if
this directory exists and if it does not, it should then create the
directory (similar to what the bosserver does for the directories it
requires).  It creates this pathname in the main program which is probably
the best place to correct this problem.  
Another observation that was made while tracking down this problem was that
the bakserver does not verify the existance of the
/opt/dcelocal/var/dfs/adm directory (created by the bosserver) before it
tries to create a log file in there.  This too should probably be corrected.

[9/3/93 public]
Abhijit can take a look at the new 1.0.3 code and make whatever fixes
are required.
Changed H/W Ref Platform from `hppa' to `all' 
Changed S/W Ref Platform from `hpux' to `all' 
Changed Responsible Engr. from `elliot_jaffe@transarc.com' to 
 `khale@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[12/17/93 public]
Closed.



CR Number                     : 8542
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : episode still doesn't link
Reported Date                 : 8/31/93
Found in Baseline             : 1.0.3
Found Date                    : 8/31/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : file/libafs/HPUX/Makefile
Sensitivity                   : public

[8/31/93 public]
The link of the episode components to to build the episode kernel
extension is still disabled.  I am modifying the makefile to
allow it.

[9/7/93 public]
Modified the Makefile and saw that Episode linked ok.



CR Number                     : 8531
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : file/episode/libefs/HPUX
Short Description             : don't know how to make hp-ux.sys
Reported Date                 : 8/30/93
Found in Baseline             : 1.0.3
Found Date                    : 8/30/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : ./src/file/episode/libefs/HPUX/Makefile
Sensitivity                   : public

[8/30/93 public]

[ /file/episode/libefs/HPUX at 21:33 (PM) Sunday ]

makepath HPUX/. && cd HPUX &&  exec make MAKEFILE_PASS=BASIC     build_all
make: don't know how to make ../../../..///u3/devobj/sb/nb_ux/obj/hp800/kernel/\DCE/hp-ux.sys (continuing)

[12/17/93 public]
Closed.



CR Number                     : 8523
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : xvnode
Short Description             : cannot do exclusive create on HP
Reported Date                 : 8/27/93
Found in Baseline             : 1.0.2a
Found Date                    : 8/27/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3
Affected File(s)              : file/xvnode/HPUX/xvfs_os2vfs.c
Sensitivity                   : public
OA
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/27/93 public]
There is a bug in nux_create (file/xvnode/HPUX/xvfs_os2vfs.c) which 
caused low/test8 to fail.
In the last version we retrieved from OSF (7/22), this function ignores its
fourth argument ("flags") and always passes NONEXCL as the fourth argument to
VOPX_CREATE.  A comment says that this is done "just like NFS".  Indeed, the
NFS server always passes NONEXCL to VOP_CREATE.  However, this is not server
code, it is just a set of wrappers for portability, and in any case, our RPC
protocol is not the NFS protocol.  The right thing for nux_create to do is
just to pass its arguments, unaltered, down to VOPX_CREATE.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/15/93 public]
I submitted a change for this several weeks ago ... under OT 8670.

[12/17/93 public]
Closed.



CR Number                     : 8521
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : HPUX interop problems with readdir
Reported Date                 : 8/27/93
Found in Baseline             : 1.0.3
Found Date                    : 8/27/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : kutils/direntry.h episode/vnops/efs_dir.c
Sensitivity                   : public

[8/27/93 public]


There appears to be some interoperability problems betwee RIOS and
HPUX whe doing a readdir operation.

From a RIOS client I did a ls of the current
directory, where the current directory is a UFS filesystem exported
from an HPUX server.

The ls never returns and causes the RIOS to hang.  I have seen this
on serveral occasions. 

Another data point, Although the 486/OSF1 is not an official dfs
reference platform, there are similar readdir problems between
486 clients and HPUX servers.  ls will not hang, but often one
cannot see files via ls although they are there when we do
an ls <filename> or ls will show the folowing:

i         i 


The permissions are set for 755.  there is no episode involved in either
case.  Note that RIOS server with 486 client works perfectly fine.

[09/02/93 public]

I've also reproduced this on the lastest build.  
It appears that there are certain directories which
will manifest this problem, while with others, the
rios client is able to execute ls successfully.

. (So if you -r
SLT ST    PID   PPID   PGRP   UID  EUID  PRI   CPU   EVENT  NAME
  2 r      202     0     0     0     0   127   120          wait
        FLAGS: swapped_in no_swap fixed_pri kproc
0452-220: Cannot read process table entry  35.
0452-220: Cannot read process table entry  68.
 69 r     4593  4d90  3c05     0     0    64     0          sh
        FLAGS: swapped_in
0452-220: Cannot read process table entry  70.
 72 r     4894  4940  4894     0     0    60   120          ls
        FLAGS: swapped_in
 77 r     4d90  3c05  3c05     0     0    64     0          sh
        FLAGS: swapped_in> proc -r
SLT ST    PID   PPID   PGRP   UID  EUID  PRI   CPU   EVENT  NAME
  2 r      202     0     0     0     0   127   120          wait
        FLAGS: swapped_in no_swap fixed_pri kproc
0452-220: Cannot read process table entry  35.
0452-220: Cannot read process table entry  68.
 69 r     4593  4d90  3c05     0     0    64     0          sh
        FLAGS: swapped_in
0452-220: Cannot read process table entry  70.
 72 r     4894  4940  4894     0     0    60   120          ls
        FLAGS: swapped_in
 77 r     4d90  3c05  3c05     0     0    64     0          sh
        FLAGS: swapped_in


> trace 72
STACK TRACE:
        [dfscmfx.ext:cm_readdir] ()
        [dfscore.ext:naix_readdir] ()
        [dfscore.ext:xglue_readdir] ()

>

[09/10/93 public]

BE WARNED:
  HPUX clients running with this change will not be able to interoperate
  (with respect to _readdir operations) with HPUX servers that do not have
  this change.  Update BOTH your clients and servers!

[12/17/93 public]
Closed.



CR Number                     : 8515
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref, users_gdref
Short Description             : Availability of read-only filesets
Reported Date                 : 8/26/93
Found in Baseline             : 1.0.2
Found Date                    : 8/26/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[8/26/93 public]
The documentation related to read-only fileset availability needs to
be updated to reflect the following.  First, if the read-write version
of a fileset (configured for scheduled replication) becomes unavailable,
the read-only versions become unavailable upon expiration of the FailAge;
if the read-only version at the same site as the readpwrite version of
a fileset (configured for release replication) becomes unavailable,
the read-only versions become unavailable upon expiration of the FailAge.
Second, if a Cache Manager is using data from a read-only fileset and a
new version of that read-only fileset becomes available, the Cache
Manager does not attempt to access that new data until the the MaxAge
expires
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/16/93 public]
The following files are affected:
	admin_gd/dfs/dfs/1_overview_dfs.gpsml
	admin_gd/dfs/dfs/2_issues_dfs.gpsml
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_ref/man8dfs/fts_rmsite.8dfs
	admin_ref/man8dfs/fts_release.8dfs
	admin_ref/man8dfs/fts_update.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 8512
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Rationalize fts syncserv and fts syncfldb commands
Reported Date                 : 8/25/93
Found in Baseline             : 1.0.2
Found Date                    : 8/25/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[8/25/93 public]
The fts syncserv and fts syncfldb commands currently have some overlapping
functionality.  In 1.0.3, the commands will be updated so that this
overlap is limited.  The fts syncserv command changes fileset headers
to match their FLDB entries, while the fts syncfldb command changes
the FLDB to match the fileset headers.  The documentation must
be updated to reflect this change.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/1/93 public]
The following files are affected:
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
	admin_ref/man8dfs/fts_suncfldb.8dfs
	admin_ref/man8dfs/fts_suncserv.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[09/07/93 public]
Closed bug.



CR Number                     : 8511
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Connectathon's performance degradesdue to excessive dcp recycling
Reported Date                 : 8/24/93
Found in Baseline             : 1.0.3
Found Date                    : 8/24/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8511-avoid-long-dc-vhash-chains
Transarc Herder               : jaffe@transarc.com

[8/24/93 public]
This bug happened on all three paltforms --- PMAX, HP and RIOS
We found this bug about one year ago on PMAX, but nobody paid any
attention then. Now, it's time.
It's reproducible by running connectathon/basic/test7 alone. The elapsed
time will jump from ~11 to ~14 around 100th run on either HP or RIOS.
The dfstrace dump showed there were excessive dcp recycling. I'll put
one dfstrace in hsiao directory.

[8/25/93 public]
A dfstrace dump has been put in notuser/hsiao directory. From this trace
you will find there are 143 trace lines like the following:
	getdowndslot recycling dcp 249b500
for each of the 100 dcache entries. Note that this trace covers only half
of the duration --- the test7 elapsed time is ~14 seconds at this time.
Another finding is that on both HP and RIOS the elapsed time of test7 
jumped right after run 99(100th run). So, this may suggest something ---
one dcache leak per run?
The following piece from a RIOS shows part of the test7 elapsed times for 
200 test7 runs:
./test7: link and rename
        200 renames and links on 10 files in 11.22 seconds
        ./test7 ok.
1
./test7: link and rename
        200 renames and links on 10 files in 10.52 seconds
        ./test7 ok.
2
./test7: link and rename
        200 renames and links on 10 files in 10.77 seconds
        ./test7 ok.
3
./test7: link and rename
        200 renames and links on 10 files in 11.63 seconds
        ./test7 ok.
4
./test7: link and rename
        200 renames and links on 10 files in 10.51 seconds
        ./test7 ok.
...
95
./test7: link and rename
        200 renames and links on 10 files in 10.74 seconds
        ./test7 ok.
96
./test7: link and rename
        200 renames and links on 10 files in 10.39 seconds
        ./test7 ok.
97
./test7: link and rename
        200 renames and links on 10 files in 10.69 seconds
        ./test7 ok.
98
./test7: link and rename
        200 renames and links on 10 files in 11.81 seconds
        ./test7 ok.
99
./test7: link and rename
        200 renames and links on 10 files in 10.40 seconds
        ./test7 ok.
100
./test7: link and rename
        200 renames and links on 10 files in 13.41 seconds
        ./test7 ok.
101
./test7: link and rename
        200 renames and links on 10 files in 13.5  seconds
        ./test7 ok.
102
./test7: link and rename
        200 renames and links on 10 files in 13.53 seconds
        ./test7 ok.
103
./test7: link and rename
        200 renames and links on 10 files in 13.11 seconds
        ./test7 ok.
104
./test7: link and rename
        200 renames and links on 10 files in 13.26 seconds
        ./test7 ok.
105
./test7: link and rename
        200 renames and links on 10 files in 13.17 seconds
        ./test7 ok.

[8/26/93 public]
I am pretty sure I know what is happening here.  I should have some
information and maybe a fix the the conference call on Monday.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[8/26/93 public]
More findings:
	The answer to why there were so many "dcp recycling" lines in the 
	dfstrace dump file is that the GetDownDSlot function can get called 
	about 14,592 times for each run of connectathon test7.
	Why?
	Note that, after each "cm flushfileset", the GetDownDSlot function 
	got virtually no call at all for each of the first 100 test7 runs.

[8/31/93 public]
Here is a brief explaination of the problem and the fix:
For environments were files and directories are deleted and
then recreated, the hash algorythym for the dcache vhash and dcache
chunk hash chains will tend to migrate all the dcach entries onto
one bucket of the hash chain.  The result is that searches on the
chain for dcache entries will grow longer since the chains are longer.
The connectathon test suite exposes this effect since it continually
deletes and recreates the nfstestdir directory.  When directories are
modified or deleted the DFS client must scan all the dcache entries
for the directory and invalidate them.  Since the hash chains
continue to grow the invalidates take longer and longer to search
the chains for the desired dcache entry.  This effect is amplified
by the fact the each entry must be brought into one of the in memory
dcache slots in order to evaluate it.  This leads to excessive
dcache recycling and further degrades performance.
This fix is to modify the dcache hash functions to include the fid
unique value so that re-incarnated fids (directories and files) 
will be distributed accross the hash buckets.
Context diffs for cm_dcache.h:
< #define	DC_CHASH(v, c) ((((v)->Vnode+(v)->Volume.low+(c))) & (DC_CHASHSIZE-1))
< #define	DC_VHASH(v)    ((((v)->Vnode+(v)->Volume.low )) & (DC_VHASHSIZE-1))
---
> #define	DC_CHASH(v, c) \
>     ((((v)->Vnode+((v)->Volume.low << 2)+(v)->Unique+(c))) & (DC_CHASHSIZE-1))
> #define	DC_VHASH(v)    \
>     ((((v)->Vnode+((v)->Volume.low << 2)+(v)->Unique)) & (DC_VHASHSIZE-1))

[8/31/93 public]
Defect Closure Form
-------------------
The connectathon tests should should steady times now even
after many loops.
Filled in Transarc Deltas with `cburnett-ot8511-avoid-long-dc-vhash-chains'

[8/31/93 public]

[8/31/93 public]
Yes Carl, you did it! The connectathon test7 has run more than 100 times 
without dgradation!

[10/8/93 public]
Re-assigning to xarc since it appears IBM has submitted
a fix there.

[10/11/93 public]
this change is in out 1.0.3 build at Transarc.
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `import'

[12/17/93 public]
Closed.



CR Number                     : 8506
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Multi-threaded programs link
with objects that are not thread safe.
Reported Date                 : 8/24/93
Found in Baseline             : 1.0.3
Found Date                    : 8/24/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : 

[8/24/93 public]
I have listed this as an enhancement, but it could easily be the source of
countless bugs.  In looking through the code I stumbled onto the fact that
a number of multi-threaded programs, e.g. bosserver, are linked with
library code that was not compiled with pthread.h.  For instance, I did a
"nm" on all the lib*.a libraries in file and did a grep for malloc.  The
following libraries had references to both malloc and cma_malloc.  This
means that some of the .o's were compiled with pthread.h and some weren't.
Thus, any multi-threaded program that happens to use one of each has a
potential bug.
./file/tools/cmd/libcmd.a
./file/util/libafsutil.a
./file/osi/libosi.a
./file/security/dfsauth/libdauth.a
./file/security/dacl2acl/libdacl2acl.a
./file/xaggr/libdfstab.a
./file/ftutil/libftutil.a
./file/gtx/libgtx.a
I suspect there are many other system calls in these and other libraries
that have the same problem (I only looked for malloc, not read, printf,
etc.).
As an additional note, the following other libraries also have objects,
some of which call malloc and some of which call cma_malloc:
./time/common/libdpeacl.a
./security/krb5/comerr/libcom_err.a
./directory/gds/ros/Product/Sources/libros.a
./libdce/libdce.a

[8/30/93 public]
Filled in Interest List CC with `kazar@transarc.com' 
Changed Responsible Engr. from `kazar' to `comer' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/28/93 public]
Changed Interest List CC from `kazar@transarc.com' to 
 `kazar@transarc.com,comer@transarc.com' 
Changed Responsible Engr. from `comer' to `khale'

[12/17/93 public]
Closed.



CR Number                     : 8501
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs_config
Short Description             : dfs_config doesn't allow
saving old configuration information upon a new install
Reported Date                 : 8/23/93
Found in Baseline             : 1.0.3
Found Date                    : 8/23/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dfs_config
Sensitivity                   : public

[8/23/93 public]

It seems to be possible to save a previous DCE configuration for re-use
after installing new software, however the same consideration does not seem
to hold for DFS.  Within dce_config in the install_common routine, there is
some provision for checking to see if DCE was previously installed and
configured and if so, the installation process does not wipe out rc.dce.
The install_dfs_common routine inside of dfs_config does not do this
checking and hence the install of new software will wipe out rc.dfs.  This
forces customers to do a CONFIGURE after every new DFS installation.

[10/14/93 public]
Working on it...

[10/15/93 public]
The dfs part is fixed, I'm checking with Tom now to figure out
who'll do the dce_config part.  We apparently don't have the
change that Cindy mentions above in our dce_config (?).

[10/15/93 public]
Added the correct code to dce_config too.

[12/17/93 public]
Closed.



CR Number                     : 8468
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : libdcedfs
Short Description             : Don't build libdcedfs except for RIOS
Reported Date                 : 8/18/93
Found in Baseline             : 1.0.3
Found Date                    : 8/18/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : ./src/Makefile
Sensitivity                   : public

[8/18/93 public]
libdcedfs is only needed by RIOS. Change ./src/Makefile so it is only built
for RIOS.  This will correct a build error on HPUX.  Here's a pseudo diff:

  .if !defined(NO_DFS) || defined(DFS_ONLY)
  BUILD_DFS = file
! .if ${TARGET_OS}!="OSF1"
  BUILD_LIBDCEDFS = libdcedfs
  .endif
  .endif
	
to:
	
  .if !defined(NO_DFS) || defined(DFS_ONLY)
  BUILD_DFS = file
! .if ${TARGET_OS} == "AIX"
  BUILD_LIBDCEDFS = libdcedfs
  .endif

[12/17/93 public]
Closed.



CR Number                     : 8467
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : Document the bakserver command's -verbose option
Reported Date                 : 8/18/93
Found in Baseline             : 1.0.2
Found Date                    : 8/18/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[8/18/93 public]
The bakserver command used to start the Backup Server process now
has a -verbose option.  The option provides debuggin information about
what the Backup Server is doing.  This new option must be documented.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[8/18/93 public]

[8/24/93 public]
Also updated the -verbose descriptions for the flserver, ftserver, repserver,
and upclient processes.
 
Affected files:
admin_ref/man8dfs/bakserver.8dfs
admin_ref/man8dfs/flserver.8dfs
admin_ref/man8dfs/ftserver.8dfs
admin_ref/man8dfs/repserver.8dfs
admin_ref/man8dfs/upclient.8df
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[9/8/93 public]
Closed bug.



CR Number                     : 8466
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Correct replication parameter discussion
Reported Date                 : 8/18/93
Found in Baseline             : 1.0.2
Found Date                    : 8/18/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[8/18/93 public]
The descriptions of the replication parameters associated with the fts
setrepinfo command are incorrect.  The descriptions of the MaxAge and
FailAge parameters need to be updated; they currently pertain to 
scheduled replication only, with no mention of how they work for
release replication.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'
Changed Interest List CC from `dimitris@transarc.com, cfe@transarc.com' to 
 `cfe@transarc.com'

[9/1/93 public]
The following files are affected:
	admin_gd/dfs/dfs/6_ftavail.gpsml
	admin_ref/man8dfs/fts_setrepinfo.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[09/07/93 public]
Closed bug.



CR Number                     : 8462
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref, user_gdref
Short Description             : Remove -cell option from fts crmount command
Reported Date                 : 8/17/93
Found in Baseline             : 1.0.2
Found Date                    : 8/17/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[8/17/93 public]
Remove references to the fts crmount command's cell option and
remove all reference to cellular mount points.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[8/18/93 public]
The following files are affected:
admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
admin_ref/man8dfs/fts_crmount
admin_ref/man8dfs/fts_lsmount
Changed Priority from `2' to `1'

[9/1/93 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[9/8/93 public]
Closed bug.



CR Number                     : 8452
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : ls hangs against a NFS mounted DFS root (/...)
Reported Date                 : 8/13/93
Found in Baseline             : 1.0.2
Found Date                    : 8/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8452-ls-hangs-w-nfs-exported-dfs
Transarc Herder               : jaffe@transarc.com

[8/13/93 public]
Here is the information from our bug database:
abstract      OC:ls hang against a NFS mounted DFS root (/...)
notes:
    <Note by sshi (Shaw-Ben Shi), 93/03/23 16:20:33, action: open>
I mounted /... of blizzard through nfs and was not able to do ls against
the mounted directory. The following log illustrate the problem:

[sshi@kazuko]>mount blizzard:/... /foo

[sshi@kazuko]>cd /foo

[sshi@kazuko]>ls
.... (Never came back)

[sshi@kazuko]>cd dfsfvt.cell.austin.ibm.com
/foo/dfsfvt.cell.austin.ibm.com

[sshi@kazuko]>ls
... (Never came back)

[sshi@kazuko]>cd fs
/foo/dfsfvt.cell.austin.ibm.com/fs
#
# ls works from the fs level
#

[sshi@kazuko]>ls
cmvc@      dfssrc/    local/     proj/      rs_aix32/  service/   u/
I tried the same thing with a NFS OS/2 client. I got the cellname 
displayed repeatedly while I did a "dir" against the mounted DFS
root. 
The problem lies in the way cm_ReadVDir traverses its list of vdirs.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/13/93 public]
Defect Closure Form
-------------------
The ls through an NFS mounted /... no works.
Filled in Transarc Deltas with `cburnett-ot8452-ls-hangs-w-nfs-exported-dfs' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8446
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts aggrinfo broken
Reported Date                 : 8/13/93
Found in Baseline             : 1.0.3
Found Date                    : 8/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : ./src/file/ufsops/ufs_agops.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[8/13/93 public]
fts aggrinfo reports incorrect results for a non-LFS partition on
HP:
$ fts aggrinfo -server /.:/hosts/fire
Non-LFS aggregate /u2 (/dev/dsk/c201d4s0): 4277973844 K free out of total 4278036833 (63570 reserved)

[8/24/93 public]
The problem probably arises from the fact that there is no HPUX conditional
code in ag_ufsStat in ufsops/ufs_agops.c.  Thus no call to VFS_STATVFS ever
gets done and random values get returned.  (You're lucky if you don't get a
panic due to division by zero in the subsequent code.)
Filled in Interest List CC with `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[12/17/93 public]
Closed.



CR Number                     : 8444
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm panics in GetAccessBits durring dump/restore test
Reported Date                 : 8/13/93
Found in Baseline             : 1.0.2
Found Date                    : 8/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8444-cm-getaccessbits-panics
Transarc Herder               : jaffe@transarc.com

[8/13/93 public]
Here is the information from our bug database:
name          6662
abstract      Cache Manager panic during dump and restore
notes:
    <Note by sshi (Shaw-Ben Shi), 93/06/28 15:24:09, action: open>
Cache manager panics while running the backup and restore script. The 
following is the procedure to recreate this problem on sos.
1.Use rgy_edit to change the default ticket life time to a short 
  time period (I used 3 hours while I reproduced this problem 
  last Friday. But I believe it can be shorter.)
2.cd /u/sshi/backup
3.run foo
4.While you see the error message for backup and restore shown up, stop
  the test case. Since only privileged users can run backup and restore, 
  backup and restore will fail whenever the ticket expires. 
5.dce_login as cell_admin 
6.Run foo again. After a while, the following error message will 
  show on the screen:
  dfs: GetAccessBits can't merge status.  Sleeping...
7.After a while, bomb...            
8.A log is saved in /u/sshi/backup/runlog. 
Test case foo is a simple  test case to run fts clone, user  
tasks, dump and restore repeatedly. It will spawn ten word processing
processes (latex), fts clone and fts dump. After all these tasks finish, 
fts restore is run to restore the root file set. The word processor 
binary is stored in dfs. 
Shepherd
--------
    <Note by carlb (Rodney Carlton Burnett), 93/06/29 10:00:54, action: note>
I have isolated the problem and have a fix.  The purpose of 
the retry loop is to sleep and retry hoping to get the number
of outstanding calls down to 1 in order to properly merge
the status and permissions of a file whose ctime has moved
backwards due to the fts restore of data which has since
changed.
The bug is that the retry logic is not releasing its reference
count on the current number of outstanding RPCs on the
volume.  Thus if multiple procesess enter the retry loop
at the sime time, non of them will ever successfully merge
in the status.
The fix is straight forward. I have discussed and confirmed this
diagnosis/fix with Transarc.
    <Note by sshi (Shaw-Ben Shi), 93/07/01 10:06:18, action: note>
I ran the test with the patches I received from Carl. The panic
problem no longer exists. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/13/93 public]
Defect Closure Form
-------------------
The above described test now passes.  This fix was
made and tested on a 102 level DCE and DFS.
Filled in Transarc Deltas with `cburnett-ot8444-cm-getaccessbits-panics' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8439
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref, users_gdref
Short Description             : Remove references to dfsatab file
Reported Date                 : 8/12/93
Found in Baseline             : 1.0.2
Found Date                    : 8/12/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[8/12/93 public]
In DFS 1.0.2, a list of the currently exported aggregates and partitions
is stored in the dcelocal/var/dfs/dfsatab file by the dfsexport command.
In DFS 1.0.3, this list is stored in the DFS kernal.  Therefore,
all references to dfsatab must be removed from the DFS documentation.
The following files are affected:
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
	admin_ref/man4dfs/dfsatab.4dfs
	admin_ref/man8dfs/dfsexport.8dfs
	admin_ref/man8dfs/fts_lsaggr.8dfs
	users_gdref/dfs/u4_share_dfs.gpsml
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[8/12/93 public]
Changed Found in Baseline from `1.0.3' to `1.0.2'

[9/1/93 public]
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[09/07/93 public]
Closed bug.



CR Number                     : 8434
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : build
Short Description             : incomplete DFS build on RIOS
Reported Date                 : 8/11/93
Found in Baseline             : 1.0.3
Found Date                    : 8/11/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/file/Makefile
Sensitivity                   : public

[8/11/93 public]
HP port added the DFS_EXTRAS variable in src/file/Makefile.
This should be for HPUX only, and not affect other platforms.
Currently, it prevents many DFS things from being built on
RIOS.

[8/11/93 public]
Re-enabled missing pieces of RIOS DFS build.  Removed references
to DFS_EXTRAS; it is an unnecessary complication.  Fixed.

[12/17/93 public]
Closed.



CR Number                     : 8431
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm setcachesize does not reduce cache
Reported Date                 : 8/10/93
Found in Baseline             : 1.0.2
Found Date                    : 8/10/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8431-cm_setcachesize-not-reducing-cache
Transarc Herder               : jaffe@transarc.com

[8/10/93 public]
The cm setcachesize command is supposed to reduce the amount of data
in the disk cache so that it is below the new cache size specified.
cm setc is currently not doing this.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/10/93 public]
Defect Closure Form
-------------------
Tested on 102a DCE and DFS code base.
To test:
Use DFS as to populate the client cache somewhat.
Run cm setcachesize and set the cache to a small value (say 500 1k blocks)
Apon completion do a cm getc to see that the cache block usage is
  now below or at the newly specified cache size.
Before the fix the cache would not shrink all the way down to the
  new value.
Filled in Transarc Deltas with 
 `cburnett-ot8431-cm_setcachesize-not-reducing-cache' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8420
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : Remove -bakgroup option from bakserver command
Reported Date                 : 8/6/93
Found in Baseline             : 1.0.2
Found Date                    : 8/6/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[8/6/93 public]
The -bakgroup option of the bakserver command needs to be removed for version
1.0.3.  It is also being removed from the command interface.  The change
affects only the file src/admin_ref/man8dfs/bakserver.8dfs in the Admin Ref.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[8/6/93 public]
Verifying new defect.

[8/24/93 public]
The -bakgroup option is also removed from the butc command.
 
Files affected:
admin_ref/man8dfs/bakserver.8dfs
admin_ref/man8dfs/butc.8dfs
 
This defect can be closed
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Affected File from `1' to `See description' 
Changed Transarc Status from `open' to `closed'

[9/8/93 public]
Closed bug.



CR Number                     : 8415
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fx
Short Description             : SAFS_ReleaseTokens uses wrong icl hool on exit
Reported Date                 : 8/5/93
Found in Baseline             : 1.0.2
Found Date                    : 8/5/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot8415-fix-safs-releasetokens-icl-hook
Transarc Herder               : jaffe@transarc.com

[8/5/93 public]
The function SAFS_ReleaseTokens in the fileserver is using the wrong
ICL trace hook on exit.  This leads to incorrect and confusing output
in the DFS trace logs.
Here is the change:
Bad_ReleaseTokens:
#ifdef FIX
    icl_Trace1(px_iclSetp, PX_TRACE_RELEASETOKENSEND, ICL_TYPE_LONG, errorCode);
#else
    icl_Trace1(px_iclSetp, PX_TRACE_RELEASETOKENS, ICL_TYPE_LONG, errorCode);
#endif
    osi_Free((char *)cookie, sizeof (struct context));
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/5/93 public]
Defect Closure Form
-------------------
Do a dfstrace -dump and verify that the correct trace entry
appears on exit of SAFS_ReleaseTokens.
Filled in Transarc Deltas with 
 `cburnett-ot8415-fix-safs-releasetokens-icl-hook' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8413
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : rmdir inconsistent
Reported Date                 : 8/5/93
Found in Baseline             : 1.0.3
Found Date                    : 8/5/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : unknown
Sensitivity                   : public

[8/5/93 public]

There is a problem with rmdir synchronization between DFS and UFS (and
also between DFS and NFS).  The following summarizes the problem:

Working in a DFS directory with permissions "drwxrwxrwx",

DFS: mkdir foo
UFS: ls		/* directory foo appears */
UFS: rmdir foo	/* no error indication */
UFS: ls		/* but directory foo is still present */
DFS: rmdir foo	/* no error indication */
DFS: ls		/* foo is gone */
UFS: ls		/* foo is gone */

This problem does not appear with files:

DFS: touch foo
UFS: ls foo	/* file foo appears */
UFS: rm foo	/* no error indication */
UFS: ls 	/* file foo is gone */
DFS: ls		/* file foo is gone */

The problem does not happen with symbolic links or device nodes.
I haven't tried other file system object types yet.  Also the
current working directory in the above test was root.dfs.

[09/13/93 public]
Sounds like a DFS problem.

[10/08/93 public]
I know we've had at least one change to readdir on HPUX 
since this bug was filed.  Mike, could you verify this 
bug still exists, and if so, we should collect some more 
debug data.  See me if you need help. Tx.

[10/18/93 public]
Fixes for the "rm -rf bug" reported by Diane Delgado, appear to have
fixed this bug as well.  It is no longer reproducible.



CR Number                     : 8406
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : typos in fts test env variable
Reported Date                 : 8/3/93
Found in Baseline             : 1.0.3
Found Date                    : 8/3/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/test/file/fts/README,src/test/file/fts/test21
Sensitivity                   : public

[8/3/93 public]
UFS_FSNAME_2 variable sometimes refered to as UFS_FS_NAME_2
causing test errors with -ufs switch.

[12/17/93 public]
Closed.



CR Number                     : 8402
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : bos
Short Description             : bos test11 fails
Reported Date                 : 8/2/93
Found in Baseline             : 1.0.3
Found Date                    : 8/2/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/file/bosserver/bossvr_ncs_procs.c
Sensitivity                   : public

[8/2/93 public]
bos test11 fails.  The root cause appears to be that installed
files have a access time of Dec 31, 1969.

[8/2/93 public]
Corrected faulty signature to utime() in HPUX case.  Fixed.

[12/17/93 public]
Closed.



CR Number                     : 8395
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : unconfig
Short Description             : unconfig scripts not ordered correctly
Reported Date                 : 8/2/93
Found in Baseline             : 1.0.3
Found Date                    : 8/2/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/config/dce.unconfig,dfs.unconfig
Sensitivity                   : public

[8/2/93 public]
Errors are returned when unconfigging a client (using dce_config)
that has a DFS server on it.  The errors are caused by attempting
to remove the CDS directory for that client before all the entries
in the directory are removed.  The fix is to delete the DFS
server CDS entries in dfs.unconfig, and modify dce.unconfig to
call dfs.unconfig first.  The directory should then be empty
when dce.unconfig attempts to remove it.

The unconfig scripts are still flawed in that they're not
smart enough to completely unconfig a DCE client that also
is running a DFS server.  For example, the script does not
remove the FLDB entries associated with the machine being 
unconfigged (or the mount points).

[8/2/93 public]
Fixed.  Also added code to dfs.unconfig to return if dfs-server
principal is not found rather than trying to find CDS entries
(for performance).

[12/17/93 public]
Closed.



CR Number                     : 8393
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref, admin_gd
Short Description             : Document DFS udebug command
Reported Date                 : 7/30/93
Found in Baseline             : 1.0.2
Found Date                    : 7/30/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[7/30/93 public]
The DFS udebug command, used to determine the status of and obtain information
about the Ubik database synchronization facility, needs to be documented.  The
manpage for the command will be named admin_ref/man8dfs/udebug.8dfs; text in
the file admin_gd/dfs/dfs/2_issues_dfs.gpsml will also need to be modified to
discuss the command at some level.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/15/93 public]
The following files are affected:
	admin_ref/man8dfs/udebug.8dfs
	admin_gd/dfs/dfs/2_issues_dfs.gpsml
Changed Interest List CC from `jeff@transarc.com, vijay@transarc.com' to 
 `vijay@transarc.com, comer@transarc.com' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Affected File from `udebug.8dfs, 2_issues_dfs.gpsml' to `See 
 description' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 8392
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfsexport -detach of native filesystems failing
Reported Date                 : 7/30/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/30/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ufsops/ufs_volops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-o-ot8392-ufsops-map-neg-one-into-EBUSY-on-detach
Transarc Herder               : 

[7/30/93 public]
In trying to verify that CR 7158 was pmax-specific, I hit the
following:
root@cobbler> cat dfstab
# blkdev aggname aggtype aggid 
/dev/lv01       lfs_root        lfs     1
/dev/lv00       /u1             ufs     2       0,,4
/dev/hd2        /usr            ufs     33      0,,7
root@cobbler> rm /opt/dcelocal/var/dfs/dfsatab
root@cobbler> dfsexport -all
dfsexport: /dev/lv01:lfs_root (id 1):  Already attached: not attached
dfsexport: /dev/lv00:/u1 (id 2):  Already attached: not attached
root@cobbler> dfsexport -aggregate /usr -detach
dfsexport: Revoking tokens for filesets on aggregate 33...
dfsexport: Could not revoke tokens for fileset 0,,7: Token requested was in conflict with another (non-revocable) token (dfs / tkm)
dfsexport: Failed to detach /dev/hd2:/usr (Device busy)
root@cobbler> dfsexport -aggregate /usr -detach -force -verbose
dfsexport: Revoking tokens for filesets on aggregate 33...
(The system then proceeded to be unresponsive - almost as if the local,
non-DFS path to /usr was not available!)
The system did NOT crash but required a reboot since no new logins were
allowed and no current logins were usable.
There was no dfs mountpoint for the cob.usr fileset associated with
aggname /usr, aggrid 33 but a fldb entry had been created first to
get the right fsid (0,,7) for the dfstab.
root@cobbler> fts lsfldb -server cobbler
cob.u1  
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
cobbler.osf.org     RW       /u1     0:00:00 hosts/cobbler  <nil>               
cob.usr  
        readWrite   ID 0,,7  valid
        readOnly    ID 0,,8  invalid
        backup      ID 0,,9  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
cobbler.osf.org     RW       /usr    0:00:00 hosts/cobbler  <nil>               
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
cobbler.osf.org     RW       lfs_root 0:00:00 hosts/cobbler  <nil>               
----------------------
Total FLDB entries that were successfully enumerated: 3 (0 failed)
Why is the aggregate reported "busy"? (NOTE: no mountpoint available
TO BE busy)
Is there some correlation with removing dfsatab?
Is there another option/combination of options instead of -force that
should have been used in this case?

[7/30/93 public]
It's certainly not standard practice to remove dfsatab, and in fact it's
not a good idea.  If you don't remove dfsatab does it still occur?
(Just a thought.  dfsexport might be complaining that it can't get a token,
or it might be getting EBUSY from the AG_DETACH call.  If it were getting
that from AG_DETACH, that would be in system-dependent pmax code.)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/30/93 public]
Hmm, rc.dfs contains a rm of dfsatab and it is recommended in the
admin guide as well. I see similar but not identical behavior on the
pmax but this CR is for the RIOS platform.
Unfortunately, we're even more short-handed on RIOS's than usual so, I
can't try this out on a RIOS again for quite awhile. If it's useful, I
can test on the pmax.

[8/2/93 public]
Filled in Interest List CC with `jaffe@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com'

[8/2/93 public]
Sorry; as Jeff K. reminded me, indeed you remove dfsatab at reboot/startup
time.  But Gail had already exported some aggregates, so removing dfsatab
at that point was clearly unusual.

[9/28/93 public]
Any word on this one?  Can this be cancelled?

[10/8/93 public]
THere doesn't seem to be any activity on this one so I'm giving it back
to Gail.  Please reassign it to me if there is still a problem.
Changed Interest List CC from `jaffe@transarc.com' to `jaffe@transarc.com, 
 comer@transarc.com' 
Changed Responsible Engr. from `comer@transarc.com' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[10/14/93 public]
Here's the scoop:
The current behavior (using a native filesystem on HP):
root@dce13> dfsexport
dfsexport: /dev/dsk/c201d0s0, lfs, 1, 0,,0
dfsexport: /dev/dsk/c201d5s0, ufs, 2, 0,,4
root@dce13> rm /opt/dcelocal/var/dfs/dfsatab
root@dce13> dfsexport -aggregate /u0 -detach
dfsexport: Revoking tokens for filesets on aggregate 2...
dfsexport: Could not revoke tokens for fileset 0,,4: Token requested was in conflict with another (non-revocable) token (dfs / tkm)
dfsexport: Failed to detach /dev/dsk/c201d5s0:/u0 (Device busy)
root@dce13> dfsexport -aggregate /u0 -detach -force
dfsexport: Revoking tokens for filesets on aggregate 2...
dfsexport: Failed to detach /dev/dsk/c201d5s0:/u0 (Unknown code  255 (65535))
SO ... you're still stuck, although not as badly as before, if you remove dfsatab when you
have a native filesystem dfsexported and you later want to detach the native filesystem.
Note that you can not recover from this even when a new dfsatab file exists. There do not
appear to be any of the previous side effects (ie. no logins allowed, immediate reboot req'd).
You can explain it away in docs if you like but it'd be friendlier not to have this liability.
I can't lower the priority due to the aging policy.

[10/15/93 public]
It's possible that you will still be unable to unexport a fileset,
even with the -force option.  What concerns me here is the -1 error
code that is being returned.  I looked through the code with Jeff Prem
and we couldn't find any place where -1 was returned.  I guess we need
to try again to reproduce this problem.
By the way, the dfsatab problem is a red herring.  This file is
written but it is never consulted for anything anymore.  dfsexport now
consults the kernel when enumerating aggregates. 
One last question.  Was there are activity in /u0 prior to starting
DFS? 
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com'

[10/15/93 public]
I'd thought the dependence on dfsatab had been removed but the above notes
implied otherwise - thanks for clarifying.
There had been activity on the /u0 partition before starting DFS on this
machine. Also,the /u0 partition contains the /opt/dcelocal directory, including
the dfs cache. This is not an ideal set up but we're very limited on the HPs.

[10/18/93 public]
I've fixed the place where I believe the -1 error code is coming from.
It will now return EBUSY.  What happens is that on detach, the code
checks to see if there is any activity in the fileset (in
tkc_flushvfsp).  If there is, it cannot detach the fileset and should
return EBUSY.
In general, it is a bad idea to export the partition containing the
DFS cache.  This could be the source of activity that's blocking the
detach.  
Changed H/W Ref Platform from `rs6000' to `all' 
Changed S/W Ref Platform from `aix' to `all' 
Filled in Affected File with `file/ufsops/ufs_volops.c' 
Filled in Transarc Deltas with 
 `comer-o-ot8392-ufsops-map-neg-one-into-EBUSY-on-detach' 
Filled in Transarc Status with `export'

[10/18/93 public]

Just triple-checking, if a UFS partition is dfsexported and there are
files open via their UFS path, you should NOT be able to detach it - is
that correct?

[12/17/93 public]
Closed.



CR Number                     : 8386
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd
Short Description             : Make OSF editorial comments in DFS Admin Guide
Reported Date                 : 7/29/93
Found in Baseline             : 1.0.2
Found Date                    : 7/29/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Description for list.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[7/29/93 public]
This defect covers incorporation of the OSF editorial comments into the DCE
DFS Admin Guide chapters.  The following files of the Admin Guide are all
affected:
 
	admin_gd/dfs/dfs/README
	admin_gd/dfs/dfs/1_overview_dfs.gpsml
	admin_gd/dfs/dfs/2_issues_dfs.gpsml
	admin_gd/dfs/dfs/3_aclgroup_dfs.gpsml
	admin_gd/dfs/dfs/4_adminkey_dfs.gpsml
	admin_gd/dfs/dfs/5_processes_dfs.gpsml
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
	admin_gd/dfs/dfs/8_cachemgr_dfs.gpsml
	admin_gd/dfs/dfs/9_backup_dfs.gpsml
	admin_gd/dfs/dfs/10_backrest_dfs.gpsml
	admin_gd/dfs/dfs/11_scout_dfs.gpsml
 
The following manpage was also affected (a change to Chapter 11 of the DFS
Admin Guide necessitated a corresponding change to this file):
 
	admin_ref/man8dfs/scout.8dfs
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/11/93 public]
All of the files listed previously have been submitted.  We are finished
incorporating the OSF editorial comments.  This defect can now be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Affected File from `See Description' to `See Description for list.' 
Changed Transarc Status from `open' to `closed'

[08/17/93 public]
Closed bug.



CR Number                     : 8381
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : glue
Short Description             : rmdir fails via local path when filesystem is exported to DFS
Reported Date                 : 7/28/93
Found in Baseline             : 1.0.3
Found Date                    : 7/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/file/xvnode/HP800/xvfs_osglue.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[7/28/93 public]
The dfs.glue system test is failing because of a problem
regarding directory removal via the local path.
The following sequence fails via the local path on a filesystem
which has been exported to DFS:
# cd xyz
# ls -ld .
drwxrwxrwx   2 root     sys           24 Jul 28 15:49 .
# mkdir abc
# rmdir abc
# mkdir abc
mkdir: cannot create abc: File exists
# ls -ld
drwxrwxrwx   3 root     sys         1024 Jul 28 15:50 .
# id
uid=0(root) gid=3(sys)
# pwd
/u1/dfs-test/xyz
#
This succeeds when done via DFS and on a local filesystem which is 
NOT exported to DFS.

[8/18/93 public]
The problem is that the test of the result of vol_StartVnodeOp in xglue_rmdir
has the wrong sense:
===================================================================
RCS file: RCS/xvfs_osglue.c,v
retrieving revision 1.3
diff -r1.3 xvfs_osglue.c
79c79
< RCSID ("$Header: /project/ot/dce/d00/d83/RCS/c008381,v 1.6 94/03/15 16:15:58 nata Exp $")
---
> RCSID ("$Header: /project/ot/dce/d00/d83/RCS/c008381,v 1.6 94/03/15 16:15:58 nata Exp $")
786c786
<     if (!(code = vol_StartVnodeOp(volp, VNOP_RMDIR, 0))) {
---
>     if (code = vol_StartVnodeOp(volp, VNOP_RMDIR, 0)) {
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[8/23/93 public]
I believe HP submitted a fix for this one.  HP: please mark bugs
as "fixed" after submitting changes to OSF.

[12/17/93 public]
Closed.



CR Number                     : 8378
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : cache manager
Short Description             : new ticket granting ticket
					     function not linked into
                                             afs_sysent switch table
Reported Date                 : 7/28/93
Found in Baseline             : 1.0.3
Found Date                    : 7/28/93
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : file/libafs/HPUX/cm_config.c
Sensitivity                   : public

[7/28/93 public]

The cm_configure fuction did not link afscall_cm_newtgt into the
afs_sysent table.

[12/17/93 public]
Closed.



CR Number                     : 8375
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm statservers fails on HPUX
Reported Date                 : 7/27/93
Found in Baseline             : 1.0.3
Found Date                    : 7/27/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/file/userInt/cm/cm.c
Sensitivity                   : public

[7/27/93 public]
cm statservers without a cell name argument is unable to figure
the name of the local cell.  I traced the problem to some fauly
pointer arithmetic in the user space cm command, though for the
life of me I can't figure out why this didn't show up previously
on AIX or PMAX.  I've tested a fix on HPUX.  I'll be regression 
testing on the RIOS and submitting RSN.

[7/30/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 8370
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : kutils
Short Description             : fix syscalls.c
Reported Date                 : 7/27/93
Found in Baseline             : 1.0.3
Found Date                    : 7/27/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : kutils/syscall.c
Sensitivity                   : public

[7/27/93 public]

There are a few problems with kutils/syscalls.c which have
been introduced with the HP port.  They are:

a. the declaration for afscall_krpcdbg should not be turned on
   for everyone (i.e. RIOS) 

b. In the routine all_configure, the AFSCALL_KLOAD call should
   be ifdef'd HPUX only.

c. In the config_rtn table, the routine at_configure should be
   ifdef'd HPUX only.

[09/01/93]


Submitted as part of AT386 port

[12/17/93 public]
Closed.



CR Number                     : 8368
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : Need scripts to reflect how 102a stress objectives tested
Reported Date                 : 7/27/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/27/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : test/systest/file/Makefile
Sensitivity                   : public

[7/27/93 public]

The max file, max directory and block-fragment testing for 102a was done
with the following scripts and datafiles which should be in the tree for
reference:
	dfs.maxfile	maxfile.data
	dfs.maxdir	maxdir.data
	dfs.block_frag	block_frag.data
The datafiles are templates - actual datafiles used are in the 102a test 
results area along with the output logs.

[7/27/93 public]
Submission done, status = fixed.

[8/30/93 public]
Build done (many times!) - status = closed.



CR Number                     : 8362
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Need to enforce blocksize >= pagesize
Reported Date                 : 7/26/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/26/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db4099-check-pagesize-in-newaggr
Transarc Herder               : jaffe@transarc.com

[7/26/93 public]
I was able to successfully newaggr a RIOS aggregate using a 1024 blocksize.
The minimum page size for the RIOS is 4k however so this resulted in a
hard crash of the system during the first attempt to write to the aggregate.
Ted Anderson and Carl Burnett are aware of this problem. They have suggested
the fix is to enforce the blocksize >= pagesize rule at the osi layer.

[7/26/93 public]
This needs to be fixed.  Its a B2 since it can be worked around, and causes 
no loss of data.  I'll let tu fix it this summer.
Changed Severity from `A' to `B' 
Changed Priority from `1' to `2' 
Changed Responsible Engr. from `jaffe@transarc.com' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[8/25/93 public]
Ted actually already fixed this one in his delta
ota-db4099-check-pagesize-in-newaggr. This fix is already in 103 code base.
Filled in Transarc Deltas with `ota-db4099-check-pagesize-in-newaggr' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 8337
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : Episode doesn't build
Reported Date                 : 7/22/93
Found in Baseline             : 1.0.3
Found Date                    : 7/22/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : ./src/file/Makefile
					     ./src/file/episode/vnops/Makefile
					     ./src/file/episode/libefs/HP800/Makefile
					     ./src/file/episode/anode/edbasic.c
					     ./src/rep/rep_peer.c
Sensitivity                   : public

[7/22/93 public]
Episode hasn't been ported to HP/UX and is causing many build failures (too
many to list here.)  Details available upon request.

[12/17/93 public]
Closed.



CR Number                     : 8333
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bos
Short Description             : NULL pointer passed as array argument to stub should be a null string
Reported Date                 : 7/21/93
Found in Baseline             : 1.0.3
Found Date                    : 7/21/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : file/userInt/bos/bos_main.c
Sensitivity                   : public

[7/21/93 public]

The bos command passes a NULL pointer as an array argument to the stub
routine BOSSVR_AddSUser (the permsStr paramater).  With the advent
of the new idl compiler this is causing the bos command to core
dump, since the ability passing NULL pointers for array arguments is not
a documented feature.  The arguments should be, instead, null strings.

We are going to review the other areas of dfs looking for similar
problems.

[07/21/93 public]

Fix submitted - there don't appear to be any more of these
in the bosserver.

[12/17/93 public]
Closed.



CR Number                     : 8332
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : readdir() skips entries
Reported Date                 : 7/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/21/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : part of the 12/93 drop from Transarc
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[7/21/93 public]
One of the 102a system test criteria was 5,000 entry directories on
RIOS and PMAX LFS and DFS exported UFS. Using the dirwrite.sh and
dirread dfs system tests, I have discovered odd behavior on PMAX LFS:
40 entries are skipped using readdir() - usually between entries 664 and
704 (test names the files in the order it creates them).
This doesn't appear to be a test problem currently.

[8/3/93 public]
Re-assigning to gmd.  If it can be reproduced on 1.0.3 HP, assign 
it back to me; otherwise cancel.

[8/3/93 public]
This bug has been reproduced at HP using the dfs.maxdir test on both dfsexported
native and lfs filesystems on the HP-UX platform. Runs of the test on
non-dfsexported native filesystems pass. Assigning to Transarc for further investigation.
Joe Wadleigh may be contacted for HP's test results.

[10/18/93 public]
This problem has already been fixed in our code base.  We verified
this by running dirwrite with 5000 entries then running dirread to
verify the results.
Changed Status from `open' to `cancel' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[10/21/93 public]
This problem still occurs.  Transarc agrees, so I am reopening it.

[11/5/93 public]
I am updating this CR to reflect some correspondence between HP and Transarc.
Briefly, there follows:
- a test program (written by Daryl Kinney) that reproduces the bug;
- a high-level description of what exactly is going wrong;
- a high-level description of the fix;
- a discussion of a deeper problem, caused by directory compaction done by
  HP's ufs filesystem, which is not solved by the present fix and cannot
  be solved in the present DFS framework.
I am in the process of testing the fix.  This OT CR will be updated as
appropriate when we are ready to drop our code to the OSF.  Meanwhile HP
is using the same fix internally (it was designed and implemented by Daryl
Kinney).
Test program:
 
#include <unistd.h>
#include <stdio.h>
#include <string.h>
#include <fcntl.h>
#include <dirent.h>
#define NFILES 10
#define MAXFILES 10000
/* Test program to replicate problems with dfs.maxdir test program.
 * Verify that it you can't expect opendir()/readdir() to continue
 * returning consistent results if you're deleting files out from
 * behind it.
 */
void main( int argc, char *argv[] ) {
    int			i, j, fd, dfp, nfiles, errs;
    char		*cp, fname[100], *prefix, c;
    int			states[MAXFILES+1];
    struct dirent	*dirp;
    DIR			*D;
    nfiles = NFILES;
    while ((c = getopt(argc, argv, "n:")) != EOF) {
	switch (c) {
	  case 'n':
	    nfiles = atol(optarg);
	    if (nfiles > MAXFILES) {
		printf("We can only handle %d files ... \n", MAXFILES);
		exit(1);
	    }
	    break;
	  case '?':
	    printf("Make a bunch of files in cur directory\n");
	    printf("Usage: %s [-n #-of-files]\n",
		   argv[0]);
	    exit(1);
	}
    }
    /* ----------------- */
    printf("*** Creating %d files\n", nfiles);
    for (i = 1; i <= nfiles; i++) {
	sprintf(fname, "%d", i);
	if ((fd = creat(fname, 0777)) < 0) {
	    printf("***** %s\n", fname);
	    perror("unable to create");
	    exit(1);
	}
	close(fd);
	states[i] = 0;
    }
    /* ----------------- */
    printf("*** Files created... making sure they all exist\n");
    if ((D = opendir(".")) == NULL) {
	perror("scan phase: opendir failed");
	exit(1);
    }
    while (dirp = readdir(D)) {
	if ((strcmp(dirp->d_name, ".") == 0) ||
	    (strcmp(dirp->d_name, "..") == 0))
	    continue;
	i = -1;
	i = atol(dirp->d_name);
	if ((i <= 0) || (i > nfiles)) {
	    printf("scan phase: Encountered bogus entry: %s\n", dirp->d_name);
	    exit(1);
	}
	states[i] = 1;
    }
    closedir(D);
    for (i = 1; i <= nfiles; i++) {
	if (states[i] != 1) {
	    printf("scan phase: didn't find file %d\n", i);
	    errs++;
	}
    }
    if (errs) exit(1);
    errs = 0;
    /* ----------------- */
    printf("*** Files all exist... now delete them\n");
    if ((D = opendir(".")) == NULL) {
	perror("delete phase: opendir failed");
	exit(1);
    }
    while (dirp = readdir(D)) {
	if ((strcmp(dirp->d_name, ".") == 0) ||
	    (strcmp(dirp->d_name, "..") == 0))
	    continue;
	i = -1;
	i = atol(dirp->d_name);
	if ((i <= 0) || (i > nfiles)) {
	    printf("delete phase: Encountered bogus entry: %s\n", dirp->d_name);
	    exit(1);
	}
	if (states[i] != 1) {
	    printf("delete phase: encountered entry (%d) with phase %d\n",
		   i, states[i]);
	}
	if (unlink(dirp->d_name) < 0) {
	    printf("delete phase: unable to delete %s\n", dirp->d_name);
	    errs++;
	}
	states[i] = 2;
    }
    closedir(D);
    /* Now make sure that the above readdir() loop showed us everything */
    for (i = 1; i <= nfiles; i++) {
	if (states[i] != 2) {
	    printf("delete phase: didn't find file %d\n", i);
	    errs++;
	}
    }
    if (errs) exit(1);
    errs = 0;
    /* ----------------- */
    printf("***Done...\n");
}
End of test program
Notes on test program:  This is compiled by itself, using c89 and
-D_HPUX_SOURCE.  Change your working directory to be in DFS, in a fileset
exported from an HP server.  Give it the argument "-n1000", so that it will
create 1000 files, check them, delete them, etc.  Sample output if the bug
is not fixed:
> /kinney/dirtest -n1000
*** Creating 1000 files
*** Files created... making sure they all exist
*** Files all exist... now delete them
delete phase: didn't find file 671
delete phase: didn't find file 672
delete phase: didn't find file 673
delete phase: didn't find file 674
delete phase: didn't find file 675
delete phase: didn't find file 676
delete phase: didn't find file 677
delete phase: didn't find file 678
Description of the problem:  At the root of the problem is that the offsets
in the directory entries returned by the AFS_Readdir RPC call (the CM calling
the PX) can be wrong if the server is on HP.  On other server platforms,
specifically AIX and Solaris, the offsets are derived from the d_off field
of the directory entries in the format returned by VOPX_READDIR.  But on HP
(and other platforms using an old readdir format, such as OSF/1), the PX does
not have d_off fields to look at and it tries to guess offsets.  The code in
SAFS_Readdir looks like this (taken from Transarc's 1.0.3 code):
#if defined(AFS_AIX31_ENV) || defined(AFS_SUNOS5_ENV)
                newdp->offset = htonl(olddp->offset);
#else
                newdp->offset = htonl(BaseOffset + incount
                                          + olddp->recordlen);
#endif
This just doesn't give a correct result.  If the fileset is Episode, the
guesses are incorrect because they implicitly assume that the successive
entries are contiguous, which they are not.  (Episode's VOPX_READDIR
implementation returns a format that only describes real entries, not the
empty space between them.)  If the fileset is UFS, the first guess may be
incorrect because it assumes that the offset of the first entry is equal to
the offset that was passed to VOPX_READDIR.  That assumption works as long
as there is an entry at the given offset, but if there is not (due to deletions
intervening between successive calls of VOPX_READDIR), a wrong offset is used.
All guesses after the first are off by the same amount.
Since the CM is relying on correct offsets, wrong offsets could result in
either a directory entry appearing more than once, or not at all.  Presumably
we are seeing the latter case.  Tracking this down is complicated considerably
by the buffering done both by the readdir library call and the CM.
Description of the fix:
A change of format is required.  In the fixed code, VOPX_READDIR takes an
extra argument, telling whether it is being called from the PX or called on
behalf of a local access.  If called on behalf of a local access, it returns
directory entries in the native format; if called by the PX, it returns 
directory entries in a possibly expanded format, which is similar to the
native format, but includes an offset field for every entry.
The implementation of this for HP UFS (xufs_readdir) calls VOPN_READDIR
(which is of course ufs_readdir), asking for 1K at a time, aligned on a 1K
boundary.  With this alignment and size, ufs_readdir will fill the buffer
by directly reading from the disk buffer, so offsets can be determined
precisely.  Entries are copied, one at a time, from the buffer filled by
ufs_readdir to the buffer supplied by the caller.  This is slow and cumbersome,
so we can expect some performance degradation compared to the old code.
Episode's efs_readdir must also be modified.  Most of the changes are in
vnd_Read, which already understood two formats, one for use by efs_readdir,
and one for use by fileset dump/restore.  Now there are are three formats.
Further problem:
HP's UFS will compact a directory page (1K bytes).  That is, when an entry is
to be added to a directory, and there is enough space on a page to add it, but
the space is rendered unusable by fragmentation, it will move existing entries
around to consolidate the free space.  This will defeat us and, indeed, will
defeat any program that does getdirentries system calls with buffers that are
less than 1K bytes.

[11/8/93 public]
Bruce is working on this problem.
Changed Responsible Engr. from `jaffe@transarc.com' to `bwl@transarc.com'

[12/9/93 public]
This will not make the 103a release, deferring to 1.1.

[1/5/94 public]
I was able to successfully run dfs.maxdir on an dfsexported hp native
filesystem.  I also ran the Test program written by Daryl Kinney
successfully creating and deleting 5000 files.  We plan to re-execute
dfs.maxdir in a large cell configuration.

[3/7/94 public]
Why is this defect still open?  It was fixed in the code that we submitted
to OSF in late December.  What's the story?

[3/7/94 public]
Its state went from open to defer to open again, following the completion
of 1.0.3a.  If it had been fixed, the OT never reflected that.
See the notes above which show its suspected fix, followed by reopening.

[3/7/94 public]
This isn't telling me what I need to know.  If anyone really believes that
this isn't fixed, where's the evidence?  What happened to the testing effort
reported by cmckeen@osf.org?  I have nothing to go on.

[3/7/94 public]

Sorry Bruce.  We were able to successfully run dfs.maxdir as part of 1.0.3a
testing.  We did not have time to re-run it in a large cell configuration.
I'm closing this CR to reflect the successful runs we did have.  I'll
run it again on 1.1 and open this CR if there are any problems.



CR Number                     : 8330
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Unsatisfied symbols:AFS4Int_v4_0_s_ifspec (data)
Reported Date                 : 7/21/93
Found in Baseline             : 1.0.3
Found Date                    : 7/21/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : fsint/Makefile
Sensitivity                   : public

[7/21/93 public]

[ /file/pxd at 07:58 (AM) Wednesday ]
c89      -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/project/dce/build/nb_ux/export/hp800/usr/shlib   -L/project/dce/build/dce1.0.3i/export/hp800/usr/shlib   -L/project/dce/build/dce1.0.2a/export/hp800/usr/shlib -L/usr/shlib -L/project/dce/build/nb_ux/export/hp800/usr/lib -L/project/dce/build/dce1.0.3i/export/hp800/usr/lib -L/project/dce/build/dce1.0.2a/export/hp800/usr/lib   -o fxd.X pxd.o  -lafsrpcd -lafssys -lnubik -licl -lcmd  -lafsutil -lncompat -ldacl -ldauth -ldacllfs -ldacl  -ldauth -ldacllfs -losi -lcommondata -lcom_err -ldce -lBSD  
/bin/ld: Unsatisfied symbols:
   AFS4Int_v4_0_s_ifspec (data)
*** Error code 1

[07/21/93 public]

fix submitted.  The problem was in the fsint/Makefile in the sed script
which is used to generate afsrpcd_sstub.c.  The sed script was refering
to names like NIDL_epvt, which have changeg to IDL_epvt with the new
IDL compiler.

[12/17/93 public]
Closed.



CR Number                     : 8320
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_ref
Short Description             : Need additional description for dfsd
Reported Date                 : 7/20/93
Found in Baseline             : 1.0.2
Found Date                    : 7/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : admin_ref/man8dfs/dfsd.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[7/20/93 public]
   
This just serves as a reminder to the document group. As a result of the work
for ot7748, there might be additional doc work with regard to 'fxd' to address
the minimum number of blocks required for the CM. 
   
The DFS/OSF group expects to finish the 7748 defect work in the week of
9/6. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/24/93 public]
changed the status from code to doc.
Changed CR in Code, Doc, or Test? from `code' to `doc'

[10/15/93 public]
Documented that minimum value for -blocks is 17 and minumum value for -files
is 2.  Elliot verified the changes.  This one can be closed.
Changed Subcomponent Name from `doc' to `admin_ref' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `admin_ref/man8dfs/dfsd.8dfs' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and
closed this bug.



CR Number                     : 8284
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : dfs.read_write_all does not work on a RIOS cell
Reported Date                 : 7/15/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/15/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : dfs.read_write_all.main
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[7/15/93 public]
The system test dfs.read_write_all is not working on the RIOS. I am having
a problem runnning the rsh command from the script. For instance:
On machine soldier, logged in as cell_admin or root I type and get the follwoing:
rsh vodka -l user_1 dce_login user_1 -dfsuser- -exec /.../tasso_cell/fs/soldier_u1/user_1/one 
rshd: 0826-813 Permission is denied 
It does not execute the script called one. 
However, on the same machine, if I am dce logged in as user_1 and I type the same command, 
the script gets excuted. 
I need to find a way to be able, as root or cell_admin, to run remotely a script that lives 
on a user's home directory which is on DFS.

[7/16/93 public]
Are you sure it's not rshd that's rejecting your request?  I take it
your .rhosts and/or /etc/hosts.equiv is set up right on vodka?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/20/93 ]

Problem fixed. Thanks to Mike Comer for his hints.
In order to be able to run a script that rsh-es into user accounts that live in 
fileset on DFS, the pathname to that directory nee to have the ACL any_other:rx.
In addition, all the /etc/hosts.equiv file need to explicitly state the name of
the user doing the rsh, even if it is root itself. I also found out that in this
during this operation the ~/.hosts is not taken into account.



CR Number                     : 8259
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /test/systest/file
Short Description             : "make" errors
Reported Date                 : 7/13/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/13/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : test/systest/file/Makefile
Sensitivity                   : public

[7/13/93 public]

[ /test/systest/file at 23:48 (PM) Monday ]
makepath file/. && cd file &&  exec make MAKEFILE_PASS=BASIC     build_all
make: don't know how to make dfs.read_write_all (continuing)
make: don't know how to make read_write_all.data (continuing)
`build_all' not remade because of errors.

[7/30/93 public]
Martha Dassarma submitted the updated Makefile for dfs.read_write_all



CR Number                     : 8258
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8230
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : Race condition in throwing data out of cache.
Reported Date                 : 7/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/12/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot8230-sending-dirty-data-loop 1.2
Transarc Herder               : jaffe@transarc.com

[7/12/93 public]
Added field Transarc Deltas with value `tu-o-ot8230-sending-dirty-data-loop 
 1.2' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `export'

[12/17/93 public]
Closed.



CR Number                     : 8244
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : assertion failure line 1653 fixed_anode.c
Reported Date                 : 7/8/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/8/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : none
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[7/8/93 public]
Newaggr'd an existing episode aggregate with the following:
root@valentine> newaggr /dev/rz1a -blocksize 1024 -fragsize 1024 -overwrite
*** /dev/rrz1a ALREADY CONTAINS AN EPISODE FILE SYSTEM
*** CONTINUING
*** Using default initialempty value of 1.
*** Using default number of (1024-byte) blocks: 102399
*** Defaulting to 1023 log blocks                             (maximum of 14 concurrent transactions).
/dev/rrz1a: Marked as not a BSD file system any more.
Done.  /dev/rrz1a is now an Episode aggregate.
Then tried to dfsexport it:
assertion failed: line 1653 , file ../../../../../src/file/episode/anode/fixed_anode.c
The only non-debugger call in the stack shown by kdb is:
	CheckIBFree()

[7/8/93 public]
Assigned to Bwl to diagnose.  If this occurs only on a pmax, then we may
be of limited use.  If necessary, is there a pmax with a network accessable 
debugger that we can telnet into?
Filled in Interest List CC with `jaffe@transarc.com, ota@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/8/93 public]
Certainly - if this occurs only on pmax, I can make one available for you.

[7/9/93 public]
Bruce tells me that gail cannot reproduce the problem.  The particular case
cannot be run on a RIOS, since there is a requirement that the block size be
at least a page, and the RIOS has a 4K page size.  We've tried to reproduce it
here on a RIOS with no luck.  Bruce is trying to understand the code paths
a bit more, with the hope that he will find some insight.  If we can't 
reproduce it or make progress, then this should at the least be moved to 1.0.3

[7/9/93 public]
Not as reproducable as I'd thought, after happening twice, haven't seen it
since. The following from Bruce:
Ted A. has tried to reproduce the problem on a RIOS here, without success.
If you can no longer reproduce it on your PMAX, we may be out of luck.
The panic was in a function called FindBlocks.  (The appearance of CheckIBFree
on the stack may be due to the fact that FindBlocks is static, while
CheckIBFree is not; and CheckIBFree is just before FindBlocks.  In other words,
if the debugger's symbol table had included FindBlocks, that's what it would
have printed instead of CheckIBFree.)
It is surprising to me that dfsexport on an aggregate with no volumes
would be causing a call to FindBlocks.
So my next step will be to run dfsexport on one of our RIOSes and see if and
when it ever calls FindBlocks.  If I learn anything useful from this, fine,
but if I don't, we may be at a dead end.
There are actually several places where dfsexport will call FindBlocks:
(1) FindBlocks
    Map
    IndexToOffset
    epia_Open
    InitLog
    GetGoodAggregate
     etc.
(2) FindBlocks
    epia_Map
    InitLog
     etc.
(3) FindBlocks
    Map
    IndexToOffset
    epia_Open ( ... bitmap ... )
    GetGoodAggregate
     etc.
(4) FindBlocks
    Map
    epix_GetBuffer
    epib_InitAllocationPool
    GetGoodAggregate
     etc.
(5) FindBlocks
    Map
    epix_GetBuffer
    CheckVolumeTablePages
    epit_LastIndex
    ag_efsVolInfo
(6) FindBlocks
    Map
    IndexToOffset
    epia_Open
    vnv_IsAVolume
    ag_efsVolInfo
To me, this is interesting, but I don't see how I can use it to come closer to
a diagnosis.
Gail's experience suggests that reproducing the problem depends on random
factors such as whatever bits happened to be lying around on the disk at the
time of the newaggr.  It may be elusive.

[8/25/93 public]
Gail,
After talking to Bruce, we can not reproduce it in our current code base 
(1.0.3, 3.28) on RIOS. The problem may be pmax specific. 
In addition, the fix in O8T362 could also prevent the user from passing
invalid sizes. 
I assigned this to you so that you can dispose it appropriately.
Changed Interest List CC from `jaffe@transarc.com, ota@transarc.com' to 
 `jaffe@transarc.com, ota@transarc.com, tu, bwl@transarc.com' 
Changed Responsible Engr. from `bwl@transarc.com' to `gmd'

[8/26/93 public]
Since the HP episode port is not available yet, there's not much I can do
with this one either. I'm dropping it to a "2" since it is not easily
reproducable but I don't want to close it until dfs.block_frag is updated
for and passes on the HP. Note that 1024/1024 was NOT an invalid size for
the pmax.

[12/16/93 public]
Assigning to Cindy because she is going to run dfs.block_frag on the HP
- BTW we did verify the fix for the rios that newaggr will NOT allow
a blocksize of less than 4k.

[12/22/93 public]
Successfully ran dfs.block_frag on the HP.



CR Number                     : 8230
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : client-only machine hangs/spings during cho
Reported Date                 : 7/2/93
Found in Baseline             : 1.0.2a
Found Date                    : 7/2/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/cm/cm_scache.c
Sensitivity                   : public
Transarc Deltas               : tu-o-ot8230-sending-dirty-data-loop
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[7/2/93 public]
During cho, machine m4, which is client-only hung after about 22 hours.
We have a crash dump which indicates that some cachemgr thread was
running at the time.  Note that the machine answers pings, but one
can't log into it nor get any response at the command line.
Here's what we have from the crash dump:
> proc -r
SLT ST    PID   PPID   PGRP   UID  EUID  PRI   CPU   EVENT  NAME
  2 r      202     0     0     0     0   127   120          wait  
        FLAGS: swapped_in no_swap fixed_pri kproc
0452-220: Cannot read process table entry  70.
0452-220: Cannot read process table entry  73.
0452-220: Cannot read process table entry  83.
0452-220: Cannot read process table entry  84.
0452-220: Cannot read process table entry  86.
0452-220: Cannot read process table entry  87.
0452-220: Cannot read process table entry  90.
 91 r     5b2d     1  5b2d     0     0    60    67          cmA0  
        FLAGS: swapped_in kproc
0452-220: Cannot read process table entry  97.
.... all other processes aren't readable
> trace 91
STACK TRACE:
        0597cfcc
> od  0597cfcc 100 char
0597cfcc: | \b \? \? \?  ! \? \? \? \? \0  H  0  a \0  8 
0597cfdc: H \0 \? \? \?  A \0 \?  0 \? \0  8 \?  d \0 \? 
0597cfec: |  c  V  p \?  d \0 \?  8  ` \0 \0  , \? \0 \0 
0597cffc: @ \? \? \? \?  a \0  8 \? \? \0  H  | \b \? \? 
0597d00c: 0  ! \0  @  N \? \0    \0 \0 \0 \0 \0 \0     A 
0597d01c:\? \0 \0 \? \0 \0 \0  H \0 \b  o  s  i  _  T  i 
0597d02c: m  e \0 \0 
Not a very good hint I'm afraid.  The plan is for us to try
to get more information and then turn the bug over to
Transarc.

[7/2/93 public]
The cm_MarkbadScache should try to invalidate dirty segament first before
calling forceREturnToken. Otherwise, it could cause a loop. 
I will have a fix here. 
Diane, talk to Andi to get it when it is available. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[06/07/93 public]
I just wanted to verify that we are seeing the same problem that
you are seeing out there at Transarc.  Basically, it looks like
we are looping in RevokeDataToken; does this sound correct?

[7/7/93 public]
Yes, it is the same problem we've seen here. The reason was given above.
Defect Closure Form
-------------------
--Other explanation below--
The fix is very straightforward. Mike and I went thorugh this. It is in
our recent build (carl.14). We are about to start our CHO test for testing
this, among other fixes. 
Elliot will drop this to you probably today.

[7/8/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/cm/cm_scache.c' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `submit'

[12/17/93 public]
Closed.



CR Number                     : 8207
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : xvolume
Short Description             : Fix error exit paths for destroy/detach/close
Reported Date                 : 6/29/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-ot8207-detach-error-cleanup-paths
Transarc Herder               : 

[6/29/93 public]
Some nonsense got left behind after the recent open-for-detach changes.
In vol_Detach(), the path that gets taken if volreg_Delete() fails still drops
vol_vollock even though its not held.  This isn't so bad since volreg_Delete()
doesn't ever fail.  In the VOL_CLOSE case of afscall_volser(), the comment
above the call to vol_Detach() is out of date, and the VOL_RELE() that it
describes should be removed.  vol_VolInactive() now does the work of
putting the struct volume on the free list.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/29/93 public]
Changed Found in Baseline from `1.0.1' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[7/1/93 public]
Defect Closure Form
-------------------
--Other explanation below--
Eyeballing it is about the best I can suggest.
Associated information:
Tested on TA build:  
dfs-103-3.19
Tested with backing build:  
Filled in Transarc Deltas with `jdp-ot8207-detach-error-cleanup-paths' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 8203
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : some of my DFS machines lose access to filesets.
Reported Date                 : 6/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/25/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : dfs.read.write_all.main
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[6/25/93 public]
 
I have created and mounted several filesets to be used by users 
as their home directories. 
When a user logs in, the home directory is for example:
 
$ pwd
/.../dce6_cell.qadce.osf.org/fs/dfsusers/user_4
$ id
uid=10132(user_4) gid=1001(systest)
 
I wrote a test that logs in each user on a machine (PMAX) and executes for 48 
hours a script found in their home directory.  The script is simple, it does
a predetermined set of operations, i.e. mkdir, rm, cp, ln, cc, mail, compress,
uncompress, tar, etc.
 
The problem I have experienced (several times already) is that sometime through 
the test, some of the clients lose access to /:/, therefore the users
login on that client machine lose their home directory, where the script and 
logs live. 
 
When I try to get at the fileset by hand I get the following messages:
 
root@blueberry> cd /:/
/bin/ksh: /:/: The specified directory is not valid.
 
and
 
root@blueberry> id
uid=0(root) gid=1(daemon) groups=0(system),3(kmem),6(mail),10(staff),31(guest),28(operator)
root@blueberry> su user_1
root@blueberry> dce_login user_1 -dfsuser-
root@blueberry> cd
/bin/ksh: /.../dce6_cell.qadce.osf.org/fs/dfsusers/user_1: The specified directory is not valid.
root@blueberry> 
On other machines in the cell, I am able to get login successfully as user_1
and look at the files.
 
On the sick clients I have done:
 
root@blueberry> cm flush
cm: Invalid argument; it is possible that . is not in DFS.
root@blueberry> 
root@blueberry> cm stat
All servers are running.
root@blueberry> cm check
All backup filesets checked.
root@blueberry> 
 
Nothing has helped to get back the lost filesets but to reboot. 
 
My questions are - Is their something else I should be looking at? 
How could I prepare myself and my machines to deal with this problem next 
time I encounter it?  
 
If this bug is a duplicate of another one please make a note of it. I searched 
OT and I did not see anything appropiate.
 
If you need more information about what I reported, let me know and I can 
gather it.

[6/25/93 public]
What is the errno that /bin/ksh is experiencing?  ``The specified directory
is not valid'' is not a useful error message, as it doesn't say what the
errno was.  (It could have been, say, ENOENT, EACCES, ETIMEDOUT, ESTALE, or
the like.)  A program like ``more'' is better at interpreting error codes
to command users.  /bin/csh would do a better job in its ``cd'' command,
also.
 
Does the ACL on /.../dce6_cell.qadce.osf.org/fs/dfsusers/user_1 allow
access by user_1?  Could you show us acl_edit output to that effect?  Klist
output?  How about the ACL on /.../dce6_cell.qadce.osf.org/fs/dfsusers
or /.../dce6_cell.qadce.osf.org/fs?
 
The ICL trace of the cmfx log on the cache manager that's experiencing the
problem would be the most detailed answer.  Do a ``cd'' command that fails
and then get an ICL dump of the cmfx log on that machine.
Filled in Responsible Engr. with `dassarma' 
Filled in Resp. Engr's Company with `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[06/28/93 public]
ONE:
This test reproduced the problem over the weekend. Here is what I
found today.
I had 4 users running a script for 48 hours. I had 3 PMAX and 4 
users.  
On PMAX1 user_1 - aborted with coredump ( I have not figured this one out)
On PMAX2 user_2 - finished the 48 CHO successfully
ON PMAX3 user_3 and user_4 - failed with the problem described in this OT. 
On a window where I had logged as user_4 I typed :
root@blueberry> cd
/bin/ksh: /.../dce6_cell/fs/dfsusers/user_4: The specified directory is not valid.
root@blueberry> more *
*: Connection timed out
root@blueberry> ls 
	  < CORRECT OUPUT>
root@blueberry> pwd
/.../dce6_cell/fs/dfsusers/user_4
root@blueberry> ls -lt
	  < CORRECT OUPUT >
I had similar results with user_3.  
It seems that after I typed the command "more *", I recovered the
user's fileset. About the same time, the file /var/adm/messages logged the
following messages:
Jun 28 09:16:08 blueberry vmunix: dfs: Warning: the rpc call to server 130.105.202.24 in cell dce6_cell timed out.
Jun 28 09:19:08 blueberry vmunix: dfs: dce errors (code 663076868) from the fx server 130.105.202.24 in cell dce6_cell
Jun 28 09:19:09 blueberry last message repeated 2 times
Jun 28 09:19:09 blueberry vmunix: dfs: fx server 130.105.202.24 in cell dce6_cell back up!
The code 663076868 is : 
663076868 (decimal), 2785c004 (hex): Invalid token ID specified (dfs / tkm)
TWO:
Here is the ACL information you asked for:
root@blueberry>  acl_edit /.../dce6_cell/fs -l
# SEC_ACL for /.../dce6_cell/fs:
# Default cell = /.../dce6_cell
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id
root@blueberry> acl_edit /.../dce6_cell/fs/dfsusers -l
# SEC_ACL for /.../dce6_cell/fs/dfsusers:
# Default cell = /.../dce6_cell
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id
root@blueberry> 
root@blueberry> acl_edit /.../dce6_cell/fs/dfsusers/user_4 -l
# SEC_ACL for /.../dce6_cell/fs/dfsusers/user_4:
# Default cell = /.../dce6_cell
mask_obj:r-xcid
user_obj:rwxcid
user:user_4:rwxc--      #effective:r-xc--
group_obj:rwx-id        #effective:r-x-id
other_obj:r-x-id
root@blueberry> 
root@blueberry> klist
DCE Identity Information:
        Warning: Identity information is not certified
        Global Principal: /.../dce6_cell/user_4
        Cell:      0063e3f4-5264-1c2b-bad9-08002b24cb74 /.../dce6_cell
        Principal: 00002794-7027-2c2b-9b00-08002b24cb74 user_4
        Group:     000003e9-6f01-2c2b-9b01-08002b24cb74 systest
        Local Groups:
                000003e9-6f01-2c2b-9b01-08002b24cb74 systest
Identity Info Expires: 93/06/29:17:57:47
Account Expires:       never
Passwd Expires:        never
Kerberos Ticket Information:
Ticket cache: /opt/dcelocal/var/security/creds/dcecred_41000010
Default principal: user_4@dce6_cell
Server: krbtgt/dce6_cell@dce6_cell
        valid 93/06/25:17:57:48 to 93/06/29:17:57:47
Server: dce-ptgt@dce6_cell
        valid 93/06/28:09:15:23 to 93/06/28:11:15:23
Client: dce-ptgt@dce6_cell      Server: krbtgt/dce6_cell@dce6_cell
        valid 93/06/28:09:15:24 to 93/06/28:11:15:23
Client: dce-ptgt@dce6_cell      Server: hosts/dce4/dfs-server@dce6_cell
        valid 93/06/28:09:15:43 to 93/06/28:11:15:23
Client: dce-ptgt@dce6_cell      Server: dce-rgy@dce6_cell
        valid 93/06/28:10:28:22 to 93/06/28:11:15:23
THREE:
I did a dfstrace dump - I am not sure if this is what you meant by 
" ICL trace of the cmfx log on the cache manager". I should say that 
I did not get the dump until about one hour after I did the 
"cd" that gave the error. If you think this trace might be good, I could
mail it to you. Please let me know.

[06/28/93 public]
We need to understand this problem, even if only for 1.0.2a release notes.
So "fixby" --> 1.0.2a unless or until determined to be deferable to 1.0.3.

[8/2/93 public]
It's been a month since there was anmy update on this defect.  Does it
continue to be a problem?  If it is still happening, get a trace by
doing:
	dfstrace clear
	cd (that fails)
	dfstrace dump > /tmp/dump.out

[8/2/93]

I was about to update this CR. 

I have not been able to reproduce this problem on the all RIOS cell running AIX 3.2.4.
This problem was either related to the PMAX load problem, or the network difficulties
experienced on AIX 3.2.3.



CR Number                     : 8188
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : fts test10 confused about crmount/delmount cases
Reported Date                 : 6/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Herder               : andi@transarc.com
Transarc Status               : submit
Transarc Deltas               : andi-o-db3728-fts-test10-confused

[6/21/93 public]
**Description Text**
 Trying to "fts crm" with a -cell arg returns a "pass" for non-zero status,
 but the test still expects to be able to do a delmount on that mountpoint.
 
 Probably we should stop trying to delete something we know isn't there.
 
 This was introduced when the "-cell" arg went out of style.
**Solution Text**
 Changed expected return from deleting nonexistent mount points from
 $DFS_PSDD_EQ_ZERO to $DFS_PASS_EQ_NONZERO.
 
 Delta: andi-o-db3728-fts-test10-confused
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `export'

[6/22/93 public]
Changed Transarc Status from `export' to `import' 
Added field Transarc Deltas with value `andi-o-db3728-fts-test10-confused'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8187
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : FTS test 9 fails when VERB_ARG = "-verbose"
Reported Date                 : 6/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Herder               : andi@transarc.com
Transarc Status               : submit
Transarc Deltas               : andi-o-db3726-test9-chokes-on-verbose

[6/21/93 public]
**Description Text**
 fts delmount is called with $av_arg.
 
 av_arg="$AUTH_ARG $VERB_ARG"    $VERB_ARG="-verbose"
 
 fts delmount chokes on the -verbose switch.
**Solution Text**
 Removed $av_arg from arguments list for fts delmount.
 Delta: andi-o-db3726-test9-chokes-on-verbose
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `export'

[6/21/93 public]

[6/22/93 public]
Changed Transarc Status from `export' to `import' 
Added field Transarc Deltas with value `andi-o-db3726-test9-chokes-on-verbose'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8183
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : potential ambiguity in DFS protocol
Reported Date                 : 6/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/21/93
Severity                      : D
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : cm/cm_{scache.h,subr.c,dcache.c,vnodeops.c},fsint/afs4int.idl, osi/RIOS/osi_port_mach.h,px/px_{intops.c,subr.c}
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-o-ot8183-fix-device-protocol-deficiency
Transarc Herder               : 

[6/21/93 public]
The DFS protocol passes major/minor device information in a single
unsigned32 field (struct afsFetchStatus, struct afsStoreStatus).
To my knowledge, it is unspecified how this field is to be parsed
to derive the major and minor numbers.  This represents an ambiguity
in the protocol which in fact we have fallen into with the reference
platforms.  The RIOS uses 16 bit fields while the PMAX uses only
8 bits.  The result is a device created on one platform will
appear to have different device numbers when accessed from the 
other.
We should arrive at a convention (if there isn't one already) and 
implement that convention identically on both platforms to head off 
interoperability problems should any of the vendors provide support 
for devices as a value add in a 1.0.2a based DFS.  In practice,
this will probably mean 16 bit fields as the convention with the 
PMAX doing the conversion.  I'm interested in feedback from all
on this one, since it is a protocol issue of sorts.

[6/28/93 public]
Assigning to Elliot to get input/fix from DFS gods at Transarc.  
- proposal is to use canonical representation on wire.  What is
  appropriate representation?  PMAX supports 8 bits major/8 bits 
  minor; RIOS and DEC ALPHA use 16/16; HP/UX uses 8/24.  Probably
  want to use full 32 bits for combination of major and minor, 
  whatever the representation.  Can't find a POSIX spec on this.
- what should DFS do when it receives a canonical major/minor
  device in a protocol request that overflows when converted to 
  native O.S. format?  (i.e. major or minor device > 2^8 on the PMAX)?
- Do people think this is important enough to fix in 1.0.2a?

 Changed Interest List CC from `kazar@transarc.com, cfe@transarc.com' to 
 `kazar@transarc.com, cfe@transarc.com, jaffe@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/16/93 public]
Deferred this to 1.0.3.
Here's what we're going to do.  We're going to define the device
number in the afsFetchStatus and afsStoreStatus structures to contain
16 bits of major number and 16 bits of minor number.  In addition, a
spare will be taken that will contain the upper 16 bits of the major
and minor number, supporting 32 bits total for both major and minor.
osi_ routines will be used to convert to/from the canonical format.
Devices that do not map from the canonical representation to a valid
device on the client (e.g. trying to access a device with a minor
number > 2^16 on an HP/UX machine from an AIX machine) will appear as
normal files but will produce failure on any attempt to access.
A 1.0.2-based AIX client will work fine except for major or minor
device numbers greater than 2^16.  In these cases, only the lower 16
bits will be looked at, aliasing the device to another.  This is a
small problem since the devices are really machine-specific anyway
(you wouldn't create such a device for a RIOS) and is not worth reving
the protocol.
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Changed Interest List CC from `kazar@transarc.com, cfe@transarc.com, 
 jaffe@transarc.com' to `kazar@transarc.com, cfe@transarc.com, 
 jaffe@transarc.com, tmm@apollo.hp.com' 
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[8/11/93 public]
Made the changes described above.
Defect Closure Form
-------------------
Tests run between rios (16/16) and a sparc (14/18):
   [sparc] mknod dev1 c 17 17
   [sparc] ls -l dev1
   crw-r--r--   1 root     root      17, 17 Aug  5 09:14 dev1
   
   [rios] ls -l dev1
   crw-r--r--   1 root     system    17, 17 Aug 05 09:14 dev1
   
   [rios] mknod dev2 c 19 19
   [rios] ls -l dev2
   crw-r--r--   1 root     system    19, 19 Aug 05 09:16 dev2
   
   [sparc] ls -l dev2
   crw-r--r--   1 root     root      19, 19 Aug  5 09:16 dev2
   
   [sparc] ls -lL /dev/tty
   crw-rw-rw-   1 root     sys       22,  0 Jun 11 09:52 /dev/tty
   
   [rios] mknod suntty c 22 0
   [rios] ls -l suntty
   crw-r--r--   1 root     system    22,  0 Aug 05 09:17 suntty
   
   [sparc] echo hello > suntty
   hello
   
   [sparc] mknod riostty c 1 0
   [sparc] ls -l riostty
   crw-r--r--   1 root     root       1,  0 Aug  5 10:44 riostty
   
   [rios] ls -l riostty
   crw-r--r--   1 root     system     1,  0 Aug 05 10:44 riostty
   [rios] echo hello > riostty
   riostty: No such device or address.
   [rios] cm setdevok .
   [rios] echo hello > riostty
   hello
   
   # fileset on sparc
   [sparc] mknod bigmin b 14 131072 
   [sparc] ls -l bigmin
   brw-r--r--   1 root     root      14,131072 Aug  5 10:53 bigmin
   
   [rios] ls -l bigmin
   brw-r--r--   1 root     system     0,  0 Aug 05 10:53 bigmin
   [rios] cat < bigmin
   bigmin: No such device or address.
   
   # fileset on sparc
   [rios] mknod bigmaj  c 65535 17
   mknod: A system call received a parameter that is not valid.
   
   # fileset on rios
   [rios] mknod bigmaj c 65535 17
   [rios] ls -l bigmaj
   crw-r--r--   1 root     system   65535, 17 Aug 05 11:01 bigmaj
   
   [sparc] ls -l bigmaj
   crw-r--r--   1 root     root       0,  0 Aug  5 11:01 bigmaj
   
   [sparc] echo hello > bigmaj
   bigmaj: No such device.
   
   # fileset on rios
   [sparc] mknod bigmin c 14 131072
   mknod: Invalid argument
   
Associated information:
Tested on TA build:  
	dce-103-3.25
Filled in Affected File with 
 `cm/cm_{scache.h,subr.c,dcache.c,vnodeops.c},fsint/afs4int.idl, 
 osi/RIOS/osi_port_mach.h,px/px_{intops.c,subr.c}' 
Filled in Transarc Deltas with `comer-o-ot8183-fix-device-protocol-deficiency' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 8182
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : assertion failure in cm_tokens.c, line 2184
Reported Date                 : 6/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/21/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-db3709-cm-panicked-in-cascade-failures
Transarc Herder               : jaffe@transarc.com

[6/21/93 public]
This is a 2 server cell with the latest nb.  Run 2 connectathons to
a single fileset from each of the machines.  Move that fileset
every 10 minutes.  Eventually the machine will hit this assertion failure.
assertion failed line 2184 cm_tokens.c
 
syscall
stat
lstat
namei
xglue_lookup
nosf_lookup
cm_lookup
cm_GetTokens
cm_GetTokensRange
cm_RestoreMoveTokens
 
Here's the code where the assertion fails:
 2178              if (AFS_hsame(volp->volume,  scp->fid.Volume)) {
  2179                  CM_HOLD(scp);
  2180                  lock_ReleaseWrite(&cm_scachelock);
  2181                  lock_ObtainWrite(&scp->llock);
  2182                  if ((scp->states & SC_RO) == 0) {
  2183                      /* Will remove these two asserts eventually */
  2184                      osi_assert(scp->modChunks == 0);
  2185                      osi_assert((scp->m.ModFlags & ~CM_MODACCESS) == 0);

[6/21/93 public]
I can't speak to whether this is an A/0 bug, but it's on Tu's plate.
Filled in Interest List CC with `jaffe@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `jaffe@transarc.com' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/21/93 public]
Diane, 
Before you the system (cm) panicked, did you see that cm had encountered
any comm failure during the moves ? If this is the case then I have the fix and
plan to test it very soon. However, if it is not the case, then please
describe what else your were running at the same time. 
Thanks,
Tu
Filled in Priority with `1'

[06/21/93 public]
There are no messges relating to timeouts on the system consoles; only
messages of the following types:
dfs: fileset 0,,7 is busy with code 691089541 (several of theses)
dfs: fileset 0,,7 moved
followed by assertion failure line 2184 cm_tokens.c
The only things running in the cell were what was described above; i.e.,
a connectathon from each machine running to the same moving fileset, and
the tests log the same log file which also live on that moving fileset.

[7/1/93 public]
This particular problem has been fixed, among other many problems, in 
the delta mentioned. I am in the process of writing test prgrams, testing it.
At end of this week, I should be able to finish running all tests in this
regard. 
Sorry for the late update. 
Filled in Transarc Deltas with `tu-o-db3709-cm-panicked-in-cascade-failures'

[7/8/93 public]
Defect Closure Form
-------------------
--Regression test program below--
For regression test, I have run the cthon, fs and low test all together
in one fileset, while at the sam time moving it. 
--Verification procedure below--
I have run some of the related adverse-filset-move test programs
(adverse_move1.itl, adverse_move1a.itl and adverse_move1b.itl) to verify
the fix and it worked fine. 
Changed Transarc Status from `open' to `export'

[7/9/93 public]
I'de like to move this problem to the 1.0.3 time frame.  We have a delta that
addresses this problem, but there were many changes, and I am loath to chance
regressing the current build.  I've adjusted the fixby field, so please shout
if this is a problem.
Changed Interest List CC from `jaffe@transarc.com, cfe@transarc.com' to 
 `jaffe@transarc.com, cfe@transarc.com, rsarbo, davel' 
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[12/17/93 public]
Closed.



CR Number                     : 8174
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Not cleaning up credential files.
Reported Date                 : 6/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/18/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/ncsubik/ubikclient.c, file/userInt/fts/volc_main.c
Sensitivity                   : public
Transarc Deltas               : fred-db3634-add-ubik_ClientCleanup-call, fred-db3656-add-cleanup-to-fts
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[6/18/93 public]
The version of fts that is currently in the 1.0.2 release does not 
clean up the credential files it creates during its operations.  In our
self-host cell, with 173 filesets, an fts clonesys will eat up 2MB of
data under the /opt/dcelocal/var/security/creds directory.  This command
is run via cron every night.  The delta's
fred-db3634-add-ubik_ClientCleanup-call and fred-db3656-add-cleanup-to-fts 
fix this problem.
The fts clonesys command is how most users will do their DFS backups.  They
will run this command, either manually or automatically, and then use
the DFS backup system to copy the fileset clones to a backup media.  I don't
think they'll be pleased to see 2MB of data disappearing every day.
Added field Transarc Deltas with value 
 `fred-db3634-add-ubik_ClientCleanup-call, fred-db3656-add-cleanup-to-fts' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/18/93 public]
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.2a'

[6/21/93 public]
This is a sev 2 defect since the credentials can be cleaned up by hand.  Thus
the problem while important is not critical/fatal.  I will propose dropping 
this to the OSF today.
Changed Priority from `1' to `2'

[6/22/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/ncsubik/ubikclient.c, 
 file/userInt/fts/volc_main.c' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8173
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fsprobe_test core dumps
Reported Date                 : 6/18/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/18/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jaffe-ot8173-remove-fsprobe_test
Transarc Herder               : jaffe@transarc.com

[6/18/93 public]
The test program fsprobe_test core dumps on both reference platforms
and hardcodes transarc host names in the source.  This should either 
be pulled out of the source base if it is not intended for general
consumption or, preferably, fixed on the reference platforms.
> # ./fsprobe_test
> 
> 
> Test of the fsprobe facility.
> 
> Sockets for the 3 AFS FileServers to be probed:
>          Host transarc.com: IP addr 0xc036e201, port 7000
>          Host vice.transarc.com: IP addr 0xc037cf02, port 7000
>          Host bigbird.transarc.com: IP addr 0x9e620303, port 7000
> Starting up the fsprobe service
> [fsprobe_Init] fsprobe_Results.stats allocated (3084 bytes)
> [fsprobe_Init] fsprobe_Results.probeOK allocated (12 bytes)
> Segmentation fault

[9/13/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Removed the program and the file fsprobe_callback.c since both were
unusable in DFS.  Test the change by building the system cleanly.
You can test the fsprobe library by using scout.
Associated information:
Tested on TA build:  dfs-osf-1.3
Tested with backing build:  dce1.0.3-0831
Added field Transarc Deltas with value `jaffe-ot8173-remove-fsprobe_test' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `export'

[12/17/93 public]
Closed.



CR Number                     : 8165
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : make the mem plumber work on PMAX
Reported Date                 : 6/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : src/file/osi/osi_misc.c
Sensitivity                   : public

[6/16/93 public]
This ot captures the changes I implemented to make the memory 
plumber work on OSF/1:

*** /d/dce1.0.2a/src/file/osi/osi_misc.c        Sat Apr 10 17:09:33 1993
--- osi_misc.c  Wed Jun 16 14:04:45 1993
***************
*** 122,128 ****

  /*static*/ long osi_memUsage = 0;
  /*static*/ long osi_allocCnt = 0;
- /*static*/ long osi_dumping = 0;
  /*static*/ long osi_purges = 0;
  long osi_BuffersAlloced = 0;
  /*static*/ struct osi_buffer *osi_freeBufferList = 0;
--- 122,127 ----
***************
*** 285,292 ****
--- 284,300 ----
      tm->addr = 0;
  #endif /* kernel */
  #else /* AIX */
+ #ifdef AFS_OSF11_ENV
+     /*
+      * This is about as machine specific as you can get.
+      * Look at the assembly to see the stack offset
+      * for the return address vs. the first argument.
+      */
+     tm->addr = *((long *)((char *)&x - (4 * sizeof (x))));
+ #else
      tm->addr =
                *((long *)((char *)&x - sizeof (x)));   /* try to find return ad
dress */
+ #endif        /* OSF/1 */
  #endif        /* AIX */
      return (char *) ((long)tm + sizeof (struct osimem));
  #else /* AFSDEBMEM */
***************
*** 339,345 ****
  #endif /* AFSDEBMEM */

  #ifdef AFSDEBMEM
-     osi_dumping = 1;
      for (tm = osi_memlist; tm; tm = tm->next) {
        val = (long *) ((long)tm + 12);
        printf("block at %x size %d addr %x: ",
--- 347,352 ----
***************
*** 356,363 ****
          printf("%x ", val[4]);

        printf("\n");
      }
-     osi_dumping = 0;
-     osi_Wakeup(&osi_dumping);
  #endif /* AFSDEBMEM */
  }

--- 363,368 ----
***************
*** 374,379 ****
--- 379,385 ----
  #ifdef AFSDEBMEM
      register struct osimem *top;
      register long i;
+     DEFINE_OSI_UERROR

      *retvalP = 0;

[6/16/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 8161
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : must dynamically allocate dslot entries @ high load
Reported Date                 : 6/16/93
Found in Baseline             : 1.0.2
Found Date                    : 6/16/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8161-allocate-dslots-dynamically
Transarc Herder               : jaffe@transarc.com

[6/16/93 public]
At high load, the CM panics if it doens't have as many dslot entries
as processes doing reads/writes.  This is bogus.
The CM should dynamically allocate more dslot entries when there are that many
processes doing DFS syscalls concurrently.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
AIM with 50 users didn't work because we ran out of dslot entries.  This isn't
a leak, since we won't allocate more dslot entries than there are files in the
cache.
Associated information:
Tested on TA build:  
[* 		(e.g. dfs-103-3.16)
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8161-allocate-dslots-dynamically' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8160
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : race between vnm_StopUse and efs_inactive
Reported Date                 : 6/15/93
Found in Baseline             : 1.0.2a
Found Date                    : 06/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Herder               : andi@transarc.com
Transarc Status               : submit
Transarc Deltas               : 

[6/15/93 public]
**Description Text**
 If someone drops the last reference to a vnode while vnm_StopUse is
 phantomizing it, efs_inactive will see the WAITFORME bit set, and will think
 that it has called itself recursively, and will just return.  The end result
 will be a vnode with a reference count of 0 and the VD_PHANTOM bit set.
 Subsequently, the strategy daemon will call vnm_RestartUse, which will call
 epif_Open to open the anode; and later the same strategy daemon will call
 vnm_Rouse, which will also open the anode, causing the first open anode to be
 forgotten.  It's then only a matter of time before someone calls
 epif_ChangeLink to close the anode and gets a panic because its ref count is 2.
 
 The fix (for now) will be that, when vnm_StopUse gets a vnode with a non-zero
 ref count, it will call VN_HOLD to raise the ref count still further; and then
 when it is finished with that vnode, it will call VN_RELE.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `new'

[6/15/93 public]

[6/15/93 public]
Changed Transarc Status from `new' to `import'

[6/21/93 public]
Changed sub-component name to "lfs".
Changed Subcomponent Name from `EPISODE' to `lfs' 
Added field Transarc Deltas with value `'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8156
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Bad contiguous block addr calculation
Reported Date                 : 6/15/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/15/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot8156-block-contiguity
Transarc Herder               : jaffe@transarc.com

[6/15/93 public]
IBM found a bug in the arithmetic that tries to calculate the best disk
block address to request in epia_Strategy:BufWrite.
Stack of strategy daemon is:
epib_Allockate
MakeBlock
AddToGroup
BufWrite
epia_Strategy
efs_strategy
efs_BioDaemon
It panics on line #2389 in block_alloc.c (IBM source tree):
  2388      start = lastBlock+1;
  2389      assert (start >= h->base);          /* don't want to use bogus hints */
  2390      if (start >= h->length+h->base) start = h->base, wait = 0;
r19 has start = FFFFFFFF
r31 has h->base = 1
The anode AllocationRock is 0xFFFFFFFE
The disk anode is:
05D0ECB8   B4002F6C 00000000 00000000 00000040   |../l...........@|
05D0ECC8   00000018 00000000 0000FA04 00000000   |................|
05D0ECD8   00000000 00000000 00000000 00000018   |................|
05D0ECE8   FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF   |................|
05D0ECF8   FFFFFFFF 00000003 000005FF 00000002   |................|
05D0ED08   FFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF   |................|
My hypothesis is that the current operation is writing to logical block
zero of this file.  It is trying to find the best contiguous block and
it does this by subtracting "5" from the first following allocated
block, in this case "3".  Well 3-5 is -2 which is what it has put in
AllocationRock.
I think all that is needed is some bounds checking on this arithmetic in
BufWrite.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/15/93 public]
Changed block_alloc.c to ignore bogus block address hints.
Defect Closure Form
-------------------
--Regression test program below--
New user space test called block-contiguity.test.  This used to fail but
now works.  I've incorporated it into all_tests.
Associated information:
Tested on TA build:  
dfs-carl-1.11
Tested with backing build:  
dce1.0.2ab2
Filled in Transarc Deltas with `ota-ot8156-block-contiguity' 
Changed Transarc Status from `open' to `export'

[6/16/93 public]
Version 1.2 of this delta includes a minor improvement to the test script.
I successfully ran all_tests overnight so this change is further qualified.

[7/23/93 public]
This problem has been ready to submit for some time.  Since it looks highly
unlikely that it will get permission to submit to 1.0.2a, I'm moving it to
1.0.3 where it belongs.
Filled in Priority with `1' 
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[12/17/93 public]
Closed.



CR Number                     : 8153
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkc
Short Description             : tkc can use token before it has been granted
Reported Date                 : 6/14/93
Found in Baseline             : 1.0.2
Found Date                    : 6/14/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8153-tkc-oralltokens-shouldnt-use-tokens-early
Transarc Herder               : jaffe@transarc.com

[6/14/93 public]
Carl found a problem where tkc uses a token before its LHELD flag
is set.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed.
Associated information:
Tested on TA build:  
dfs-103-3.16
Filled in Transarc Deltas with 
 `kazar-ot8153-tkc-oralltokens-shouldnt-use-tokens-early' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8151
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_GetSLock uses uninit'd variable in error path
Reported Date                 : 6/13/93
Found in Baseline             : 1.0.2
Found Date                    : 6/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8151-fix-error-trashing-in-cm-getslock
Transarc Herder               : jaffe@transarc.com

[6/13/93 public]
The cm_GetSLock function can trash its error code because it
references undefined variables in that path (the variable "perms" in
particular).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed.
Associated information:
Tested on TA build:  
dfs-103-3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8151-fix-error-trashing-in-cm-getslock' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8150
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_evalmountpoint can trash mt pt
Reported Date                 : 6/13/93
Found in Baseline             : 1.0.2
Found Date                    : 6/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8150-fix-evalmountpoint-race
Transarc Herder               : jaffe@transarc.com

[6/13/93 public]
There's a race condition in EvalMountPoint that makes it look like the
mount point has been evaluated before it has.  Someone  who passes
through the mount point in this wwindow will see a spurious ENODEV or
ENOENT failure, but it will get better milliseconds later.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed.
Associated information:
Tested on TA build:  
dfs103-3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8150-fix-evalmountpoint-race' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8149
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : yet more cases where we can grant incompatible tokens
Reported Date                 : 6/12/93
Found in Baseline             : 1.0.2
Found Date                    : 6/12/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8149-tkm-fix-wvt-lock-release-problem
Transarc Herder               : jaffe@transarc.com

[6/12/93 public]
The place where we look up what tokens are held on the WVT in order
to determine what optimmistic tokens we can grant releases the
fid lock, which means that we can lett a conflicting grant come iin
at this point.
A simiilar lock release occurs in the async grant code.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Tested as part of IBM system test.
Associated information:
Tested on TA build:  
dfs-103-3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8149-tkm-fix-wvt-lock-release-problem' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8145
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : need to update dfs.glue doc
Reported Date                 : 6/11/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/11/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : src/test/systest/file/README
Sensitivity                   : public

[6/11/93 public]

With the new acl changes, it became apparent that an extra step
is required in dfs.glue setup if root.dfs is in lfs. While I'm
at it, I want to doublecheck that the README is as complete as
the Porting and Testing Guide entry for dfs.glue AND to add the
option of NOT running the cache_mgr functional tests.

[6/16/93 public]
Assigned to gail for possible re-assignment within the systest
group.

[7/7/93 public]
Submitted README with NOTES on RSH requirement and ANY_OTHER acl
requirement. Enhancements to dfs.glue will be done under another
defect.

[7/30/93 public]
Changing from fix to closed state.



CR Number                     : 8132
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bomb
Short Description             : libbomb.a and bomb.h not
installed corectly
Reported Date                 : 6/9/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/9/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/bomb/Makefile
Sensitivity                   : public

[6/9/93 public]
_IDIR lines were not provided for the targets 'bomb.h' and 'libbomb.a' in
src/file/Makefile.  This results in these files being installed to 
	install/<target_machine>/opt/dce1.0/_MISSING_IDIR_

Adding the following lines to the makefile:

bomb.h_IDIR = /share/include/dcedfs/
libbomb.a_IDIR= /usr/lib/

will result in the files being installed to the correct directory.

[6/9/93 public]
Fix submitted today.

[6/10/93 public]
Fix confirmed in nightly build.



CR Number                     : 8118
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs_admin_gdref
Short Description             : Fix doc bugs identified in DCE Release Notes
Release Notes, Section 1.10.3.3
Reported Date                 : 6/9/93
Found in Baseline             : 1.0.2
Found Date                    : 6/9/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1doc
Affected File(s)              : See Description
Sensitivity                   : public
Transarc Status               : closed
Transarc Deltas               : 
Transarc Herder               : 

[6/9/93 public]

[6/9/93 public]
The DFS documentation bugs mentioned in the DCE Release Notes, Section
1.10.3.3, will be fixed in the DCE Administration Guide: Extended Services.

[6/25/93 public]
The documentation defects indicated in the Release Notes apply to all DFS
documents wherever possible.  This defect now covers changes to the DFS
Admin Guide chapters, the DFS Admin Ref reference pages, and the DFS User's
Guide and Ref chapters and reference pages.
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Filled in Subcomponent Name with `admin_gd, admin_ref, users_gdref' 
Changed Short Description from `Doc bugs identified in DCE' to `Doc bugs 
 identified in DCE Release Notes' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[7/30/93 public]
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[10/21/93 public]
Keith and I have gone through the 1.0.2 DFS documentation release notes.  Some
of the notes have been obviated by code changes; some are being moved to the
code portion of the DFS release notes; most have been addressed under other
defect numbers.  Of the 1.0.2 release notes, the following pair remain valid:
 
.ML
.LI
Many commands in the \*Lbos\*O and \*Lfts\*O command suites include a
\*L-localauth\*O option. The option allows you to direct the \*Lbos\*O or
\*Lfts\*O program to use the DFS server principal of the machine on which the
command is issued as the identity of the issuer of the command. Because the
command executes with the identity of the machine, the DFS server principal of
the machine from which the command issued must be included in the necessary
administrative lists; otherwise, the command fails. For example, including the
\*L-localauth\*O option with a \*Lbos\*O command issued from machine \*Lfs1\*O
to machine \*Lfs2\*O succeeds only if the DFS server principal of machine
\*Lfs1\*O is included in the \*Ladmin.bos\*O list on machine \*Lfs2\*O.
.P
To simplify this requirement, create a group that contains all DFS server
principals and include that group in the \*Ladmin.fl\*O, \*Ladmin.bos\*O, and
\*Ladmin.ft\*O lists on all trusted DFS server machines in your cell. If your
cell includes administrative domains, the latter two lists may require special
domain-specific groups.
.LI
As necessary, the documentation correctly indicates when you need to include
double quotes around an argument to a command for the command to execute
properly. In such cases, however, the documentation mistakenly uses directional
quotes, `` '', as examples of double quotes. Legal values are " " or \*C' '\*O.
A command that requires double quotes will not execute properly if you include
the directional quotes currently employed in the documentation.
.LE
 
We are currently preparing the 1.0.3 documentation release notes for forwarding
to OSF; we will include these two remaining release notes in the 1.0.3 notes.
Filled in Interest List CC with `kdu@transarc.com' 
Changed Fix By Baseline from `1.0.3' to `1.1'

[5/6/94 public]
From the 1.0.3 DFS release notes, the note that pertains to displaying
nonexistent Initial Creation ACLs has been incorporated into the text.  The
only file affected was
 
./dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
 
The new text was verified by myself and Rajesh Agarwalla.
 
Changed Interest List CC from `kdu@transarc.com' to `' 
Filled in Affected File with `See Description'

[5/10/94 public]
Included information for the release note about reserved disk space for DCE
LFS aggregates.  The following files were modified:
 
./dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./dce_books/dfs_admin_gdref/gd/ftmgmt.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_aggrinfo.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/newaggr.8dfs

[5/12/94 public]
Included information from the 1.0.3 release notes about the need to manually
remove an existing fileset before using the DFS Backup System to restore it to
a different site.  (I also corrected some existing problems with Backup System
documentation while I was in there.)  The following files were affected:
 
./dce_books/dfs_admin_gdref/gd/backrest.gpsml
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoredisk.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoreft.8dfs

[5/19/94 public]
Two of the original problems from the release notes remain to be fixed:
 
  o Further documentation pertaining to administrative lists and groups
 
  o Correction of the use of double quotes
 
The first problem from the release notes is a duplicate of defect 5780; all
information related to this problem is now included in defect 5780, so defect
8118 no longer covers that issue.
 
The second (and final) problem from the release notes has been corrected.
The following files were modified in response to this defect:
 
./dce_books/dfs_admin_gdref/gd/overview.gpsml
./dce_books/dfs_admin_gdref/gd/issues.gpsml
./dce_books/dfs_admin_gdref/gd/aclgroup.gpsml
./dce_books/dfs_admin_gdref/gd/adminkey.gpsml
./dce_books/dfs_admin_gdref/gd/processes.gpsml
./dce_books/dfs_admin_gdref/gd/ftavail.gpsml
./dce_books/dfs_admin_gdref/gd/ftmgmt.gpsml
./dce_books/dfs_admin_gdref/gd/backup.gpsml
./dce_books/dfs_admin_gdref/gd/backrest.gpsml
./dce_books/dfs_admin_gdref/ref/man4dfs/BosConfig.4dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_adddump.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_addftentry.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_apropos.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_labeltape.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_readlabel.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bak_restoreft.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bos_addkey.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bos_apropos.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bos_create.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/bos_setrestart.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/cm_apropos.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfs_intro.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/dfstrace_apropos.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_apropos.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_dump.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_restore.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/fts_zap.8dfs
./dce_books/dfs_admin_gdref/ref/man8dfs/salvage.8dfs
 
The changes required straightforward editorial correction.  I verified all of
the changes myself.  This defect can mercifully be closed.
 
Changed Subcomponent Name from `admin_gd, admin_ref, users_gdref' to 
 `dfs_admin_gdref' 
Changed Short Description from `Doc bugs identified in DCE Release Notes' to 
 `Fix doc bugs identified in DCE Release Notes' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.1' 
Changed Transarc Status from `open' to `closed'

[10/20/94 public]
Closed bug.



CR Number                     : 8112
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkc
Short Description             : tkc has token granting race condition
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2
Found Date                    : 6/7/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8112-fix-tkc-racing-revoke-code-locking-problem
Transarc Herder               : jaffe@transarc.com

[6/7/93 public]
This should be fixed for IBM's release.  Insufficient locks are held when
handling the tkc token revoke race condition path.
If a thread getting a token blocks trying to get the tokenID hash table
lock, while trying to hash in a just-returned token, then if a revoke occurs,
the revoke will fail to find the token in the tokenID hash table (it hasn't been
added yet), and will call the code to register the racing revoke call.  The
registration of the race is done w/o holding the vcache entry's lock, so it
will execute immediately.  However, the EndRacingRevoke call has already been
done by the first thread, and it is holding only the vcache entry lock.  So,
there is no concurrent token granting call visible to tkm_EndRacingRevoke, and
it does nothing.
Fix is to grab the vcache lock when queueing a revoke.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed.
Associated information:
Tested on TA build:  
dfs-103-3.116
Tested with backing build:  
Filled in Transarc Deltas with 
 `kazar-ot8112-fix-tkc-racing-revoke-code-locking-problem' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8107
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Volume needs to be quiesced before detach
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2
Found Date                    : 6/7/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : jdp-ot8107-quiesce-volume-before-detaching
Transarc Status               : submit
Transarc Herder               : 

[6/7/93 public]
An excerpt from the ORBIT defect (6405) appears below:
>    <Note by pehkonen (Jean E. Pehkonen), 93/06/06 17:50:16, action: open>
>
>I was running connectathons from 2 different clients in the cell when I
>attempted to do a dfsexport -detach on the aggregate which contains root.dfs
>(The connectathons were running against directories in root.dfs, also).  One
>of the clients was on test4; the other on test5 when I attempted the dfsexport.
>
>dfsexport hung with the following stack:
>vms_delete
>vnvm_recycle
>vnm_Inuse
>vol_efsDetach
>vol_Detach
>ag_UnRegisterVolume
>afscall_aggr
>kafs_syscall
>
>The stacks for the Episode daemon processes were:
>
>e_wait
>e_sleep
>osi_aix_Sleep
>Lock_Obtain
>vol_efsHold
>volreg_Lookup
>efs_getvolume
>efs_BioDaemon
>efs_vmm_proc
>procentry
The problem is that vol_efsDetach() calls vnm_Inuse() while holding
volp->v_lock.  vnm_Inuse() is waiting for the strategy daemon to run, which
in turn tries to obtain volp->v_lock.  Deadlock.
This deadlock is related to a higher-level, more serious problem.  In
particular, there is no code to wait for existing vnode ops to finish and
to keep new ones from starting.  The upshot is that detaching an aggregate
while there is activity in any of its filesets may lead to data loss and/or
panics.
The proposed fix is to call (internally) vol_open() before proceeding with
the detach.  This will have the desired effect of waiting for existing vnode
ops to finish while keeping new ones from starting.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[6/8/93 public]
Our testing group hit something similar. They were trying to detach an
aggregate before shutting down a machine. The stack for dfsexport is:
epig_CloseAggregate
ag_efsDetach
ag_PutAggr
afscall_aggr
kafs_syscall
The Episode daemon showed:
e_sleep
osi_Wakeup
osi_SleepWI
epia_Async
efs_vmm_proc
procentry
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[6/11/93 public]
Added field Transarc Herder with value `'

[6/11/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Generate some load in one or more filesets that are all contained in a
particular aggregate, and then detach the aggregate while the load is running.
You will need to use the "-force" flag to "dfsexport -detach".
Associated information:
Tested on TA build:  
dfs-carl-1.11
Tested with backing build:  
dce1.0.2ab2
Filled in Transarc Deltas with `jdp-ot8107-quiesce-volume-before-detaching' 
Changed Transarc Status from `open' to `export'

[6/15/93 public]
Changed Transarc Status from `export' to `import'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8106
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Panic during fts move in Episode
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/7/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-db3765-no-backing-up-in-copyacl
Transarc Herder               : jaffe@transarc.com

[6/7/93 public]
The following excerpt from mail describes the problem:
Date: Thu, 3 Jun 1993 16:44:29 -0500
From: dstokes@sunlight.austin.ibm.com
Well, I've recreated this a couple of times now, on
two different machines.  I started running the move
-verbose, so I can tell how far it is getting.  It
is panicing during the dump of the incremental:
# fts move dce.build.xxx blizzard epi3 blizzard epi4 -verbose
Cloning fileset 0,,695 to 0,,698 (name dce.build.xxx.move-temp, Reclone=0)
Creating token keep-alive thread.
Revocation socket is: inet/129.35.67.212/1191
Creating RPC listener thread.
About to get token 404 for fileset 0,,695 from blizzard.austin.ibm.com.
Listening for net calls (calling rpc_server_listen)
Connecting to file server on blizzard.austin.ibm.com...
STKN_InitTokenState called with flags of 0
...connection done (result 0)
Token 739139029,,12890, type 0x404, obtained on fileset 0,,695.
Releasing token (1) for fileset 0,,695 from blizzard.austin.ibm.com.
Releasing token on file server...done.
Creating dest fileset 0,,699...done
Creating tag keep-alive thread.
Dumping from clone 0,,698 (src) to fileset 0,,699 (dest)...done
Deleting the clone 0,,698...done
About to get token 404 for fileset 0,,695 from blizzard.austin.ibm.com.
Connecting to file server on blizzard.austin.ibm.com...
STKN_InitTokenState called with flags of 0
...connection done (result 0)
Token 739139029,,12891, type 0x404, obtained on fileset 0,,695.
Dumping from original 0,,695 (src) (incr from VV 0,,3568) to fileset 0,,699 (des
t)...
=BOOM=
Its interesting that ftserver thinks it needs to do an incremental,
give that there is no activity on the fileset while the move is
happening.  I assume episode must be doing some cleaning up after
the copy or something??
Anyway, here is what I did to recreate it:
1) fts create
2) fts crm
3) acl_edit -ic on the mount point
   sec_acl_edit> m any_other:rx
4) acl_edit -io on the mount point
   sec_acl_edit> m any_other:rx
5) rc.afs
6) klog
7) cp -r from dce build into the new fileset until
   Disk quota exceeded message starts showing up.
   ^C out of the cp.
8) fts lsq
   It is usually between 98-100% full.
9) fts move from aggra to aggrb on the same server.
I don't think AFS has anything to do with it, but I've
tried copying off the local disk and out of DFS, and
didn't hit the problem.  Maybe the types of files in
a DCE build contribute.  AFS is the only place I can
copy a build from.  It copies the export tree first,
so is getting lots of little files (headers) and then
starts in on the libraries before the fileset fills up.
Would a metamucil of the aggregates after the failure
be helpful??  I noticed when I came up, salvage complained
about the destination:
Salvaging /dev/lv11
Will run recovery on /dev/lv11
recovery statistics:
        Elapsed time was 5713 ms
        289 log pages recovered consisting of 10794 records
        Modified 33 data blocks
        8201 redo-data records, 3 redo-fill records
        0 undo-data records, 0 undo-fill records
Ran recovery on dev 1/1
In volume dce.build.21e 0,,683 (avl #5)
  Volume is marked as inconsistent, not walked
Processed 1 vols 513 anodes 0 dirs 0 files 0 acls
Done.  /dev/rlv11 checks out as Episode aggregate.
The original fileset does come back online and the
data appears to all be there.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/23/93 public]
Not a test defect.
Changed CR in Code, Doc, or Test? from `test' to `code'

[7/7/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
See Dawn's procedure given above.
Associated information:
Tested on TA build:  
dfs-carl-1.11
Filled in Reported by Company's Ref. Number with `(Sybase) 3765' 
Filled in Transarc Deltas with `cfe-db3765-no-backing-up-in-copyacl' 
Changed Transarc Status from `open' to `export'

[7/23/93 public]
Filled in Priority with `1' 
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[12/17/93 public]
Closed.



CR Number                     : 8105
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fshost
Short Description             : fshs_RevokeToken messes up return parm
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2
Found Date                    : 6/7/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8105-fix-revoke-token-loop
Transarc Herder               : jaffe@transarc.com

[6/7/93 public]
fshs_RevokeToken can screw up its return parameters when it is asked to
revoke a lot of tokens from a single host in once shot.
This is probably pretty minor, since it is probably pretty rare that enough
tokens are revoked at one time.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed.
Associated information:
Tested on TA build:  
dfs-103-3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8105-fix-revoke-token-loop' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8102
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : CM_TRWAITING flag is obsolete
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2
Found Date                    : 6/7/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8102-remove-obsolete-trwaiting-flag
Transarc Herder               : jaffe@transarc.com

[6/7/93 public]
Remove all references to the CM_TRWAITING flag, since it is now obsolete.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed in 103 development.
Associated information:
Tested on TA build:  
dfs-103-3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8102-remove-obsolete-trwaiting-flag' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8100
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : rpc call timeout too low
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2
Found Date                    : 6/7/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8100-increase-rpc-call-timeout
Transarc Herder               : jaffe@transarc.com

[6/7/93 public]
The RPC call timeout (not com timeout) is too high for the DFS client;
really, it shouldn't be used at alll, but we'll set it to a pretty conservative
number.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed in 103 development.
Associated information:
Tested on TA build:  
dfs-103-3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8100-increase-rpc-call-timeout' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8097
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : px
Short Description             : missing gluevn calls from setattr calls
Reported Date                 : 6/5/93
Found in Baseline             : 1.0.2
Found Date                    : 6/5/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8097-px-add-gluevn-wrappers-to-setattr-calls
Transarc Herder               : jaffe@transarc.com

[6/5/93 public]
The code in postsetexistingstatus forgets to call setattr with the gluevn
guard.  This means that VM integrated systems can deadlock getting tokens
from the glue code invoked by VOP_GETPAGE / VOP_STRATEGY / whatever.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed in 103 dev.
Associated information:
Tested on TA build:  
103b16
Tested with backing build:  
Filled in Transarc Deltas with 
 `kazar-ot8097-px-add-gluevn-wrappers-to-setattr-calls' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8096
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_open can lose open token in race case
Reported Date                 : 6/5/93
Found in Baseline             : 1.0.2
Found Date                    : 6/5/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8096-fix-superfluous-getslock-call
Transarc Herder               : jaffe@transarc.com

[6/5/93 public]
cm_open can lose open token in race case, since it calls getslock spuriously
(left over merge conflict?), before bumping the open count.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed in 103 dev.
Associated information:
Tested on TA build:  
103 3.16
Filled in Transarc Deltas with `kazar-ot8096-fix-superfluous-getslock-call' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8095
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : tkm can GC locked tokens
Reported Date                 : 6/5/93
Found in Baseline             : 1.0.2
Found Date                    : 6/5/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8095-watch-for-locked-tokens-in-tkm-gcfid
Transarc Herder               : jaffe@transarc.com

[6/5/93 public]
The tkm GC function doesn't watch for locked tokens.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed in 103.
--Verification procedure below--
--Other explanation below--
Associated information:
Tested on TA build:  
3.16
Tested with backing build:  
Filled in Transarc Deltas with 
 `kazar-ot8095-watch-for-locked-tokens-in-tkm-gcfid' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8088
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : px
Short Description             : px can fail to set atime/mtime properly
Reported Date                 : 6/3/93
Found in Baseline             : 1.0.2
Found Date                    : 6/3/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[6/3/93 public]
When setting atime and mtime as part of store data, px_PostSetExistingStatus
forgets to clear the bits telling the main setattr call to set the atime / mtime.
This generates spurious protection failures on certain platforms.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/6/93 public]
Fixed in 103.
Associated information:
Tested on TA build:  
103 3.16
Tested with backing build:  
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8087
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : libdce didn't build
Reported Date                 : 6/3/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/3/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/security/dacl2acl/Makefile
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/3/93 public]
Build failed to to "Error Code 8" while attempting to build libdce
problem stems from
"../../../../src/rpc/runtime/comnaf.c", line 902.42: 1506-016 (E) Operands must be pointers to compatible types.
 which is related to (via includes) to ./rpc/runtime/krbclt.c

[6/3/93 public]
I changed krbclt.c which compiles as its own object file.  I don't believe that
comnaf.c includes other .c files.  I doubt that this problem was caused by
removing an if clause in krbclt.c.  Can you please provide a build log, or
more detailed information to deduce the cause of the problem?
Filled in Interest List CC with `jaffe@transarc.com' 
Changed Responsible Engr. from `jaffe' to `paananen' 
Changed Resp. Engr's Company from `tarc' to `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/3/93 public]
Here's a little more information on the problem.  It seems that a reference
is being made to the symbol "osi_Alloc" which is no longer to be found in
libdce.
Here are excerpts from the build logs:
PMAX:
project/dce/build/nb_pmax/tools/pmax/macho/ld 
	-R 
	-export_default libdce:
	-L/project/dce/build/nb_pmax/export/pmax/usr/shlib 
	-L/usr/shlib
	-L/project/dce/build/nb_pmax/export/pmax/usr/lib 
	-L/project/osc/build/osc1.1.1/export/pmax/usr/ccs/lib 
	-o 
	libdce.so.X libdce_1.o libdce_2.o libdce_3.o libdce_4.o  
	-lc -lbsd   
libdce_2.o: Undefined symbol "osi_Alloc" referenced
*** Error code 1 (continuing)
`build_all' not remade because of errors.
RIOS:
ld 
	-o shr.o *.o 
	-bloadmap:shr.o.map 
	-bM:sre 
	-bE:libdce.syms
	-bI:/lib/syscalls.exp 
	-bI:../../../src/libdce/RIOS/syms.imp 
	-L/u1/devobj/sb/nb_rios/export/rios/usr/lib 
	-T512 -H512 -lbsd -lc -lm -lcurses -lkrb5 -lslog 
	-lgda_util -lafssys -ldacl2acl 2> shr.o.error
.osi_Alloc
*** Error code 8 (continuing)
`build_all' not remade because of errors.
The complete build logs are in
/afs/dce/project/dce/build/dce1.0.2a-0601/logs
Note that everything else in the nightly build results area
(dce1.0.2a-0602) is from the previous night's build.

[6/3/93 public]
There was a missing CFLAG argument in the dacl2acl Makefile which caused
that version of epi_id.o to use osi_Alloc instead of malloc.  
Changed Status from `open' to `fix' 
Changed Fix By Baseline from `1.0.3' to `1.0.2a' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/security/dacl2acl/Makefile'

[6/4/93 public]
 Problem still exists on PMAX
Changed Status from `fix' to `open'

[ /libdce at 19:25 (PM) Thursday ]
/project/dce/build/nb_pmax/tools/pmax/macho/ld 
  -R 
  -export_default libdce:          
  -L/project/dce/build/nb_pmax/export/pmax/usr/shlib 
  -L/usr/shlib 
  -L/project/dce/build/nb_pmax/export/pmax/usr/lib
  -L/project/osc/build/osc1.1.1/export/pmax/usr/ccs/lib 
  -o libdce.so.X libdce_1.o libdce_2.o libdce_3.o libdce_4.o  
  -lc 
  -lbsd   
libdce_2.o: Undefined symbol "osi_Alloc" referenced

reference the file in /u1/devobj/sb/nb_pmax/src/file/security

dfssec_errorStrings.c:  rtnValP = (char *)osi_Alloc(sizeof(dce_error_string_t));


PMAX libdce didn't build with Elliot's fix because the only change was
a Makefile (which because we are doing incrementals Build didn't detect
any change and no new objs were generated.) I removed 
./security/dacl2acl/epi_id.o
./security/dacl_lfs/epi_id.o
./security/dacl_lfs.klib/epi_id.o
from the the ./obj/pmax/file tree to force a rebuild and a new libdce
to be created.

[12/17/93 public]
Closed.



CR Number                     : 8084
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : admin_ref, admin_gd
Short Description             : fts crserverentry wrong flag descr
Reported Date                 : 6/2/93
Found in Baseline             : 1.0.2
Found Date                    : 6/2/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[6/2/93 public]
On the man page for 'fts crserverentry', the following description
is wrong:
-principal name
        Specifies the abbreviation for the DFS server principal
        to be registered in the FLDB for the machine.  The machine's
        principal name in the Registry Database must match this
        name.
While coding the mkdfs command, we tried two different Registry
principals, neither of which worked:  'hosts/hostname/self' and
'hosts/hostname/dfs-server'.  What did work was work (and this
is what dce_config uses) was 'hosts/hostname', which is not a
Registry principal.
I don't know what is really meant here--I'll leave that to
Transarc or the DFS team to puzzle out.

[06/03/93 public]
Assigned to writer Jeff Kaminski at Transarc.

[10/13/93 public]
Filled in Interest List CC with `jeff@transarc.com, comer@transarc.com' 
Changed Responsible Engr. from `jeff@transarc.com' to `kdu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `'

[10/14/93 public]
The following files are affected:
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_ref/man8dfs/fts_crserverentry.8dfs
	admin_ref/man8dfs/fts_edserverentry.8dfs
Changed Subcomponent Name from `admin ref' to `admin_ref, admin_gd' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `See description' 
Filled in Transarc Status with `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 8082
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fix AIX compiler problems
Reported Date                 : 6/1/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/1/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/episode/anode/anode.c,file/episode/anode/strategy.c,file/episode/vnops/efs_vnodeops.c, file/export/RIOS/Makefile
Sensitivity                   : public
Transarc Deltas               : jaffe-add-alignment-to-export.ext, jaffe-fix-carl-1.11-aix-compiler-bug
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[6/1/93 public]
the AIX 3.2.3 compiler has problems with the following files:
file/episode/anode/anode.c
file/episode/anode/strategy.c
file/episode/vnops/efs_vnodeops.c
In addition, the dfsexport linkage line is missing a -H8 to force the 
library to double word alignment.  All of these changes are necessary for
AIX 3.2.3 builds.
Added field Transarc Deltas with value `jaffe-add-alignment-to-export.ext, 
 jaffe-fix-carl-1.11-aix-compiler-bug' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/1/93 public]
This was tested in dfs-carl-1.11 against dce1.0.2a2b2
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with 
 `file/episode/anode/anode.c,file/episode/anode/strategy.c,file/episode/vnops/ef
 s_vnodeops.c, file/export/RIOS/Makefile' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8080
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : Data inconsistency --- DFS clients didn't get updated data
Reported Date                 : 6/1/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/1/93
Severity                      : B
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/tkm/tkm_fidHash.[ch]
Sensitivity                   : public
Transarc Deltas               : kazar-ot8080-fix-tkm-race 1.1
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[6/1/93 public]
Test config: 	three PMAXes
	santa --- DFS server and DFS client
	jolt --- DFS client
	dce10 --- DFS client
Test scenarios:
	on santa --- 
		1. iostat 5 > /:/io.dat(via DFS) 
 
		   The "iostat 5" command updates io.dat every five
	 	   seconds.
		2. tail -f /u1/io.dat(via UFS)
	on jolt --- 
		1. one copy of connectathon
		2. tail -f /:/io.dat
	on dce10 --- 
		1. one copy of connectathon
		2. tail -f /:/io.dat
Data inconsistency --- Both jolt and dce10 didn't get updated data.
	After few minutes of testing the two "tail -f /:/io.dat" screens
	from the two clients didn't have updates of the io.dat file for at 
	least 15 minutes. Normally, the tails sync with "iostat 5" --- 
	updates screen every five seconds. 
	Starting another "tail -f /:/io.dat" from either jolt or dce10
	updated the two hanging "tail -f" screens right away. Without 
	doing so we have no idea when the two tails will get updated.

[6/1/93 public]
Change to fixby 1.0.2a.

[6/2/93 public]
Changed Responsible Engr. from `jaffe' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/2/93 public]
A continuous dfstrace is in notuser/hsiao/dfstrace.iostat.5.dump. This
particular dfstrace was started when I discovered that a "tail -f /:/io.dat" 
session on "dce10" hang. Then, to release hang, another "tail -f /:/io.dat" 
on "dce10" was started. So, for a period of about six minutes, both tail 
sessions were working normally and then hang together.
Note that to reproduce this bug you don't have to run any connectathon to 
stress DFS server of clients. A PMAX server and a PMAX client are enough:
	on the server --- iostat 5 > /:/io.dat
	on the client ---- tail -f /:/io.dat

[6/2/93 public]
Would you please send your dfstrace to me via e-mail ? thanks. In the meantime,
I will try to run this on AIX to see what happens.

[6/2/93 public]
With a very good luck, I was able to reproduce it on AIX machines. (It is
actually an accident that it got reproduced)
                                                                            
The situation was : tail -f was hung there for almost 15 miniutes after working
for a while and then suddenly got better. To make the story short. Here is what 
I think how it happened: 
               
The TKM granted both 0xc0c (write) and 0x400 (read) tokens 
to CM1 and CM2 respectivly without revoking any conflicting ones, WHEN one
of the token request is in already TKM's pending queue. 
Note that I don't know how it got better
later on. The only reason I can think of is that CM daemon's periodical 
renewing tokens could cause TKM to revoke conflicting WRITE tokens. 
I have discussed it with Kazar: we are in the mode of "understand the problem and
no solution yet". 
The following is the picture of what happened based on the trace dump.
       CM1,			Server				CM2
       Machines#1		Machines#1			Machine#2
                                                            
      iotat 5 > iostat &			              tail -f iostat
AFS_GetToken (0xc0c)		
(asking for write tokens)
                  
			processing GetToken 0xc0c
			for CM1
                                                                
			revoke conflicting tokens 
			from CM2
			END of revoke 
                                                              
			end processing GetToken,
			returns 0xc0c
							AFS_GetToken (0x400)
							(asking for a read token)
                                                                                 
		     ->	processing GetToken 0x400 for
		    |	CM2
		    |	- processing revoke tokens 0xc0c
		    |	    from CM1
  =>TKN_RevokeToken |
    revoke 0xcoc    |
                    |
End AFS_GetToken    |
 		    |
AFS_GetToken(0xc0c) |
(realizing token    |
has been revoked)   |
                    |
       		    |	processing GetToken 0xc0c
		    |	for CM1
		    |
		    |	end processing GetToken 0xc0c
		    |	returns 0xc0c to CM1 ******
CM1 gets 0xc0c	    |
		    |
		    |	- end processing revoke tokens 
		    |
		    |	end processing GetToken 0x400
		    ->	return 0x400 to CM2  ********
                                                               
							CM2 gets 0x400 token
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `aix' 
Filled in Subcomponent Name with `tkm' 
Changed Interest List CC from `dfs-team, pshaw, hathaway' to `dfs-team, pshaw, 
 hathaway, kazar'

[6/3/93 public]
This defect should be for all S/W platforms.
Changed S/W Ref Platform from `aix' to `all'

[6/3/93 public]
This bug did not exit for a dfsvmunix from my April 25th nightly build.
I just replaced the dfsvmunix with this old version for the DFS server(santa),
and haven't seen any "tail -f /:/io.dat" hang yet on dce10 for more than
an hour.

[6/3/93 public]
Yes, you are quite right. The bug was accidently introduced recently. We
be able to fix it shortly.

[6/8/93 public]
Filled in Transarc Deltas with `kazar-ot8080-fix-tkm-race 1.1' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `import'

[6/10/93 public]
I have re-built my dfsvmunix with the fixes in tkm and have been running my 
"tail -f /:/io.dat" test for more than one hour without encoutering this 
failure --- This test used to fail within 15 minutes consistently.

[6/13/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/tkm/tkm_fidHash.[ch]' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8079
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : dcache leak in cm_HandleLink
Reported Date                 : 6/1/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/1/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8079-fix-handlelink-dcache-leak
Transarc Herder               : jaffe@transarc.com

[6/1/93 public]
There's a dcache leak in cm_HandleLink, in an error path.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/3/93 public]
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[7/6/93 public]
Fixed in 103.
Associated information:
Tested on TA build:  
103 3.16
Tested with backing build:  
Filled in Transarc Deltas with `kazar-ot8079-fix-handlelink-dcache-leak' 
Changed Transarc Status from `open' to `export'

[7/7/93 public]

[12/17/93 public]
Closed.



CR Number                     : 8077
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm can return data token without clearing dnlc
Reported Date                 : 5/30/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/30/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8077-cleanup-token-return-when-error
Transarc Herder               : jaffe@transarc.com

[5/30/93 public]
There are some error paths in which the CM could return a token
without clearing the dnlc.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/1/93 public]
Mike, Please change the fixby field if this needs to be in 1.0.2a
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[7/7/93 public]
Fixed in 103.
Associated information:
Tested on TA build:  
103 3.16
Filled in Transarc Deltas with `kazar-ot8077-cleanup-token-return-when-error' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8076
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : acl cache token-race comment is incomplete
Reported Date                 : 5/30/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/30/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8076-fix-fetchacl-comment
Transarc Herder               : jaffe@transarc.com

[5/30/93 public]
I'm getting tired of figuring out this code over and over due to
the fact that the comment describing why it works is incomplete.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/1/93 public]
Mike, if this needs to be in 1.0.2a, then please update the Fix by field.
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[7/7/93 public]
Fixed in 103.
Tested on TA build:  
103 3.16
Filled in Transarc Deltas with `kazar-ot8076-fix-fetchacl-comment' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8072
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_setattr unlocks unlocked vnode
Reported Date                 : 5/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/28/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8072-setattr-error-path-is-bad
Transarc Herder               : jaffe@transarc.com

[5/28/93 public]
cm_setattr can release an unlocked scache entry sometimes, since it
forgets whether it got a non-zero return code from cm_GetSLock, or from
further on down the line.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/1/93 public]
Mike, if this needs to be in 1.0.2a, then please update the fix by field.
Changed Found in Baseline from `1.0.2' to `1.0.2a' 
Changed Fix By Baseline from `1.0.2' to `1.0.3'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8072-setattr-error-path-is-bad' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8066
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : glue
Short Description             : system can panic when rename target in AFS
Reported Date                 : 5/27/93
Found in Baseline             : 1.0.2
Found Date                    : 5/27/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8066-fix-rename-xdev-panic
Transarc Herder               : jaffe@transarc.com

[5/27/93 public]
The system can panic when a rename is done from a local UFS exported
FS into AFS.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8066-fix-rename-xdev-panic' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8064
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : need locking in cm_dnamehash.c
Reported Date                 : 5/27/93
Found in Baseline             : 1.0.2
Found Date                    : 5/27/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8064-add-dnlc-locking
Transarc Herder               : jaffe@transarc.com

[5/27/93 public]
Even without an MP, there are places where the code in the name cache pkg
can block.  Add some locking.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8064-add-dnlc-locking' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8063
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : px
Short Description             : server doesn't let execute-only files be fetched
Reported Date                 : 5/26/93
Found in Baseline             : 1.0.2
Found Date                    : 5/26/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8063-allow-x-rights-fetches
Transarc Herder               : jaffe@transarc.com

[5/26/93 public]
The server needs to let X-only files be fetched by CMs, so that they can be
executed by the remote client.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8063-allow-x-rights-fetches' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8062
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : need to reload flserver info on cm checkf
Reported Date                 : 5/26/93
Found in Baseline             : 1.0.2
Found Date                    : 5/26/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8062-refresh-flserver-info-on-checkfilesets
Transarc Herder               : jaffe@transarc.com

[5/26/93 public]
Subject says it all: reload cell info periodically in case
the FS node changes in CDS.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with 
 `kazar-ot8062-refresh-flserver-info-on-checkfilesets' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8057
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : fts release fails intermittently
Reported Date                 : 5/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/25/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3
Affected File(s)              : src/test/file/rep/rtest3
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[5/25/93 public]
Two machine cell, RIOS and PMAX - running rep functional tests 
from RIOS.  Caught a "repserver forward fileset" error from
an fts release command in rtest3.  I've seen this sporadically 
in the past from other replication functional tests.  I'll put 
the test output in ~notuser/rep_failures/rtest3.out.

[5/25/93 public]
Just assigning to me.  This is a test defect, not a code defect.
Changed CR in Code, Doc, or Test? from `code' to `test' 
Filled in Interest List CC with `pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/26/93 public]
If this is an expected (normal?) failure from fts release, it
should probably be documented somewhere.  I'll open a doc bug.
I'm assuming the workaround  is simply to re-run fts release?
I'm dropping the priority since there's a workaround.

[5/27/93 public]
I'm still looking over the rtest3.out file, but the problem of encountering
a busy fileset when you're doing some fileset operation is indeed inherent
in the system.  I don't know if any assertions about that appear in the
documentation.  It's a test defect that the tests don't deal with these
essentially expected failures.  The work-around is indeed to re-execute the
command that failed.
Changed Interest List CC from `pakhtar@transarc.com' to `pakhtar@transarc.com, 
 jeff@transarc.com'

[5/27/93 public]
I've e-mailed with Craig and agreed to fix this myself.

[8/3/93 public]
Added code to serialize invocations of fts release in rtest3.
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 8055
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 8048
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_ResetStatus is called in too many places
Reported Date                 : 5/25/93
Found in Baseline             : 1.0.2
Found Date                    : 5/25/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : lots in cm.
Sensitivity                   : public
Transarc Deltas               : kazar-ot8055-adjust-reset-status-calls
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/25/93 public]
There are a # of calls to cm_ResetStatus that are superfluous.  In addition,
we should be careful to execute the appropriate revoke processing code when
doing a reset status.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/25/93 public]
Mike K has already done work under this OT, but OT 8048 will be fixed by
it also.  Upping its priority to match that of 8048.
Filled in Inter-dependent CRs with `8048' 
Changed Severity from `B' to `A' 
Changed Priority from `2' to `1' 
Changed Fix By Baseline from `1.0.2' to `1.0.2a'

[6/2/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Run cache tests (cache consistency, low, cthon).
Associated information:
Tested on TA build:  dfs-carl (at ibm)
Tested with backing build:  dce1.0.2a-0427
Filled in Transarc Deltas with `kazar-ot8055-adjust-reset-status-calls' 
Changed Transarc Status from `open' to `export'

[6/2/93 public]
Changed Transarc Status from `export' to `import'

[6/13/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `lots in cm.' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8052
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : more quourum problems during cho
Reported Date                 : 5/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/25/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/ncsubik/vote.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[5/25/93 public]
 
This ot is to track the flserver/quorum problems we've seen during
cho with the dfs.carl drop.  After less than 24 hours of cho the
flserver on the pmax is reporting UNOQUORUM for operations and
appears to be in some perpetual recovery mode.  Also, the flserver
on one of the rios has an abnormal looking flserver icl log in that
after about 2 hours into the test there does not appear to be any
voting activity.
 
here is part of the log from the pmax server:
time 679.937511, pid 23: ubik_BeginTrans returns (transPtr=0x0, errorcode=668147713) 
time 679.937511, pid 23: InitVLdbase returns 668147713 
time 679.937511, pid 23: VL_GetEntryByID returns 668147713 
time 679.937511, pid 23: VL_GetEntryByName returns 668147713 
time 682.035161, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 686.046879, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 690.058597, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 694.078127, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 698.093751, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 702.101563, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 706.113281, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 710.124999, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 714.199213, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 718.214837, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 718.410137, pid 5: ubik_thrPoolLookup, use ubik thread pool 
time 718.445355, pid 23: VL_Probe(connp=0x1018b0c8) entered 
time 722.226555, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 726.253897, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 730.269521, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 733.359359, pid 5: ubik_thrPoolLookup, use ubik thread pool 
time 733.367171, pid 23: VL_GetEntryByName (connp=0x1018b0c8, volname=0,,4, aentry=0x102dbdf0) entered 
time 733.367171, pid 23: VL_GetEntryByID (connp=0x1018b0c8, volid=0,,4, voltype=-1, aentry=0x102dbdf0) entered 
time 733.371077, pid 23: Init_VLdbase (transPP=0x102db730, locktype=1, op=503) entered 
time 733.371077, pid 23: ubik_BeginTrans (transMode=0) 
time 733.371077, pid 23: ubik version lock attempt to hold 
time 733.371077, pid 23: ubik version lock held 
time 733.371077, pid 23: ubik version lock attempt to release 
time 733.371077, pid 23: ubik version lock released 
time 733.371077, pid 23: ubik_BeginTrans returns (transPtr=0x0, errorcode=668147713) 
time 733.371077, pid 23: InitVLdbase returns 668147713 
time 733.371077, pid 23: VL_GetEntryByID returns 668147713 
time 733.371077, pid 23: VL_GetEntryByName returns 668147713 
time 734.281239, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 738.299575, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 742.315841, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 746.332107, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 750.336172, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 754.398668, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 758.406480, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 762.422104, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 766.461164, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 770.472882, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 774.484600, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 778.496318, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 782.504130, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 786.523660, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 790.531472, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 794.539348, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 798.547160, pid 10: recovery running on host 130.105.5.78 in state 0x0 
time 802.554972, pid 10: recovery running on host 130.105.5.78 in state 0x0 
here's the log from the rios:
 
time 245.446219, pid 12: VL_GetEntryByName (connp=0x20158518, volname=0,,403, aent
ry=0x2028cff0) entered
time 272.565539, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 272.649247, pid 14: VL_GetEntryByName (connp=0x201ac798, volname=0,,403, aent
ry=0x202bfff0) entered
time 300.969544, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 300.988753, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 301.013071, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 301.074153, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 301.153328, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 301.161867, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 301.177364, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 301.266065, pid 15: VL_GetEntryByName (connp=0x201ad448, volname=0,,496, aent
ry=0x202d3ff0) entered
time 301.848543, pid 16: VL_GetEntryByName (connp=0x201adb68, volname=0,,496, aent
ry=0x202f2ff0) entered
time 301.849957, pid 17: VL_GetEntryByName (connp=0x201ade28, volname=0,,496, aent
ry=0x20306ff0) entered
time 301.856113, pid 18: VL_GetEntryByName (connp=0x201ae128, volname=0,,496, aent
ry=0x20325ff0) entered
time 301.857776, pid 19: VL_GetEntryByName (connp=0x201ae428, volname=0,,496, aent
ry=0x20339ff0) entered
time 301.859168, pid 20: VL_GetEntryByName (connp=0x201e3288, volname=0,,496, aent
ry=0x20358ff0) entered
try=0x2036cff0) entered
time 398.636295, pid 9: ubik_thrPoolLookup, use default thread pool
time 602.390167, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 633.101588, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 782.244200, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 813.131450, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 962.101348, pid 0: Current time: Mon May 24 18:01:22 1993
time 962.101348, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 993.004395, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 119.210509, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 148.426144, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 306.204314, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 338.889463, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 478.949990, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 508.772165, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 658.967860, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 689.174803, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 790.443599, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 838.952825, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 868.876424, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 952.556422, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 968.893222, pid 0: Current time: Mon May 24 18:18:32 1993
time 968.893222, pid 5: ubik_thrPoolLookup, use ubik thread pool
After this point in time there is no vl or voting activity only
ubik_thrPoolLookup log entries.
Note that an fts lsfldb hangs (here's the rpc debug output); (the server
we're trying to connect to is the rios from above):
 
root@singsing> fts lsfldb
CN: call_rep->none partially bound binding handle
CN: call_rep->20137ae8 assoc->20138368 desc->0 establishing connection & negotiating presentation syntax
CN: call_rep->20137ae8 assoc->20138368 desc->0 connection request initiated to 130.105.5.3[135]
(rpc__cn_network_req_connect) desc->6 desired_sndbuf 4096, desired_rcvbuf 4096
(rpc__cn_network_req_connect) desc->6 actual sndbuf 4096, actual rcvbuf 4096
CN: assoc->20138368 call_rep->none Receiver thread starting...
CN: assoc->20138368 call_rep->none Entering receive loop...
CN: Attemping to lock global mutex
CN: Global mutex locked
CN: call_rep->20137ae8 assoc->20138368 desc->6 connection established
CN: call_rep->20137ae8 assoc->20138368 desc->6 negotiating for abstract syntax->e1af8308-5d1f-11c9-91a4-08002b14a0fa,3 context_id->1 call_id->1
CN: call_rep->20137ae8 assoc->20138368 desc->6 transfer_syntax[0]->8a885d04-1ceb-11c9-9fe8-08002b104860,2
CN: call_rep->20137ae8 assoc->20138368 desc->6 sent 72 bytes
CN: assoc->20138368 call_rep->none Receiver awake ... Connection established
CN: call_rep->20137ae8 assoc->20138368 desc->6 received 60 bytes
CN: call_rep->20137ae8 assoc->20138368 desc->6 presentation negotiation succeeded
CN: call_rep->20137ae8 call transmit...
CN: call_rep->20137ae8 call transmit...
CN: call_rep->20137ae8 call transmit...
CN: call_rep->20137ae8 call transmit...
CN: call_rep->20137ae8 call transmit...
CN: call_rep->20137ae8 call transceive...
CN: call_rep->20137ae8 assoc->20138368 desc->6 sent 156 bytes
CN: call_rep->20137ae8 assoc->20138368 desc->6 received 152 bytes
CN: call_rep->20137ae8 call end
(ep_get_endpoint) call_rep->none binding_rep->201365d8 endpoint mapper returned 1024
CN: call_rep->20137ae8 assoc->201370b8 desc->0 establishing connection & negotiating presentation syntax
CN: call_rep->20137ae8 assoc->201370b8 desc->0 connection request initiated to 130.105.5.3[1024]
(rpc__cn_network_req_connect) desc->7 desired_sndbuf 4096, desired_rcvbuf 4096
(rpc__cn_network_req_connect) desc->7 actual sndbuf 4096, actual rcvbuf 4096
CN: assoc->201370b8 call_rep->none Receiver thread starting...
CN: assoc->201370b8 call_rep->none Entering receive loop...
CN: Attemping to lock global mutex
CN: Global mutex locked
CN: call_rep->20137ae8 assoc->201370b8 desc->7 connection established
CN: call_rep->20137ae8 assoc->201370b8 desc->7 negotiating for abstract syntax->afa8bd80-7d8a-11c9-bef4-08002b102989,1 context_id->1 call_id->3
CN: call_rep->20137ae8 assoc->201370b8 desc->7 transfer_syntax[0]->8a885d04-1ceb-11c9-9fe8-08002b104860,2
CN: call_rep->20137ae8 assoc->201370b8 desc->7 sent 72 bytes
CN: assoc->201370b8 call_rep->none Receiver awake ... Connection established
CN: call_rep->20137ae8 assoc->201370b8 desc->7 received 60 bytes
CN: call_rep->20137ae8 assoc->201370b8 desc->7 presentation negotiation succeeded
CN: call_rep->20137ae8 call transceive...
CN: call_rep->20137ae8 assoc->201370b8 desc->7 sent 48 bytes
CN: call_rep->20137ae8 assoc->201370b8 desc->7 received 64 bytes
CN: call_rep->20137ae8 call end
CN: call_rep->0 assoc->201370b8 desc->7 received 16 bytes
CN: assoc->201370b8 call_rep->none No longer receiving...Close socket
CN: assoc->201370b8 call_rep->none Entering receive loop...
CN: Attemping to lock global mutex
CN: Global mutex locked
CN: call_rep->0 assoc->20138368 desc->6 received 16 bytes
CN: assoc->20138368 call_rep->none No longer receiving...Close socket
CN: assoc->20138368 call_rep->none Entering receive loop...
CN: Attemping to lock global mutex
CN: Global mutex locked

[5/26/93 public]
Raising priority since this has been seen in system test as well as
cho and it makes it real difficult to do anything useful in a 
multi-flserver configuration.

[5/27/93 public]
Adding Vijay and Mike C to the CC list.
Filled in Interest List CC with `vijay@transarc.com, comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[06/1/93 public]
This seems to happen quite regularly with cho.  
Here we've got one flserver which gets several exceptions raised in ncscubik/remote.c
From there on the icl log for this flserver looks rather abnormal:
time 877.556333, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 958.281581, pid 22: SUBIKDISK_SendFile (handle=0x20140688, file=0, len=208896, pipe=
0x201407c0)
time 958.409540, pid 22: SUBIKDISK_SendFile (version 738552177.108)
time 975.317373, pid 22: ubik version lock attempt to hold
time 975.317677, pid 22: ubik version lock held
time 976.376561, pid 11: recovery running on host 130.105.5.3 in state 0x0
time 977.615097, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 978.289959, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 978.688817, pid 22: ubik RPC generated exception (rpc_x_ss_pipe_comm_error) exceptio
n raised, in file ../../../../src/file/ncsubik/remote.c at line 804
time 978.689336, pid 22: ubik version lock attempt to release
time 978.689559, pid 22: ubik version lock released
time 978.689787, pid 22: SUBIKDISK_SendFile(handle=0x20140688) returns (errorcode=6681477
40)
time 978.891782, pid 24: SUBIKDISK_SendFile (handle=0x20140bd8, file=0, len=208896, pipe=
0x20144e60)
time 978.892071, pid 24: SUBIKDISK_SendFile (version 738552177.108)
time 978.932510, pid 24: ubik version lock attempt to hold
time 978.932837, pid 24: ubik version lock held
time 979.315237, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 979.520123, pid 24: ubik RPC generated exception (rpc_x_ss_pipe_comm_error) exceptio
n raised, in file ../../../../src/file/ncsubik/remote.c at line 804
time 979.520636, pid 24: ubik version lock attempt to release
time 979.520882, pid 24: ubik version lock released
time 979.521411, pid 24: SUBIKDISK_SendFile(handle=0x20140bd8) returns (errorcode=6681477
40)
.....
time 662.017586, pid 10: SUBIKVOTE_Beacon(host=130.105.5.3) returns (vote=738576022, erro
rcode=0)
time 662.017927, pid 10: received 2 votes from 3 servers
time 666.608140, pid 11: recovery running on host 130.105.5.3 in state 0x0
time 668.479005, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 672.437311, pid 11: recovery running on host 130.105.5.3 in state 0x0
time 877.823148, pid 0: Current time: Fri May 28 03:03:57 1993
time 877.823148, pid 24: ubik RPC generated exception (rpc_x_ss_pipe_comm_error) exceptio
n raised, in file ../../../../src/file/ncsubik/remote.c at line 804
time 877.986265, pid 24: ubik version lock attempt to release
time 877.986536, pid 24: ubik version lock released
time 877.986778, pid 24: SUBIKDISK_SendFile(handle=0x20140998) returns (errorcode=6681477
40)
time 910.138376, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 910.944460, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 911.318558, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 377.700316, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 378.153752, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 378.420139, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 378.603038, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 378.806149, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 378.884732, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 381.070987, pid 32: ubik_thrPoolLookup, use default thread pool
time 381.305804, pid 5: ubik_thrPoolLookup, use default thread pool
time 381.428747, pid 32: ubik_thrPoolLookup, use default thread pool
time 381.499758, pid 32: ubik_thrPoolLookup, use default thread pool
time 381.549972, pid 5: ubik_thrPoolLookup, use default thread pool
time 381.669969, pid 32: ubik_thrPoolLookup, use default thread pool
time 381.713084, pid 5: ubik_thrPoolLookup, use default thread pool
time 383.112807, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 383.397825, pid 25: SUBIKDISK_Probe (handle=0x20144c48)
time 385.431249, pid 5: ubik_thrPoolLookup, use ubik thread pool
time 385.578410, pid 25: SUBIKDISK_SendFile (handle=0x20144c48, file=0, len=208896, pipe=
0x20140920)
time 385.578691, pid 25: SUBIKDISK_SendFile (version 738571591.57)
time 393.806985, pid 5: ubik_thrPoolLookup, use ubik thread pool
From here on we only get "use default thread pool" log entries.

[06/02/93 public]
Just noting that we saw the exception again last night preceeding the
quorum problems:
time 913.148427, pid 5: ubik_thrPoolLookup, use ubik thread pool 
time 913.246077, pid 5: ubik_thrPoolLookup, use ubik thread pool 
time 916.671831, pid 23: ubik RPC generated exception (rpc_x_ss_pipe_comm_error) exception raised, in file ../../../../src/file/ncsubik/remote.c at line 804 
aised, in file ../../../../src/file/ncsubik/remote.c at line 804 
time 916.804635, pid 23: ubik version lock attempt to release 
time 916.808541, pid 23: ubik version lock released 
time 916.808541, pid 23: SUBIKDISK_SendFile(handle=0x10414c90) returns (errorcode=668147740) 
This server continued to perform normal operations for a few minutes after
hitting this exception and then it went into the catatonic mode where it's
only logging  "ubik_thrPoolLookup" entries.

[6/3/93 public]
One thing that might be useful to look at would be to, for testing purposes,
assert when such an exception happens in SUBIKDISK_SendFile. If we get core
files and ICL logs of all the ubik servers at the time such a problem happens
we could determine the problem. One way to do this is to assert in the
remote.c:SUBIKDISK_SendFile when such an exception is raised, and in
recovery.c:urecovery_Interact at the point where the UBIKDISK_SendFile RPC
is made and an exception would be raised here too. The other thing might
be to add ICL trace calls in the push and pull routines in upipe.c and see
if the pull routine gets cancelled for some reason or fails to get the 
low level lock via ubik_hold(). We can't seem to reproduce this problem on 
a all-rios cell here.

[06/08/93 public]
more data:
Looks like threads are having trouble locking ubik_llock.
There are 17 threads with the following stack trace:
(gdb) bt 7
#0  0x7311dd80 in cma__dispatch ()
#1  0x7311d77c in cma__block ()
#2  0x73136940 in pthread_cond_wait ()
#3  0x455d98 in afslk_ObtainWrite ()
#4  0x4169c8 in VL_GetEntryByName ()
#5  0x400de4 in op1_ssr ()
#6  0x731e3c28 in rpc__dg_execute_call ()
Other threads:
(gdb) switch_thread 11
(gdb) bt 7
#0  0x7311dd80 in cma__dispatch ()
#1  0x7311d77c in cma__block ()
#2  0x7312b3bc in cma__int_mutex_block ()
#3  0x73134bb4 in pthread_mutex_lock ()
#4  0x455cec in afslk_ObtainWrite ()
#5  0x434130 in ubik_hold ()
#6  0x4499a0 in urecovery_Interact ()
(More stack frames follow...)
(gdb) switch_thread 10 
(gdb) bt 7
#0  0x7311dd80 in cma__dispatch ()
#1  0x7311d77c in cma__block ()
#2  0x7312b3bc in cma__int_mutex_block ()
#3  0x73136aa0 in pthread_cond_wait ()
#4  0x455d98 in afslk_ObtainWrite ()
#5  0x434130 in ubik_hold ()
#6  0x448294 in ubeacon_Interact ()
(More stack frames follow...)
#0  0x7311dd80 in cma__dispatch ()
#1  0x7311d77c in cma__block ()
#2  0x7312b3bc in cma__int_mutex_block ()
#3  0x73134bb4 in pthread_mutex_lock ()
#4  0x455cec in afslk_ObtainWrite ()
#5  0x434130 in ubik_hold ()
#6  0x431440 in ubik_BeginTrans ()
#7  0x413840 in Init_VLdbase ()
#8  0x41841c in VL_ReplaceEntry ()
#9  0x409da0 in op16_ssr ()
#10 0x731e3c28 in rpc__dg_execute_call ()
#11 0x73185494 in cthread_call_executor ()
(More stack frames follow...)
(gdb) switch_thread 23
(gdb) bt 7
#0  0x7311dd80 in cma__dispatch ()
#1  0x7311d77c in cma__block ()
#2  0x7312b3bc in cma__int_mutex_block ()
#3  0x73134bb4 in pthread_mutex_lock ()
#4  0x455cec in afslk_ObtainWrite ()
#5  0x434130 in ubik_hold ()
#6  0x4333c8 in SUBIKVOTE_Beacon ()

[6/8/93 public]
Yes, it does look like ubik is getting dead locked by some thread that fails
to unlock the ubik_llock. I saw the same behavior here on a rios, the ICL
trace of which is appended below. My guess right now is an exception or
cancellation happenning on a thread that is holding this lock, although I'm
not sure. At present, I'm doing a code audit to find such places where this
could happen.
Changed Interest List CC from `vijay@transarc.com, comer@transarc.com' to 
 `vijay@transarc.com, comer@transarc.com, dawn%liz.austin.ibm.com@transarc.com'

[06/10/93 public]

Added disabling cancels to the voting routines.

Vijay, you can open a separate ot for the rest of the
robustness changes you are making.

[12/17/93 public]
Closed.



CR Number                     : 8051
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm uses return parm from AFS_Lookup on ENOENT case
Reported Date                 : 5/25/93
Found in Baseline             : 1.0.2
Found Date                    : 5/25/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8051-afs-lookup-used-unset-out-param
Transarc Herder               : jaffe@transarc.com

[5/25/93 public]
cm uses return parm from AFS_Lookup on ENOENT case, which is bad, since
the parameter isn't *set* when we return an error!
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8051-afs-lookup-used-unset-out-param' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8045
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cache not invalidated when token expires
Reported Date                 : 5/24/93
Found in Baseline             : 1.0.2
Found Date                    : 5/24/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8045-clean-caches-on-token-expiration
Transarc Herder               : jaffe@transarc.com

[5/24/93 public]
DFS caches are not invalidated when tokens expire.  They should be, since
otherwise we could be left with a cache inconsistency.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8045-clean-caches-on-token-expiration' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8044
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvage doesn't repair aggregate with bad back block reference
Reported Date                 : 5/24/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/24/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : ota-db3472-renumber-salvage-msgs (at least 1.2)
Transarc Herder               : 
Transarc Status               : submit

[5/24/93 public]
I did a clean shutdown of my file server and when the system
rebooted dfsexport failed because of the following problem
which salvage claims to have repaired but somehow it is not
repaired:
root@fire  # salvage -verbose /dev/rz1d
Salvaging /dev/rz1d
Will run recovery on /dev/rz1d
 SectorSize 512; TotalBlocks 24334; BlockSize 8192;
 FragmentSize 1024; FirstBlock 1; NLogBlocks 800;
 NumBigChunks 3042; minBlkSize 512; minBlkCount 389367;
 FileSysCreateTime 736090223;
 FileSysClean 0; FileSysEmpty 0; FileSysMountedAs ``''
/dev/rrz1d: Episode file sys created Thu Apr 29 09:30:23 1993
Device /dev/rrz1d, major 8, minor 11; total 24334 8192-byte blocks.
Block size 8192, frag size 1024, firstBlock 1, nBlocks 24334
Principal superblock at byte 65536, nLogBlocks 800.
In volume m3.lfs1 0,,1226 (avl #6)
  in anode (#26) EOF @ (0,100875)
    index 8 using indirect tree 0 at depth 0: 0x800055ae
      Backing block reference is to incorrect block: 0xffffffff
      -> removing block reference
oughtRestore m3.lfs1:/test/cthon/basic.pmax/test4 volume index: 6 anode index 26
Processed 2 vols 708 anodes 14 dirs 686 files 1 acls
Done.  Some inconsistencies repaired salvaging /dev/rrz1d
root@fire  # salvage /dev/rz1d -verbose
Salvaging /dev/rz1d
Will run recovery on /dev/rz1d
 SectorSize 512; TotalBlocks 24334; BlockSize 8192;
 FragmentSize 1024; FirstBlock 1; NLogBlocks 800;
 NumBigChunks 3042; minBlkSize 512; minBlkCount 389367;
 FileSysCreateTime 736090223;
 FileSysClean 0; FileSysEmpty 0; FileSysMountedAs ``''
/dev/rrz1d: Episode file sys created Thu Apr 29 09:30:23 1993
Device /dev/rrz1d, major 8, minor 11; total 24334 8192-byte blocks.
Block size 8192, frag size 1024, firstBlock 1, nBlocks 24334
Principal superblock at byte 65536, nLogBlocks 800.
In volume m3.lfs1 0,,1226 (avl #6)
  in anode (#26) EOF @ (0,100875)
    index 8 using indirect tree 0 at depth 0: 0x800055ae
      Backing block reference is to incorrect block: 0xffffffff
      -> removing block reference
oughtRestore m3.lfs1:/test/cthon/basic.pmax/test4 volume index: 6 anode index 26
Processed 2 vols 708 anodes 14 dirs 686 files 1 acls
Done.  Some inconsistencies repaired salvaging /dev/rrz1d
I will try to put the aggregate out in the cell; I'm compressing it
now and it's taking awhile.

[5/24/93 public]
Filled in Interest List CC with `ota@transarc.com, pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/25/93 public]
At first blush this looks like two bugs: first the aggregate should not
have errors (whether the shutdown was clean or not!), and second the
salvager should be able to repair simple problems like this on a single
pass.  In practice there is likely to be some connection.
Be sure to metamucil the aggregate (file/episode/ravage/metamucil)
before compressing it.  This typically saves a great deal of space.  If
you can make it available I'd be happy to figure out what is going on.
Changed Subcomponent Name from `salvage' to `lfs'

[5/25/93 public]
We believe that the salvager's failure to do the repair can be explained
by bug db3472.  The error that required the repair is another matter.  We
would like to look into it some more.
Filled in Transarc Deltas with `ota-db3472-renumber-salvage-msgs (at least 
 1.2)' 
Filled in Transarc Status with `export'

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8043
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : unable to rmsite if agg not attached
Reported Date                 : 5/22/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/22/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/userInt/fts/volc{.h,_main.c,_subr.c,_vnodeops.c}
Sensitivity                   : public
Transarc Deltas               : cfe-ot8043-rmsite-on-detached-aggr
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/22/93 public]
 
BUILD:	dfs.carl
CONFIG: 4 mach cell, 3 pmax, 1 rios, 3 flservers, 4 ftservers
TEST:	dfs.repfs_checklist
 
In order to leave lfs_aggr4, the lfs aggregate on the rios, in its
current "unrecoverable" state for further analysis (CR 8038), it is not
attached/dfsexported. However, it IS the site of scheduled replicas. I
can not rmsite, as a system administrator would, to inform the system
NOT to attempt replication to lfs_aggr4. In the case of root.dfs and epi.2,
lfs_aggr4 is only ONE of its scheduled replication sites but an attempt to fts
update epi.2 hangs, even though it should at least succeed to the OTHER
scheduled replication site. I did NOT attempt to fts update root.dfs, see
the output of fts statrep -long below.
 
Also, on the machine that houses the rw root.dfs, any reference thru /:
produces:
dfs: fileset (0,,19) error (code 691089410) on server 130.105.201.7 in cell p102a_cell.qadce.osf.org.
Lastly, response time in the cell is horrendous, could this be the cause of
any of it? 
 
fts lsft -fileset epi.2 -server /.:/hosts/dce5
No fileset of ix 0 (type 0, readWrite) in FLDB entry for epi.2 (0,,25)
epi.2  
        readWrite   ID 0,,25  valid
        readOnly    ID 0,,26  valid
        backup      ID 0,,27  invalid
number of sites: 3
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDel
ay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner     
valentine.osf.org   RW       lfs_aggr2 0:00:00 hosts/valentine<nil>     
dce5.osf.org        RO       lfs_aggr3 0:30:00 hosts/dce5     <nil>     
cobbler.osf.org     RO       7       0:30:00 hosts/cobbler  <nil>     
 
root@cobbler> cat /opt/dcelocal/var/dfs/dfstab
# blkdev aggname aggtype aggid 
/dev/hd1        lfs_aggr4       lfs     7
 
root.dfs  
        readWrite   ID 0,,18  valid
        readOnly    ID 0,,19  valid
        backup      ID 0,,20  valid
number of sites: 3
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDel
ay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner   
valentine.osf.org   RW,BK    lfs_aggr 0:00:00 hosts/valentine<nil>   
cobbler.osf.org     RO       7       0:30:00 hosts/cobbler  <nil>   
dce5.osf.org        RO       lfs_aggr3 0:30:00 hosts/dce5     <nil> 
 
root@cobbler> fts statrep -server /.:/hosts/dce5
Status of rep server /.:/hosts/dce5 (dce5.osf.org) at Sat May 22 16:12:13 1993
Replicas managed: 2; host/connection blocks: 1
IDs: 1 allocated, 1 in use, 0 re-used
Next forced keep-alive in -738101533 secs, at Wed Dec 31 19:00:00 1969
Primary comm blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
Primary keep-alive blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
Active replicas on /.:/hosts/dce5:
 
root.dfs, cell 134228757,,3182450524: src 0,,18 (lfs_aggr) (on valentine.osf.org) => dce5.osf.org 0,,19 (lfs_aggr3)
   flags 0x20001, volstates 0x10421206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,9743; curVV: 0,,442; WVT ID = 738009408,,22247
   Lost token 7777114 ago; token expires 13295 hence; new version published 738101601 ago
   vvCurr 730324487.658298 (7777114 ago); vvPingCurr 738100496.463040 (1105 ago)
   Last update attempt 738100501.236519 (1100 ago); next scheduled attempt 0.000000 (-738101601 hence)
   Status msg: Calling FTSERVER_Forward() on primary
 
epi.2, cell 134228757,,3182450524: src 0,,25 (lfs_aggr2) (on valentine.osf.org) => dce5.osf.org 0,,26 (lfs_aggr3)
   flags 0x20001, volstates 0x10421206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,9743; curVV: 0,,442; WVT ID = 738009408,,22247
   Lost token 7777114 ago; token expires 13295 hence; new version published 738101601 ago
   vvCurr 730324487.658298 (7777114 ago); vvPingCurr 738100496.463040 (1105 ago)
   Last update attempt 738100501.236519 (1100 ago); next scheduled attempt 0.000000 (-738101601 hence)
   Status msg: Calling FTSERVER_Forward() on primary
 
epi.2, cell 134228757,,3182450524: src 0,,25 (lfs_aggr2) (on valentine.osf.org) => dce5.osf.org 0,,26 (lfs_aggr3)
   flags 0x20000, volstates 0x10421206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,0; curVV: 0,,8451; WVT ID = 0,,0
   Lost token 7777117 ago; token expires -738101604 hence; new version published 738101604 ago
   vvCurr 730324487.498152 (7777117 ago); vvPingCurr 0.000000 (738101604 ago)
   Last update attempt 0.000000 (738101604 ago); next scheduled attempt 0.000000 (-738101604 hence)
   Status msg: <<empty>>
 
Note that somehow, epi.2's curVV is > than srcVV and that the root.dfs srcVV
and curVV values imply the many attempts to do root.dfs scheduled replication
have failed.

[5/22/93 public]
Downgrading severity/priority - removing update complaint from short
description- apparently I had not waited long enough after starting
the repserver on dce5 before doing the stat. Here is the current stat:
 
root@dce12> fts statrep -server /.:/hosts/dce5 -long
Status of rep server /.:/hosts/dce5 (dce5.osf.org) at Mon May 24 09:25:47 1993
Replicas managed: 2; host/connection blocks: 1
IDs: 0 allocated, 0 in use, 0 re-used
Next forced keep-alive in 86357 secs, at Tue May 25 09:25:04 1993
Primary comm blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
Primary keep-alive blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
 
Active replicas on /.:/hosts/dce5:
 
root.dfs, cell 134228757,,3182450524: src 0,,18 (lfs_aggr) (on valentine.osf.org) => dce5.osf.org 0,,19 (lfs_aggr3)
   flags 0x20001, volstates 0x10423206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,9743; curVV: 0,,9743; WVT ID = 738009408,,22469
   Lost token 7881599 ago; token expires 4817 hence; new version published 738249948 ago
   vvCurr 730368349.580772 (7881599 ago); vvPingCurr 738240365.004646 (9583 ago)
   Last update attempt 0.000000 (738249948 ago); next scheduled attempt 0.000000 (-738249948 hence)
   Status msg: GetToken returned 0x404 token, ID 738009408,,22469.
 
epi.2, cell 134228757,,3182450524: src 0,,25 (lfs_aggr2) (on valentine.osf.org) => dce5.osf.org 0,,26 (lfs_aggr3)
   flags 0x20001, volstates 0x10423206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,8457; curVV: 0,,8457; WVT ID = 738009408,,22470
   Lost token 7881604 ago; token expires 4814 hence; new version published 738249953 ago
   vvCurr 730368349.541712 (7881604 ago); vvPingCurr 738240367.004545 (9586 ago)
   Last update attempt 0.000000 (738249953 ago); next scheduled attempt 0.000000 (-738249953 hence)
   Status msg: GetToken returned 0x404 token, ID 738009408,,22470.

[5/27/93 public]
(a) I changed fts to allow rmsite to accept as an aggregate specification,
    in addition from a validate-able aggregate name or ID, an unvalidate-able
    aggregate ID.  Thus, you would now be able to say
	fts rmsite epi.2 cobbler.osf.org 7
    and it would do it.  In addition, the repserver on cobbler would stop
    trying to maintain a local replica.
(b) The ``dfs: fileset (0,,19) error (code 691089410) on server...'' message
    is due to the FLDB entry listing a site for root.dfs.readonly that
    doesn't represent an available fileset.  An ``rmsite'' command would
    clear that up.
(c) I have to assume in the first ``fts statrep dce5 -long'' that there aren't
    really three replicas being managed: the listing gives the epi.2 name
    twice, giving it the data for root.dfs the first time.  This looks like
    a copy-and-paste error.  The second ``statrep'' command shows something
    sensible.
 
Defect Closure Form
-------------------
--Verification procedure below--
Issue the ``fts rmsite'' command to remove a scheduled replica that resides
on an aggregate that has been detached.  With the old fts, this fails; with
the new fts, if you use a numeric ID, this works.
 
Associated information:
Tested on TA build:  dfs-carl 1.10
Tested with backing build:  dce1.0.2a-0427
Filled in Interest List CC with `jeff@transarc.com' 
Added field Transarc Deltas with value `cfe-ot8043-rmsite-on-detached-aggr' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `export'

[6/2/93 public]
Changed Transarc Status from `export' to `import'

[6/13/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with 
 `file/userInt/fts/volc{.h,_main.c,_subr.c,_vnodeops.c}' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8042
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : error from FTSERVER_SetStatus causing infinite loop in fts move
Reported Date                 : 5/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/21/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot8042-signal-local-pipe-errors
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/21/93 public]
We hit a problem during fts move of an unloaded fileset (8026).
Somehow now the fts move command is looping in an error state
doing the same thing over and over again (unsucessfully)
 
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021076.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021114.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021144.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021178.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021216.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021246.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021277.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021308.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: Transaction doesn't exist (dfs / fts)
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738021338.
 
Since it seems like we can't fix the situation which is causing
the error, the command should probably terminate and clean up after itself.

[5/21/93 public]
Filled in Interest List CC with `tu@transarc.com, jaffe@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/24/93 public]
The infinite loop isn't in the code whose output is shown here, which is the
output from an auxiliary thread in fts.  The infinite loop is somewhere else
in the fts-move process, probably in the token grant where fts requests the
spot-there token to revoke the here tokens from CMs and other clients.  If the
real infinite loop were fixed, fts would exit and the loop whose output is
shown here would terminate with the process as a whole.
 
Thus, could you please include in this OT report the rest of the output from
the failing fts command?  Thanks.

[5/25/93 public]
Assigning to Diane (Dianne?) pending further information.
Changed Interest List CC from `tu@transarc.com, jaffe@transarc.com' to 
 `tu@transarc.com, jaffe@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `delgado' 
Changed Resp. Engr's Company from `tarc' to `osf'

[5/27/93 public]
The FtLog of the target file server would contain relevant information about
why the destination fileset's transaction was no longer to be found.

[6/8/93 public]
Changed Interest List CC from `jaffe@transarc.com, cfe@transarc.com' to 
 `jaffe@transarc.com, cfe@transarc.com, dstokes@transarc.com'

[6/15/93 public]
Dawn Stokes gave me a thread trace of a hung-up ftserver that pointed the
finger in this case at a thread that wouldn't die.  The thread in question
was the one that ftserver spawns to do the vols_DumpVolume when servicing
an FTSERVER_Forward RPC request.  This thread was blocked on a write to a
local pipe (which was basically piped to a remote FTSERVER_Restore()),
but the call executor had received a pipe exception from the _Restore call
and was trying to pthread_join() the blocked dumper.
 
The fix was to add a procedure to be done to the pipe veneer to cause all
current operations on the pipe to fail and not block.  This procedure is
then called from vols_Forward (in the ftserver) if the FTSERVER_Restore()
call returned failure of any sort.  The dumper thread un-blocks from any
local-pipe write, and it then does pthread_exit, whereupon the RPC call
executor thread can join up with it and the FTSERVER_Forward() procedure
can return to fts.
 
Defect Closure Form
-------------------
--Regression test program below--
I tested this with Fred's fts-move-with-I/O exerciser.
 
--Verification procedure below--
Run ``fts move'' on a loaded system (not necessarily a loaded fileset).
If you ever get a message in FtLog referring to an rpc_x_ss_pipe_comm_error,
do ``fts statft'' commands to look for ftserver transactions that have a
reference count of 1 and an age of more than 3 minutes.  If you find any of
these, the bug is probably not fixed.  If there are such transactions, this
didn't fix the problem.
 
Associated information:
Tested on TA build:  dfs-carl-1.11
Tested with backing build:  dce1.0.2ab2
Changed Interest List CC from `jaffe@transarc.com, cfe@transarc.com, 
 dstokes@transarc.com' to `jaffe@transarc.com, cfe@transarc.com, 
 dstokes@transarc.com, delgado' 
Changed Responsible Engr. from `delgado' to `cfe' 
Changed Resp. Engr's Company from `osf' to `tarc' 
Filled in Transarc Deltas with `cfe-ot8042-signal-local-pipe-errors' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `export'

[6/16/93 public]
Changed Transarc Status from `export' to `import'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8038
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvage reports bogus VV errors after repserver ops
Reported Date                 : 5/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/21/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db2355-file-status-VV -r1.2
Transarc Herder               : jaffe

[5/21/93 public]
BUILD:	dfs.carl
CONFIG:	4 mach cell, 3 flservers, 1 rios w/ 1 lfs aggregate
TEST:	dfs.repfs_checklist
Rebooting after dfs.repfs_checklist (more specifically, after CR 8024),
I was unable to recover the aggregate on the rios.
This is what was on the aggregate before the reboot:
fts lsheader -aggr lfs_aggr4 -server cobbler
Total filesets on server cobbler aggregate lfs_aggr4 (id 7): 5
epi.4                    0,,42 RW     10 K alloc     10 K quota On-line
epi.a.readonly           0,,55 RO   1063 K alloc   1063 K quota On-line
epi.b                    0,,57 RW     16 K alloc     16 K quota On-line
epi.gmd                  0,,50 RW      9 K alloc      9 K quota On-line
root.dfs.readonly        0,,19 RO     17 K alloc     17 K quota On-line
Total filesets on-line 5; total off-line 0; total busy 0
This is what happened during rc.dfs after the reboot:
+ /opt/dcelocal/bin/salvage -rec -verify /dev/hd1 
Verifying /dev/hd1
Will run recovery on /dev/hd1
recovery statistics:
        Elapsed time was 1984 ms
        14 log pages recovered consisting of 841 records
        Modified 19 data blocks
        648 redo-data records, 1 redo-fill record
        0 undo-data records, 0 undo-fill record
Ran recovery on dev 1/1
Volume 10 version 0.20fd Lower than Anode 120 Version 0.2147
Volume 10 version 0.20fd Lower than Anode 121 Version 0.2148
Volume 10 version 0.20fd Lower than Anode 122 Version 0.214d
Volume 10 version 0.20fd Lower than Anode 123 Version 0.214e
In volume epi.2.readonly 0,,26 (avl #10)
  Volume version 0,,8445 smaller than largest anode version number seen 0,,8526
Volume 11 version 0.20fd Lower than Anode 120 Version 0.2147
Volume 11 version 0.20fd Lower than Anode 121 Version 0.2148
Volume 11 version 0.20fd Lower than Anode 122 Version 0.214d
Volume 11 version 0.20fd Lower than Anode 123 Version 0.214e
In volume epi.2.clone 0,,74 (avl #11)
  Volume version 0,,8445 smaller than largest anode version number seen 0,,8526
unsafe <no name was available>: volume index: 11 anode index 291
unsafe <no name was available>: volume index: 11 anode index 290
unsafe <no name was available>: volume index: 11 anode index 289
unsafe <no name was available>: volume index: 11 anode index 288
unsafe <no name was available>: volume index: 10 anode index 291
unsafe <no name was available>: volume index: 10 anode index 290
unsafe <no name was available>: volume index: 10 anode index 289
unsafe <no name was available>: volume index: 10 anode index 288
Processed 7 vols 217 anodes 43 dirs 152 files 18 acls
Done.  Some inconsistencies found verifying /dev/rhd1
+ [ 1 -eq 0 ] 
+ x_exit \tWARNING: PROBLEM RECOVERING /dev/hd1 
I'm in the process of dd'ing/compressing the aggregate for you.

[5/21/93 public]
Compressed output is still very large - you may want to log in remotely or
analyze this a different way - let me know.

[5/25/93 public]
The basic problem is that the salvager you have checks VVs on the first
pass not during the directory traverse as it should.  This has several
problems and has been fixed for the IBM product by delta
ota-db2355-file-status-VV.  It should be appearing in a dfs-carl build
near you soon.
In more detail what is happening is that the indexes the salvager is
complaining about are those reserved to ACLs.  This is odd because ACLs
do not have VVs.  However, until very recently because VVs were
partially maintained at the wrong level the primitve creation functions
assign ACL contains a VV upon creation, even though this VV is
inaccessible and is never updated.  The was restore works is to insert each file, and explicitly set its VV along with its other status information.  If the file has an ACL that ACL is set using a setacl call.  This will often create a new
 ACL (which in the 1.0.2 code base will assign it a new VV).  Then after all files have been restored the filesets VV is set to match the value in the dump.  This will often result in a lower VV value than that used by any ACL created duri
ng the restore process.  This leads to the errors you described.
The aforementioned delta causes the ACL never to get a VV and the
salvager not to try to check the VV of auxiliary containers.  It also
doesn't check VVs on inconsistent filesets (another potential source of
erroneous salvager output).
Filled in Subcomponent Name with `lfs' 
Changed Short Description from `unable to recover agg' to `salvage reports 
 bogus VV errors after repserver ops' 
Filled in Interest List CC with `cfe, jaffe' 
Changed Status from `open' to `cancel' 
Filled in Reported by Company's Ref. Number with `2355' 
Added field Transarc Deltas with value `ota-db2355-file-status-VV -r1.2' 
Added field Transarc Herder with value `jaffe' 
Added field Transarc Status with value `closed'

[5/25/93 public]
Ted has pointed out the problem.  If the OSF would like this fix, we can 
work on getting it to you for 1.0.2a.  This is a non-trivial change, and so
you may wish to wait.
Changed Status from `cancel' to `open' 
Filled in Affected File with `some' 
Changed Transarc Status from `closed' to `export'

[5/26/93 public]
Dropping the severity and priority now that this has been diagnosed. Thanks
for the warning - this may very well not make 102a. Jeff - can you add this
to your "potential release note" list?

[6/3/93 public]

[12/17/93 public]
Closed.



CR Number                     : 8029
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : upclient
Short Description             : upclient cant make it past an invalid obj in request list
Reported Date                 : 5/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/21/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : update/{server,client}.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot8029-upclient-cant-recover-from-bad-request
Transarc Herder               : jaffe@transarc.com

[5/21/93 public]
If upclient requests an object that the upserver is not exporting, it
will never make it past the bad entry.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/21/93 public]
To clarify, it is the entries *before* the bad one that will never be
retrieved since they are pushed onto a list left to right.
Defect Closure Form
-------------------
touch /opt/dcelocal/var/dfs/foo
Have an upserver export /opt/dcelocal/var/dfs/foo
Run upclient ... /opt/dcelocal/var/dfs/foo /opt/dcelocal/var/dfs/bar
		-time 3 -file uplog
previously, a message would be written to the log file but foo would
never be retrieved.  Now, the message still appears but foo is
retrieved. 
Also, run the clint and server a couple of times and see that the old
log is moved to <logfile>.old.
Associated information:
Tested on TA build:  
	dfs-102/dfs-carl2 1.10
Tested with backing build:  
	dce1.0.2a-0427
Filled in Interest List CC with `pehkonen%liz.austin.ibm.com@transarc.com, 
 jeff@transarc.com' 
Filled in Affected File with `update/{server,client}.c' 
Filled in Transarc Deltas with 
 `comer-ot8029-upclient-cant-recover-from-bad-request' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8026
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts move under load failure
Reported Date                 : 5/20/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/20/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : tu-o-ot8026-cm-more-bullitproof
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/20/93 public]
This is a placeholder to track the problems Tu and I saw with 
fts move on dfs.carl when Tu was visiting OSF.
 
The configuration was:
 
2 machine cell, 1 RIOS, 1 PMAX.  Core components, single flserver
on RIOS.
 
The test was: Tu's script moving a fileset back and forth every
10 minutes.  The load on the fileset was a connectathon being
run from the RIOS, and a fs and low (not-moderate), both being run 
from the pmax. 
 
The fts move failed on the ninth iteration with the following
error:
 
Token requested was in conflict with another (non-revocable) token.
 
This is unfortunately not very reproducable.

[5/20/93 public]
just adding myself and Dr. K to CC
Changed Interest List CC from `tu@transarc.com' to `cfe@transarc.com, 
 kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[05/21/93]
 
To keep from logging too many bugs, we're placing fts move problems
under this one.
 
Here's another problem which I encountered today.  I was moving a
fileset from fire (pmax) to alcatraz(rios) in preparation for
cho testing.  Well this move has taken about 1.5+ hour so far
and has not yet completed; there is no load on this fileset and
the fileset is not large.
 
There are two tests running on other filesets in this cell:
 
a. acl tests
b.  low.moderate
 
Info about the fileset being moved:
 
root@fire  # cm whereis .
The file '.' resides in the cell 'leprosy', in fileset 'm1.lfs2', 
on host fire.osf.org.
root@fire  # fts lsquota .
Fileset Name          Quota    Used  % Used   Aggregate
m1.lfs2               30000    4850    16%     9% = 18606/194672 (LFS)
root@fire  # 
 
Here is the FtLog from Fire:
 
93-May-21 16:43:06 trans 10005688 (Id=5, 0,,2218/4) is 5967 seconds old (ref count 1)93-May-21 16:43:06 (trans 10005688: desc=0, time=738011019, ctime=738010772, states=0x10040006, accs=0x183, acce=691089523)
93-May-21 16:43:37 trans 10005688 (Id=5, 0,,2218/4) is 5998 seconds old (ref count 1)93-May-21 16:43:37 (trans 10005688: desc=0, time=738011019, ctime=738010772, states=0x10040006, accs=0x183, acce=691089523)
93-May-21 16:44:07 trans 10005688 (Id=5, 0,,2218/4) is 6028 seconds old (ref count 1)93-May-21 16:44:07 (trans 10005688: desc=0, time=738011019, ctime=738010772, states=0x10040006, accs=0x183, acce=691089523)
 
Here's the tail end of the icl.ftserver from Fire:
 
Here's the FtLog from alcatraz:
93-May-21 13:59:47 ftserver_CreateVolume: created m1.lfs2 as 0,,496 on aggr 2
93-May-21 14:01:12 Restoring fileset 0,,496/2
93-May-21 14:04:17 SFTSERVER_Restore: Exception while restoring: (rpc_x_ss_pipe_comm_error) exception raised
93-May-21 15:21:29 trans 20119e28 (Id=3, 0,,496/2) is 181 seconds old (ref count 0)
93-May-21 15:21:29 (trans 20119e28: desc=0, time=738015508, ctime=738010786, states=0x12810015, accs=0x1c4, acce=691089523)
 
Here's the icl.ftserver from alcatraz
 
time 89.853857, pid 8: ftserver_FindTrans(3) entered 
time 89.854145, pid 8: ftserver_FindTrans(#1) returns 0x20119e28 (t_aggrId = 2, t_volId = 0,,496) 
time 89.949235, pid 8: vol_syscall(11, 0, 0x40000040, 0x201c2098, 
time 89.949520, pid 8:  0) entered 
time 90.072001, pid 8: vol_syscall returns 0 
time 90.072258, pid 8: vol_syscall(32, 0, 0x1, 0x1, 
time 90.072504, pid 8:  0xdeadbeef) entered 
time 90.123084, pid 8: vol_syscall returns 0 
time 90.123343, pid 8: ftserver_PutTrans({..., 3, 2, 0,,496, 0, ...} 
time 90.123602, pid 8: ftserver_PutTrans(#2) returns 0 
time 90.123836, pid 8: SFTSERVER_SetStatus(#4) returns 0 
time 95.131459, pid 7: ftserver_GCTrans(void) entered 
time 95.131726, pid 7: ftserver_GCTrans returns 0 
time 127.605313, pid 7: ftserver_GCTrans(void) entered 
time 127.936581, pid 7: ftserver_GCTrans returns 0 
time 153.005663, pid 8: SFTSERVER_Monitor(0x20119c78, OUT <entriesp>) entered 
time 154.362750, pid 8: SFTSERVER_Monitor returns 0, *entriesp = {1, ftserver_transStatus_val[]} 
time 158.187436, pid 7: ftserver_GCTrans(void) entered 
time 158.187702, pid 7: ftserver_GCTrans returns 0 
time 214.695286, pid 7: ftserver_GCTrans(void) entered 
time 215.155241, pid 7: ftserver_GCTrans returns 0 
time 246.406861, pid 7: ftserver_GCTrans(void) entered 
time 246.504270, pid 7: ftserver_GCTrans returns 0 
time 274.245793, pid 0: Current time: Fri May 21 15:18:26 1993 
time 274.245793, pid 8: SFTSERVER_SetStatus(0x20119d78, 3, 0x40000040, 0x201c2798) entered 
time 275.371593, pid 8: ftserver_FindTrans(3) entered 
time 275.371885, pid 8: ftserver_FindTrans(#1) returns 0x20119e28 (t_aggrId = 2, t_volId = 0,,496) 
time 275.510246, pid 8: vol_syscall(11, 0, 0x40000040, 0x201c2098, 
time 275.510539, pid 8:         0) entered 
time 276.034911, pid 8: vol_syscall returns 0 
time 276.035172, pid 8: vol_syscall(32, 0, 0x1, 0x1, 
time 276.035417, pid 8:         0xdeadbeef) entered 
time 276.089883, pid 8: vol_syscall returns 0 
time 276.090148, pid 8: ftserver_PutTrans({..., 3, 2, 0,,496, 0, ...} 
time 276.090407, pid 8: ftserver_PutTrans(#2) returns 0 
time 276.090642, pid 8: SFTSERVER_SetStatus(#4) returns 0 
time 276.652285, pid 7: ftserver_GCTrans(void) entered 
time 276.652557, pid 7: ftserver_GCTrans returns 0 
time 307.257726, pid 7: ftserver_GCTrans(void) entered 
time 307.364923, pid 7: ftserver_GCTrans returns 0 
 
Here's the output from the screen on the system running the
fts move:
 
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738016985.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017108.
FTSERVER_SetStatus for trans 3 (mask=40000040) failed
Error: communications failure (dce / rpc)
[server binding due to RPC error]
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017172.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017295.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017444.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017571.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017695.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017829.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738017957.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738018082.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738018204.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738018328.
move KA trans 3: dest id 0,,496: states := 0x12810015, TO := 738018450.

[6/8/93 public]
Changed Interest List CC from `cfe@transarc.com, kazar@transarc.com' to 
 `cfe@transarc.com, kazar@transarc.com, dstokes@transarc.com'

[6/9/93 public]
I've been running a test here at Transarc that often encounters an RPC
communication failure.  The basic test is to have three heavyweight
processes, each of which reads and writes files in a fileset, while a
fourth process moves that fileset from place to place.  At first, we
believed that the comm_failure was due to thread scheduling problems in
the ftserver, where CMA threads weren't allowing the RPC listener thread
to run often enough.  This was a problem, but fixing it didn't fix
everything.  In particular, there was also a class of system calls that
would sit in the kernel for many seconds at a time, so that the various
user-space threads wouldn't have any opportunity to run regardless of
what CMA might do.  These calls could require up to about 170 seconds to
execute, with many requiring 30 seconds or more.
 
The system call in question is the VOL_AGOPEN call, which prepares a
fileset for a sequence of low-level fileset operations (via the volops).
 Its time is naturally divided into the time necessary for waiting until
all vnode operations on the fileset have terminated, followed by the
time necessary for flushing any dirty data back from the VM system to
the disk.  We have a little timing data describing what happens when
writing data from VM to the disk, but as yet not much data describing
the concurrent vnode operations.
 
This note describes the impact of the problem and our current
assessments about solutions for it.
 
The impact of the problem as it presently stands is that fileset
operations may not, in some cases, gracefully coexist with concurrent
use of the files in a fileset.  At present, this problem has been
observed only with moving filesets under moderate load, though a
comparable situation may pertain to other fileset operations.  The
problem does not produce data loss, but the desired fileset operation
might not take place.  Administrators are informed or warned of the
situation.
 
Here is a table of operations and likely impact:
 
    fileset move: the move may not take place.  Data is cleaned up.
 
    release: a comm_failure may occur but the fileset will be released anyway.
 
    fileset clone: the clone will be created in spite of a comm_failure,
but if the fldb entry for the fileset didn't indicate that a clone
existed, it will not be updated to include that indication.
 
    destroying a clone fileset: the clone will be deleted, but if a
comm_failure occurs, the FLDB entry will still indicate the presence of
the clone.  Repeating the ``fts delete'' command will complete the task.
 
    replication: the repserver will encounter comm_failure responses
from the ftserver, but it should retry its operations transparently. 
However, when the repserver itself delays for a long-running system
call, fts processes may encounter comm_failure responses, to commands
such as ``fts lsreplicas'', ``fts statrepserver'', ``fts release'',
``fts update'', and ``fts addsite''.
 
These situations represent the worst cases.
 
In addition to measuring what's going wrong here, I have done some work
at improving the problem, and we are discussing what further steps to
take.  The simplest improvement is to increase the RPC comm_failure
timeout from the default 30 second interval to 2 minutes; this allows
most cases that failed with the 30 second value to succeed, and in fact
allows them to succeed often enough that we can debug other problems.
 
There is other work that is clearly correct and resasonable.  In
particular, we intend to prevent the file exporter from holding the
count of activeVnops high for the entire duration of its call, but
rather to hold it high only while that call is not waiting for the
result of an RPC pipe.  This work will reduce the latency with which the
active-vnode operations are being shut down when quiescing a fileset.
 
There is more speculative work possible.  One strategy is that a process
that does possibly-long-lived system calls could do them via a slave
heavyweight process, so that the slave could block in the kernel and the
high-level caller could use wrapped CMA reads from interprocess pipes. 
This is unattractive for several reasons, not least of which is the
tradeoff between filesystem-architecture dependence of knowing which
volop syscalls are slow, vs. the performance penalty of carrying out all
the volop syscalls in this manner.
 
A second strategy would be to start a helper daemon in the kernel to
carry out the long-running volop calls, and define some mechanism for
user processes to determine whether the volop call had yet finished.  We
would lean toward a polling method of determining whether it was done,
simply to reduce exposure to the vagaries of signal handling in a
threaded environment.
 
Comments?

[6/10/93 public]
Changed Interest List CC from `cfe@transarc.com, kazar@transarc.com, 
 dstokes@transarc.com' to `cfe@transarc.com, kazar@transarc.com, 
 dstokes@transarc.com, comer@transarc.com'

[6/16/93 public]
Just a note for now.  I have confidence that the fix for OT 8042
will handle the ftserver's bad handling of rpc pipe exceptions that Diane
mentions above, causing the FTSERVER_Forward() operation to terminate rather
than continue for 1.5 hours and more.  In the same fix, I've also increased
the ftserver-to-ftserver RPC timeout, which should reduce the incidence of
the problem.
 
I've also put together several fixes that affect both ftserver threads and
the latency with which the kernel can respond to VOL_AGOPEN requests, some
of which work was alluded to above.  These are relatively lightweight
fixes, though, not doing any massive overhaul either to how the ftserver
does its volops (e.g. through a companion process) or to the kernel's
treatment of, and interface to, long-running volume operations.  They seem
to fix all but pathological cases, though.
 
None of these fixes affects the initial complaint of this OT, though, which
was fts reporting an inability to obtain a token, generally indicating a
CM that refused to revoke a token that conflicts with the requested token.

[6/28/93 public]
We have more data for the initial (only remaining) problem reported in this
OT, which was the ``token conflicted with a non-revokable token'' message.
Two things conspire to cause the problem.  First, the DFS token architecture
always allows the server to hold tokens that a CM doesn't know are being
held; the thinking is that if a CM doesn't care or know about a token,
it will allow the token to be revoked trivially should a conflict arise.
Not that the CM seeks tokens that it intends to discard, but just that the
protocol allows errors of this direction to occur.  (Errors in the opposite
direction, where a CM thinks that it holds a token that the server doesn't
think is granted, are not tolerated.)
 
The second fundamental piece of the story is that when the CM re-obtains
tokens after a server crash or after a fileset move, there isn't really room
for inter-CM negotiation for tokens, so the protocol used is that the CMs
always ask for an exact token grant (only those token types that they
possessed before the crash or move), and it asks that no revocations take
place.  This last rule is actually in place to keep infinite recursions from
taking place, consuming all available threads, deadlocking, and like that.
 
So when you create a new fileset on server A, and you move it to server B,
in the process, all the CMs having tokens granted on the fileset from server
A re-acquire comparable tokens on server B using this no-revocation protocol.
They re-acquire all the tokens they care about in that manner.  But various
sources of noise could allow server A to think that tokens are still granted
from it (server A) with respect to the moved fileset.  Ordinarily, these would
be cleaned up by various forms of pressure, like TKM GCing and the like.
But let's say that before that happens, the admin moves the fileset back
from server B to server A.  Well, in the process all the CMs will then ask
server A for all the tokens they think they possess and care about, using
the special no-revoke protocol.  But if these requests then collide with some
of the junk tokens that server A thought it still had granted on that
fileset, then oops, the token manager returns TKM_ERROR_TOKENCONFLICT.
This translates to the ``non-revokable token'' message we first saw.
 
A secondary bug caused the CM not to handle TOKENCONFLICT errors correctly
when it was using a no-revoke token grant protocol.  This is being fixed,
and the CM now responds quickly to the problem.
 
The initial problem is being fixed by sweeping the token manager clean of
outstanding tokens as soon as a move target fileset is created, thus allowing
any CMs that are later pointed at that fileset to be able to acquire their
tokens without requiring revocations, without encountering conflicts, and
without undue grief.
Changed Interest List CC from `cfe@transarc.com, kazar@transarc.com, 
 dstokes@transarc.com, comer@transarc.com' to `cfe@transarc.com, 
 kazar@transarc.com, dstokes@transarc.com, comer@transarc.com, davel' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `open'

[6/29/93 public]
Filled in Transarc Deltas with `tu-o-ot8026-cm-more-bullitproof' 
Changed Transarc Status from `open' to `import'

[6/29/93 public]
Ideally, the fix for this would include delta
cfe-ot8026-trace-fids-for-gettoken 1.2, but just Tu's fix will keep the
CM from looping almost-forever.  The cfe-ot8026 delta will minimize or
eliminate the possibility of the token conflict's occurring.  I'm not sure
what has been imported at this point.

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8023
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fx
Short Description             : fetchdata should not perform access chks on symlinks
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.1
Found Date                    : 5/19/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/px/px_subr.c
Sensitivity                   : public
Transarc Deltas               : cburnett-ot8023-fx-treat-symlinks-as-public-access
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/19/93 public]
Currently the fileserver (px_rdwr) performs access checks on sym
links before calling the underlying physical filesystem to read
the link.
Instead the fileserver should ignore the access check for sym links
and call the filesystem.  Both LFS and UFS treat allow open access to
sym links so should the fileserver.
Otherwise access through DFS mountpoints (which are really symlinks)
can fail (even if the fileset they point has wide open permissions)
for anonymous accesses.
This behavior will become very evident when that latest ACL fixes
from Transarc are incorperated into the DFS code base.
This can become very confusing to an administrator depending
on the io ACL of the parent directly where the symlink (mountpoint)
is created.  And since the ACL on the mountoint itself cannot
be viewed or modified with acl_edit, it is almost impossible to
figure out what is wrong if access checking is performed symlink.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'
[public]
Defect Closure Form
-------------------
Validated access though a mountpoint as an anonymous user.
The following procedure can be used. (note LFS filesets must be used).
create a fileset
create a mountpoint to a fileset in a dir which denies anon access
acl_edit the path to the root of the fileset to add any_other:rwxid perms
Now reboot the client
After rebooting and reconfiguring DFS-
Try an ls on the path to the newly recreated fileset.
This should succeed.
Without this fix the ls would fail while traversing the mountpoint.
Filled in Transarc Deltas with 
 `cburnett-ot8023-fx-treat-symlinks-as-public-access' 
Changed Transarc Status from `open' to `export'

[5/19/93 public]

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/px/px_subr.c' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8021
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : tkm may screwup if async grant token returned before grant
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2
Found Date                    : 5/19/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8021-protect-tkm-returntoken-against-races
Transarc Herder               : jaffe@transarc.com

[5/19/93 public]
The token manager is vulnerable to an async grant token being returned
while it is still pending, since it doesn't check to see if some other
thread is trying to grant the token.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with 
 `kazar-ot8021-protect-tkm-returntoken-against-races' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8020
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : lose rw fileset on detach attempt
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/19/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/xaggr/export.c, file/xaggr/ag_init.c, file/userInt/fts/volc_tokens.c, file/ftutil/ftutil.c
Sensitivity                   : public
Transarc Deltas               : jdp-db3549-ignore-unattached-filesets-on-detach
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/19/93 public]
CONFIG:	4 mach cell, 3 flservers
BUILD:	dfs.carl
TEST:	dfs.repfs_checklist
 
root@valentine> fts lsheader -aggr lfs_aggr2 -server valentine
Total filesets on server valentine aggregate lfs_aggr2 (id 2): 3
epi.2                    0,,25 RW   1605 K alloc   1605 K quota On-line
epi.a.readonly           0,,55 RO   1063 K alloc   1063 K quota On-line
epi.b.readonly           0,,58 RO     17 K alloc     17 K quota On-line
Total filesets on-line 3; total off-line 0; total busy 0
 
root@valentine> cat /opt/dcelocal/var/dfs/dfstab
# blkdev aggname aggtype aggid 
/dev/rz1a       lfs_aggr        lfs     1
/dev/rz1e       lfs_aggr2       lfs     2
/dev/rz1b       /u2     ufs     4       0,,36
 
root@valentine> dfsexport /dev/rz1e -detach
dfsexport: Revoking tokens for filesets on aggregate 2...
dfsexport: Failed to detach /dev/rz1e:lfs_aggr2 (Device busy)
root@valentine> fts release epi.a
Released fileset epi.a successfully
root@valentine> dfsexport /dev/rz1e -detach
dfsexport: Revoking tokens for filesets on aggregate 2...
Can't get 0xd0c token on 0,,25, server valentine.osf.org: fileset already deleted/moved (dfs / xvl)
dfsexport: Could not revoke tokens for fileset 0,,25: fileset already deleted/moved (dfs / xvl)
dfsexport: Failed to detach /dev/rz1e:lfs_aggr2 (fileset already deleted/moved (dfs / xvl))
root@valentine> cm whereis /:/epi_2
The file '/:/epi_2' resides in the cell 'p102a_cell.qadce.osf.org', 
in fileset 'epi.2.readonly', on host dce5.osf.org.
root@valentine> cm whereis /:/root_dfs/epi_2
dfs: fileset (0,,25) error (code 691089410) on server 130.105.5.27 in cell p102a_cell.qadce.osf.org.
cm: '/:/root_dfs/epi_2': No such device
root@valentine> fts lsheader -aggr lfs_aggr2 -server valentine
Total filesets on server valentine aggregate lfs_aggr2 (id 2): 3
**** Fileset 0,,25 is busy: No such device ****
epi.a.readonly           0,,55 RO   1063 K alloc   1063 K quota On-line
epi.b.readonly           0,,58 RO     17 K alloc     17 K quota On-line
Total filesets on-line 2; total off-line 0; total busy 1
 
I had not been using epi.2 but it is a rw fileset with scheduled replication.
>From fts lsfldb:
 
epi.2  
        readWrite   ID 0,,25  valid
        readOnly    ID 0,,26  valid
        backup      ID 0,,27  invalid
number of sites: 2
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner 
valentine.osf.org   RW       lfs_aggr2 0:00:00 hosts/valentine<nil>
dce5.osf.org        RO       lfs_aggr3 0:30:00 hosts/dce5     <nil>

[5/19/93 public]
Jeff Prem has seen something like this; reassigned to him.
FYI, though, if the ``dfsexport -detach'' fails, some filesets will in general
have been detached.  Not all, but some.  The fix for the moment is to get
a repaired ``dfsexport'' program and retry the -detach attempt.
Filled in Interest List CC with `comer@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `comer@transarc.com' to `jdp@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/19/93 public]
This problem has been addressed by the delta named in the Transarc Delta
field, but I'll leave the handling of the export/delivery issues to the
Transarc integration group.
I've downgraded this, since there was no data lost.
Changed Interest List CC from `comer@transarc.com, cfe@transarc.com' to 
 `comer@transarc.com, cfe@transarc.com, jaffe' 
Changed Severity from `A' to `B' 
Filled in Priority with `2' 
Filled in Reported by Company's Ref. Number with `3549' 
Filled in Transarc Deltas with 
 `jdp-db3549-ignore-unattached-filesets-on-detach'

[5/20/93 public]
You're correct - no data loss - I still think having to reboot to recover
is not an acceptable work around however - compromising at 1.

[5/20/93 public]
This is ready to go.  Is it something that should be in 1.0.2a?
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `export'

[5/20/93 public]
Yes - but I believe we're switching to individual submissions and would
like individual submissions for A0's to occur 1st, then A1's, then B0's
(if any), then B1's ... of course, that's the ideal - requests for
individual submissions to the dfs-drb are the way to go.

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/xaggr/export.c, file/xaggr/ag_init.c, 
 file/userInt/fts/volc_tokens.c, file/ftutil/ftutil.c' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8019
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : rep tests clean up replicas in wrong order
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2
Found Date                    : 5/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot8019-bad-rep-test-cleanups
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/19/93 public]
At least one rep test (rtest2) cleans up the R/O sites of a release-replicated
setup in a bad order, causing the repserver to log horrible-looking messages.
There are no other ill effects.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/24/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Rep tests no longer produce horrible messages in the RepLog, claiming that
some release-replicated fileset exists with no staging replica.
 
Associated information:
Tested on TA build:  dfs-carl2, a.k.a. dfs-carl 1.10
Tested with backing build:  dce1.0.2a-0427
Filled in Transarc Deltas with `cfe-ot8019-bad-rep-test-cleanups' 
Changed Transarc Status from `open' to `export'

[6/14/93 public]

[6/15/93 public]
Changed Transarc Status from `export' to `import'

[6/21/93 public]
We'de like to get this into 1.0.2a, since it is a functional test failure, and
all functional tests are supposed to pass.
Changed Fix By Baseline from `1.0.3' to `1.0.2a'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8018
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : scout
Short Description             : Syntax of scout command is incorrect
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2
Found Date                    : 5/19/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot8018-fix-scout-syntax-to-match-docs
Transarc Herder               : jaffe@transarc.com

[5/19/93 public]
The syntax for the DFS scout program is currently the following:
   scout -server <FileServer name(s) to monitor>...
   [-basename <base server name>] [-frequency <poll frequency, in seconds>]
   [-host] [-attention <specify attention (highlighting) level>...]
   [-debug <turn debugging output on to the named file>] [-help]
It needs to be changed to the following to match the documentation:
   scout -server <machine>... [-basename <common_prefix>] [-host]
   [-frequency <seconds>] [-attention <stat/threshold_pair>...]
   [-debug <filename>] [-help]
Essentially, the argument names are all wrong; the proper options appear to
be in place.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/20/93 public]
Defect Closure Form
-------------------
--Verification procedure below--

 [scout] ./scout -help
Usage: ./scout  -server <machine>... [-basename <common_prefix>]

 [-host] [-frequency <seconds>] [-attention <stat/threshold_pair>...]
 [-debug <filename>] [-help]
I also made sure that -host and -freq still work since I had to swap them.
Associated information:
Tested on TA build:  
	dfs-102/dfs-carl2 1.10
Tested with backing build:  
	dce1.0.2a-0427
Filled in Interest List CC with `pehkonen%liz.austin.ibm.com@transarc.com' 
Filled in Transarc Deltas with `comer-ot8018-fix-scout-syntax-to-match-docs' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8017
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cache manager should return tokens better
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2
Found Date                    : 5/19/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8017-dont-wait-for-refcount-on-tokens
Transarc Herder               : jaffe@transarc.com

[5/19/93 public]
There's a potential deadlock when the CM fails to store back status 
while doing a token revocation, due to its waiting for a ref count
to drop that isn't going to drop.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8017-dont-wait-for-refcount-on-tokens' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8016
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : butc
Short Description             : bak scantape fails to read the same tape twice
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2
Found Date                    : 5/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/butc/recoverDb.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-o-ot801-bak-scantape-fails-to-read-same-tape-twice
Transarc Herder               : 

[5/19/93 public]
Doing a bak scantape on the same tape twice, results in an error on the
second attempt to scan the tape. The output from butc is:
Are there more tapes? (y/n) y
******* OPERATOR ATTENTION *******
Device :  /dev/rmt0 
Put in the tape whose contents are to be scanned
Hit return when done
Thanks, now proceeding with tape scanning operation.
**********************************
aborting - this dump cannot be processed correctly
This tape may not have a valid dfs backup dump on it
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[5/19/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[9/24/93 public]
Changed Responsible Engr. from `vijay@transarc.com' to `abhijit@transarc.com'

[9/24/93 public]
Get the mail address right
Changed Responsible Engr. from `abhijit@transarc.com' to `khale@transarc.com'

[11/9/93 public]
   
Defect Closure Form
-------------------
   This fixes a problem wherein butc immediatedly aborts a scantape if any
tape after the first has the wrong header. While this behavior is correct,
it should be more tolerant of operator error. The fix changes butc to prompt
MAXTRIES times and then abort if the wrong tape is still being put in.
--Verification procedure below--
A.
   1.  Mount a known good backup tape and do a scantape.
   2.  When butc asks if there are more tapes, answer yes but don't change
       tapes.
   3.  Hit <cr> when asked.
   4.  butc should complain about an out of sequnece tape but should not
       abort the scantape job.
   5.  Kill the job from the bak window.
B.
   1.  For a dump that scans at least 2 tapes, mount the first tape
       and do a scantape.
   2.  When butc asks if there are more tapes, answer yes but don't change
       tapes.
   3.  Hit <cr> when asked.
   4.  butc should complain about an out of sequnece tape but should not
       abort the scantape job.
   5.  Now insert the correct second tape.
   6.  butc should successfully scan this tape.
Associated information:
Tested on TA build:  
dfs-103-3.38
Tested with backing build:  
dce1.0.2ab4
Filled in Interest List CC with `davecarr@transarc.com' 
Filled in Affected File with `file/butc/recoverDb.c' 
Filled in Transarc Deltas with 
 `khale-o-ot801-bak-scantape-fails-to-read-same-tape-twice'

[12/17/93 public]
Closed.



CR Number                     : 8015
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : token manager
Short Description             : tkm token states managed poorly
Reported Date                 : 5/19/93
Found in Baseline             : 1.0.2
Found Date                    : 5/19/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot8015-fix-tkm-state-machine
Transarc Herder               : jaffe@transarc.com

[5/19/93 public]
TKM has a number of places where token states are managed in a strange
fashion.  Since certain states represent locked tokens, while others
represent unlocked tokens, this is a significant reliability problem.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot8015-fix-tkm-state-machine' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 8010
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bosserver
Short Description             : bosserver lsadmin trips over bad cell uuids
Reported Date                 : 5/17/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/17/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : security/dacl/dacl_sec_acl.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot8010-dacl-handle-unknown-foreign-cell-uuid-on-print
Transarc Herder               : jaffe@transarc.com

[5/17/93 public]
If an admin list contains an foreign entry but the cell uuid in
the entry is bad, an lsadmin of the list will fail (and stop printing)
at that point.  Instead, it should print the uuid.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/19/93 public]
Defect Closure Form
-------------------
--Regression test program below--
I did a bos lsadmin with existing lists and they worked fine, as
expected. 
--Verification procedure below--
I did the same test on an admin list that had both a foreign realm and
foreign use entries.  It printed the numeric representation, as expected.
Associated information:
Tested on TA build:  
	dfs-102/dfs-carl2 1.10
Tested with backing build:  
	dce1.0.2a-0427
Filled in Affected File with `security/dacl/dacl_sec_acl.c' 
Filled in Transarc Deltas with 
 `comer-ot8010-dacl-handle-unknown-foreign-cell-uuid-on-print' 
Changed Transarc Status from `open' to `export'

[5/20/93 public]
Downgrading to 2, doesn't appear critical to 1.0.2a.

[12/17/93 public]
Closed.



CR Number                     : 8007
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bosserver
Short Description             : bosserver may hang in loop
Reported Date                 : 5/17/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/17/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/bosserver/bossvr_thread_bnodeTimeout.c
Sensitivity                   : public
Transarc Deltas               : comer-ot8007-fix-bosserver-hang
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/17/93 public]
The bosserver may hang in a loop because of an uninitialized variable.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/17/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
This problem only came up after compiling -O; and it happened consistently
after do so.  After the fix, no more hangs.  
Associated information:
Tested on TA build:  
	dfs-102/dfs-carl2 2.10
Tested with backing build:  
	dcde1.0.2a-0427
Filled in Transarc Deltas with `comer-ot8007-fix-bosserver-hang' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/bosserver/bossvr_thread_bnodeTimeout.c' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8006
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : panic from dacl_FreeAcl_Entries
Reported Date                 : 5/14/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/14/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/security/dacl/dacl_print.c
Sensitivity                   : public
Transarc Deltas               : comer-db3512-initialize-entries_allocated-on-create
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/14/93 public]
This happened during cho.  We let cho run for about 24 hours and
then attempted to move a fileset;  After the third try the
move completed and the destination server panice'd with
the following stack trace:
panic: badd addr 
SAFS_RemoveFile
vrele
xglue_inactive
nosf_inactive
efs_inactive
dacl_ReleaseAcl
dacl_FreeAcl_Entries
osi_free
osi_kfree
free
panic
The address passed to free looks like it's 0.

[5/18/93 public]
I believe the comer might have seen this problem.
Filled in Interest List CC with `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/18/93 public]
If not, I've seen something very similar.  I believe  the problem is
an uninitialized member of the DACL structure.  It is fixed in
comer-db3512-initialize-entries_allocated-on-create.  Elliot, could
you make sure this is imported?

[05/20/93 public]
Fine, If you've got the fix then I'm reassigning it to you.  Also, I've
elevated this to priority 0 since it does show up during cho. 
I request that you submit the (tested) fix individually, as I don't wan't
to wait for the next drop to get it; this is a much more serious problem
than the other defects which I've seen going into import state - Thanks.

[5/20/93 public]
submitted.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/security/dacl/dacl_print.c' 
Filled in Transarc Deltas with 
 `comer-db3512-initialize-entries_allocated-on-create' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `submit'

[06/17/93 public]

Have not seen the problem since we received the fix.



CR Number                     : 8005
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : xxx_obj entries in initial acls are templates only
Reported Date                 : 5/14/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/14/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/episode/vnops/efs_access.c, file/episode/vnops/efs_vnodeops.c, file/episode/vnops/fuser.test, file/libafs/RIOS/dfscore.exp, file/security/dacl/dacl.p.h, file/security/dacl/dacl_default.c, f
ile/security/dacl/dacl_init.c, file/security/dacl/dacl_validate.c
Sensitivity                   : public
Transarc Deltas               : rajesh-ot8005-use-xxx_obj-entries-in-initial-ACLs-as-template-only
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/14/93 public]
This report aims to describe the ACL model, that we reached consensus 
on today. The model uses user_obj, group_obj and other_obj in dummy
initial acls or actual initial acls as a template only.
Scenario:
	Cell A: is the storing cell
	D1 :  directory stored in cell A owned by a principal in cell A.
	Cell X : another cell.
	The acl for D1 has an anyother entry allowing "wi" permissions.
User foo@cellX creates a subdir D2 in D1, with mode bits 777. 
Case 1: D1 has no initial container (IC) acl or  initial file acl (IF).
	D2 gets an access acl but neither IC nor IO acl. The access acl 
	has a default realm of cell X. It has  user_obj, group_obj,
	other_obj entries ONLY with permissions being derived from
	the requested mode (to mkdir system call) filtered
	through the umask.
Case 2: D1 has IC acl.
	
	D2 gets an access acl. The access acl 
	has a default realm of cell X. It has user_obj, group_obj,
	other_obj entries with permission for each being the
	intersection of the requested mode ( to mkdir ) and the
	user_obj, group_obj, other_obj entries, in the IC acl on D1, 
	respectively.
	D2 gets an IC acl. The user_obj, group_obj, other_obj entries 
	are exactly the same as in D1 IC acl. 
	If D1 IC acl has any of user, group, foreign_user,
	foreign_group entries then will undergo the realm transformations.
	All other entries in D1's IC acl (including foreign_other) will
	be copied without any realm transformations.
The other cases where D1 has IO acl or both IC /IO acl follow from case2.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/18/93 public]
First a clarification - If case 2 above, if D1's IC acl has user, group, 
foreign_user or foreign_group entries, then the corresponding entries in 
BOTH D2's regular acl and in D2's IC acl will have undergone realm
transformations.
Defect Closure Form
-------------------
--Verification procedure below--
I ran test_vnodeops and all the ACL tests which pass thus ensuring
no regressions.
Additonally I manually tried various sequences of operations that
to confirm that the code did work the way as outlined in the new
model above as the ACL tests do not test foreign_users, though 
test_vnodeops does some of that.
I tested tried many sequences - the variations being :
- the directories being owned by member of storing cell or by unauth user
- the directories having no explicit IC nor IO acl, having only 1 of 
  IC/IO acl, or having both.
- In case of dirs with explicit initial acls, set combinations of
  various entries:
  user, group, any_other, foreign_other (including one for default realm of 
  acl), mask_obj. Also foreign_user and foreign_group entries 
  when default realm of acl was the unauthenticated cell.
- check access on resulting dirs and files
However, I did not try each and every possible combination. Since I do
not yet have a multi-cell machine setup, I have not been able to try
creating files/dirs from cells other than the storing cell and
unauthenticated cell. Also addition of foreign_{user,group,other}
entries for cells other than the storing cell need to be tried.  (As
foreign_{user,other} entry for unauthenticated cell cannot be added as
acl_edit does not recognize that cell and generates an error.  Also
foreign_group for unauth cell does not have any meaning.)
If you find that there are other interesting variations, I would
appreciate hearing from you.
I have not listed all the tests, as then this report would become 
v. long. But if there is interest, I can write up a note describing
the interesting cases to try in more detail.
An example of sequence of operations I tried is:
Example 1:
As dce user rajesh:
#  klist | grep prin
Default principal: rajesh@bugacl
# pwd
/.../bugacl/fs/rajesh
# ls -l
total 8
-rw-r--r--   1 rajesh   system        57 May 18 07:05 README
drwxrwxr-x   4 rajesh   system       352 May 18 07:10 d00
drwxrwxr-x   2 rajesh   system       288 May 18 07:13 d01
drwxr-xr-x   2 rajesh   system       256 May 18 07:05 d10
drwxr-xr-x   2 rajesh   system       256 May 18 07:05 d11
NOTE: d01 has explicit regular ACL, IO ACL but no IC ACL.
# cd d01
# acl_edit . -l
# SEC_ACL for .:
# Default cell = /.../bugacl
mask_obj:rwx-id
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
any_other:rwx-id
# acl_edit . -io -l
# Initial SEC_ACL for objects created under: .:
# Default cell = /.../bugacl
mask_obj:rwx---
user_obj:rwxc--
user:ravi:r-x---
group_obj:r-x---
group:aclgog:r-x---
other_obj:r-x---
foreign_other:/.../bugacl:rwx---
any_other:r-x---
# acl_edit . -ic -l
# Initial SEC_ACL for directories created under: .:
# Default cell = /.../bugacl
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
As unauth user:
# klist | grep prin
Default principal: hosts/power7/self@bugacl
# whoami
root
# pwd
/.../bugacl/fs/rajesh
# ls -l
total 8
-rw-r--r--   1 rajesh   system        57 May 18 07:05 README
drwxrwxr-x   4 rajesh   system       352 May 18 07:10 d00
drwxrwxr-x   2 rajesh   system       288 May 18 07:13 d01
drwxr-xr-x   2 rajesh   system       256 May 18 07:05 d10
drwxr-xr-x   2 rajesh   system       256 May 18 07:05 d11
# cd d01
# touch unauthfile3
# ls -l
total 0
-rw-rw-r--   1 42949672944294967294      0 May 18 07:13 unauthfile1
-rw-rw-r--   1 42949672944294967294      0 May 18 07:31 unauthfile2
-rw-rw-r--   1 42949672944294967294      0 May 18 07:35 unauthfile3
# acl_edit unauthfile3 -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for unauthfile3:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
mask_obj:rw----
user_obj:rw-c--
foreign_user:/.../bugacl/ravi:r-x---    #effective:r-----
group_obj:r-x---        #effective:r-----
foreign_group:/.../bugacl/aclgog:r-x--- #effective:r-----
other_obj:r-----
foreign_other:/.../bugacl:rwx---        #effective:rw----
any_other:r-x---        #effective:r-----
Associated information:
Tested on TA build:  dfs-102-2.10
Tested with backing build: dce1.0.2b23 
Filled in Transarc Deltas with 
 `rajesh-ot8005-use-xxx_obj-entries-in-initial-ACLs-as-template-only' 
Changed Transarc Status from `open' to `export'

[6/2/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/episode/vnops/efs_access.c, 
 file/episode/vnops/efs_vnodeops.c, file/episode/vnops/fuser.test, 
 file/libafs/RIOS/dfscore.exp, file/security/dacl/dacl.p.h, 
 file/security/dacl/dacl_default.c, file/security/dacl/dacl_init.c, 
 file/security/dacl/dacl_validate.c' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8004
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : tkm leaks tkns for RO filesets
Reported Date                 : 5/14/93
Found in Baseline             : 1.0.1
Found Date                    : 5/14/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/tkm/tkm_getToken.c
Sensitivity                   : public
Transarc Deltas               : cburnett-ot8004-tkm-plug-ro-fs-tkn-leaks
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/13/93 public]
Tokens granted by the token manager on read only filesets
are never returned to the tokens manager's freelist.
Eventually the token manager will run out of tokens.
This can be produced in the following way:
in a loop:
   clone a medium sized fileset (30 meg with lots of files)
   create a mountpoint to the clone
   tar the clone to /dev/null
   delete the clone (backup fileset)
   repeat the loop.
Eventually this will fail with a TKM_NO_MEM error.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/13/93 public]
Defect Closure Form
-------------------
The above mentioned test will not run without all the
tokens getting eaten up!
Filled in Transarc Deltas with `cburnett-ot8004-tkm-plug-ro-fs-tkn-leaks' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/tkm/tkm_getToken.c' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8003
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fshost
Short Description             : revert anon id back to -2 (nobody)
Reported Date                 : 5/13/93
Found in Baseline             : 1.0.1
Found Date                    : 5/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/fshost/fshs_prutils.c
Sensitivity                   : public
Transarc Deltas               : cburnett-ot8003-revert-anon-back-to-minus-2
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/13/93 public]
Per recent discussions with Transarc about the ACL sementics,
it was decided that the recent change to make the anonymous
id -3 in DFS should be reverted back to -2.  This corresponds
to the nobody entry in the DCE registry.  Transarc has already
opened a doc defect I believe to make the necessary doc changes.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/13/93 public]
Defect Closure Form
-------------------
Files created by non dce_login user should be onwed
by id -2
Filled in Transarc Deltas with `cburnett-ot8003-revert-anon-back-to-minus-2' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[5/21/93 public]
submitted to OSF.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/fshost/fshs_prutils.c' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 8002
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : null ACL anon access done incorrecly by lfs
Reported Date                 : 5/13/93
Found in Baseline             : 1.0.1
Found Date                    : 5/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/episode/vnops/efs_access.c
Sensitivity                   : public
Transarc Deltas               : cburnett-ot8002-lfs-null-acl-anon-evaluation
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/13/93 public]
LFS is performing access checking incorrectly for anonymous request
when there is a "NULL" ACL on the file.  It is comparing against
the equivelant of other_obj, when it should actually be denying
access since there is an implied empty any_other perm set for the
NULL ACL case.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/13/93 public]
Defect Closure Form
-------------------
This can be tested in the following way:
Create a LFS fileset and a mountpoint to it.
Try to ls it as an non dce_login user (anonymous)
It should fail.
Now as a privilidged user (cell_admin) add an ACL entry
for any_other which allows access.
Now the ls as an anonymous user should work.
Filled in Transarc Deltas with `cburnett-ot8002-lfs-null-acl-anon-evaluation' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[6/2/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/episode/vnops/efs_access.c' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7999
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : compat
Short Description             : shutdown duplicate server code prints confusing msg
Reported Date                 : 5/13/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : ncscompat/compat_serverdup.c
Sensitivity                   : public
Transarc Deltas               : comer-ot7999-remove-dup-server-warning-message
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/13/93 public]
The shutdown duplicate server code prints a message if it fails in
contacting what it thinks is a duplicate server.  The rpc call it's
making is rpc_mgmt_is_server_listening, which may fail.  Since the
code does not try to recover from this and since the server is
probably not really there, the error message should just be removed.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/13/93 public]
Removed the message.
Defect Closure Form
-------------------
--Regression test program below--
Run the bosserver.  Run a second bosserver.  Verify that when the
second returns, there is only one running.
--Verification procedure below--
Kill the bosserver and its managed processes by signal.  Restart it.
There should be no message printed and the bosserver should start normally.
Associated information:
Tested on TA build:  
	dfs-102 2.10
Tested with backing build:  
	dce1.0.2b23
Filled in Interest List CC with `dstokes%liz.austin.ibm.com@transarc.com' 
Changed Fix By Baseline from `1.0.2' to `1.0.2a' 
Filled in Affected File with `ncscompat/compat_serverdup.c' 
Filled in Transarc Deltas with `comer-ot7999-remove-dup-server-warning-message' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[5/20/93 public]
Downgrading to 2.

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7983
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : too many arguments to function `epis_VerifyAggrPaths'
Reported Date                 : 5/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/12/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/episode/salvage/paths.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/12/93 public]
in /file/episode/salvage
/project/dce/build/nb_pmax/tools/pmax/macho/gcc -B/project/dce/build/nb_pmax/tools/pmax/macho/  -c    -D_SHARED_LIBRARIES -O  -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\"  -G 0 -pedantic -Wpointer-arith -nostdinc -Wtrigr
aphs -Wcomment    -I- -I. -I/project/dce/build/nb_pmax/src/file/episode/salvage -I../anode -I/project/dce/build/nb_pmax/src/file/episode/anode -I/project/dce/build/nb_pmax/export/pmax/usr/include -I/project/osc/build/osc_sbox1.1.1/export/p
max/usr/include -I/project/osc/build/osc1.1.1/export/pmax/usr/include -pic-lib   -L/project/dce/build/nb_pmax/export/pmax/usr/shlib -L/usr/shlib -L/project/dce/build/nb_pmax/export/pmax/usr/lib -L/project/osc/build/osc1.1.1/export/pmax/usr
/ccs/lib  -lcom_err -ldir -lanode -llwpdummy -ledsk -llogbuf -lasync  -lxcred -losi -ltools  -ldfskutil -ledsk -lcmd -lftutil -lafssys  -ldce -lbsd   ../../../../../src/file/episode/salvage/paths.c
../../../../../src/file/episode/salvage/paths.c: In function epis_VerifyDevPaths:
../../../../../src/file/episode/salvage/paths.c:720: warning: argument passing between incompatible pointer types
../../../../../src/file/episode/salvage/paths.c:720: too many arguments to function `epis_VerifyAggrPaths'
*** Error code 1 (continuing)
`build_all' not remade because of errors.

[5/12/93 public]
Removed extra argument to epis_VerifyAggrPaths.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/episode/salvage/paths.c' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[12/17/93 public]
Closed.



CR Number                     : 7982
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : undefined symbols in xaggr
Reported Date                 : 5/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/12/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/xaggr/Makefile
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/12/93 public]
in /file/xaggr
/project/dce/build/nb_pmax/tools/pmax/macho/gcc -B/project/dce/build/nb_pmax/tools/pmax/macho/  -%ld," -warn_nopic -glue"  -O        -L/project/dce/build/nb_pmax/export/pmax/usr/shlib -L/usr/shlib -L/project/dce/build/nb_pmax/export/pmax/u
sr/lib -L/project/osc/build/osc1.1.1/export/pmax/usr/ccs/lib -o dfsexport.X export.o aggrDesc.o volDesc.o  -lvolc -lftserver -lnubik -licl -lfldb -lncompat -lafs4int -ldauth -lrep -lcommondata -ldacl  -ldacllfs  -ldfsncs -lafsutil -lbomb  
-lcmd -lftutil -lafssys -ldfstab -losi -lcom_err  -ldce -lbsd  
bomb_core.o: Undefined symbol "dfsh_HashIn" referenced
bomb_core.o: Undefined symbol "dfsh_HashInit" referenced
bomb_core.o: Undefined symbol "dfsh_HashInitKeyString" referenced
bomb_core.o: Undefined symbol "dfsh_HashLookup" referenced
*** Error code 1 (continuing)

[5/12/93 public]
Corrected order of libraries.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/xaggr/Makefile' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[12/17/93 public]
Closed.



CR Number                     : 7981
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Undefined symbol "compat_MakeDfsName" referenced
Reported Date                 : 5/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/12/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/icl/Makefile
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/12/93 public]
in /file/icl
/project/dce/build/nb_pmax/tools/pmax/macho/gcc -B/project/dce/build/nb_pmax/tools/pmax/macho/  -%ld," -warn_nopic -glue"  -O        -L/project/dce/build/nb_pmax/export/pmax/usr/shlib -L/usr/shlib -L/project/dce/build/nb_pmax/export/pmax/u
sr/lib -L/project/osc/build/osc1.1.1/export/pmax/usr/ccs/lib -o dfstrace.X icl_dumpCommand.o  -licl -lncompat -ldacl -ldauth -ldacllfs  -ldacl -ldauth -ldacllfs -lafssys -lcom_err  -lcmd -losi -ldce -lbsd  
dfsauth_localprincipal.o: Undefined symbol "compat_MakeDfsName" referenced
*** Error code 1 (continuing)
`build_all' not remade because of errors.

[5/12/93 public]
Corrected order of libraries for dfstrace.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/icl/Makefile' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[12/17/93 public]
Closed.



CR Number                     : 7980
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : there are still dfs memory usage problems
Reported Date                 : 5/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/12/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : vnops/efs_access.c
Sensitivity                   : public
Transarc Deltas               : rajesh-db3681-fix-mem-leak-in-vnops-acl-code
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/12/93 public]
During cho, Machine 3 (PMAX) ran out of memory, causing the machine
to hang.  This was a less than 24 hour run.  We looked at the
kernel memory statistics and discovered that DFS is using
905 of the 1169 kmempages, which means that even though there
was some (but not much) kernel memory available we still could not use it.
This ot is to investigate the DFS memory usage problem further and attempt
to address it.

[06/02/93 public]
We hit many other serious problems before we are ever up long enough
to hit this one.  Since pmax won't be around after 1.0.2a, there's
probably not much point in looking at this anyway.

[06/10/93 public]
We've hit this again during cho - less than 24 hours.  
vm_page_free_count is 14 and the system is hung.

[06/16/93 public]
We added the Transarc pc tracing and found that vnax_CorrectAclRealm was by far
the largest memory abuser.  The following output constitues the trace from osidump
every 5 minutes - note that vnax_CorrectAclReal has allocated more than 400k in
about 100 minutes:
PC 802ed15c, min    120, max     120, total   26400, count 220
PC 802ed15c, min    120, max     120, total   26400, count 220
PC 802ed15c, min    120, max     120, total   53520, count 446
PC 802ed15c, min    120, max     120, total   79920, count 666
PC 802ed15c, min    120, max     120, total  105600, count 880
PC 802ed15c, min    120, max     120, total  133440, count 1112
PC 802ed15c, min    120, max     120, total  167160, count 1393
PC 802ed15c, min    120, max     120, total  205080, count 1709
PC 802ed15c, min    120, max     120, total  236640, count 1972
PC 802ed15c, min    120, max     120, total  268560, count 2238
PC 802ed15c, min    120, max     120, total  294960, count 2458
PC 802ed15c, min    120, max     120, total  322800, count 2690
PC 802ed15c, min    120, max     120, total  350760, count 2923
PC 802ed15c, min    120, max     120, total  355680, count 2964
PC 802ed15c, min    120, max     120, total  355680, count 2964
PC 802ed15c, min    120, max     120, total  355680, count 2964
PC 802ed15c, min    120, max     120, total  355680, count 2964
PC 802ed15c, min    120, max     120, total  381120, count 3176
PC 802ed15c, min    120, max     120, total  407640, count 3397
PC 802ed15c, min    120, max     120, total  434640, count 3622
PC 802ed15c, min    120, max     120, total  465360, count 3878

[06/16/93 public]
The simplest configuration that we have seen this on is a 2 pmax cell running 
connectathon and the fileset on which connectathon is running moves back and
forth every ten minutes, eventually after about a day, one of the servers will
run out of memory.

[6/16/93 public]
I've got a two line fix for this problem that I would be happy to submit asap.
We've been running our solaris port with this change for some time.
It fixes a leak in the dacl code.  I will request permission to submit it
from dfs-drb.
Filled in Affected File with `vnops/efs_access.c' 
Added field Transarc Deltas with value 
 `rajesh-db3681-fix-mem-leak-in-vnops-acl-code' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `import'

[6/16/93 public]
Filled in Reported by Company's Ref. Number with `3681'

[6/16/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `import' to `submit'

[06/17/93 public]

Over a 19 hour period the dfs kmem  usage peaked at 2497584 with this
fix instead of its usual 3.9 meg!



CR Number                     : 7979
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : error in building /libdcedfs
Reported Date                 : 5/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/12/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : libdcedfs/Makefile
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/12/93 public]
ld -o shr.o *.o *.exp -bloadmap:shr.o.map -bM:sre  -bE:libdcedfs.syms -bI:/lib/syscalls.exp  -L/u1/devobj/sb/nb_rios/export/rios/usr/lib -T512 -H512 -lkrb5 -lbsd -lc -ldce 2> shr.o.error
._bomb_ShouldExplode
._bomb_Explode
_bomb
*** Error code 8 (continuing)

[5/12/93 public]
Needed to add libbomb.a to the libdcedfs Makefile.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `libdcedfs/Makefile' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[12/17/93 public]
Closed.



CR Number                     : 7977
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : tkm can hang under very obscure conditions
Reported Date                 : 5/11/93
Found in Baseline             : 1.0.2
Found Date                    : 5/11/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot7977-alloc-wvtoken-sans-locks
Transarc Herder               : jaffe@transarc.com

[5/11/93 public]
TKM can hang if it is token GC mode, and the token allocation that triggered the
GC is the WV token.  This scenario causes it to lock things out of order, and
if the load is high enough, this can cause a deadlock.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot7977-alloc-wvtoken-sans-locks' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7976
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : scout
Short Description             : scout doesn't detect when server is back up
Reported Date                 : 5/11/93
Found in Baseline             : 1.0.2
Found Date                    : 5/11/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3383-misc-scout-fixes
Transarc Herder               : jaffe@transarc.com

[5/11/93 public]
Scout did not indicate that a File Server machine was back up
even though I could perform file operations to that server.
Setup: 2 machine cell - 1 server machine, 1 client machine;
	scout was running on the client machine
The server machine was rebooted.  When scout detected that the
server machine was no longer responding to probes, scout highlighted
the File Server's name and the values for that machine in the other
columns were removed.  This is correct.  But when the File Server
machine came back up, scout never un-highlighted the name and 
restored the other values even though I could get to files on
that File Server from the client machine.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[5/11/93 public]
I Believe that fred has tracked this problem down.  Fred: Please reassign
as appropriate.
Changed Responsible Engr. from `jaffe@transarc.com' to `fred@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/11/93 public]
Generally, scout will have to do a rpc_reset_binding() call so that it can
find the re-incarnated fileserver.

[5/17/93 public]
Jean -- This problem should have been fixed in a delta I made last week that I 
think you have.  Could you please verify whether or not the problem still exists?
The delta was comer-db3383-misc-scout-fixes.  
Filled in Interest List CC with `comer@transarc.com'

[5/18/93 public]
Mike- Yes, this works now with the delta you mentioned. Thanks! 
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[5/18/93 public]
imported into dfs-carl 1.11
Filled in Affected File with `some' 
Changed Responsible Engr. from `fred@transarc.com' to `comer@transarc.com' 
Filled in Transarc Deltas with `comer-db3383-misc-scout-fixes' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `import'

[5/18/93 public]

[6/21/93 public]
The OSF does not want this in 1.0.2a, so we move it back to export.
Changed Transarc Status from `import' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7971
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : the -cell flag should be deleted from fts crmount cmd
Reported Date                 : 5/10/93
Found in Baseline             : 1.0.2
Found Date                    : 5/10/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/userInt/fts/fts_userint.c, test/file/fts/test{10,12,14,16,18}
Sensitivity                   : public
Transarc Deltas               : cfe-ot7971-pull-cell-option-from-crmount
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/10/93 public]
Support for cellular mount points should be dropped from DFS.
Cellular mount points are mount points for filesets
from a foreign cell.  They are created by using the -cell flag
on the fts crmount command to specify that the fileset you are
mounting resides in a foreign cell.  The problem is that you
can't cross a cellular mount point from a DFS client, unless
the foreign cell's filespace has already been accessed using the 
global pathname to that cell.  
The problem is that the Cache Manager can not determine the 
DFS junction in the foreign cell.  This means that it can 
not find the foreign cell's flserver and can not get information 
about the foreign fileset.   
The reason the Cache Manager can not determine the DFS junction in 
the foreign cell's namespace is that it is not well-known so DFS can
not assume it is /.../<cellname>/fs.  
Cellular mount points are obsolete and left over from AFS 3.  I propose
that support get dropped from DFS and the -cell flag get deleted from 
the fts crmount command.  
To recreate the problem:
Setup two cells for intercell access with each other and
put the appropriate acls for foreign access on the fileset from 
cella you planned to mount in cellb.  In cellb, mount a fileset from cella:
# fts crm /.:/fs/tmp cella_fs -cell cella.austin.ibm.com
Then attempt to cross the mount point:
# ls -la /.:/fs/tmp
ls: 0653-341 The file /.:/fs/tmp does not exist.
The fts crmount command was successful but you will not be able to
cross the mount point.  Since the foreign cell, cella, has never
been accessed before using a global pathname, the Cache Manager can not 
determine the filesystem junction in cella.  
Doing something that uses a global path to the foreign cell, 
for example ls /.../cella.austin.ibm.com/fs, gets the information
set up so you can now use the mount point created above to access
the foreign fileset.  
For example:
Access cella's DFS file space through a global name
# ls /.../cella/fs
.rw       common    lfs1_fs2  lfs2_fs1  lfs2_fs2  u
Now access through the cellular mount point succeeds
# ls -la /.:/fs/tmp
total 6
drwxrwxrwx   3 guest    system       288 Apr 23 08:44 .
drwxrwxrwx   2 root     system       288 Apr 27 10:54 ..
drwxrwxrwx   2 guest    12           256 Apr 23 08:44 rjf
Filled in Responsible Engr. with `kazar@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/12/93 public]
I removed the -cell option from the ``fts crmount'' command and updated
the fts tests not to use that option.
 
Defect Closure Form
-------------------
--Verification procedure below--
Issue the ``fts help crmount'' command and notice that the -help option
isn't available.
 
Associated information:
Tested on TA build:  dfs-carl 1.10
Tested with backing build:  dce1.0.2a-0427
Changed Interest List CC from `kazar@transarc.com cfe@transarc.com 
 jsk@transarc.com' to `kazar@transarc.com, cfe@transarc.com, 
 jsk@transarc.com,jeff@transarc.com' 
Changed Fix By Baseline from `1.0.2' to `1.0.2a' 
Changed Responsible Engr. from `kazar@transarc.com' to `cfe@transarc.com' 
Filled in Transarc Deltas with `cfe-ot7971-pull-cell-option-from-crmount' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[6/1/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/userInt/fts/fts_userint.c, 
 test/file/fts/test{10,12,14,16,18}' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7969
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm checks for read perm incorrectly when O_RDONLY and O_RDWR both not set
Reported Date                 : 5/10/93
Found in Baseline             : 1.0.2
Found Date                    : 5/10/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : src/file/cm/cm_vnodeops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot7969-fix-cm-open-for-read-checks
Transarc Herder               : jaffe@transarc.com

[5/10/93 public]
The CM checks for read permission even when you're the owner and you're opening the
file for write-only or null modes.  It shouldn't.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot7969-fix-cm-open-for-read-checks' 
Changed Transarc Status from `open' to `export'

[10/14/93 public]
I am not usre how Transarc found this bug since it seems to pass low/test8
on RIOS (which is the test that looks for it in step f1.401.222, which is
trying to open an existing file with modes 222 using open flags O_CREAT and
O_WRONLY).  In any case, the test fails on HPUX so I have submitted the fix
that Transarc made (as referenced in their comments above).

[12/17/93 public]
Closed.



CR Number                     : 7967
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : scout
Short Description             : scout: print "Server" over server column
Reported Date                 : 5/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/10/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[5/10/93 public]
Change the documentation to reflect the "Server" label over the 
server column.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/13/93 public]
The following files are affected:
	admin_gd/dfs/dfs/11_scout_dfs.gpsml
	admin_ref/man8dfs/scout.8dfs
Filled in Interest List CC with `comer@transarc.com' 
Filled in Affected File with `See description' 
Changed Responsible Engr. from `jeff@transarc.com' to `kdu@transarc.com'

[10/14/93 public]
The following file is affected:
	admin_gd/dfs/dfs/11_scout_dfs.gpsml
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified changes in latest doc build and
closed this bug.



CR Number                     : 7966
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : LFS builds anyaccess perm set incorrectly
Reported Date                 : 5/10/93
Found in Baseline             : 1.0.2
Found Date                    : 5/10/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/episode/vnops/efs_access.c,file/security/dacl/dacl_getmode.c
Sensitivity                   : public
Transarc Deltas               : bwl-ot7966-bad-anon-access
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/10/93 public]
Description from Carl Burnett:
The DCE LFS filesystem is building the anonymousAccess permission set
that is passed back to DFS clients in the FetchStatus structure
incorrectly.  The anyaccess set is used by clients to make a quick
check for allowed permissions which may avoid having to do the permission
checks for the current requester.  If the anyaccess set does not allow
the desired permission, then additional checking must take place which
may require additions RPC calls to the fileserver.
Currently it is building it in the following way:
--------------------------------------------------
For the NULL ACL case it is using the intersection of the
unix mode bits for owner, group, and user.  If the corresponding
permission is set in all 3 sets, then the permission turned for
the any access perm.
For the ACL case, anyaccess is built by takeing the intersection
of the owner, group, other, and mask_obj ACL perms.
It should be working in the following way:
-------------------------------------------
For the NULL ACL case, no perms should be granted in the
anyaccess set since it is implied the the access for any_other
is 0.
For the ACL case, the anyaccess perms should be set in the minimum
of all the ACL entries, the owner_obj, group_obj, other_obj,
and forign_other entries.  This means all the acl entries must
be scanned to determine the lowest permission set.
Also an entry for any_other must exist.  If one does not exist, the
permissions for anyaccess should be set to zero.
If an any_other entry does exist, then the permission is the lesser
of the any_other entry and the intersection of all the scan entries.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/10/93 public]
Changed Interest List CC from 
 `demail!carl,lhughes@austin.ibm.com,ota@transarc.com' to 
 `demail!carl,lhughes@austin.ibm.com,ota@transarc.com, rajesh@transarc.com'

[5/10/93 public]
I believe the severity of this CR should be B 1 and not B 2.
Because the anyaccess permission set is built incorrectly, once
data is cached on a client machine, users on that client machine
who should be denied access to the data will be allowed to get the data
from the cache.  This is a serious security hole.

[5/11/93 public]
Changed Priority from `2' to `1'

[5/18/93 public]
I am exporting this delta on Bruce's behalf.
Defect Closure Form
-------------------
--Other explanation below--
I tested this delta by setting breakpoints in vnax_GetAnonAccess in 
the kernel debugger.
Associated information:
Tested on TA build:  dfs-102-2.10
Tested with backing build:  dce1.0.2b23
Filled in Transarc Deltas with `bwl-ot7966-bad-anon-access' 
Changed Transarc Status from `open' to `export'

[6/2/93 public]
This change is part of the set of acl changes that must be in 1.0.2a.
Changed Fix By Baseline from `1.0.3' to `1.0.2a'

[6/2/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with 
 `file/episode/vnops/efs_access.c,file/security/dacl/dacl_getmode.c' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7964
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : file server
Short Description             : file server can, rarely, handle calls before inits
Reported Date                 : 5/9/93
Found in Baseline             : 1.0.2
Found Date                    : 5/9/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot7964-fix-px-init
Transarc Herder               : jaffe@transarc.com

[5/9/93 public]
There are race conditions in the file server init code that can
cause it to fail if it gets a call before a the base pkgs init
themselves.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot7964-fix-px-init' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7961
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : dacl_CheckAccessAllowed does not grant "c" to member of sys admin group
Reported Date                 : 5/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/7/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/security/dacl/dacl_pac.c
Sensitivity                   : public
Transarc Deltas               : cburnett-ot7961-cperm-not-set-for-celladmin
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/7/93 public]
For an ACL with a foreign cell (e.g. unauthenticated cell), as the
default realm, the cell_admin of the storing cell, cannot modify the
ACL unless the ACL has appropriate entries that allow
cell_admin@storingCell control right.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/12/93 public]
The cause of the bug is outlined below. 
This turns out to be a cache manager bug. Cm_setacl calls cm_AccessOK
to check if the issuer (cell_admin) has dacl_control_right.
cm_AccessOK calls GetAccessBits which returns no rights at all for cell_admin
(zero). Hence cm_setacl returns EACCESS which results in acl_edit
generating a "not authorized message". 
The relevant ICL trace output and some more info from debugger is 
given below. 
File n owned by -2 is being modified by cell_admin. ls -l on n shows
-rw-rwxr--   1 42949672944294967294      0 May 12 20:39 n
The acl on the file is:
# acl_edit n -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for n:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
mask_obj:rwx---
user_obj:rw-c--
group_obj:rwx---
other_obj:r-----
foreign_other:/.../bugacl:r-----
# acl_edit n -m other_obj:rx
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
ICL output:
time 702.700096, pid 30540: cm_lookup 62ea36c n
time 702.700215, pid 30540: gettokens vp 62ea36c, rights.low 0x404
time 702.700325, pid 30540: nh_dolookup dvp 62ea36c, name n
time 702.700387, pid 30540: gettokens vp 62ea36c, rights.low 0x404
time 702.700549, pid 30540: cm_GetScache vp 62eadb0, volume.low 0x1, vnode 0x16
time 702.700643, pid 30540: found fid f49f1236.1.16.15 (hex)
time 702.700694, pid 30540: lookup returning vp 62eadb0
time 702.700730, pid 30540: checkerror returning code 0
time 702.707068, pid 30540: cm_setacl vp 62eadb0
time 702.707175, pid 30540: gettokens vp 62eadb0, rights.low 0x400
time 702.707265, pid 30540: cm_GetACLCache vp 62eadb0 pag 1090519047
time 702.707311, pid 30540: foundaclcache rights 1
time 702.707365, pid 30540: checkerror returning code 13	<<<<<<<<<<
time 702.711154, pid 30540: inactive vp 62eadb0
(bin/kdbx) where
cm_setacl(vp = 0x62eadb0, aclp = 0x6310000, cvp = (nil), dw = 0x0, sw = 0x0, cre
dp = 0x6328ed8), line 4906 in "cm_vnodeops.c"
xvn_setacl(vp = 0x62eadb0, aclp = 0x6310000, svp = (nil), dw = 0x0, sw = 0x0, cr
edp = 0x6328ed8), line 403 in "xvfs_vnode.c"
afscall_vnode_ops(code = 0x1, parm1 = 0x20101ef8, parm2 = 0x20117f48, parm3 = 0x
5c, parm4 = 0x0, retvalP = 0x2ff97f88), line 137 in "xvnops_call.c"
kafs_syscall(0xa, 0x1, 0x20101ef8, 0x20117f48, 0x5c, 0x0) at 0x5dd4180
(bin/kdbx) stop in cm_AccessOK

[11] stop in cm_AccessOK
(bin/kdbx) c

[11] stopped in cm_AccessOK at line 692 in file "cm_subr.c"
  692       aclBits= GetAccessBits(scp, rights, rreqp, setLock);
(bin/kdbx) n
stopped in cm_AccessOK at line 693 in file "cm_subr.c"
  693       return ((aclBits & rights) == rights); /* true if all rights bits on
 */
(bin/kdbx) p aclBits
0x0					<<<<<<< no access for cell_admin
(bin/kdbx) p rights
0x8
Changed Subcomponent Name from `dacl' to `cm' 
Changed Short Description from `cell_admin cannot modify the acl with a foreign 
 cell as default realm' to `cm_AccessOK does not grant "control" right to 
 members of sys admin group.' 
Changed Interest List CC from `comer@transarc.com, ota@transarc.com, 
 bwl@transarc.com, gmd@osf.org, lhughes@transarc.com, kazar@transarc.com' to 
 `comer@transarc.com, ota@transarc.com, bwl@transarc.com, gmd@osf.org, 
 lhughes@transarc.com, kazar@transarc.com, jsk'

[5/13/93 public]
Mike K. mentioned that the cause of this bug was found yesterday by
Carl at IBM in dacl_epi_CheckAccessAllowed. I further tracked code
paths of how acls are handled by cm and realised that it is indeed
dacl_epi_CheckAccessAllowed that is at fault. cm_AccessOK calls
GetAccessBits which calls AFS_FetchStatus, if the user's access
rights are not cached or cached but not valid any more.
SAFS_FetchStatus calls efs_getxattr leading to vnva_GetAttr which
calls vnax_GetAccess which calls dacl_epi_CheckAccessAllowed to find
out access allowed for a particular user. These rights are cached by cm
for future requests, with an expiration time.
In order to grant members of the sys admin group, "c" right on all
files/dirs, the code in dacl_epi_CheckAccessAllowed should check to
see if the realm of the requester is same as that of the local cell
and then if the requester belongs to the sys admin group.  But the
code is instead comparing the local cell id against the default realm
stored in the ACL!!.
I'll find out from Carl if he has already opened another OT for this and turn this
into a duplicate of the other if so.
Changed Subcomponent Name from `cm' to `dacl' 
Changed Short Description from `cm_AccessOK does not grant "control" right to 
 members of sys admin group.' to `dacl_CheckAccessAllowed does not grant "c" to 
 member of sys admin group'

[5/13/93 public]
I have the fix and will submit it to Transarc.
Changed Responsible Engr. from `rajesh@transarc.com' to `demail1!carl' 
Changed Resp. Engr's Company from `tarc' to `ibm'

[5/13/93 public]
Defect Closure Form
-------------------
This fix can be tested in the following way:
create a file as an anon user
dce_login as cell_admin
do a chmod on the file
it should now work, where it failed before the fix.
Filled in Transarc Deltas with `cburnett-ot7961-cperm-not-set-for-celladmin' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[6/2/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/security/dacl/dacl_pac.c' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7959
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : Modification of a acl with default realm other than the storing cell has problems
Reported Date                 : 5/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/7/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/security/dacl/dacl_sec_acl.c,file/security/dacl/epi_id.c
Sensitivity                   : public
Transarc Deltas               : rajesh-ot7959-dacl-unauth-bindings-and-uuid-expansion
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/7/93 public]
Scenario: 
An unauthenticated non-root user foo creates a DFS file in cell A. The
unauthenticated user belongs to the "unauthenticated cell". The user
adds a foreign_user entry with control right for user in the storing
cell i.e. prakash@cellA. The unauth user foo is no longer able to list
the acl.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/17/93 public]
foo is able to ls -l the file, chmod the file etc. Cell_admin can list the ACL.
afscall_vnode_ops seems to be returning a valid acl back to acl_edit.
I am currently in the process of building acl_edit with -g to enable
further tracking of the problem.
Log of operations:

 [7959] klist
No DCE identity available: No currently established network identity for which c
ontext exists (dce / sec)
Kerberos Ticket Information:
klist: No credentials cache file found (dce / krb) while setting cache flags (ti
cket cache /tmp/krb5cc_4970)

 [7959] whoami
rajesh

[rajesh] cd /:
dfs: set auth binding failed (code 382312714)
dfs: Warning: created unauthenticated binding

[:] cd unauth-nonroot/7959

 [7959] touch file4
dfs: set auth binding failed (code 382312714)
dfs: Warning: created unauthenticated binding

 [7959] ls -l file4
-rw-r--r--   1 42949672944294967294      0 May 13 12:35 file4

 [7959] acl_edit file4 -l
Warning - binding to registry is unauthenticated
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for file4:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
user_obj:rw-c--
group_obj:r-----
other_obj:r-----

 [7959] acl_edit file4 -m foreign_user:/.../bugacl/rajesh:r
Warning - binding to registry is unauthenticated
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce / sec)
INFO: Local cell will be used for operations requiring default cell info.

 [7959] acl_edit file4 -l
Warning - binding to registry is unauthenticated
ERROR: SEC_ACL_EDIT status 1
Unable to bind to object file4
As cell_admin:

 [7959] dce_login cell_admin -dce-
Password must be changed!
%  klist | grep -i Prin
        Global Principal: /.../bugacl/cell_admin
        Principal: 00000064-1151-2bf0-b400-02608c2ef49f cell_admin
Default principal: cell_admin@bugacl
% acl_edit file4 -l
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce /
 sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for file4:
# Default cell = fffffffe0000.00.00.00.00.00.00.00.00
mask_obj:r-----
user_obj:rw-c--
foreign_user:/.../bugacl/rajesh:r-----
group_obj:r-----
other_obj:r-----

[5/18/93 public]
The originating point of this failure has been found.
acl_edit after getting the acl from DFS thru a syscall, calls a
function in our dacl layer, sec_acl_ParseAcl, to convert the acl to
the security acl format. sec_acl_ParseAcl calls dacl_To_SecAcl, which
calls dacl_FindFullUuid for each user, group, foreign_user and
foreign_group entries in the ACL. In the example above, dacl_FindFullUuid
calls sec_id_gen_name for the foreign user /.../bugacl/rajesh. 
sec_id_gen_name returns error sec_login_s_no_current_context, which 
is mapped to DACL_ERROR_SEC_RGY_PGO_ERROR by dacl_FindFullUuid, 
which is mapped by acl_edit to EPERM.
Changed Interest List CC from `ota@transarc.com, bwl@transarc.com, gmd@osf.org, 
 lhughes@transarc.com, kazar@transarc.com' to `ota@transarc.com, 
 bwl@transarc.com, gmd@osf.org, lhughes@transarc.com, kazar@transarc.com, 
 comer@transarc.com'

[5/21/93 public]
The fix is explained below:
Acl_edit obtained the filesystem ACL from the DFS kernel and then
called sec_acl_ParseAcl to convert the DFS style acl into a security
style acl. Sec_acl_ParseAcl, under the impression, that the user and
group uuids stored in the DFS acl were short ones (only 32 significant
bits) ALWAYS made calls to the security service to fetch the
corresponding full uuid (128 significant bits). The calls (e.g.
sec_id_gen_name) were made using the default authenticated binding,
which failed, as the caller was unauthenticated, with error
sec_login_s_no_current_context, which the acl conversion code mapped
to DACL_ERROR_SEC_RGY_PGO_ERROR and then to 1. Hence acl_edit printed
out an error status of 1.
With great amount of help from Mike Comer (thanks Mike), the fix was
implemented and tested. The fix has 2 aspects:
- Call security to convert the uuid given by DFS to a full uuid
  only if the uuid stored by DFS is a short one.
- If we need to contact the security service for the conversion,
  first determine if we are authenticated or not. If authenticated, then
  make the calls as in the old code with the default authenticated
  binding handle. If we are unauthenticated, create an unauthenticated
  binding handle first and use this handle to make calls to security service.
Log:
% % klist
No DCE identity available: No currently established network identity for which context exists (dce / sec)
Kerberos Ticket Information:
klist: No credentials cache file found (dce / krb) while setting cache flags (ticket cache /tmp/krb5cc_4970)
%
% acl_edit.new file4 -l
dfs: set auth binding failed (code 382312714)
dfs: Warning: created unauthenticated binding
Warning - binding to registry is unauthenticated
Unknown default cell from ACL - ERROR: Cell UUID is not a valid cell name (dce / sec)
INFO: Local cell will be used for operations requiring default cell info.
# SEC_ACL for file4:
# Default cell = fffffffd0000.00.00.00.00.00.00.00.00
mask_obj:rwx-id
user_obj:rw-c--
foreign_user:/.../power7.dce.transarc.com/rajesh:r-x---
group_obj:r-----
foreign_group:/.../power7.dce.transarc.com/system:rwx-id
other_obj:r-----
foreign_other:/.../power7.dce.transarc.com:r-x---
any_other:rwx---
--Verification procedure below--
Login into a client or server machine as a non-privileged user.  Do
not dce_login. Do a klist. It should print that you have no network
network credentials.  Now try to list an acl that is known to have
foreign_user and foreign_group entries. The operation should succeed.
Associated information:
Tested on TA build:  dfs-carl2
Tested with backing build:  dce1.0.2a-0427
Filled in Transarc Deltas with 
 `rajesh-ot7959-dacl-unauth-bindings-and-uuid-expansion' 
Changed Transarc Status from `open' to `export'

[6/2/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with 
 `file/security/dacl/dacl_sec_acl.c,file/security/dacl/epi_id.c' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7954
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : strategy daemon panic on deleted file
Reported Date                 : 5/7/93
Found in Baseline             : 1.0.2
Found Date                    : 5/7/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : bwl-ot7954-biodaemon-panic
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/7/93 public]
If the strategy daemon gets a request to clean a dirty page for a file that
has been deleted, the call to efs_getvolume panics.  (A null volume handle is
passed to epiv_GetIdent and it doesn't like it.)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/18/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
We have run 24 hour CHO on this problem, and we no longer see this problem
Associated information:
Tested on TA build:  dfs-carl-1.10
Tested with backing build:  dce1.0.2-0427
Changed H/W Ref Platform from `pmax' to `rs6000' 
Changed S/W Ref Platform from `osf1' to `aix' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Filled in Transarc Deltas with `bwl-ot7954-biodaemon-panic' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7946
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, users_gdref
Short Description             : Correct/update DFS ACL documentation
Reported Date                 : 5/6/93
Found in Baseline             : 1.0.2
Found Date                    : 5/6/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See Description.
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[5/6/93 public]
The following information needs to be corrected or updated in the DFS ACL
documentation:
o The anonymous identity is changed to nobody.  The uid and gid of nobody are
  both -2.
o The nobody identity is no longer from the local cell.  The identity is now
  a user from a fabricated, unknown cell.
o On a DCE LFS fileset, nobody acquires permissions from the any_other entry
  of an object's ACL.  Because the identity is now a member of a foreign
  (albeit fictitious) cell, it no longer matches on other_obj.
o On a non-LFS fileset, unauthenticated users and all foreign users,
  authenticated or unauthenticated, acquire the other mode bits of an object.
o The "local" cell of an ACL is the local cell of the user who creates the
  ACL, not the cell in which the ACL happens to reside.
o When a foreign user creates an object, the ACL of the object gets a
  foreign_other entry for users from the cell in which the object resides.
  The other_obj entry matches users from the cell of the user who creates the
  object; the foreign_other entry will match users from the cell in which the
  object resides.
  The object also receives a mask_obj (because it has a non_obj entry). 
  Because an unauthed user is technically a foreign user (from an unknown
  cell), the ACL of an object created by an unauthed user also gets a
  foreign_other entry for users from the cell in which the object resides.
o The w permission on an implicit Initial Object Creation ACL will no longer
  be expanded to include i and d (as it is on an implicit Initial Container
  Creation ACL).  The i and d permisions have no meaning for a file, so the
  expansion is pointless.  Note that, if a user explicitly adds i or d to an
  entry on the ACL of a file, the permissions will be maintained.
That summarizes the many changes.  Please excuse the brevity of the summaries.
Correct any blatant errors; thanks.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/21/93 public]
Over the past few months, DFS ACLs have continued to evolve.  The following
list documents the current changes that need to be made:
 
o The anonymous identity is changed to nobody.  The uid and gid of nobody are
  both -2.
 
o The nobody identity is no longer from the local cell.  The identity is now
  a user from a fabricated, unknown cell.
 
o On a DCE LFS fileset, nobody acquires permissions from the any_other entry
  of an object's ACL.  Because the identity is now a member of a foreign
  (albeit fictitious) cell, it no longer matches on other_obj.
 
o On a non-LFS fileset, unauthenticated users and all foreign users,
  authenticated or unauthenticated, acquire the other mode bits of an object.
 
o The "local" cell of an ACL is the local cell of the user who creates the
  ACL, not the cell in which the ACL happens to reside.
 
o When a foreign user creates an object, the other_obj entry matches users from
  the "local" cell of the user who creates the object, not the cell in which
  the object resides.
 
o The w permission on an implicit Initial Object Creation ACL will no longer
  be expanded to include i and d (as it is on an implicit Initial Container
  Creation ACL).  The i and d permisions have no meaning for a file, so the
  expansion is pointless.  Note that, if a user explicitly adds i or d to an
  entry on the ACL of a file, the permissions will be maintained.
 
o The umask of the process that creates an object is considered only if the
  parent directory of the object does not have the proper Initial Object/
  Container Creation ACL.  Otherwise, it is still considered as documented.
 
In addition, the following text attempts (and, doubtless, fails) to clarify
these changes to DFS ACLs:
 
The DFS documentation does not currently describe ACL inheritance with respect
to file and directory objects created by foreign users. When a foreign user
creates an object, the ACLs of the object are defined with respect to the cell
of the foreign user. The appropriate Initial Creation ACL of the object's
parent directory determines the entries included in the Object ACL of the
object, but the entries in the Object ACL are created with respect to the cell
of the foreign user.
 
For example, suppose a foreign user creates a file in a directory whose Initial
Object Creation ACL grants the following permissions:
 
   user_obj:rwxc--
   group_obj:r-x---
   other_obj:r-x---
 
The Object ACL of the new file inherits these entries directly from the Initial
Object Creation ACL, but the entries correspond to users from the cell of the
user who created the file, not from the cell of the directory in which the file
is created. So the other_obj entry grants r and x permissions to users from the
cell of the foreign user who created the file, not from the cell of the
directory in which the file exists.
 
Similarly, suppose a foreign user creates a subdirectory in a directory whose
Initial Container Creation ACL grants the following permissions:
 
   user_obj:rwxcid
   group_obj:r-x-id
   other_obj:r-x---
 
The Object ACL of the new subdirectory inherits these entries from the Initial
Container Creation ACL, but the entries again apply to users from the cell of
the user who created the subdirectory, not from the cell of the directory in
which the subdirectory is created. Additionally, the subdirectory inherits the
Initial Object Creation and Initial Container Creation ACLs directly from its
parent directory, but the entries in these Initial Creation ACLs correspond to
the cell of the user who created the subdirectory (the same cell to which the
subdirectory's Object ACL corresponds).
 
If the Initial Creation ACL of an object's parent directory contains entries
for users in the cell of the parent directory, the entries are changed to
foreign entries as necessary. For example, suppose the Initial Container
Creation ACL of a new subdirectory's parent directory grants the following
permissions:
 
   user_obj:rwxcid
   group_obj:r-x-id
   other_obj:r-x---
   user:rajesh:rwx---
 
If the subdirectory is created by a foreign user, the entry for the user rajesh
is changed to the following in the Object ACL of the subdirectory:
 
   foreign_user:<cellname>/rajesh:rwx---
 
where <cellname> is the name of the cell with respect to which the parent
directory's ACLs are defined (user rajesh's local cell). The entry for user
rajesh is also changed to a foreign_user entry in the Initial Container
Creation ACL that the subdirectory inherits from the parent directory. The
user entry for rajesh is also changed to a foreign_user entry in the Object
ACL of a new file and in the Initial Object Creation ACL inherited by a new
subdirectory if the parent directory's Initial Object Creation ACL contains
such a user entry.
 
Foreign entries (foreign_user, foreign_group, and foreign_other) are usually
inherited directly from the appropriate Initial Creation ACL; they are not
typically manipulated during inheritance. Thus, you can include a foreign_other
entry on a directory's Initial Creation ACLs to ensure that users from the
cell with respect to which the ACLs are defined are automatically granted
permissions on an object created in the directory by a foreign user. For
example, suppose the Initial Container Creation ACL in the previous example
also included the following entry:
 
   foreign_other:<cellname>:rwx---
 
where <cellname> is the name of the cell with respect to which the parent
directory's ACLs are defined. A subdirectory created in the directory inherits
this foreign_other entry, unchanged, directly from the parent's Initial
Container Creation ACL. With two exceptions, this inheritance is true of all
foreign entries, regardless of the local cell of the user who creates an
object.
 
The sole exceptions occur when a foreign_user or foreign_group entry defined
with respect to the cell of a foreign user who creates an object exists in the
appropriate Initial Creation ACL. In this case, the foreign entry is changed
to a local entry in the ACLs of the new object because the foreign entry is a
local entry in the cell with respect to which the new object's ACLs are
defined. For example, suppose the Initial Container Creation ACL in the
previous example also includes the following entry:
 
   foreign_user:<foreign_cellname>/mike:rwx---
 
where <foreign_cellname> is the name of the cell of a foreign user who creates
a subdirectory in the directory. When the foreign user from the named cell
creates the subdirectory, the foreign_user entry for mike is changed to a user
entry on the subdirectory's Object ACL because it is now defined with respect
to the same cell as the subdirectory's Object ACL (mike's local cell). So the
previous entry would be changed as follows:
 
   user:mike:rwx---
 
The entry is changed in the same way in the Initial Container Creation ACL that
the subdirectory inherits from the parent directory. The same changes are also
made to the Object ACL of a file created in the directory and to the Initial
Object Creation ACL inherited by a new subdirectory if the parent directory's
Initial Object Creation ACL includes such a foreign_user entry.
 
In the case where a directory does not have the appropriate Initial Creation
ACL, the umask of a user who creates an object in the directory is used as the
basis of the Object ACL of the object. An Object ACL is explicitly created for
an object created by a foreign user, even if the object's parent directory does
not have the appropriate Initial Creation ACL. Entries on the explicit ACL are
associated with the cell of the user who creates the object, not with cell of
the object's parent directory.
 
Note: Initial Object Creation and Initial Container Creation ACLs are never
      created for a subdirectory whose parent directory does not have the
      corresponding Initial Creation ACLs. Also note that the umask is never
      used to determine the ACL permissions granted by an object whose parent
      directory has the appropriate Initial Creation ACL.
 
I hope this information helps to clarify things; documentation changes will be
based on the text of this note.
Changed Interest List CC from `lhughes@austin.ibm.com' to 
 `lhughes@austin.ibm.com, rajesh@transarc.com'

[6/23/93 public]
Another ACL issue that needs to be resolved and documented pertains to changing
the default realm (cell) of an ACL.  The acl_edit command provides an interface
for changing an ACL's default realm.  We need to determine who can change the
default realm of an object ACL and an initial ACL.  We also need to think about
just what it all means (should inital ACLs be allowed to have different default
realms than the object ACL of their corresponding object) and how it really
should work.  The related code work is presently under consideration.

[8/12/93 public]
The following files have been submitted in response to this defect:
 
	admin_gd/dfs/dfs/1_overview_dfs.gpsml
	admin_gd/dfs/dfs/3_aclgroup_dfs.gpsml
	users_gdref/dfs/u3_protect_dfs.gpsml
 
All of the changes noted in this defect have been addressed across the DFS ACL
documentation.  This includes the privileges required to modify the default
cell of an object's ACL.  This one can be closed.
 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `See Description.' 
Changed Transarc Status from `open' to `closed'

[08/17/93 public]
Closed bug.



CR Number                     : 7934
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : bak labeltape not returning error
Reported Date                 : 5/5/93
Found in Baseline             : 1.0.2
Found Date                    : 5/5/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-ot7934-bak-return-appropriate-exit-code
Transarc Herder               : 

[5/5/93 public]
bak labeltape commands with either no tape in the drive or a write-protected 
tape returns 0.  This command should return 1 in these instances.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[5/5/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/12/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Run bak commands in non-interactive mode, e.g. bak labeltape -tcid 2, in 
both working and error cases. The working cases should return 0 status code
and non-working case should return 1 exit status. This problem was present
for all bak commands, and all of them should be fixed.
Associated information:
Tested on TA build:  
dfs-102-2.10
Tested with backing build:  
dce1.0.2b23
Filled in Transarc Deltas with `vijay-ot7934-bak-return-appropriate-exit-code'

[7/26/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
--Other explanation below--
Associated information:
Tested on TA build:  
Tested with backing build:

[7/26/93 public]
Defect Closure Form
-------------------
--Other explanation below--
This was marked for export earlier with the defect closure info as given above
but somehow reverted to open state. Marked for export *again*.

[7/26/93 public]
marked for export. field was empty.
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 7933
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : -family with null argument not giving error msg.
Reported Date                 : 5/5/93
Found in Baseline             : 1.0.2
Found Date                    : 5/5/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-ot7933-bak-bad-error-message
Transarc Herder               : 

[5/5/93 public]
bak is not parsing a null argument correctly for the following commands:
# bak addftfamily -family
bak: Missing required parameter '-family'
# bak rmftfam -family
bak: Missing required parameter '-family'
The proper error message to give back in these cases is:
 -family requires non-null argument
(This is what other DFS commands give back).
Also, If I give it a null argument as "" , I get:
# bak addftfamily -family "" 
bak: [bc_ParseVolumeSet] Bad fileset header line: 'volumeset  
'
# echo $?
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[5/5/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/12/93 public]
There are two problems mentioned above. 
1. When "bak addftfam -family" is entered without any argument to the -family
switch, the error message printed comes from the cmd package common to many
dfs programs. When the switch is optional, you get "<switch> requires 
non-null argument", and when the switch is required, you get "missing
required parameter <switch>". This is true for bak, fts and other dfs
commands. This could be raised as an anomaly against the cmd package for 
giving out two different error messages for a similar problem, but this is
definitely not a bak error.
2. The second problem is the incoherent message put out by bak when giving
the empty string as a parameter to addftfamily. This should of course be
fixed.

[5/18/93 public]
Defect Closure Form
-------------------
--Regression test program below--
An error message cleanup pass was made to ensure that incoherent messages
such as the one above are not put out by the backup system. Almost all error
messages now have the format
bak: <error msg from catalog> (dfs / bak); <error msg from bak>
To test this, run 
bak addftfamily -family ""
and you will see the error message,
bak: bad fileset family name (dfs / bak) ; Failed to create fileset family ''
Associated information:
Tested on TA build:  
dfs-102-2.10
Tested with backing build:  
dce1.0.2b23
Filled in Transarc Deltas with `vijay-ot7933-bak-bad-error-message'

[7/26/93 public]
Defect Closure Form
-------------------
--Other explanation below--
Marked for export *again*.
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 7928
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : acl tests
Short Description             : setup description incomplete
Reported Date                 : 5/4/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/4/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : andi-ot7928-fix-acltest-setup-description
Transarc Herder               : andi@transarc.com

[5/4/93 public]
**Description Text**
The setup description in README.tmp instructs you to chown
{results,scratch} but that's insufficient -- the cwd and
logfile need to be writeable by the test-user as well.
Added field Transarc Herder with value `andi@transarc.com'

[5/4/93 public]
Defect Closure Form
-------------------
--Other explanation below--
document that the user running the test must be able to create files
in all directories associated with the acl test (including the current dir).
Also removed the ugly README.tmp stuff and included it in the natural 
README file.
Associated information:
Tested on TA build:  dfs-carl2
Tested with backing build:  dce1.0.2a-0427
Added field Transarc Deltas with value 
 `andi-ot7928-fix-acltest-setup-description' 
Added field Transarc Status with value `export'

[12/17/93 public]
Closed.



CR Number                     : 7925
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : REPSERVER
Short Description             : repserver runs out of stack on osf/1
Reported Date                 : 04/29/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/29/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/rep/rep_main.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-db3486-more-stack-for-repserver
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Here's a stack trace that seems to have overflowed.
 (gdb) where
 #0  0x7322eaa8 in cma__transfer_thread_ctx ()
 #1  0x731d1d80 in cma__dispatch ()
 #2  0x731d177c in cma__block ()
 #3  0x731c1220 in cma__int_wait ()
 #4  0x732152c4 in cma_select ()
 #5  0x73210d8c in cma_connect ()
 #6  0x732d9ba8 in rpc__socket_connect ()
 #7  0x73279328 in rpc__cn_network_req_connect ()
 #8  0x73272014 in request_conn_action_rtn ()
 #9  0x732867e0 in rpc__cn_sm_eval_event ()
 #10 0x732656cc in rpc__cn_assoc_open ()
 #11 0x7325fe90 in rpc__cn_assoc_request ()
 #12 0x732756d8 in allocate_assoc_action_rtn ()
 #13 0x732867e0 in rpc__cn_sm_eval_event ()
 #14 0x7326b840 in rpc__cn_call_start ()
 #15 0x73238790 in rpc_call_start ()
 #16 0x7325a1b0 in op3_csr ()
 #17 0x7324a4b4 in ep_get_endpoint ()
 #18 0x73249e88 in rpc_ep_resolve_binding ()
 #19 0x7326b1d4 in rpc__cn_call_start ()
 #20 0x73238790 in rpc_call_start ()
 #21 0x733e7f38 in op0_csr ()
 #22 0x733676a0 in sec_rgy_login_get_info ()
 #23 0x73445a34 in sec_login_pvt_get_login_info ()
 #24 0x734397a4 in sec_login_validate_identity ()
 #25 0x7343aadc in sec_login_valid_from_keytable ()
 #26 0x549a90 in dfsauth_client_EstablishLocalAuthContext (refresh=1)
     at ../../../../../src/file/security/dfsauth/dfsauth_client.c:432
 #27 0x549f8c in dfsauth_client_RefreshLocalAuthContext ()
     at ../../../../../src/file/security/dfsauth/dfsauth_client.c:598
 #28 0x54a200 in dfsauth_client_InitAuthContext (useNoAuth=0, useLocalAuth=1)
     at ../../../../../src/file/security/dfsauth/dfsauth_client.c:673
 #29 0x54a2dc in dfsauth_client_InitBindingAuth (
     serverBindingHandleP=0x1012b738, useNoAuth=0, useLocalAuth=1,
     serverPrincipalNameP=0x1012b338 "/.../manwe.com/hosts/piglet/dfs-server")
     at ../../../../../src/file/security/dfsauth/dfsauth_client.c:718
 #30 0x40bf18 in GetIncrementalDump (rp=0x100b64c0, lv=0x10006f88, versp=0x0,
     timep=0x1012b798) at ../../../../src/file/rep/rep_main.c:2769
 #31 0x411270 in StartVolumeRetrieval (rp=0x100b64c0)
     at ../../../../src/file/rep/rep_main.c:3555
 #32 0x4138cc in ReplicaWantsAdvance (rp=0x100b64c0, needAdvP=0x1012b870,
     delp=0x1012b868) at ../../../../src/file/rep/rep_main.c:4004
 #33 0x4145cc in StartImporting () at ../../../../src/file/rep/rep_main.c:4130
 #34 0x417ddc in BackgroundProcessThread (arg=0x0)
     at ../../../../src/file/rep/rep_main.c:4977
 #35 0x7321e9e8 in cma__thread_base ()
 #36 0x7322e9a8 in cma__execute_thread ()
 #37 0x7322e990 in cma__execute_thread ()
**Solution Text**
 Delta cfe-db3486-more-stack-for-repserver.  Running rep/rtest1 to check
 for regressions.  The code is much like what's in the ftserver for the
 comparable problem, so I have high hopes.
 
 Entered as OT 
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `OPEN'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/rep/rep_main.c' 
Changed Transarc Status from `OPEN' to `import' 
Added field Transarc Deltas with value `cfe-db3486-more-stack-for-repserver'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/1/93 public]
Closing and setting fixby back to 1.0.2a - status field inadvertently
left in fix state.

[12/17/93 public]
Closed.



CR Number                     : 7924
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Spurious dir entries with long names appear at dump time
Reported Date                 : 04/27/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/27/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_dir.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3479-impossible-dir-entry-names
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Occasionally, a fileset dump/restore will fail because a dumped
 directory entry's name member has an impossibly large length.  I don't know
 that the problem is yet, but the first cut will be to add some
 assertions to try to catch the problem at the source.
 
 Jeffrey Prem
 Tue Apr 27 17:29:33 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_dir.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `jdp-db3479-impossible-dir-entry-names'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[6/10/93 public]
Hey Jeff, how long is "impossibly long"?  And did it get really fixed,
or were assertions just put in?  I'm trying to figure out how to
verify this one . . .  thanks.

[6/11/93 public]
"Impossibly long" is greater than MAXNAMLEN or NAME_MAX, depending on whether
you're running on AIX or OSF/1.
As far as I know, this one was never really fixed.  Only the assertions were
added, but I still have no greater understanding of what the bug is/was.  Has
anyone seen this lately?

[12/17/93 public]
Closed.



CR Number                     : 7923
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage needs to avoid zero as error reason number
Reported Date                 : 03/20/92
Found in Baseline             : 1.0.2a
Found Date                    : 03/20/92
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/sal_errors.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3472-renumber-salvage-msgs
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 The error msg returned by epiz_CheckEOF had been given a number of zero
 (0).  which was being used by the caller to mean no error was detected.
 This should be remedied by renumbering all the msgs to avoid zero.
 [Tue Apr 27 11:07:59 1993]
**Solution Text**
 Delta: ota-db3472-renumber-salvage-msgs -r1.2
 
 ota:
 
 While renumbering, sort the msgs for clarity.
 [Tue Apr 27 11:08:09 1993]
 
 ota:
 
 Must make the NOOP msgs non-zero so that it doesn't ignore these repairs.
 [Wed Apr 28 11:41:24 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/sal_errors.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3472-renumber-salvage-msgs'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7922
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : ftutil's pipe functions need some sanity checking.
Reported Date                 : 04/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftutil/local_pipe.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-db3464-pipe-assertions
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ftutil/local_pipe.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `cfe-db3464-pipe-assertions'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7921
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage better validation of length field
Reported Date                 : 04/22/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/dir/dir_salvage.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3455-better-free-area-check
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 The CheckFreeArea function wasn't detecting the case where the length
 indicates a point between pnhimark and the very end of the page.  This
 was causing the rebuilder to trust the length field when it shouldn't.
 [Thu Apr 22 12:17:34 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3455-better-free-area-check
 [Thu Apr 22 12:17:37 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/dir/dir_salvage.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3455-better-free-area-check'

[5/5/93 public]
Changed Short Description from `See description below' to `salvage better 
 validation of length field'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7920
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager must check logged bits of file module anodes
Reported Date                 : 04/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/file.c, file/episode/anode/sal_errors.h, file/episode/anode/salvage.h, file/episode/salvage/paths.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3444-check-file-logged-bits
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 Verify that file and user plists are not logged and that directories,
 ACLs and system plists are logged.
 
 Check that initial ACLs are not used on regular files.
 
 Fix strange counting of nFiles.  Now counts first occurance of
 non-directories.
 [Thu Apr 22 10:53:40 1993]
**Solution Text**
 Delta: ota-db3444-check-file-logged-bits -r1.2
 
 ota:
 [Thu Apr 22 10:53:44 1993]
 
 ota:
 
 Fix spelling error: auxilary => auxiliary.
 [Fri Apr 23 09:15:00 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/file.c, 
 file/episode/anode/sal_errors.h, file/episode/anode/salvage.h, 
 file/episode/salvage/paths.c, file/episode/salvage/walk.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3444-check-file-logged-bits'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7919
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : start bosserver threads after forking
Reported Date                 : 04/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bosserver/bossvr_main.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3443-start-bosserver-threads-after-forking
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 THis delta sets up a pipe between the parent and child bosserver
 processes so that the bosserver can fork before spawning threads.
 The parent process waits until either the child end of the pipe goes
 away or until it gets a code indicating whether or not initialization
 succeeded. 
 Michael Comer
 Wed Apr 21 16:14:45 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/bosserver/bossvr_main.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3443-start-bosserver-threads-after-forking'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/1/93 public]
Closing and setting fixby back to 1.0.2a - status field inadvertently
left in fix state.

[12/17/93 public]
Closed.



CR Number                     : 7918
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage must check volIndex in anode pages
Reported Date                 : 04/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/21/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/anode_block.c, file/episode/anode/anode_block.h, file/episode/anode/fixed_anode.c, file/episode/anode/fixed_anode.h, file/episode/anode/volume_
table.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3439-verify-anode-block-volIndex
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 The verify volume table function was not checking that the volIndex
 field of the anode blocks in the volume were correct.  This field is
 used to set the corresponding field in the anode handle.  Eventually the
 salvager gets confused because the volIndex in the anode handle has an
 unexpected value.
 [Wed Apr 21 08:39:06 1993]
 
 ota:
 
 [Wed Apr 21 11:29:21 1993]
**Solution Text**
 Delta: ota-db3439-verify-anode-block-volIndex
 
 ota:
 
 Change the AnodeBlockOkay function to take the volIndex which it checks.
 
 Revamp lots of code in fixed anode to call AnodeBlockOkay with the
 volIndex appropriately.  Part of this was to change the was CreateAVL
 and InitAggregate are called so that they take the AVL index instead of
 the anode's offset in the superblock.  This simplifies the callers and
 allows better checking to be done at lower levels.
 [Wed Apr 21 11:29:27 1993]
 
 ota:
 
 While I was modifying anode_block.c change FormatAnodeBlock to use
 osi_Time to produce the stamps.  It used to use the bsize which is no
 where near variable enough.
 [Wed Apr 21 12:59:59 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/anode_block.c, file/episode/anode/anode_block.h, 
 file/episode/anode/fixed_anode.c, file/episode/anode/fixed_anode.h, 
 file/episode/anode/volume_table.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3439-verify-anode-block-volIndex'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7917
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : vnm_CreateVolume error recovery hangs
Reported Date                 : 04/20/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_misc.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-db3438-create-vol-error-recovery
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 When an error code is returned from dir_MakeDir in vnm_CreateVolume, we
 call OSI_VN_RELE without first ending the transaction.  In a debug kernel
 extension, RememberThreadTrans would panic; in a non-debug kernel, the process
 hangs waiting for its own transaction to finish.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_misc.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `bwl-db3438-create-vol-error-recovery'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7916
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : fts restore complains about garbage after end of dump
Reported Date                 : 04/19/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftserver/ftserver_restore.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3435-allow-garbage-after-end-of-dump
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 fts restore (actually, vols_RestoreVolume() in ftserver_restore.c)
 reports an error if it detects any data beyond the end of the dump
 (after processing the VOLS_DUMP_HEADER_END opcode).  This causes a
 problem if the restore is being done from a device that pads the last
 block.
 Jeffrey Prem
 Mon Apr 19 18:26:53 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ftserver/ftserver_restore.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value 
 `jdp-db3435-allow-garbage-after-end-of-dump'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[6/10/93 public]
Since we've never seen this one, so we don't have those kind of
devices on our machines, whatever kind they are, I'm closing this.

[12/17/93 public]
Closed.



CR Number                     : 7915
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager should scan badblocks better
Reported Date                 : 04/19/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/big_bitmap.test, file/episode/anode/fixed_anode.c, file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, file/episode/anode/salvage.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3432-cleanup-badblocks-scanning
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 Instead of trying to fold badblock scanning into the used block
 processing, do it explicitly early on.
 [Mon Apr 19 14:47:11 1993]
**Solution Text**
 Delta: ota-db3432-cleanup-badblocks-scanning -r1.4
 
 ota:
 
 Move the scan bad blocks file processing code to aggregate.c where it
 can be near the make bad blocks file code used by newaggr.  This also
 means we call it during salvage directly after verifying the badblock
 anode, which we do explicitly (not part of the general traverse) anyway.
 
 This change uncovered a bug in the existing make bad block file code
 which left the unused portion of the badblock frame zeroed.  Fix this.
 [Tue Apr 20 15:55:03 1993]
 
 ota:
 
 In the above changes I also put "#if !defined(KERNEL)" around most of
 the aggregate creation code.  An unexpect result was that AVL_INDEX
 wasn't defined any more for use inside the kernel.
 
 Move that define out of the body of epig_CreateAggregate.
 [Fri Apr 23 09:23:06 1993]
 
 ota:
 
 Set ss->volIx and ss->anodeIx during bad block scan so error messages
 are reported properly.
 [Mon Apr 26 10:17:58 1993]
 
 ota:
 
 Don't return an uninitialized code variable from BitmapSetup if there is
 no bad block index listed in the superblock.
 [Tue Apr 27 11:50:14 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/big_bitmap.test, file/episode/anode/fixed_anode.c, 
 file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, 
 file/episode/anode/salvage.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3432-cleanup-badblocks-scanning'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7914
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager must handle repairing AVL out of existence
Reported Date                 : 04/19/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/19/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/fixed_anode.c, file/episode/anode/volume_table.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3429-check-AVL-as-vt
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 If the first block of the AVL is removed we don't notice this until the
 beginning of the dir scan pass.  We should be making additional checks
 earlier.
 [Mon Apr 19 11:29:28 1993]
 
 ota:
 
 Instead of fixing this directly, add checks to make sure we don't allow
 COW block references in non-COW anodes.  Since the block map of the AVL
 is so tightly constrained, we don't really need a general solution here.
 This check prevents the possible errors in the first block of the AVL,
 others will be checked pretty naturally.
 [Mon Apr 19 14:41:28 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3429-check-AVL-as-vt
 [Mon Apr 19 14:41:52 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/fixed_anode.c, 
 file/episode/anode/volume_table.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3429-check-AVL-as-vt'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7913
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage needs to avoid modifications to backing filesets
Reported Date                 : 04/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/sal_errors.h, file/episode/salvage/paths.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3426-avoid-touching-backing-volumes
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 Still more places where we attempt to modify backing filesets when we
 discover problems.  Avoid this.
 [Fri Apr 16 16:09:04 1993]
 
 ota:
 
 In aggregate.c set return code from epib_Free before insisting it be
 zero.
 
 In backing filesets:
   Don't delete orphaned indexes.
   Don't complain if errors reported earlier weren't repaired.
 [Mon Apr 19 11:23:40 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3426-avoid-touching-backing-volumes
 [Mon Apr 19 11:23:54 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/sal_errors.h, file/episode/salvage/paths.c, 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3426-avoid-touching-backing-volumes'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7912
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : verifydb fails on empty database
Reported Date                 : 04/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bakserver/ol_verify.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-db3424-bakserver-bad-malloc
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 a bad malloc in verifydb causes it to fail when the database is newly created
 and empty.
**Solution Text**
 -------------------------------------------------------------------------------
 There were actually two bad mallocs that cause data corruption. Both these
 were fixed. A simple test to run is
 
 0. Remove the backup database files, and startup a bakserver.
 1. bak addhost <hostname> 1
 2. bak addhost <hostname> 2
 3. bak rmhost 1
 4. bak rmhost 2
 5. verifydb
 
 This should show Database NOT_OK. With this fix, the above test should
 show Database OK.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/bakserver/ol_verify.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `vijay-db3424-bakserver-bad-malloc'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7911
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : icl rpc interface may incorrectly report a server already running
Reported Date                 : 04/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/icl/icl_rpcnsi.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3423-disable-failing-icl-dup-server-code
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 There was code in the icl RPC interface exporter that was checking the
 namespace for a server already running.  This was returning false
 positives.  This delta removes the code, which is really extraneous
 anyway.
 Michael Comer
 Fri Apr 16 15:03:16 1993
**Solution Text**
 Michael Comer
 Fri Apr 16 15:03:24 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/icl/icl_rpcnsi.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3423-disable-failing-icl-dup-server-code'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7910
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : vols_Forward leaks memory
Reported Date                 : 04/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftserver/ftserver_vprocs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3422-plug-vols-forward-leak
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 There are several memory leaks in vols_Forward().  First, the call to
 pthread_attr_create() should be compliemented with a call to
 pthread_attr_delete() after creating the dumper thread.  Second, the call to
 pthread_join() should be followed by a call to pthread_detach() in order to
 free the resources held by the thread.  Finally, there are several leaks that
 occur during the call to dfsauth_client_InitBindingAuth(); however, it is not
 clear that these are DFS problems.  The problem is described further in the
 contents of a mail message to Bill Sommerfeld at HP/Apollo:
 
 Date: Thu, 15 Apr 1993 17:49:22 -0400 (EDT)
 From: Jeffrey_Prem@transarc.com
 To: sommerfeld@apollo.hp.com
 CC: Carl_Burnett@transarc.com, Beth_Bottos@transarc.com,
     Pervaze_Akhtar@transarc.com, Craig_Everhart@transarc.com,
     BBoard <bb+transarc.afs.dce.fileset@transarc.com>
 Subject: Possible memory leaks in security code
 
 Bill,
 
 I'm debugging memory leaks in the DFS ftserver that may be
 in/related-to security code.  When preparing for a fileset move, the
 ftserver establishes a login context in order to talk to another
 ftserver.  I've included some rough memory usage numbers (from ps
 output).
 
 After calling each of the following routines, memory usage has
 increased by the specified amount:
 
         sec_login_setup_identity:       112k
         sec_login_valid_from_keytable:  56k
         rpc_binding_set_auth_info:      20k
 
 Are these numbers reasonable?  I guess I could be convinced that they
 were an expected one-time cost; however, on subsequent move requests,
 the ftserver refreshes its login context, causing even more memory to
 be allocated:
 
         sec_login_refresh_identity:     28k
         sec_login_valid_from_keytable:  12k
 
 The second set of numbers are actually more troublesome, since the
 growth occurs each time a fileset is moved (actually, twice each time).
 
 Are there any known bugs along these lines, or, failing that, do you
 have any ideas?  Should we be purging our identity/context before
 refreshing? 
 
 Thanks,
 Jeff
 Jeffrey Prem
 Fri Apr 16 10:07:24 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ftserver/ftserver_vprocs.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `jdp-db3422-plug-vols-forward-leak'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7909
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : core dump when dumping more than one log
Reported Date                 : 04/15/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/icl/icl_dump.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3419-premature-close-of-logfile-on-multilog-dump
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 There was a misplaced fclose that was causing the log file to be
 closed once for each log.  The second close resulted in a segv. 
 
 Michael Comer
 Thu Apr 15 15:06:44 1993
**Solution Text**
 Move close.
 Michael Comer
 Thu Apr 15 15:06:53 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/icl/icl_dump.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3419-premature-close-of-logfile-on-multilog-dump'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7908
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage does not validate aux ptrs
Reported Date                 : 04/14/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/file.c, file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, file/episode/anode/salvage.h, file/episode/salvage/paths.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3413-check-file-status-auxes
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 The aux indexed in the file status are bad.  In this case (salt:file8a)
 they point to an ordinary file.  The first routine that notices this is
 epif_GetStatus who panics.  We need to validate these fields explicitly
 before we stumble over them when we can't fix the problem.
 [Wed Apr 14 16:40:39 1993]
**Solution Text**
 Delta: ota-db3413-check-file-status-auxes -r1.6
 
 ota:
 
 Cleanup and validate the code that salvages aux containers.  The
 existing code is barely used at all.
 [Fri Apr 16 16:07:06 1993]
 
 ota:
 
 Initialize SETINCON bit to zero for each new volume.
 [Fri Apr 23 09:13:09 1993]
 
 ota:
 
 Also clear the BACKINGVOLUME and SETINCON bits near the end of DirWalk
 so that ZLC repairs can happen without marking the volume inconsistent.
 
 More spelling and typo correction of msgs.
 [Fri Apr 23 10:44:06 1993]
 
 ota:
 
 Fix some problems with output of aux related msgs.
 [Fri Apr 23 13:16:16 1993]
 
 ota:
 
 Add the anode's with bad auxiliary containers to the badAcls or
 badPlists list in addition to the mayRestore list.
 [Fri Apr 23 15:55:28 1993]
 
 ota:
 
 Don't clear SETINCON at end of DirWalk or we'll never see it!  Only
 clear BACKINGVOLUME at this point so subsequent errors do not cause
 SETINCON to be set if it isn't already set.  Also clear BACKINGVOLUME
 upon return from DirWalk (in case we return early) so that we see the
 marking of the volume "INCONSISTENT".
 [Tue Apr 27 15:11:19 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/file.c, 
 file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, 
 file/episode/anode/salvage.h, file/episode/salvage/paths.c, 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3413-check-file-status-auxes'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7907
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager panics recursively opening AVL
Reported Date                 : 04/14/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/salvage.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3412-omit-ah-when-calling-Open
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 For no really good reason, the salvager passes in the allocation handle
 when opening anodes to in EnumerateAnodeItems.  This causes a panic if
 the anode is the AVL.
 [Wed Apr 14 15:49:33 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3412-omit-ah-when-calling-Open
 
 In epiz_EnumerateAnodeItems, instead of passing the allocation handle to
 epia_Open, just explicitly set it after open returns and before calling
 the specified proc.
 [Wed Apr 14 16:06:02 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/salvage.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3412-omit-ah-when-calling-Open'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7906
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage does not check for EMPTY frag block addr
Reported Date                 : 04/14/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/fixed_anode.c, file/episode/anode/sal_errors.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3410-check-for-empty-frag-block
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 The checks of fragmented anode miss checking for the block address being
 "EMPTY".  This is illegal if the fragment bit is set, the anode should
 be made-empty.
 [Wed Apr 14 15:09:36 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3410-check-for-empty-frag-block
 
 Check for and truncate fragmented anodes whose block address is EMPTY.
 [Wed Apr 14 15:44:24 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/fixed_anode.c, 
 file/episode/anode/sal_errors.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3410-check-for-empty-frag-block'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7905
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage cannot clear COW bits during verify
Reported Date                 : 04/14/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/fixed_anode.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3406-handle-MaybePatchBacking-in-verifyonly
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 ota:
 
 The mechanism that clears the COW bits of block references in anodes
 with bad backing pointers gets confused when -verify is specifed.
 [Wed Apr 14 12:03:33 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3406-handle-MaybePatchBacking-in-verifyonly
 
 Don't attempt to verify the backing block references of a an anode whose
 backing anode is invalid.  If we're doing repairs we just patch the
 block address, but in verifyonly mode we must be careful.
 [Wed Apr 14 15:07:38 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/fixed_anode.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3406-handle-MaybePatchBacking-in-verifyonly'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7904
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : ICL should dump timestamps periodically
Reported Date                 : 04/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/icl/Makefile, file/icl/icl.h, file/icl/icl_dump.c, file/icl/icl_error.et, file/icl/icl_log.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3396-make-icl-dump-timestamps
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The timestamps for each ICL record have only 10 bits for the seconds
 field.  This means that they may not be very useful for figuring out
 clock time.
 Michael Comer
 Fri Apr 16 15:13:39 1993
**Solution Text**
 Add a "lst timestamp" field to the log structure and write out a
 timestamp record if the difference between when now and the last write
 > 1024 seconds.  The timestamp record defines a new ICL type.
 Michael Comer
 Fri Apr 16 15:13:54 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/icl/Makefile, file/icl/icl.h, 
 file/icl/icl_dump.c, file/icl/icl_error.et, file/icl/icl_log.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `comer-db3396-make-icl-dump-timestamps'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7903
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : Uses wrong statistic value for reporting active work stations.
Reported Date                 : 04/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/px/px_intops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : fred-db3393-fx-fix-ws-count-for-scout
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The AFS_GetStatistics function uses the value goodHosts to report the
 number of workstations currently connected to the file exporter.  It should
 be using the value usedHosts.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `FIXED'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/px/px_intops.c' 
Changed Transarc Status from `FIXED' to `import' 
Added field Transarc Deltas with value `fred-db3393-fx-fix-ws-count-for-scout'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7902
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Need to establish vld_lastIndex when beginning whole-volume loop
Reported Date                 : 04/09/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_volops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3389-establish-lastindex-in-volume-loops
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The initialization code at the beginning of efsSomeClone(),
 vol_efsClone(), and vol_efsDestroy needs to initialize the
 `vld_lastIndex' member of the voldata structure.  The current code
 only initializes this member in vol_efsCreate() and vol_efsOpen().
 This causes problems when any two of the three routines mentioned
 above is called without closing the volume and reopening it in
 between because vld_lastIndex may have a bogus value.  The most
 obvious case is when a clone fails part-way through and we then
 attempt to destroy it.  Since the clone was opened right after it was
 created, vld_lastIndex is 0, dooming to failure the attempt to
 immediately destroy it.
 
 Jeffrey Prem
 Fri Apr  9 16:26:35 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_volops.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value 
 `jdp-db3389-establish-lastindex-in-volume-loops'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7901
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Deleting non-empty fileset causes disk corruption
Reported Date                 : 04/09/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/09/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/anode_block.h, file/episode/anode/basic.test, file/episode/anode/test_anode.c, file/episode/anode/volume.c, file/episode/anode/volume_table.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3388-bad-cleanup-when-deleting-non-empty-volume
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Interest List CC: jdp
 Inter-dependent CRs: 3389
 
 ota:
 
 Jeff and I discovered that a fileset clone which fails with an ENOSPC
 error will then try to cleanup by deleting the fileset.  This causes the
 episode/anode layer code to fail (because the fileset isn't empty) and
 the cleanup code here is also in error.  But the consequences are that
 it frees the fileset's anode index even though the fileset hasn't been
 deleted.
 [Fri Apr  9 16:13:28 1993]
 
 ota:
 
 [Tue Apr 13 11:14:12 1993]
**Solution Text**
 ota:
 
 Delta: ota-db3388-bad-cleanup-when-deleting-non-empty-volume
 
 The first step was to add a -force option to delete_volume so that we
 could generate the NOTEMPTY error on demand from test_anode.  This
 allowed the problem to be easily reproduced.
 
 Then I added an assert in epit_Free to insist that the index being freed
 was either freshly allocated or correctly deleted.
 
 The actual fix involved moderately significant reorganization of delete
 and close routines in both aggregate.c and volume.c.
 [Tue Apr 13 11:14:14 1993]
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/anode_block.h, file/episode/anode/basic.test, 
 file/episode/anode/test_anode.c, file/episode/anode/volume.c, 
 file/episode/anode/volume_table.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3388-bad-cleanup-when-deleting-non-empty-volume'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7900
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Need to bulletproof against invalid file types
Reported Date                 : 04/09/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/09/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_vattr.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot3386-bullet-proof-chmod
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The AFS/DFS translator is making a StoreStatus RPC call to do a chmod, in
 which it isn't zeroing out the S_IFMT bits of the mode.  Random S_IFMT bits
 are getting passed all the way through to the epif_SetStatus call, and the
 file becomes unusable.  This could be fixed in the PX, but safest is to fix
 it in Episode (i.e. vnva_SetAttr).
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_vattr.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `bwl-ot3386-bullet-proof-chmod'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7899
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : Add RPC interface to user-level ICL traces
Reported Date                 : 04/06/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/flserver/Makefile, file/flserver/flserver.c, file/ftserver/ftserver_main.c, file/icl/Makefile, file/icl/icl_dump.c, file/icl/icl_dumpCommand.c, file/icl/icl_error.et, file/icl/icl_rpc.acf,
file/icl/icl_rpc.c, file/icl/icl_rpc.idl, file/icl/icl_rpcnsi.c, file/icl/icl_userint.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3378-add-rpc-interface-to-user-level-icl
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Now that our servers are using ICL tracing, we need to have a way to
 do dfstrace-type things to them.  THis delta will add that and will
 incorporate the functionality into dfstrace.
 Michael Comer
 Tue Apr  6 12:36:37 1993
**Solution Text**
 RPC interface added; changes made to dfstrace to call it.
 Michael Comer
 Fri Apr  9 13:25:35 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/flserver/Makefile, file/flserver/flserver.c, 
 file/ftserver/ftserver_main.c, file/icl/Makefile, file/icl/icl_dump.c, 
 file/icl/icl_dumpCommand.c, file/icl/icl_error.et, file/icl/icl_rpc.acf, 
 file/icl/icl_rpc.c, file/icl/icl_rpc.idl, file/icl/icl_rpcnsi.c, 
 file/icl/icl_userint.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3378-add-rpc-interface-to-user-level-icl'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7898
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : fix up random problems with ICL traces in flserver and ftserver
Reported Date                 : 04/06/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/06/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/flserver/fl_trace.et, file/ftserver/Makefile, file/ftserver/ftserver_trace.et, file/ncsubik/utrace.et
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3377-tweak-icl-tracing-in-flserver-and-ftserver
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 fl_trace.et and utrace.et don't have dummy first entries.
 the ftserver trace catalog doesn't conform to the 'dfszxx' trace
 "standard".
 Michael Comer
 Tue Apr  6 11:47:20 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/flserver/fl_trace.et, 
 file/ftserver/Makefile, file/ftserver/ftserver_trace.et, 
 file/ncsubik/utrace.et' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3377-tweak-icl-tracing-in-flserver-and-ftserver'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7897
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FXD
Short Description             : fxd takes negative numbers for arguments
Reported Date                 : 04/01/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/01/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/pxd/pxd.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3366-check-for-negative-arguments-on-pxd
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 -hostlife, -hostrpc, -pollinterval, -maxlife, and -maxrpc all take
 negative numbers.  They shouldn't.
 Michael Comer
 Thu Apr  1 15:18:00 1993
**Solution Text**
 Read specified values into long, check to see that they are >= 0, then
 copy values into unsogned long.
 
 Michael Comer
 Thu Apr  1 16:12:05 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/pxd/pxd.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3366-check-for-negative-arguments-on-pxd'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7896
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Need to push the ZLC list into new clone
Reported Date                 : 04/01/93
Found in Baseline             : 1.0.2a
Found Date                    : 04/01/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_volops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3363-copy-ZLC-list-during-clone
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 As in the reclone case, we need to copy a fileset's file status array
 (the ZLC list) into a newly created clone.  This will be done in
 vol_efsClone() after each anode has been cloned and before turning off
 the inconsistent bit.
 
 Jeffrey Prem
 Thu Apr  1 11:21:37 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_volops.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `jdp-db3363-copy-ZLC-list-during-clone'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7895
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : newaggr -noaction fails if aggr is attached
Reported Date                 : 03/31/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/31/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/newaggr.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3361-dont-lock-aggr-on-newaggr-noaction
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 "newaggr -noaction" will fail if the aggregate is attached.  There is
 no good reason for this since no modifications will be made to the aggregate.
 
 Jeffrey Prem
 Wed Mar 31 15:10:40 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/newaggr.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value 
 `jdp-db3361-dont-lock-aggr-on-newaggr-noaction'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7894
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BOSSERVER
Short Description             : adjust bosserver auto-shutdown interval for restart
Reported Date                 : 03/31/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/31/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bosserver/bossvr_thread_childWatch.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot3360-bosserver-catch-more-bogus-startups
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The bosserver currently auto-shuts down a bnode if it restarts more
 than 10 times in 30 seconds.  This is not long enough for a DCE
 process, which may take longer than a few seconds to restart.  This
 delta will make the criterion 10 restarts in 3 minutes to accomodate
 this.  
 Michael Comer
 Wed Mar 31 13:23:26 1993
**Solution Text**
 Changed 30 seconds to 3 minutes.
 Michael Comer
 Thu Apr  1 10:25:13 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `FIXED'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/bosserver/bossvr_thread_childWatch.c' 
Changed Transarc Status from `FIXED' to `import' 
Added field Transarc Deltas with value 
 `comer-ot3360-bosserver-catch-more-bogus-startups'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7893
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ALL
Short Description             : Provide bomb point utilities
Reported Date                 : 03/30/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/30/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/Makefile, file/bak/Makefile, file/bomb/Makefile, file/bomb/README, file/bomb/bomb.p.h, file/bomb/bomb_core.c, file/bomb/bomb_errors.et, file/bomb/bomb_set.sh, file/bomb/bomb_test.c, file/bu
tc/Makefile, file/fsprobe/Makefile, file/scout/Makefile, file/util/Makefile, file/util/hash.c, file/util/hash.h, file/xaggr/Makefile
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3358-add-bomb-point-utilities
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 This defect # will be used to add testing utilities for error
 injection ("bomb points").
 Jeffrey Prem
 Tue Mar 30 18:38:58 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/Makefile, file/bak/Makefile, 
 file/bomb/Makefile, file/bomb/README, file/bomb/bomb.p.h, 
 file/bomb/bomb_core.c, file/bomb/bomb_errors.et, file/bomb/bomb_set.sh, 
 file/bomb/bomb_test.c, file/butc/Makefile, file/fsprobe/Makefile, 
 file/scout/Makefile, file/util/Makefile, file/util/hash.c, file/util/hash.h, 
 file/xaggr/Makefile' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `jdp-db3358-add-bomb-point-utilities'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/01/93 public]
Closing and setting fixby back to 1.0.2a - status field inadvertently
left in fix state.

[12/17/93 public]
Closed.



CR Number                     : 7892
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager must verify block map of COW and backing anodes
Reported Date                 : 03/29/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/29/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/fixed_anode.c, file/episode/anode/fixed_anode.h, file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, file/episode/anode/salvage.h, f
ile/episode/anode/verify.h, file/episode/anode/volume.c, file/episode/salvage/paths.c, file/episode/salvage/salvage_main.c, file/episode/salvage/salvager.p.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3352-diff-COW-and-backing-blockmaps
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Mike Stolarchuk made a first stab at comparing the block map of the COW
 and its backing anode.  That code couldn't be made to work but I just
 commented it out (many months ago).  Now we really need to do this
 correctly.
 
 The aggregate that brought this to my attention was the salt anode9d
 test which produced the following crash:
 
 (global package/walk.c rev. 4.34 #376 assertion failed) alfs_MBZ: code(609947652) was not zero
 
 This error code is:
 translate_et: 609947652: the buffer is not valid (dfs / lgb)
 
 The logic here is very convoluted.  The bottom line is that a COW anode
 refers to a block (with the 0x80000000 bit set) which is actually free.
 There are no checks to detect this (and related) problem(s).
 [Ted Anderson Mon Mar 29 13:52:02 1993]
**Solution Text**
 Delta: ota-db3352-diff-COW-and-backing-blockmaps -r1.2
 
 During block map checking verify that the COW and backing anodes have
 compatable structures.
 
 In the process we revitalize the COWFIXPASS of the salvager which had
 largely atrophied.  We now do COW fixing on every pass.  However, since
 we allow arbitrarily long backing chains we may have to repeatedly check
 for COW block map errors if we modified any block map on the previous
 pass.  To avoid loops, we only modify the block maps of non-COW files on
 the first pass (or second pass if duplicates are being resolved).
 
 Also cleanup many ReportError calls.
 []
 
 ota:
 
 Accidently reversed FRAG and BLOCK reason msgs in epiz_CheckBlock so
 swap them.
 [Tue Apr 27 11:13:35 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/fixed_anode.c, file/episode/anode/fixed_anode.h, 
 file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, 
 file/episode/anode/salvage.h, file/episode/anode/verify.h, 
 file/episode/anode/volume.c, file/episode/salvage/paths.c, 
 file/episode/salvage/salvage_main.c, file/episode/salvage/salvager.p.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3352-diff-COW-and-backing-blockmaps'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7891
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage must set freelist of page without any free entries
Reported Date                 : 03/29/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/29/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/dir/dir_salvage.c, file/episode/ravage/bash.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3351-always-set-flist
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The loop in dirs_Salvage that copies each good entry from the old
 directory to the new one doesn't handle the case where there are no free
 entries at all.  It leaves the dir page header flist pointing to the
 first entry even if it isn't free!
 [Ted Anderson Mon Mar 29 13:25:49 1993]
**Solution Text**
 Delta: ota-db3351-always-set-flist
 
 This problem is triggered by the saved aggregate:
 ~ota/de/salvage-data/power7.scratch2.920929.assert-dir-salvage.32K.16K.Z
 
 The fix is to set the page header flist field to zero if there were no
 free entries copied to the new directory in the current page.
 [Ted Anderson Mon Mar 29 13:44:29 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/dir/dir_salvage.c, 
 file/episode/ravage/bash.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3351-always-set-flist'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7890
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ICL
Short Description             : user-level ICL tracing messages should use thread number, not pid
Reported Date                 : 03/29/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/29/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/icl/icl_log.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3350-make-user-level-icl-print-ti
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The ICL trace code currently prints out the pid in each trace message.
 This is not terribly useful for a multithreaded process.  It should
 use the thread ID instead.
 Michael Comer
 Mon Mar 29 12:59:34 1993
**Solution Text**
 Change icl_log.c to get and save the thread's ID.
 
 Michael Comer
 Mon Mar 29 13:25:31 1993
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/icl/icl_log.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `comer-db3350-make-user-level-icl-print-ti'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7889
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage should suppress copious visible quota errors
Reported Date                 : 03/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, file/episode/anode/salvage.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3339-suppress-visible-quota-repair-msgs
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 If we know that the aggregate predates visible quota, then suppress the
 extremely copious visible quota errors.
 [Ted Anderson Thu Mar 25 12:27:39 1993]
**Solution Text**
 Delta: ota-db3339-suppress-visible-quota-repair-msgs
 
 Set a new suppress-printing bit in the errorData flags word for the
 BADVISIBLE error if the aggregate predates the visible quota changes.
 This disables the printing phase of ReportError.
 [Ted Anderson Fri Mar 26 10:03:37 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/sal_errors.h, file/episode/anode/salvage.c, 
 file/episode/anode/salvage.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3339-suppress-visible-quota-repair-msgs'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7888
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage dhops never return correct exit code
Reported Date                 : 03/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/25/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/salvage/salvage_dhops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3337-fix-sdh_Read-return-code
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Interest List CC: bwl
 
 Apparently the dhops READ function is supposed to return 1 for success
 and 0 for failure.  However, the salvager repair dhops return 0 to mean
 success.  This means that they always appear to fail!  Strange but true.
 [Ted Anderson Thu Mar 25 11:52:50 1993]
**Solution Text**
 Delta: ota-db3337-fix-sdh_Read-return-code
 
 Rename all the ops vector procedures from "sdh" to "rdh" for Repair Dir
   Handle.  This elimintates the name duplication with the other salvage
   dhops which are used only for verification.
 In rdh_Read fail if request is beyond EOF.
 Return 1 on success.
 [Ted Anderson Fri Mar 26 16:15:45 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/salvage/salvage_dhops.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3337-fix-sdh_Read-return-code'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7887
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : umask should not be considered in presence of explicit ACLs
Reported Date                 : 03/25/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_vattr.c, file/episode/vnops/efs_vnodeops.c, file/xvnode/RIOS/xvfs_aix2vfs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : rajesh-db3333-umask-and-acls
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 [ rajesh 3/25/93 ]
 
 When a file is created, the umask value is used to filter out permission
 bits even if the parent directory has an explicit initial object ACL.
 This should not be the case.
**Solution Text**
 rajesh:
 
 -------------------------------------------------------------------------------
 
 Modify naix_create and naix_mkdir not to filter the file/dir mode
 using the umask value. Added a new parameter to vnva_FileCreate to
 hold the umask value.  Do not filter the modebits of an ordinary file
 or a directory by the umask value if the parent dir has the
 corresponding initial object/container ACL.  Modify efs_create,
 efs_mkdir to fetch the umask value and pass it to vnva_FileCreate.
 Modify the other caller of vnva_FileCreate, efs_symlink to pass a
 dummy umask (zero) to match the new interface.
 [Tue Apr 27 11:07:38 1993]
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `FIXED'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_vattr.c, 
 file/episode/vnops/efs_vnodeops.c, file/xvnode/RIOS/xvfs_aix2vfs.c' 
Changed Transarc Status from `FIXED' to `import' 
Added field Transarc Deltas with value `rajesh-db3333-umask-and-acls'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7886
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTSERVER
Short Description             : Need to do better cleanup after failed clone
Reported Date                 : 03/24/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftserver/ftserver_vprocs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-db3331-better-clone-cleanup
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 In ftserver_Clone() after the first call to VOL_BULKSETSTATUS(), we
 need to cleanup by calling ftserver_DeleteVolume() instead of
 ftutil_Destroy().  The latter routine destroys the fileset, but does
 not bring the read/write back online.  The former routine will do the
 right thing.
 Jeffrey Prem
 Wed Mar 24 18:25:37 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ftserver/ftserver_vprocs.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `jdp-db3331-better-clone-cleanup'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7885
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager does not handle repaired dirs without . and ..
Reported Date                 : 03/24/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/24/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/dir/dir_salvage.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3327-repair-dirs-wo-.-..-correctly
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Inter-dependent CRs: 3263, 3337
 
 The salvager crashes in both the salt test (e.g. anode9c) and the ravage
 test (e.g. zeroall 8192) when applied to epi-blake.  The assert failure
 is:
 
 (global package/walk.c rev. 4.30 #346 assertion failed) Got code (0) from dir_Lookup on "."
 
 The problem is that the change I made in delta
 ota-db3263-call-dirb_Annul-during-repair was in error, but for some
 reason didn't get noticed at the time.
 [Ted Anderson Wed Mar 24 10:27:01 1993]
 
 After fixing this problem several problems surfaced in dirs_Salvage
 having to with creating the free list in the rebuilt directory and
 setting pnhimark correctly.
 [Ted Anderson Thu Mar 25 08:44:19 1993]
 
 The aggregate power7.epi0.911112.findentry-loop.no_rcvr.Z showed up
 several other problems with these changes to the dirs_Salvage.  See for
 example db 3337.
 
 After fixing that problem an error with the new termination condition
 appeared, which resulted in an infinite loop if a directory didn't end
 with a free piece.
 [Ted Anderson Thu Mar 25 12:44:38 1993]
 
 More problems with directory repair.  These show up as this assert:
 
 The assert subroutine failed: dirIsOkay != 0, file ../../../../../src/file/episode/salvage/walk.c, line 366
 
 First is that duplicate names are now correctly skipped, however, the
 corresponding slot in the new directory must be added to the first list.
 This is reproduced by the above mentioned power7 aggr.
 
 Second, the code that update the freelist in case of concatenating
 adjacent free areas broken.  It was updating the previous free entry's
 length at the beginning of the piece, thus over writing the first byte
 of the entry MAGIC.  This is reproduced by salt test dir6b.
 [Ted Anderson Thu Mar 25 14:42:55 1993]
 [Ted Anderson Thu Mar 25 16:29:24 1993]
**Solution Text**
 Delta: ota-db3327-repair-dirs-wo-.-..-correctly
 
 [Ted Anderson Thu Mar 25 08:44:26 1993]
 
 And revision 1.2 thereof.
 [Ted Anderson Thu Mar 25 16:29:31 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/dir/dir_salvage.c, 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3327-repair-dirs-wo-.-..-correctly'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7884
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Do not need call to vm_makep from efs_vmrdwr
Reported Date                 : 03/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/23/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/RIOS/efs_aixvmm.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-db3320-no-more-vmmakep
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The loop in efs_vmrdwr, calling vm_makep to create pages when writing data,
 is not necessary.  In AIX 3.2, vm_move takes care of this.  Taking advantage
 of this, besides cleaning up the code, will improve performance when memory
 is very tight.
**Solution Text**
 Delta bwl-db3320-no-more-vmmakep.
 
 Entered as OT 
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/RIOS/efs_aixvmm.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `bwl-db3320-no-more-vmmakep'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7883
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Another quota res deadlock due to race condition
Reported Date                 : 03/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/23/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/RIOS/efs_aixvmm.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-db3319-recompute-file-length
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 IBM found a deadlock in quota reservation during their PVT testing.  Carl
 determined that it was due to a race condition:  efs_vmrdwr was calling
 efs_getlength to determine the file size, then calling efs_reserve, then
 using the file size, but efs_reserve had dropped the file lock and another
 process had changed the size of the file.
**Solution Text**
 Entered as OT 
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/RIOS/efs_aixvmm.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `bwl-db3319-recompute-file-length'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7882
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : efs_rmdir should check that object being removed is a dir.
Reported Date                 : 03/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_vnodeops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-db3299-check-removed-obj-type
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 In efs_vnodeops.c, the entire efs_rmdir function never checks to see
 if the object being deleted is a directory, or not.  All of the
 underlying code seems to treat this correctly (remove is on an object
 of any type in vnd_Delete).  Local access catches this (file system
 independent code verifies the object is a directory) but via DFS, this
 is not caught.
 
 IBM saw this when doing an "rmdir" on a mount point (symbolic link
 with funny mode bits).
**Solution Text**
 bwl-db3299-check-removed-obj-type
 
 Entered as OT 
 Entered as OT 
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_vnodeops.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `bwl-db3299-check-removed-obj-type'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7881
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salt modifies non-existent spares
Reported Date                 : 03/22/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/scavenge/volume.pl
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3314-fix-fileset-spare-usage
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Inter-dependent CRs: OT5926, OT5950
 Interest List CC: blake
 
 This is really a problem of merging the Visible Quota changes from
 OT5926 and the new salvager tests (salt) from OT5950.  The former
 changes certain fileset header spares to hold the visible quota, but the
 latter tests them as spares.
 [Ted Anderson Mon Mar 22 10:46:43 1993]
 
 Further, there are references to the obsolete names of the quota fields.
 [Ted Anderson Mon Mar 22 16:04:09 1993]
**Solution Text**
 Delta: ota-db3314-fix-fileset-spare-usage
 
 Only check 5 sparel's in fileset header.
 [Ted Anderson Mon Mar 22 13:35:31 1993]
 
 Change all references to quota.usage and quota.limit to allocated.usage
 and allocated.limit.  Add a comment noting that we must add the
 corresponding tests for visible quota.
 [Ted Anderson Mon Mar 22 16:04:19 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/scavenge/volume.pl' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3314-fix-fileset-spare-usage'

[5/4/93 public]
This code is not part of the current OSF code base.
Changed Status from `open' to `cancel'

[12/17/93 public]
Closed.



CR Number                     : 7880
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FTS
Short Description             : syncfldb and syncserv don't work well with UFS or a fileset that has only a backup representative.
Reported Date                 : 03/17/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/17/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_volops.c, file/ftserver/ftserver_vprocs.c, file/ufsops/ufs.h, file/userInt/fts/volc_queue.c, file/userInt/fts/volc_vldbsubr.c, file/userInt/fts/volc_vprocs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-db3303-syncfldb-syncserv-errors
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Validation Text**
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_volops.c, 
 file/ftserver/ftserver_vprocs.c, file/ufsops/ufs.h, 
 file/userInt/fts/volc_queue.c, file/userInt/fts/volc_vldbsubr.c, 
 file/userInt/fts/volc_vprocs.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `cfe-db3303-syncfldb-syncserv-errors'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7879
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UFSOPS
Short Description             : Reported UFS/JFS quota info should be based on actual size/usage
Reported Date                 : 03/16/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftserver/ftserver_restore.c, file/ufsops/ufs_agops.c, file/ufsops/ufs_volops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-ot3298-report-actual-ufs-usage
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 To prevent restores of JFS/UFS filesets into Episode from failing due to
 quota limits, the current ufs volume ops make up an arbitrary (large) value
 for both allocLimit and visQuota.  This causes confusion to users that
 do an "fts lsquota" on a JFS/UFS fileset.  Instead, we will report the
 filesystem size and usage.
 
 To ensure that this doesn't cause problems while restoring, we will change the
 ftserver restore code to artificially increase the limits before starting
 the restore.  After the restore completes, the limits will be set to their
 previously dumped values.
 
 Jeffrey Prem
 Tue Mar 16 16:45:16 1993
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `NEW'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ftserver/ftserver_restore.c, 
 file/ufsops/ufs_agops.c, file/ufsops/ufs_volops.c' 
Changed Transarc Status from `NEW' to `import' 
Added field Transarc Deltas with value `jdp-ot3298-report-actual-ufs-usage'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7878
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage fails to make crucial IB checks
Reported Date                 : 03/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/fixed_anode.c, file/episode/anode/sal_errors.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3267-do-more-IB-checks
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The salvager, in ScanIndirect, fails to make some obvious checks on the
 indirect block header.  This results in panics later on when those
 checks are made again and fail.
 [Ted Anderson Fri Mar 12 13:56:43 1993]
**Solution Text**
 Delta: ota-db3267-do-more-IB-checks
 
 Calculate skip and pass it to IndirectBlockOkay.
 
 Use epiz_ReportError when rejecting an indirect block with BADINDIRECT
 error.  This simplifies some code in ScanIndirect and its caller.
 [Ted Anderson Fri Mar 12 16:14:18 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/fixed_anode.c, 
 file/episode/anode/sal_errors.h' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `ota-db3267-do-more-IB-checks'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7877
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BACKUP
Short Description             : butc misuses osi_Free
Reported Date                 : 03/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/12/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : many
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-db3266-butc-bad-call-to-osi-free
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 butc makes a call to osi_Free but that block was allocated by some DCE 
 library that did not call osi_Alloc. This results in a program crash 
 because osi_Free is now doing checks to make sure this block was allocated
 by osi_Alloc and is of the right size. These changes went into 2.7 or 2.8
 and so I'm seeing this problem now for the first time. The fix is to 
 convert the osi_Free to a simple free(3).
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `many' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value `vijay-db3266-butc-bad-call-to-osi-free'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7876
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : List stores pioctl call copying wrong data.
Reported Date                 : 03/12/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/12/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_pioctl.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : fred-db3265-copy-hypers-correctly-in-liststores
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The pioctl interface for the liststores call does not copy enough 
 memory to record the volume ids of the failing stores.  It uses sizeof
 long and not sizeof afsHyper.
**Solution Text**
 Changed the code to use sizeof(afsHyper) for copying, and only copy the 
 number of servers that are found to be down, not the maximum.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `CLOSED'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/cm/cm_pioctl.c' 
Changed Transarc Status from `CLOSED' to `import' 
Added field Transarc Deltas with value 
 `fred-db3265-copy-hypers-correctly-in-liststores'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7875
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : See description below
Reported Date                 : 03/11/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/11/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/sal_errors.h, file/episode/dir/dir_main.c, file/episode/dir/dir_salvage.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3263-call-dirb_Annul-during-repair
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 When the salvager repairs a directory it sometimes uses dir buffers and
 sometimes it doesn't.  We need to be consistent about when we use normal
 "dir_" functions (which use dir buffers) and when dir_Salvage just
 writes directly on the anode.
 [Ted Anderson Thu Mar 11 15:52:22 1993]
 
 This change exposed several other bugs.
 
 The salvager currently inserts missing "." and ".." entries at fixed
 offsets.  This is not safe to do.  Instead we need to call Lookup to
 find a free piece.
 
 Also the code that removed directory entries cooresponding to damaged or
 invalid fids was willing to remove "." and "..".  This seemed dubious,
 so instead we check "." and ".." first to make sure they point to the
 expected fids.  If not reset their fids before doing the recursive
 descent.
 [Ted Anderson Fri Mar 12 12:20:40 1993]
**Solution Text**
 Delta: ota-db3263-call-dirb_Annul-during-repair
 
 The delta name suggests that we just invalidate the dir buffers before
 each call DHOP_WRITE.  This would work but there are there turn out to
 be lots of calls to DHOP_WRITE and to functions like dir_Lookup which
 will refill the dir buffer.  This seems like wasted effort.
 
 Instead just have dir_Salvage use the dir buffers like every other
 function in the dir package.  This is reasonably simple and easier to
 understand than the current scheme.
 
 Creating missing "." and ".." entries with valid directory offsets
 obtained from dir_Lookup.
 
 Reorganize handling of invalid "." and ".." entries.
 [Ted Anderson Fri Mar 12 12:20:54 1993]
 
 Delta: Rev 1.2 of ota-db3263-call-dirb_Annul-during-repair
 
 I added a repairing message for DIRERROR explaining that the directory
 is being rebuilt.
 
 Also changed all repair messages to start with lowercase letter and a
 verb ending in "ing" (e.g. "removing block reference").
 [Ted Anderson Fri Mar 12 13:42:00 1993]
 
 Subsequently imported into 3.7 ota-db3263-call-dirb_Annul-during-repair 1.2
 Entered as OT 
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/sal_errors.h, 
 file/episode/dir/dir_main.c, file/episode/dir/dir_salvage.c, 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3263-call-dirb_Annul-during-repair'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7874
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager handles zero pnhimark badly
Reported Date                 : 03/11/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/11/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/dir/dir_salvage.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3261-handle-zero-pnhimark
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 There are at least two problems with salvaging a directory that has a
 pnhimark of zero.  The first is that the code variable which is returned
 on exit is not initialized if the directory appears empty (as it does if
 the pnhimark is zero).  This causes the Enumerate routine in walk.c to
 think that it got an I/O error reading the old directory or writing the
 new directory.  So it panics.
 
 The other problem is that it should probably ignore the pnhimark during
 salvage and just use the file size.
 
 [Ted Anderson Thu Mar 11 11:48:20 1993]
 
 An assert I added recently that requires that either the directory be OK
 or the code be non-zero is inappropriate for verify-only operation.
 [Ted Anderson Thu Mar 11 14:08:06 1993]
**Solution Text**
 Delta: ota-db3261-handle-zero-pnhimark
 
 Initialize code in dir_salvage.
 
 Move bad assert in Enumerate routine in walk.c.
 
 Use container length instead of trusting pnhimark in directory being
 salvaged.  Note that since this change hides the uninitialized code
 error I tested that separately but there is no regression for that part
 of the fix.
 [Ted Anderson Thu Mar 11 14:08:20 1993]
 
 Imported into 3.6 ota-db3261-handle-zero-pnhimark 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/dir/dir_salvage.c, 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value `ota-db3261-handle-zero-pnhimark'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7873
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : BAKSERVER
Short Description             : bakserver should support setting debug levels on cmd line
Reported Date                 : 03/08/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bakserver/database.c, file/bakserver/db_alloc.c, file/bakserver/db_dump.c, file/bakserver/db_hash.c, file/bakserver/db_lock.c, file/bakserver/db_text.c, file/bakserver/dbs_dump.c, file/baks
erver/ol_verify.c, file/bakserver/procs.c, file/bakserver/server.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-db3231-bakserver-handle-debug-levels-interactively
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 Debug messages are printed to the log depending on the value in the debugging
 variable. At present. there is no way of setting this variable other than using
 inside a debugger. Ideally this variable should be set via the command line
 so that users could restart the bakserver with this cmd line option to see
 more detailed messages about what the bakserver is doing.
**Solution Text**
 -------------------------------------------------------------------------------
 
 bakserver now has a -verbose switch that prints limited verbosity on bakserver
 actions. More verbosity can be obtained by setting the variable "verbose" to
 levels > 1 and <= 10 using a debugger. The higher the value, more the output.
 ICL logging of backup will be done at a later date.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/bakserver/database.c, 
 file/bakserver/db_alloc.c, file/bakserver/db_dump.c, file/bakserver/db_hash.c, 
 file/bakserver/db_lock.c, file/bakserver/db_text.c, file/bakserver/dbs_dump.c, 
 file/bakserver/ol_verify.c, file/bakserver/procs.c, file/bakserver/server.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `vijay-db3231-bakserver-handle-debug-levels-interactively'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/01/93 public]
Closing and setting fixby back to 1.0.2a - status field inadvertently
left in fix state.

[12/17/93 public]
Closed.



CR Number                     : 7872
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage needs more checks for modifying backing filesets
Reported Date                 : 03/08/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/file.c, file/episode/anode/sal_errors.h, file/episode/salvage/paths.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3230-avoid-repairing-backing-fileset
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The salvager currently possesses a mechanism to handle the case where it
 want to repair a directory in a backing fileset.  We also need to use
 the mechanism in other cases where attempts are made to repair such a
 fileset.  In this case we are repairing a link count.  There are at
 least a few other cases like this.  Just mark the fileset inconsistent
 and do nothing.
 [Ted Anderson Mon Mar  8 14:22:24 1993]
**Solution Text**
 Delta: ota-db3230-avoid-repairing-backing-fileset
 
 Don't call fileset link count error repairs on backing filesets.
 Instead propagate the error up, and set fileset INCONSISTENT bit.
 
 We decide to go ahead and repair the ZLC even of a backing fileset.
 [Ted Anderson Mon Mar  8 16:15:14 1993]
 
 I also marked the corresponding OT as exported.  Please do not be
 confused by this.  It is the same delta in both cases.
 [Ted Anderson Tue Mar  9 09:37:37 1993]
 
 <restoring accidently trashed solution>
 [Ted Anderson Mon Mar 22 13:41:14 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/file.c, 
 file/episode/anode/sal_errors.h, file/episode/salvage/paths.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3230-avoid-repairing-backing-fileset'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7871
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager needs more index changes in AddCopiesVerify
Reported Date                 : 03/08/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/salvage.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3229-check-index-in-AddCopiesVerify
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Yet another case where a bogus anode causes some trouble in managing
 copies-map.  In this case, the backing anode, whose goodness we cannot
 readily verify, has an index of zero.  It could as easily have an index
 greater than lastIndex, which would be even harder to check.  Instead of
 panicing return an error which our caller can use to unclone the
 copy-on-write file.
 [Ted Anderson Mon Mar  8 13:42:46 1993]
**Solution Text**
 Delta: ota-db3229-check-index-in-AddCopiesVerify
 
 Don't panic on bad indexes.  Caller can handle errors more
 appropriately.
 [Ted Anderson Wed Mar 10 16:41:23 1993]
 
 Imported into 3.6 ota-db3229-check-index-in-AddCopiesVerify 1.1 
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/salvage.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3229-check-index-in-AddCopiesVerify'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7870
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : assert that delete and ZLC update are in same trans
Reported Date                 : 03/08/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/file.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3226-assert-delete-zlc-same-trans
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 There is some complex logic used in delete to manage transaction sizes
 better that can be used in some cases.  However, it is (or should be)
 disabled on paths where the ZLC list is updated to ensure that the
 file's deletion and removal from the ZLC are in the same transaction.
 Add an assertion that this is really working.
 [Ted Anderson Mon Mar  8 11:43:44 1993]
**Solution Text**
 Delta: ota-db3226-assert-delete-zlc-same-trans
 
 Assert that transaction used to remove file from the ZLC is the same
 transaction that comes back from deleting the file.
 [Ted Anderson Mon Mar  8 13:06:41 1993]
 
 Imported into 3.6 ota-db3226-assert-delete-zlc-same-trans 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/file.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3226-assert-delete-zlc-same-trans'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7869
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvage not skipping dead volumes in dirscan pass
Reported Date                 : 03/08/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/08/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/salvage.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3224-detect-deadanodes-as-deadVolumes
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 When the salvager trashes a volume it is supposed to remember not to try
 and use it later.  In particular, the dirscan pass should not try to
 open such a volume and try to traverse its directory structure.
 
 The epiz_ReportError maintains two lists, deadVolumes and deadFiles, to
 keep track of such beasts.  Somewhere along the line the marking of dead
 anodes as volume instead of the files broke, so the trashed volumes were
 put on the deadFiles list instead of the deadVolumes list.  This results
 in assertion failures in paths.c (and probably other places).
 [Ted Anderson Mon Mar  8 10:47:42 1993]
**Solution Text**
 Delta: ota-db3224-detect-deadanodes-as-deadVolumes
 
 Improve test for volume anodes versus regular file anodes in
 epiz_ReportError.
 [Ted Anderson Mon Mar  8 11:04:29 1993]
 
 Imported into 3.6 ota-db3224-detect-deadanodes-as-deadVolumes 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/salvage.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3224-detect-deadanodes-as-deadVolumes'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7868
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager chokes when copies field set on AVL
Reported Date                 : 03/05/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/fixed_anode.c, file/episode/anode/sal_errors.h, file/episode/anode/salvage.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3222-check-copies-of-anodes-in-AVL
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The copies verification handling doesn't work if the anode in question
 is the AVL.  But really we shouldn't allow and anode in the AVL to have
 a non-zero copies field.  This includes the AVL itself as well as the
 bitmap and all volume table anodes.
 [Ted Anderson Fri Mar  5 16:00:14 1993]
**Solution Text**
 Delta: ota-db3222-check-copies-of-anodes-in-AVL
 
 Check anodes in AVL for non-zero copies and COW bit.  Clear if found.
 [Ted Anderson Mon Mar  8 09:24:03 1993]
 
 Imported into 3.6  ota-db3222-check-copies-of-anodes-in-AVL 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/fixed_anode.c, 
 file/episode/anode/sal_errors.h, file/episode/anode/salvage.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3222-check-copies-of-anodes-in-AVL'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7867
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager ignores bad index in AVL anode
Reported Date                 : 03/05/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/sal_errors.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3221-check-AVL-anode-index
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The salvager calls epix_InitAggregate with only the superblock and avl
 offset.  It doesn't check the resulting anode's index early enough to
 avoid crashing while trying to decorate the special anodes.
 [Ted Anderson Fri Mar  5 15:07:57 1993]
**Solution Text**
 Delta: ota-db3221-check-AVL-anode-index
 
 Fail if avl.index doesn't match SB.reservedIndex.
 [Ted Anderson Fri Mar  5 15:23:04 1993]
 
 Imported into 3.6 ota-db3221-check-AVL-anode-index 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/sal_errors.h' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value `ota-db3221-check-AVL-anode-index'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7866
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : code uninitialized in epit_VerifyVolumeTable
Reported Date                 : 03/05/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/sal_errors.h, file/episode/anode/volume_table.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3220-VerifyVolumeTable-must-init-code
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 If a volume table is empty then epit_VerifyVolumeTable can return an
 uninitialized code.
 [Ted Anderson Fri Mar  5 11:49:32 1993]
 
 We need to detect and reject empty AVLs.
 [Ted Anderson Fri Mar  5 14:04:07 1993]
**Solution Text**
 Delta: ota-db3220-VerifyVolumeTable-must-init-code
 
 The problem is not so much that the error code is uninitialized as that
 the AVL is empty and so it goes around the per-page loop zero times.
 Reject zero-length volume tables.  This will take care of empty AVLs.
 [Ted Anderson Fri Mar  5 14:04:19 1993]
 
 Imported into 3.6 ota-db3220-VerifyVolumeTable-must-init-code 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/sal_errors.h, file/episode/anode/volume_table.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3220-VerifyVolumeTable-must-init-code'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7865
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager calls epiz_FindAnode with bogus index
Reported Date                 : 03/05/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/05/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3218-check-avl-indexes-before-lookup
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Inter-dependent defects: 3213
 
 Need to be more careful when calling epiz_FindAnode using the SB indexes
 to determine if any important containers were trashed.
 
 This is virtually the same precaution added to another part of the code
 under db3213.
 [Ted Anderson Fri Mar  5 10:43:47 1993]
**Solution Text**
 Delta: ota-db3218-check-avl-indexes-before-lookup
 
 Verify index is non-zero before calling epiz_FindAnode.
 [Ted Anderson Fri Mar  5 11:10:51 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `EXPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c' 
Changed Transarc Status from `EXPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3218-check-avl-indexes-before-lookup'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7864
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager segfaults if SB nBlocks too large
Reported Date                 : 03/04/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/block_alloc.c, file/episode/anode/sal_errors.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3215-check-SB-bitmap-fbsize-inconsistency
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 While some sanity checking of superblock nBlocks is done against the
 device's actual size the code sort of mindlessly calls osi_Alloc on the
 value in nBlocks.  This causes a segfault when osi_Alloc returns NULL.
 [Ted Anderson Thu Mar  4 16:00:32 1993]
**Solution Text**
 Delta: ota-db3216-alloc-bits-array-safely
 
 Add firstBlock testing to superblock verification and bullet-proof logic
 to bound firstBlock and nBlocks and their sum by the size of the actual
 device.  Use this realistic upperbound to allocate replacement bitmap.
 [Ted Anderson Fri Mar  5 10:27:36 1993]
 
 Imported into 3.6 ota-db3216-alloc-bits-array-safely 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/block_alloc.c, file/episode/anode/sal_errors.h' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3215-check-SB-bitmap-fbsize-inconsistency'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7863
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager should detect incorrect superblock frag size sooner
Reported Date                 : 03/04/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3215-check-SB-bitmap-fbsize-inconsistency
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Inspite of the fact that the bitmap can validate block and fragment size
 errors in the superblock the salvager seems to ignore such
 inconsistencies.  Instead it produces a cascade of the allocation size
 errors and then eventually the bitmap verifier panics.  Better to
 bailout with a helpful error immediately.
 [Ted Anderson Thu Mar  4 13:26:25 1993]
**Solution Text**
 Delta: ota-db3215-check-SB-bitmap-fbsize-inconsistency
 
 Check bitmap block/frag size ratio against values in async_device.  If
 they are inconsistent return NOTABITMAP from bitmap initialization
 routine.
 [Ted Anderson Thu Mar  4 14:40:53 1993]
 
 Imported into 3.6 ota-db3215-check-SB-bitmap-fbsize-inconsistency 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3215-check-SB-bitmap-fbsize-inconsistency'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7862
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager not rejecting too small fragment sizes
Reported Date                 : 03/04/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/04/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3214-reject-tiny-fragsizes
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Interested CC: jdp
 
 Amazingly the salvager is willing to accept a fragment size of 16 only
 because it is a power of two.  It clearly also needs to check for
 reasonable bounds (i.e. 1K<=fsize<=bsize).
 [Ted Anderson Thu Mar  4 09:06:48 1993]
**Solution Text**
 Delta: ota-db3214-reject-tiny-fragsizes
 
 Check fragment size early on and reject superblocks containing bogus
 values.  Simplify bad super block reporting.  Add more sanity checks to
 block and fragment sizes.
 [Ted Anderson Thu Mar  4 12:20:45 1993]
 
 ota-db3214-reject-tiny-fragsizes 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value `ota-db3214-reject-tiny-fragsizes'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7861
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : salvager panics labeling special AVL anodes
Reported Date                 : 03/03/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3213-check-avl-indexes-before-decorating
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The salvager tries to decorate the special AVL anodes with helpful
 names, but fails to check first that the indexes it is using are legal
 (non-zero).
 [Ted Anderson Wed Mar  3 15:58:33 1993]
**Solution Text**
 Delta: ota-db3213-check-avl-indexes-before-decorating
 
 Tested by simple_test which bashes these avl indexes.
 [Ted Anderson Wed Mar  3 16:27:29 1993]
 
 Imported into 3.6 ota-db3213-check-avl-indexes-before-decorating 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c' 
Changed Transarc Status from `IMPORT' to `import' 
Added field Transarc Deltas with value 
 `ota-db3213-check-avl-indexes-before-decorating'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7860
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : epiv_CreateAnode does not clear handle on error
Reported Date                 : 03/03/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/03/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/volume.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3212-zero-handle-on-error
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The volops routine efsSomeClone depends on the handle returned by
 epiv_Createanode being zero on error exit.  This isn't an advertised
 feature but is probably a good thing to take care of.  The code in
 dfs-102-2.6 is panics due to this on quota errors.  The code in 2.7,
 specifically the changes made in delta
 "ota-ot7041-clone-problems-when-over-quota" remove this problem.  Yet
 the underlying risk remains.  This should be fixed.
 
 Interested CC: bwl, cfe, jdp
 [Ted Anderson Wed Mar  3 15:35:28 1993]
 [Ted Anderson Thu Mar  4 08:19:58 1993]
**Solution Text**
 Delta: ota-db3212-zero-handle-on-error
 
 Initialize output parameter to zero on entry and only assign the anode
 on sucessful completion.
 [Ted Anderson Thu Mar  4 08:20:06 1993]
 
 Imported into 3.6 ota-db3212-zero-handle-on-error 1.1
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/volume.c' 
Changed Transarc Herder from `andi@transarc.com' to 
 `andi@transarc.comjaffe@transarc.com' 
Changed Transarc Status from `IMPORT' to `IMPORTimport' 
Added field Transarc Deltas with value `ota-db3212-zero-handle-on-error'

[5/4/93 public]
Changed Transarc Herder from `andi@transarc.comjaffe@transarc.com' to 
 `andi@transarc.com' 
Changed Transarc Status from `IMPORTimport' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7859
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : Alternate superblock handling still no good
Reported Date                 : 03/03/93
Found in Baseline             : 1.0.2a
Found Date                    : 03/03/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/aggregate.c, file/episode/anode/sal_errors.h, file/episode/salvage/salvage_main.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3208-detect-bad-superblock
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 The handling of alternate superblocks is still not adequate.  The
 salvager will use an alternate if the primary is trashed which results
 in missing the bulk of the damage.  There is some code that notices but
 the resulting exit code it lost.  At least this latter effect needs to
 be addressed.
 
 See also OT 3666 (and delta ota-ot3666-decline-alternate-superblock).
 [Ted Anderson Wed Mar  3 11:28:07 1993]
 
 Fixed in delta ota-db3208-detect-bad-superblock.  This is verifies my
 much more reliable exit code behavior as reported by the scavenge
 package's simple_test.
 [Ted Anderson Wed Mar  3 14:44:40 1993]
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/aggregate.c, 
 file/episode/anode/sal_errors.h, file/episode/salvage/salvage_main.c' 
Changed Transarc Herder from `andi@transarc.com' to 
 `andi@transarc.comjaffe@transarc.com' 
Changed Transarc Status from `IMPORT' to `IMPORTimport' 
Added field Transarc Deltas with value `ota-db3208-detect-bad-superblock'

[5/4/93 public]
Changed Transarc Herder from `andi@transarc.comjaffe@transarc.com' to 
 `andi@transarc.com' 
Changed Transarc Status from `IMPORTimport' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7858
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : test_anode needs better testing of clone volops
Reported Date                 : 03/20/92
Found in Baseline             : 1.0.2a
Found Date                    : 03/20/92
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/basic.test, file/episode/anode/test_anode.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3204-volume-qualify-fids
Transarc Herder               : andi@transarc.com

[5/3/93 public]
**Description Text**
 Augment test_anode.c so that fid naming can be qualified by volume
 name/id.  Otherwise it won't allow the obvious clone operation which is
 create an anode in one volume whose fid is the same as that of an anode
 in another volume.
 
 The need for this is driven by investigation of OT 7353, which reports
 an assertion failure during anomolous clone operations.
 
 ota 930302:
 
 Fixed in delta ota-db3204-volume-qualify-fids.
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `IMPORT'

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/basic.test, 
 file/episode/anode/test_anode.c' 
Changed Transarc Herder from `andi@transarc.com' to 
 `andi@transarc.comjaffe@transarc.com' 
Changed Transarc Status from `IMPORT' to `IMPORTimport' 
Added field Transarc Deltas with value `ota-db3204-volume-qualify-fids'

[5/4/93 public]
Changed Transarc Herder from `andi@transarc.comjaffe@transarc.com' to 
 `andi@transarc.com' 
Changed Transarc Status from `IMPORTimport' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7856
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : FILE EXPORTER
Short Description             : file exporter makes an unnecessary RPC to FLDB on delete
Reported Date                 : 02/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/23/93
Severity                      : C
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/px/px_remove.c, file/px/px_repops.c, file/px/px_vlutils.c, file/userInt/fts/volc.h, file/userInt/fts/volc_tokens.c, file/userInt/fts/volc_vldbsubr.c, file/userInt/fts/volc_volsint.c, file/x
volume/volume.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-db3190-remove-unnecessary-fldb-call-from-remove
Transarc Herder               : jaffe@transarc.com

[5/3/93 public]
**Solution Text**
 Delta:
 
 comer-db3190-remove-unnecessary-fldb-call-from-remove 1.1
 
 was imported into build dfs-103-3.6
**Validation Text**

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/px/px_remove.c, file/px/px_repops.c, 
 file/px/px_vlutils.c, file/userInt/fts/volc.h, file/userInt/fts/volc_tokens.c, 
 file/userInt/fts/volc_vldbsubr.c, file/userInt/fts/volc_volsint.c, 
 file/xvolume/volume.h' 
Added field Transarc Deltas with value 
 `comer-db3190-remove-unnecessary-fldb-call-from-remove' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7855
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : UBIK
Short Description             : udebug prints host addresses in the wrong byte order
Reported Date                 : 02/15/93
Found in Baseline             : 1.0.2a
Found Date                    : 02/15/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ncsubik/udebug.c, file/ncsubik/vote.c, test/file/ubik/udebug.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-db3165-udebug-prints-host-addresses-wrong-byte-order
Transarc Herder               : jaffe@transarc.com

[5/3/93 public]
**Description Text**
 udebug prints the host addresses in the wrong byte order. A htonl is missing
 in udebug.c
 -------------------------------------------------------------------------------
**Solution Text**
 -------------------------------------------------------------------------------
 
 When the IP addresses were passed back from the UBIKVOTE_Debug and 
 UBIKVOTE_SDebug RPCs they were passed back in network order. This
 causes lots of problems when making the RPC from RIOS to PMAX. This
 fix does ntohl on the server to pass addresses in host order and
 does htonl in the client before assigning it to the field in the
 sockaddr, or before passing it to inet_ntoa. I have tested udebug
 on local and remote paths between rios and pmax. 
 
 Ready to be exported.
 
 Imported into 3.6 vijay-db3165-udebug-prints-host-addresses-wrong-byte-order 1.2

[5/4/93 public]
imported into dfs-carl 1.8

[5/4/93 public]
Filled in Affected File with `file/ncsubik/udebug.c, file/ncsubik/vote.c, 
 test/file/ubik/udebug.c' 
Added field Transarc Deltas with value 
 `vijay-db3165-udebug-prints-host-addresses-wrong-byte-order' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7854
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : sysv locking allows slice and dice of file.
Reported Date                 : 5/3/93
Found in Baseline             : 1.0.2
Found Date                    : 5/3/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : lots
Sensitivity                   : public
Transarc Deltas               : kazar-fix-sysv-lock-problem
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[5/3/93 public]
get sys v lock retried when releasing a lock that might allow a queued
lock to proceed with slice and dice
imported in dfs-carl 1.8
Added field Transarc Deltas with value `kazar-fix-sysv-lock-problem' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `import'

[5/11/93 public]
Filled in Subcomponent Name with `cm' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7852
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : tkm can generate read reference to freed storage
Reported Date                 : 5/3/93
Found in Baseline             : 1.0.2
Found Date                    : 5/3/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot7852-fix-tkm-fileid-reference
Transarc Herder               : jaffe@transarc.com

[5/3/93 public]
The token manager can occasionally generate a reference to free storage when 
a normal token grant request is queued behind a whole fileset request.
The temporary fix is to copy the file ID into the token structure, instead of just
copying in the reference.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot7852-fix-tkm-fileid-reference' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7846
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : bak ftinfo doesn't display clone date correctly
Reported Date                 : 4/29/93
Found in Baseline             : 1.0.2
Found Date                    : 4/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-sb3296-date-specific-restores-dont-work
Transarc Herder               : jaffe@transarc.com

[4/29/93 public]
The clone date never appears to be correct when dumping filesets.  I modified
root.dfs then did an fts clone on it and dumped the fileset to tape.  Doing
a bak ftinfo on the fileset shows the following:
    DumpID   parentID lvl creation date   clone date     tape name
 736131557          0 0  04/29/93 19:59  12/31/69 18:00 Flintstone.incr.1
 
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[4/29/93 public]
Changed Short Description from `bak ftinfo does display clone date correctly' 
 to `bak ftinfo doesn't display clone date correctly' 
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `' 
Added field #Transarc Status with value `'

[5/7/93 public]
This is a duplicate of a part of Sybase bug 3296, which has been
fixed. 
Changed Status from `open' to `cancel' 
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `'

[5/7/93 public]
We've got a fix, we'll put it in a build soon.
Changed Status from `cancel' to `open' 
Filled in Transarc Deltas with `khale-sb3296-date-specific-restores-dont-work' 
Filled in Transarc Status with `export' 
Added field Transarc Herder with value `jaffe@transarc.com'

[12/17/93 public]
Closed.



CR Number                     : 7827
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7929
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : fldb_InitUbikHandle sometimes fails
Reported Date                 : 4/29/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/29/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : not_available
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/29/93 public]
I have seen this failure coming from fts commands and from fts tests runs.
This does not happen every time that I run the tests though:
fldb_InitUbikHandle: ubik client init for /.../danger/fs failed (auth ticket expired (dce / rpc)).
The latest occurrence of this was while I was running fts clonesys in a loop 
overnight.  This occurs even when the tickets for the users running the tests 
are set to expire after 2 or 3 days, and during cho runs I have seen this message
occur about 3 times during the fts test runs (and well before the ticket expiration
time of the user running the tests). 
There does not seem to be anything of interest in the FlLog which would indicate
what might be happening here.

[4/30/93 public]
Filled in Interest List CC with `pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/30/93 public]
It would be useful to get a few pieces of information about the setup.
1. What is the default lifetime for tickets in the cell
2. What is the max lifetime for tickets in the cell
( You can get the above two from rgy_edit )
3. What is the cell setup. Is the fts command running on a machine that is
   different from where the filesets reside. 
4. If you say yes to question 3, is there a clock skew between those machines.
   fts clonesys is known to slow down the clock on the server.
5. Is dts running and properly configured? This should keep the clocks in sync.
6. What other activities (tests) are you runnning on the machine and 
   aggregate(s) where clonesys is happenning.
Mike Comer thinks that clock skew between the client and server could cause the
client to use credentials with expired tickets to go through to the server.

[05/10/93 public]
Just noting that this still happens.  This is the regular cho 
setup without the ACL tests.  It runs fts short,  connectathon
and and low in a 4 machine setup (Ask Elliott for more details).
----------------------
Total FLDB entries that were successfully enumerated: 83 (0 failed)
Passed
Running fts delfldbentry
fts: Missing required parameter '-fileset'
Passed
fldb_InitUbikHandle: ubik client init for /.../leprosy/fs failed (auth ticket expi
red (dce / rpc)).
Passed
fts delfldbentry: neither -server nor -aggregate may be used with -fileset.
Passed
Could not delete FLDB entry (vol= 0,,1250, type=-1)
Error: communications failure (dce / rpc)
test4: Test case, E64: Failed
----------------------
Total FLDB entries that were successfully deleted: 5 (0 failed)

[5/10/93 public]
This could be a few things.  If it happens again, please do the
following:
1) check the times on all machines.  A clock skew of over 5 minutes
will cause this problem.
2) check the dfs-server tickets in the caller's cache to see when they
expired.  You can do this 'klist -e'.  Also not the time the failure
occurred.  Did a server ticket expire coincident with the outage?
There is a bug in the server ticket refresh code where a ticket may be
returned but that ticket expires before it is used.  There is an open
OT on this: 7929.
These are the only two places you should see auth_tkt_expired if your
TGT is really valid.

[6/8/93 public]
Changed Interest List CC from `pakhtar@transarc.com' to `pakhtar@transarc.com, 
 dstokes@transarc.com'

[6/8/93 public]
This is believed to be fixed by the fix for 7929.  OSF will verify.

[06/18/93 public]

We have now not seen this during during cho since the fix for 7929.



CR Number                     : 7820
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cache mgr
Short Description             : cm prints spurious messages when there are no tickets
Reported Date                 : 4/28/93
Found in Baseline             : 1.0.2
Found Date                    : 4/28/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot7820-zap-spurious-noauth-warning
Transarc Herder               : jaffe@transarc.com

[4/28/93 public]
cm prints spurious messages when there are no tickets, but it should only 
do this when we're getting an *unexpected* error.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot7820-zap-spurious-noauth-warning' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7818
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : some directores which are listed via 'ls' are non-existent
Reported Date                 : 4/28/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/28/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[4/28/93 public]
I've seen this once about a month ago, as well as today.  Basically
the problem is that we have a directory which shows up via 'ls' and
we are allowed to cd to it; when we do that and then do an ls from 
that directory we get: the file '.' does not exist.  The aggregate
on which the fileset resides was salvaged prior to exporting, and
there was a time when the directory in question behaved normally.
# cd /.:/fs/m3.lfs1/test
# cd cthon
# ls
ls: 0653-341 The file . does not exist.
#  cd /.:/fs/m3.lfs1/test
# ls -l 
total 1
drwxr-xr-x   4 guest    staff        320 Apr 23 12:17 cthon
time 301.866219, pid 20999: end AFS_GetTime, code 0 
time 301.866282, pid 20999: cm_Analyze: conn 594fa00, code 0, user 0 
time 301.866718, pid 28469: Ping Servers done at time 2bde952d 
time 301.954956, pid 19815: cm_lookup 5ee411c cthon 
time 301.955124, pid 19815: gettokens vp 5ee411c, rights.low 0x404 
time 301.955256, pid 19815: nh_dolookup dvp 5ee411c, name cthon 
time 301.955343, pid 19815: gettokens vp 5ee411c, rights.low 0x404 
time 301.955559, pid 19815: cm_GetScache vp 5ee3ff8, volume.low 0x1e7, vnode 0x2 
time 301.955668, pid 19815: found fid 6c5545e1.1e7.2.1 (hex) 
time 301.955727, pid 19815: lookup returning vp 5ee3ff8 
time 301.955776, pid 19815: checkerror returning code 0 
time 301.955942, pid 19815: in cm_access, vp 5ee3ff8, mode 0x40 
time 301.956055, pid 19815: gettokens vp 5ee3ff8, rights.low 0x400 
time 303.675386, pid 23702: cm_lookup 5ee3ff8 . 
time 303.675535, pid 23702: gettokens vp 5ee3ff8, rights.low 0x404 
time 303.675667, pid 23702: nh_dolookup dvp 5ee3ff8, name . 
time 303.675746, pid 23702: gettokens vp 5ee3ff8, rights.low 0x404 
time 303.675891, pid 23702: checkerror returning code 2 
I don't really know how to reproduce this yet.

[04/28/93 public]
Just in case this is relevant:
# acl_edit /.../danger/fs/m3.lfs1/test/cthon
sec_acl_edit> l
# SEC_ACL for /.../danger/fs/m3.lfs1/test/cthon:
# Default cell = /.../danger
user_obj:rwxcid
group_obj:r-x---
other_obj:r-x---
sec_acl_edit> 
drwxr-xr-x   4 guest    staff        320 Apr 23 12:17 cthon
The owner of this file is cell_admin (100)
# fgrep guest /etc/passwd
guest:!:100:100:/usr/guest:/usr/guest:

[5/19/93 public]
Changed Responsible Engr. from `pakhtar@transarc.com' to `kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/19/93 public]
The trace shows that we got an ENOENT from the server for this file.  There were quite
a number of fixes for problems like this made in the 103 code base, and which are
also present in IBM's product.
I'm thus going to recommend that we close this bug and reopen it if we see it again in
the 1.0.3 tree.
Filled in Transarc Status with `closed'

[8/31/93 public]
Diane, 
Per our telephone conversation, this is one of OTs that either is fixed or
could not be reproduced. Reassigned it to you.
Filled in Interest List CC with `tu' 
Changed Responsible Engr. from `kazar@transarc.com' to `delgado@osf.org' 
Filled in Transarc Herder with `jaffe@transarc.com'

[10/25/93 public]
for tracking purposes, I'm marking this as import.  The 1.0.3a Transarc
code base does not seem to have this problem.
Changed Transarc Status from `closed' to `import'

[12/17/93 public]
Closed.



CR Number                     : 7815
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : correct mountpoint .. semantics
Reported Date                 : 4/28/93
Found in Baseline             : 1.0.1
Found Date                    : 4/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7815-correct-mtpt-dotdot-semantics
Transarc Herder               : jaffe@transarc.com

[4/28/93 public]
The semantics for determining .. when crossing a DFS mountpoint durring
lookup are not working as intended.  The intended semantic is that when
lookup crosses a mountpoint, .. for the mountpoint is re-evaluated
so that a getcwd() call will return the absolute path that represents
the most recent cd.  Currenly DFS sticks with the .. established by the 
first lookup that caused evaluation of the mountpoint.
The following test can be used to illustrate:
	fts crmount -dir /:/usr/test -fileset usr.test
	cd /:/usr/test
	getdir
	fts crmount -dir /:/bin/test -fileset usr.test
	cd /:/usr/test
	getdir
	cd /:/bin/test
	getdir
getdir is a simple program which calls the getcwd() call.
In the current code that last call to getdir would return /.../cell/fs/usr/test
when it should return /.../bin/test
Here is the source for the getdir program:
#include <stdio.h>
#include <unistd.h>
#include <sys/errno.h>
main()
{
        char str[256];
        int size=255;
        getcwd(str, size);
        if (str[0] == '\0') {
                printf("getcwd failed %d\n", errno);
        } else {
                printf("curr dir is <%s>\n", str);
        }
        exit(0);
}
I have discusses and validated the changes with Transarc.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/28/93 public]
Defect Closure Form
-------------------
The above test now passes.  I also ran connectathon as
a regression check.
Filled in Transarc Deltas with `cburnett-ot7815-correct-mtpt-dotdot-semantics' 
Changed Transarc Status from `open' to `export'

[10/8/93 public]

Aged relative to Aug 1.

[11/8/93 public]
Changing responsible engineer to Transarc since the fix is in their
hands now.  We expect to see this in 1.0.3a since it is an aged bug.

[12/17/93 public]
Closed.



CR Number                     : 7814
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : dfsbind has slow leak
Reported Date                 : 4/27/93
Found in Baseline             : 1.0.2
Found Date                    : 4/27/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7814-stop-up-dfsbind-leak
Transarc Herder               : jaffe@transarc.com

[4/27/93 public]
dfsbind is leaking memory.  This leakage doesn't seem to show up
until after considerable runtime.  On dce.14e, I have one client
only machine (but core component server) that has been up 16 days.
Here is the ps output:
 [dstokes@sunlight]/u/dstokes:495> ps v 14429 
   PID    TTY STAT  TIME PGIN  SIZE   RSS   LIM  TSIZ   TRS %CPU %MEM COMMAND
 14429      - S     4:44 4538  1288    92 32768   267     0  0.0  0.0 dfsbind 
On blizzard, (up for 13 days),
# ps v 17208
   PID    TTY STAT  TIME PGIN  SIZE   RSS   LIM  TSIZ   TRS %CPU %MEM COMMAND
 17208      - S     0:26  325  1172  1112 32768   267   124  0.0  2.0 /usr/lpp/
On dfswitch where I have a ps cron job running, it appears that
dfsbind starts off at about 748K, and after 6 hours grows a very
small amount (4K), then after approximately 10 hours, grows 128K.
Then in the next half hour after the 128K growth, it grows another
368K.  This sounds suspiciously like the default credential time.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/93 public]
Defect Closure Form
-------------------
My conjecture here is that ICL is causing the growth in process size.
Dfsbind allocates 2 60kword ICL logs but the associated pages
apparently do not show up in the process size until they are touched.
This gives the impression that dfsbind is leaking core when it is
really just writing new pages of log.  There are still small leaks in
dfsbind and the libraries it calls but these are small enough that
they are not worth tracking down.  For example, each time the kernel
looks up the dfs junction, which happens infrequently, 48 bytyes is
leaked.  There is also a large amount of caching by the security code
that looks like a leak but that may not be.  In any case, things seem
to stabilize. 
The delta associated with this defect closes up several small holes
and closes security contexts.
Regression test
---------------
Ran dfsbind successfully under the dfs-103-3.25 build.  The changes
were inspected, as well.
Filled in Transarc Deltas with `comer-ot7814-stop-up-dfsbind-leak' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7813
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Stale host errors due to 10-minutes gaps between server pings
Reported Date                 : 4/27/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/27/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : osi/OSF1/osf_net_osf.c, osi.klib/Makefile, osi.klib/OSF1/Makefile, osi.klib/RIOS/Makefile
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/27/93 public]
The following dfstrace dump show a "stale host" error and a 10-minutes 
gap between two "running Ping Servers" for the DFS client. 
This is one of the many faiure cases we have seen in these two days.
One phenomenon is that if it happens, it is likely to happen again.
Performance implication is obvious as the DFS client has to do token
recovery every time when there is a stale error. And, we don't know if there 
is any data integrity implication due to this bug.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Platform : PMAX/osc1.1.1
DCE build : last weekend
DCE config : santa --- security server, cds server and dfs server
	     jolt ---- DCE clients
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
time 260.873340, pid 0: running Ping Servers
time 260.873340, pid 0: Ping Servers queuing at time 2bdc3904
time 260.877246, pid 0: Refresh Tokens at time 2bdc3904, opcount 1
time 260.877246, pid 0: flushactivescaches starting 1 concurrent storebacks
time 260.877246, pid 0: servertokenmgt running 2 subops
time 260.877246, pid 0: pingserver server c29a4200
time 260.877246, pid 0: cm_ConnByHosts server type 0x200, connp c297a2e0
time 260.877246, pid 0: cm_ConnByHost using conn c2b28c40, service 0x20000
time 260.877246, pid 0: begin AFS_GetTime
time 260.881152, pid 0: start UpdateTokensLifetime
time 260.881152, pid 0: in cm_FlushQueuedServerTokens for server c29a4200
time 260.885058, pid 0: cm_ConnByHosts server type 0x200, connp 0
time 260.885058, pid 0: cm_ConnByHost using conn c2b28c40, service 0x20000
time 260.885058, pid 0: begin AFS_ReleaseTokens for 1 tokens
time 260.885058, pid 0: running FlushActiveSCaches
time 260.885058, pid 0: SyncSCache vp c3ecc81c
time 260.885058, pid 0: scanstatus vp c3ecc81c, flags 0x10
time 260.885058, pid 0: cm_ConnByMHosts server type 20001
time 260.885058, pid 0: cm_ConnByHosts server type 0x200, connp 0
time 260.885058, pid 0: cm_ConnByHost using conn c2b28cc0, service 0x20001
time 260.885058, pid 0: begin AFS_StoreStatus
time 260.888964, pid 0: Refresh Tokens done at time 2bdc3904
time 260.888964, pid 0: running write through dslots
time 260.896776, pid 0: end AFS_GetTime, code 0
time 260.896776, pid 0: cm_Analyze: conn c2b28c40, code 0, user 0
time 260.896776, pid 0: in cm_FlushQueuedServerTokens for server c29d1900
time 260.900682, pid 0: Ping Servers done at time 2bdc3904
time 260.900682, pid 0: running Check Down Servers
time 260.900682, pid 0: running major renewlazyreps
time 260.900682, pid 0: Renewlazyreps starts 0 subjobs
time 260.904588, pid 0: end AFS_ReleaseTokens, code 0
time 260.904588, pid 0: cm_Analyze: conn c2b28c40, code 0, user 0
time 260.943648, pid 0: end AFS_StoreStatus, code 0
time 260.943648, pid 0: cm_Analyze: conn c2b28cc0, code 0, user 0
time 260.943648, pid 0: mergestatus vpc3ecc81c, tokenp 0, flags 0x1
time 260.943648, pid 0: mergestatus ctime OK for merging, local mod flags 0x0
time 260.943648, pid 0: mergestatus add pag 0 rights 0x5
time 260.943648, pid 0: addacl vp c3ecc81c pag 0 rights 0x5
time 260.943648, pid 0: get TGT time 0 for pag 0
time 260.943648, pid 0: checkerror returning code 0
time 260.943648, pid 0: checkerror returning code 0
time 860.991996, pid 0: running Ping Servers
time 860.991996, pid 0: Ping Servers queuing at time 2bdc3b5c
time 860.991996, pid 0: Refresh Tokens at time 2bdc3b5c, opcount 1
time 860.991996, pid 0: flushactivescaches starting 0 concurrent storebacks
time 860.991996, pid 0: running write through dslots
time 860.999808, pid 0: servertokenmgt running 2 subops
time 860.999808, pid 0: pingserver server c29a4200
time 860.999808, pid 0: cm_ConnByHosts server type 0x200, connp c297a030
time 860.999808, pid 0: cm_ConnByHost using conn c2b28c40, service 0x20000
time 860.999808, pid 0: begin AFS_GetTime
time 861.003714, pid 0: start UpdateTokensLifetime
time 861.007620, pid 0: in cm_FlushQueuedServerTokens for server c29d1900
time 861.007620, pid 0: in cm_FlushQueuedServerTokens for server c29a4200
time 861.007620, pid 0: Refresh Tokens done at time 2bdc3b5d
time 861.007620, pid 0: running Check Down Servers
time 861.007620, pid 0: running major renewlazyreps
time 861.007620, pid 0: Renewlazyreps starts 0 subjobs
time 861.007620, pid 0: GC conns 0x0
time 861.007620, pid 0: stomping binding c2b28cc0
time 861.015432, pid 0: end AFS_GetTime, code 572616705
time 861.015432, pid 0: cm_Analyze: conn c2b28c40, code 572616705, user 0
time 861.015432, pid 0: cm_Analyze: volerr subcode 735853404
time 861.015432, pid 0: cm_Analyze: host stale
time 861.015432, pid 0: cm_ConnByHosts server type 0x200, connp 22217001
time 861.015432, pid 0: cm_ConnByHost using conn c2b28c40, service 0x20000
time 861.015432, pid 0: stomping binding c2b28c40
time 861.019338, pid 0: start AFS_SetContext conn c2b28c40 srv level 0x20000
time 861.066210, pid 0: TKN_InitState called
time 861.066210, pid 0: TKN_InitState's server is c29a4200
time 861.066210, pid 0: TKN_InitState start TSR

[4/27/93 public]
Looks like a tpq problem from here.  Hope it's simple.
Changed Interest List CC from `dfs-team, hathaway' to `dfs-team, hathaway, cfe, 
 tu' 
Changed Responsible Engr. from `tu' to `comer' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/27/93 public]
It might be.  I don't know what it could be, though.  I've added some
ICL tracing to the TPQ package that would tell us for sure.  We'd then
have to reproduce the problem.  Can you guys get this to happen with
any regularity?

[4/28/93 public]
I don't know how to re-produce it. But, it has happened so many times.
If you run "dfstrace dump -follow cmfx -sleep 5 > dfstrace.log" and see
the 10 minutes-gap between two server pings, then there is a chance to get the 
stale host error when you run some DFS tests . 
Please send me, Diane or Ron your new instrumentation for monitoring the TPQ
after you have tested it out.

[4/29/93 public]
Thanks to Jean, I've got some trace data to look at!  It points the
finger at osi_Wait().  Everything works fine as long as the the tpq
dispatcher thread isn't being awakened prematurely.  When it is
allowed to sleep without interruption, it isn't sleeping for the right
amount of time.  I have a trace message before the call to sleep that
gives the amount of time it intends to sleep and a trace message when
it wakes up.  So, I have a trace the says the dispatcher is sleeping
for 30 seconds and a trace that says the dispatcher is awake, and the
messages are 10 minutes apart.
Could someone at OSF take a look at the OSF implementation of
osi_Wait() and make sure it does this right thing?

[4/29/93 public]
Mike, 
	From the following trace segment I found that c2b1ddc0 was be 
	waken up at 30 seconds later, but it woke up abot 10 minutes later.
	Do you have any idea about this? Have I mis-interpreted the trace?
	I have many similar trace segments like this.
Jean
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
time 266.406566, pid 0: tpq_DispatcherThread(c2b1ddc0) awake!!
time 266.406566, pid 0: tpq_FindQueue(pool=c2b1ddc0, priority=0, now=736090378, ...)
time 266.406566, pid 0: (tpq) GCQueue(pool=c2b1ddc0, priority=0, now=736090378)
time 266.406566, pid 0: tpq_FindQueue(pool=c2b1ddc0,...) returns nextExpired=90, graceExpired=0, entry=0
time 266.406566, pid 0: tpq_FindQueue(pool=c2b1ddc0, priority=1, now=736090378, ...)
time 266.406566, pid 0: (tpq) GCQueue(pool=c2b1ddc0, priority=1, now=736090378)
time 266.406566, pid 0: tpq_FindQueue(pool=c2b1ddc0,...) returns nextExpired=30, graceExpired=0, entry=0
time 266.406566, pid 0: tpq_FindQueue(pool=c2b1ddc0, priority=2, now=736090378, ...)
time 266.406566, pid 0: (tpq) GCQueue(pool=c2b1ddc0, priority=2, now=736090378)
time 266.406566, pid 0: tpq_FindQueue(pool=c2b1ddc0,...) returns nextExpired=60, graceExpired=0, entry=0
time 266.406566, pid 0: tpq_GCThreads(c2b1ddc0) called
time 266.406566, pid 0: tpq_DispatcherThread(c2b1ddc0) sleep with interval=30
time 866.417826, pid 0: tpq_DispatcherThread(c2b1dd80) awake!!
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1dd80, priority=0, now=736090978, ...)
time 866.417826, pid 0: (tpq) GCQueue(pool=c2b1dd80, priority=0, now=736090978)
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1dd80,...) returns nextExpired=600, graceExpired=0, entry=0
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1dd80, priority=1, now=736090978, ...)
time 866.417826, pid 0: (tpq) GCQueue(pool=c2b1dd80, priority=1, now=736090978)
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1dd80,...) returns nextExpired=600, graceExpired=0, entry=0
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1dd80, priority=2, now=736090978, ...)
time 866.417826, pid 0: (tpq) GCQueue(pool=c2b1dd80, priority=2, now=736090978)
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1dd80,...) returns nextExpired=600, graceExpired=0, entry=0
time 866.417826, pid 0: tpq_GCThreads(c2b1dd80) called
time 866.417826, pid 0: tpq_DispatcherThread(c2b1dd80) sleep with interval=600
time 866.417826, pid 0: tpq_DispatcherThread(c2b1ddc0) awake!!
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1ddc0, priority=0, now=736090978, ...)
time 866.417826, pid 0: (tpq) GCQueue(pool=c2b1ddc0, priority=0, now=736090978)
time 866.417826, pid 0: tpq_FindQueue(pool=c2b1ddc0,...) returns nextExpired=600, graceExpired=1, entry=c2b1dd40
time 866.417826, pid 0: tpq_DispatcherThread(c2b1ddc0) ready to run c2b1dd40, priority=0, graceExpired=1
time 866.417826, pid 0: tpq_DispatcherThread(c2b1ddc0) has thread to run c2b1dd40, thread=c2b30b80
time 866.417826, pid 0: tpq_DispatcherThread(c2b1ddc0) sleep with interval=0
time 866.417826, pid 0: tpq_DispatcherThread(c2b1ddc0) awake!!

[4/29/93 public]
Looking at the osi code, I have a guess as to why this ius happening.
There are two dispatcher threads running since there is a standard
thread pool and an aux thread pool.  In the trace, they both show up
as pid 0.  Does this mean that they are sharing a u.u_procp as well?
If so, the two dispatchers are both passing the same 2 arguments to
timeout (function and u.u_procp) and apparently the auxillary thread
pool is winning (since it doesn't have anything to run and therefore
WANTS to wait for 10 minutes).
So, the problem does, indeed, seem to be with osi_Wait().  Passing
some thread-unique value to timeout should be sufficient to solve this
problem, though I have no idea what this value might be.
How about if someone at OSF takes this?

[04/30/93 public]

We are taking a look at it right now.

[5/3/93 public]

We are going to modify the osi_wait routine for OSF1 to use
tsleep (which does a sleep with a timeout) and to use a disticnt
wait channel  (as opposed to the shared waitV structure declared 
in osi_net.c).

[05/12/93 public]


fix submitted.

[12/17/93 public]
Closed.



CR Number                     : 7804
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : repserver gets quota errors even
when there is space available
Reported Date                 : 4/26/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/26/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/rep/rep_main.c
Sensitivity                   : public
Transarc Deltas               : cfe-ot7804-big-quotas-for-repserver-too
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/26/93 public]
I set up scheduled replication for a fileset which succeeded several times
and thereafter failed with Disc quota exceeded messaged.  The fts commands
show that there is clearly space available.
The master is living on alcatraz (rios) and the read only lives on
fire (pmax).
m1.lfs1  
        readWrite   ID 0,,484  valid
        readOnly    ID 0,,485  valid
        backup      ID 0,,486  invalid
number of sites: 2
  Sched repl: maxAge=0:03:00; failAge=1d0:00:00; reclaimWait=1:30:00; minRepDelay=0:00:11; defaultSiteAge=0:00:45
   server           flags     aggr   siteAge principal      owner               
alcatraz.osf.org    RW       m1.aggr1 0:00:00 hosts/alcatraz <nil>               
fire.osf.org        RO       m3.aggr1 0:02:00 hosts/fire     <nil>               
Here's the RepLog for Fire.
 
93-Apr-26 12:26:40 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 12:28:01 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 12:29:33 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 12:30:50 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 12:32:10 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 12:33:30 0,,485: can't clone 0 to 1: Disc quota exceeded
root@fire  # efts lsft /dev/rz1d
NAME                       ID          
m3.lfs1                   0,,487       
m1.lfs1.readonly          0,,485       
root@fire  # cd /.:/fs/m3.lfs1
root@fire  # fts lsquota .
Fileset Name          Quota    Used  % Used   Aggregate
m3.lfs1                5000    2774    55%     6% = 12119/194672 (LFS)
root@fire  # fts lsquota /.:/fs/m1.lfs1.readonly
Fileset Name          Quota    Used  % Used   Aggregate
m1.lfs1.readonly       5000    2864    57%     6% = 12119/194672 (LFS)
root@fire  # fts aggrinfo -server /.:/hosts/fire -aggr m3.aggr1
LFS aggregate m3.aggr1 (/dev/rz1d): 182553 K free out of total 194672
root@fire  # 
# cd /.:/fs/m1.lfs1
# fts lsq .
Fileset Name          Quota    Used  % Used   Aggregate
m1.lfs1                5000    2882    57%     6% = 3984/65528 (LFS)
# fts aggrinfo -server /.:/hosts/alcatraz -aggr m1.aggr1
LFS aggregate m1.aggr1 (/dev/lv02): 61544 K free out of total 65528
I increased the  quota both on the master and on the read-only but
this did not fix the problem:
root@fire  # fts setquota -file m1.lfs1 -size 8000
root@fire  # fts setquota -file 0,,485 -size 8000
root@fire  # fts lsquota -file m1.lfs1
Fileset Name          Quota    Used  % Used   Aggregate
m1.lfs1                8000    2767    34%     5% = 3869/65528 (LFS)
root@fire  # fts lsquota /.:/fs/m1.lfs1.readonly
Fileset Name          Quota    Used  % Used   Aggregate
m1.lfs1.readonly       8000    2864    35%     6% = 12119/194672 (LFS)
root@fire  # 
Sync'ing the aggregates did not change anything either.
# fts lsr -file m1.lfs1 -server /.:/hosts/fire
m1.lfs1, cell 39881775,,1817520727: src 0,,484 (m1.aggr1) (on alcatraz.osf.org) => fire.osf.org 0,,485 (m3.aggr1)
   flags 0x20001, volstates 0x10421206.  NumKAs 0; lastKA sweep=Mon Apr 26 13:30:17 1993
   srcVV: 0,,5011140; curVV: 0,,5011074; WVT ID = 735507659,,68765
   Lost token 11276 ago; token expires 12812 hence; new version published 16624 ago
   vvCurr 735845417.446636 (11276 ago); vvPingCurr 735855105.055972 (1588 ago)
   Last update attempt 735839869.841142 (16824 ago); next scheduled attempt 735856717.845048 (24 hence)
   Status msg: can't clone 0 to 1: Disc quota exceeded
# 
root@fire  # tail /opt/dcelocal/var/dfs/adm/RepLog
93-Apr-26 16:06:25 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:07:45 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:09:05 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:10:25 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:11:48 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:13:06 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:14:25 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:15:46 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:17:06 0,,485: can't clone 0 to 1: Disc quota exceeded
93-Apr-26 16:18:26 0,,485: can't clone 0 to 1: Disc quota exceeded
The contents of the master:
root@fire  # cm whereis .
The file '.' resides in the cell 'leprosy', in fileset 'm1.lfs1', 
on host alcatraz.osf.org.
root@fire  # ls
basic       basic.pmax  datefile    tests.init
root@fire  # 
The r/o hasn't been updated yet:
# cm whereis .
The file '.' resides in the cell 'leprosy', in fileset 'm1.lfs1.readonly', 
on host fire.osf.org.
# ls
basic       basic.pmax  fstab

[4/26/93 public]
Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/26/93 public]
I think I know what's happening.  Is it true that the current R/O fileset
has at least (1000000/252) or about 3900 files?

[04/27/93 public]
 
The R/0 file set has 145 files on it.

[4/27/93 public]
Yes, I was overly generous in computing how much space a clone fileset would
require (in my estimate above).  The repserver was not generous enough.
 
Defect Closure Form
-------------------
--Verification procedure below--
Create a scheduled replica and put a copy of the low tests in it.  If the
repserver can propagate the fileset, the bug is fixed; if it cannot do the
propagation but fails with ``cannot clone 0 to 1: Disc quota exceeded'' or
some such, as shown with ``fts statrep <server> -long'', you still have the
problem.  Assuming that the replica aggregate itself has plenty of space, of
course.
 
Associated information:
Tested on TA build:  dfs-102-2.9
Tested with backing build:  dce1.0.2b21
Filled in Transarc Deltas with `cfe-ot7804-big-quotas-for-repserver-too' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/rep/rep_main.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7797
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : need call timeouts
Reported Date                 : 4/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/23/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/23/93 public]
Based on 7756, this is the CR request that fts and repserver contain
defensive code to timeout calls rather than hang forever. See 7756
for an example.

[5/19/93 public]
Upping to a 1 so this makes 102a.

[6/22/93 public]
The change for this is part of the fts move submission that we are preparing.
Changed Responsible Engr. from `cfe' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `import'

[6/29/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7782
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : free list defaults are too small
Reported Date                 : 4/22/93
Found in Baseline             : 1.0.2
Found Date                    : 4/22/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/dfsbind/main_helper.c
Sensitivity                   : public
Transarc Deltas               : jaffe-ot7782-increase-dfsbind-free-list-length
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/22/93 public]
We are seeing this error pretty frequently on our CHO and system
tests.  I've looked at the code, and it seems that default is to have
two threads in dfsbind for handling requests.  I believe that this
limit is too small.
Here is my plan:
First, at Transarc (and maybe OSF), we will run future tests with
explicit arguments to dfsbind to up the number of threads.
	dfsbind -expressprocs 2 -regularprocs 2
If this seems to help, I propose that we change the defaults to this 
value.  The tests that we are running are not particularly heavy, and
since dfsbind queues up the call, it may be that this is one cause of
the long timeouts that we are seeing.
I expect that we can submit this change by tomorrow afternoon
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[04/22/93 public]
Are you changing the defaults compiled into the program
or the defaults in /etc/rc.dfs and dce_config?  If your
changing the defaults specified in the program, you
should check with Jeff K. to see that this has no documentation
impact (I believe the current defaults are documented somewhere).

[4/22/93 public]
I'de like to change the defaults in the program.  I'm checking with Jeff K.
for doc impact.

[4/26/93 public]
submitted on 4/26 also in dfs-102-2.11
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Transarc Deltas with `jaffe-ot7782-increase-dfsbind-free-list-length' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7781
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : ufsops
Short Description             : JFS FS ops lose file ownership on restores
Reported Date                 : 4/21/93
Found in Baseline             : 1.0.1
Found Date                    : 4/21/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ufsops/RIOS/aix_volops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7781-jfs-fs-cr-op-loses-uid-gid
Transarc Herder               : jaffe@transarc.com

[4/21/93 public]
The JFS fileset op for creating objects is not recording the
file ownership, and thus is losing file ownership when dumped
filesets are restored into a JFS fileset.  vol_ufsCreate_mach()
for the RIOS platform needs to set the uid and gid on the object
from the passed in vattr structure.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/22/93 public]
Defect Closure Form
-------------------
File ownership is now correct when fileset dumps are restored
into a JFS fileset.
Filled in Transarc Deltas with `cburnett-ot7781-jfs-fs-cr-op-loses-uid-gid' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ufsops/RIOS/aix_volops.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7780
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : CM file access to restored fileset hangs
Reported Date                 : 4/21/93
Found in Baseline             : 1.0.1
Found Date                    : 4/21/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_subr.c, file/cm/cm_tknimp.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7780-protect-cm-from-ctime-regression
Transarc Herder               : jaffe@transarc.com

[4/21/93 public]
Here are the contents from our bug system which describe the problem
and the fixes.
We have a testcase that does dumps and restores, and this test hung on trying
to verify the restored files.
Also note the that the src_fileset and the dest_fileset is on two different 
machines. Those machines are also configured as clients.
The test case will create the mount points and destination fileset as needed,
but only does so every so often. This run was on exsisting filesets/mountpoints.
The steps the test case did is as follows:
Note that both the src and the destination fileset existed and were mounted.
0) dce_login lars -dce- (lars is member of the subsys/dce/dfs-admin group)
1) fts clone src_fileset
2) fts dump src_fileset.backup -time 0 -file /tmp/fileset_dump
3) fts restore dest_fileset -server /.:/host/morgan -aggr dest_aggr -overwrite -file /tmp/fileset_dump
4) cm checkfilesets
5) find src_mount_point -print
6) find dest_mount_point -print
7) Verify files:	
	        for file in `find ${BACKUP_PATH} -print` ; do
                if [ -d $file ] ; then
                        continue
                fi
                cmp $file ${REST_FILESET_MOUNT_POINT}${file##${BACKUP_PATH}}
                if [[ $? -ne 0 ]] ; then
                        echo "Failed to compare $file with ${REST_FILESET_MO
UNT_POINT}${file##${BACKUP_PATH}}"
                        RC=1
                fi
        done
8) DATE=date
9) Change file in src_fileset
10) fts clone src_fileset
11) fts dump src_fileset -time $DATE -file /tmp/fileset_dump
12) fts restore dest_fileset -host /.:/hosts/morgan -aggr dest_aggr -overwrite -file /tmp/fileset_dump
13) cm checkfilesets
14) find src_fileset_mount_point -print
15) find dest_fileset_mount_point -print
16) Verify files: Same loop as above. Hung on first file.
    <Note by carlb (Rodney Carlton Burnett), 93/04/09 18:18:26, action: modify>
The problem is that the last restore moved the ctime of the file backwards
in time, and the cache manager doesnt handle that well at all in some
cases.  In this case the CM was in an infinate loop trying to get
the file permissions, because it could not load the acl cache since its idea
of the file's changetime  was newer than what it got from the fileserver.
To protect against this the CM should turn off the SC_STATD flag on a file
when its status tokens are revoked.  The restore will revoke the tokens
and cause the CM to accept the new status (including file permissions) from
the server.  In addtion I will add some infinite loop protection to
GetAccessBits.  I have discussed and validated these fixes with
Transarc.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/21/93 public]
Defect Closure Form
-------------------
These fixes have were validated by our test group which originally
found the problem.  We have been running with these changes without
regression for a week or so.
Filled in Transarc Deltas with 
 `cburnett-ot7780-protect-cm-from-ctime-regression' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/cm/cm_subr.c, file/cm/cm_tknimp.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[6/10/93 public]
closed.

[12/17/93 public]
Closed.



CR Number                     : 7773
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : sync files with Transarc
Reported Date                 : 4/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/21/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/ftserver/ftserver_dump.c, file/ftserver/ftserver_vprocs.c, file/rep/rep_main.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/21/93 public]
There are three files which differ between OSF and Transarc 1.0.2a sources.
These files affect ftserver dump/restore/move.  This OT is a placeholder
for the submission of these files.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/21/93 public]
submitted on 4/21.
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7767
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : assertion failed - tkm_locks.c
Reported Date                 : 4/21/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/21/93
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/tkm/tkm_fidHash.c, file/tkm/tkm_fidHash_private.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-fix-tkm-expiration-assert
Transarc Herder               : jaffe@transarc.com

[4/21/93 public]
BUILD:	102a build available 4/20/93
CONFIG:	3 pmax cell, 3 flservers, root.dfs in lfs
valentine, sync site of flservers AND home to both the read-write and read-only
and backup versions of root.dfs, dropped into kdb with:
assertion failed line 634, file src/file/tkm/tkm_locks.c
kdb> $k
tkm_Lock_CheckExcLocked
tkm_FidHash_Remove
tkm_FidHash_FindPlace
Nothing going on in the cell except that I had just updated the namespace
to remove dce5, one of the flservers from the /.:/fs group.
Oh - there was a fts crmount in progress at the time of the assertion.

[4/21/93 public]
Filled in Interest List CC with `pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar' to `kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/21/93 public]
Got this assertion again - also during a crmount - perhaps this is not a
coincidence? kdb $k showed the same stack trace.

[4/23/93 public]
Yet again - with a fts delmount - $k produces same (short) stack trace.
kdb> <sp,40/p included the following symbols:
tkm_Lock_CheckExcLocked
tkm_FidHash_Remove
tkm_TokenSetIterator_Next
tkm_Token_CheckRevokeReqd
bucket
VEC_tlbmiss
malloc
osi_Time
tkm_FidHash_FindPlace
tkm_CheckTypeAndSetConflict
tkm_FidHash_RevokeConflicts

[4/26/93 public]
Mike notes that this problem has been fixed in kazar-fix-tkm-expiration-assert.
We'll drop it asap to the OSF and into dfs-102
Changed Interest List CC from `pakhtar@transarc.com' to 
 `pakhtar@transarc.com,shl@transarc.com' 
Filled in Transarc Deltas with `kazar-fix-tkm-expiration-assert' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `export'

[4/26/93 public]
submitted on 4/26 also in 102-2.10
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `file/tkm/tkm_fidHash.c, 
 file/tkm/tkm_fidHash_private.h' 
Changed Transarc Status from `export' to `import'

[12/17/93 public]
Closed.



CR Number                     : 7766
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : fxd
Short Description             : fxd maintaining connection with rpcd, secd
Reported Date                 : 4/20/93
Found in Baseline             : 1.0.1
Found Date                    : 4/20/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7766-fxd-release-rpc-connections, comer-ot7766-fxd-release-rpc-connections-for-sun
Transarc Herder               : jaffe@transarc.com

[4/20/93 public]
fxd is not closing sockets is has open to communicate with
rpcd and secd before diving into the kernel.  This results 
in the inability to restart rpcd (and maybe secd, but you
don't get that far), without rebooting.  This only shows
up in a multi-machine cell.  Here is the scenario:
Host coredump is your dce core component server and a dfs fileserver.
Host haunted is a second dfs server
When fxd is started on the second server, connections are
established with the core component server rpcd and secd, 
probably for some of the initial work fxd does in user
space.  The calls being made are primarily rpc and security
and they take the first protocol sequence they get (tcp).
These can be seen using netstat -a (the state here is
initially ESTABLISHED, but then moves to the two WAIT
states shown below):
On coredump:
# netstat -a | pg
Active Internet connections (including servers)
Proto Recv-Q Send-Q  Local Address          Foreign Address        (state)
tcp        0      0  coredump.1025          haunted.1074           FIN_WAIT_2
tcp        0      0  coredump.135           haunted.1073           FIN_WAIT_2
On haunted:
# netstat -a | pg
Active Internet connections (including servers)
Proto Recv-Q Send-Q  Local Address          Foreign Address        (state)
tcp       48      0  haunted.austin.i.1074  coredump.austin..1025  CLOSE_WAIT
tcp       48      0  haunted.austin.i.1073  coredump.austin..135   CLOSE_WAIT
 
The only way to release this connection is to reboot one of 
the two machines.  The problem shows up when you try to restart
rpcd:
# dce.clean
Stopping DCE daemons:
        killing dtsd
        killing cdsadv
        killing secd
        killing sec_clientd
        killing rpcd
# rc.dce
Starting DCE daemons:
        starting /opt/dcelocal/bin/rpcd
(rpcd) Verify that no other rpcd/llbd is running: (0x16c9a003) cannot bind socke
t (dce / rpc)
        *** rpcd did not start
 
It was not trivial to determine that these two connections actually
belonged to fxd, we kidnapped two of the AIX kernal gurus and it
took them about 2 hours of wading through the file table.  But it
is fairly easy to observe once you know what to look for.  I took
fxd out of rc.dfs and started it by hand after the rest of DFS was
up, and like clockwork, the two open sockets show up.
We have successfully reproduced this problem on the b22 reference
port.
I tried to debug this further, but the connections go away by 
themselves if there is any kind of pthread_delay or breakpoint
in dbx prior to diving into the kernel.  Maybe this is a work-around,
but it would be nice to know where these are coming from.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/3/93 public]
Defect Closure Form
-------------------
This will clear up the problem but it has dependencies on ot7850 (at
least at IBM).
--Verification procedure below--
Run fxd with tcp connections to secd and rpcd.  After fxd starts up,
these connections should be dropped.  This was verified at IBM.
Associated information:
Tested on TA build:  
	dfs-102-2.9
Tested with backing build:  
	dce1.0.2b19
Filled in Transarc Deltas with `comer-ot7766-fxd-release-rpc-connections, 
 comer-ot7766-fxd-release-rpc-connections-for-sun' 
Changed Transarc Status from `open' to `export'

[5/18/93 public]
Changed Transarc Status from `export' to `import'

[5/20/93 public]
We tried this at fix in dfs-carl and we get errors from CMA.  The errors are:
 % See 'cma_dump.log' for state information.
 %Internal DCE Threads problem (version CMA BL10+), terminating execution.
 % Reason: assertion failure:  cma__io_available:  i exceeded cma__g_file_num 
   before nfound reached zero.
        at line 867 in ../../../src/threads/cma_thread_io.c
Another retry got the errors:
        running fxd...
 **** Assertion warning. cma__close_general:  Closing a file with threads waiting to read.
     at line 508 in ../../../src/threads/cma_thread_io.c
 **** Assertion warning. cma__close_general:  Closing a file with threads waiting for exception.
     at line 514 in ../../../src/threads/cma_thread_io.c
 **** Assertion warning. cma__close_general:  Closing a file with read mask bit set.
Changed Transarc Status from `import' to `export'

[5/20/93 public]
Down-graded the priority until I get some action on ot8025.
Changed Priority from `1' to `2'

[5/20/93 public]
I think ot8025 should be duped to ot 6886.  ot7850 is the one we opened
for this, it is also duped to 6886.

[5/20/93 public]
Please try not to manipulate priorities/severities other than to reflect
the true nature of the problem reported. In particular, changing a priority
0 or 1 defect to priority 2 says "this bug is not important enough to be
in the next scheduled release". The dfs-drb should decide that.

[5/20/93 public]
You're absolutely right.    This defect dosen't belong in 1.0.2a,
however.  Moving it to 1.0.3.
Changed Fix By Baseline from `1.0.2a' to `1.0.3'

[5/20/93 public]
Watch the bouncing bug...downgrading to 2.

[12/17/93 public]
Closed.



CR Number                     : 7762
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : root.dfs scheduled rep - nogo
Reported Date                 : 4/20/93
Found in Baseline             : 1.0.2b23
Found Date                    : 4/20/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : see CR 7896
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/20/93 public]
As part of the dfs.repfs_checklist, root.dfs is replicated.
I tried scheduled replication but I'm not able to verify it
with the explicit readonly mount point - even after flushing.
The RepLogs on both valentine and dce5 contain nothing except
an old reference to a quorum problem that occurred during a
reboot/restart (ie. expected/explainable). The flservers achieved
quorum and udebug on all 3 machines agree that valentine is the
sync site. I believe I've waited long enough (an hour) for the mount points
to be replicated - what's happening?
 
root@dce12> fts update root.dfs -all -localauth
fts update: Repserver on dce5.osf.org requested to update fileset 0,,2
fts update: Repserver on valentine.osf.org requested to update fileset 0,,2
 
root@dce12> fts lsreplicas root.dfs -all
On dce5.osf.org:
root.dfs, cell 134228757,,3182436262: src 0,,1 (lfs_aggr) (on dce5.osf.org) => dce5.osf.org 0,,2 (lfs_aggr)
   flags 0x20081, volstates 0x10421206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,78; curVV: 0,,63; WVT ID = 735322968,,141
   Lost token 2896 ago; token expires 12025 hence; new version published 735330834 ago
   vvCurr 735327938.159623 (2896 ago); vvPingCurr 735328459.005218 (2375 ago)
   Last update attempt 735327192.835398 (3642 ago); next scheduled attempt 735330799.622487 (-35 hence)
   Status msg: getFreeVolID: could not get a new vol ID: no quorum elected (dfs / ubk)/
 
On valentine.osf.org:
root.dfs, cell 134228757,,3182436262: src 0,,1 (lfs_aggr) (on dce5.osf.org) => valentine.osf.org 0,,2 (lfs_aggr2)
   flags 0x20081, volstates 0x10423206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,78; curVV: 0,,78; WVT ID = 735322968,,127
   Lost token 3515 ago; token expires 11651 hence; new version published 2726 ago
   vvCurr 735327321.286562 (3515 ago); vvPingCurr 735328087.005305 (2749 ago)
   Last update attempt 735328091.895937 (2745 ago); next scheduled attempt 735328723.140066 (-2113 hence)
   Status msg: ReplicaWantsAdvance: all OK at 735328110
 
root@valentine> fts lsfldb -fileset root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  valid
        backup      ID 0,,3  invalid
number of sites: 2
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:00:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner               
dce5.osf.org        RW,RO    lfs_aggr 0:30:00 hosts/dce5     <nil>               
valentine.osf.org   RO       lfs_aggr2 0:30:00 hosts/valentine<nil>             
 
root@dce12> ls /:
dce12_u3     epi_1        epi_1_ro     root_dfs     root_dfs_ro  val_u2
root@dce12> ls /:/root_dfs
dce12_u3     epi_1        epi_1_ro     root_dfs     root_dfs_ro  val_u2
root@dce12> ls /:/root_dfs_ro
root_dfs
root@dce12> cm flush /.:/fs/root_dfs_ro
root@dce12> cm flush /.:/fs
root@dce12> ls /:/root_dfs_ro
root_dfs

[4/20/93 public]
You may need to reboot the CM after establishing that root.dfs is replicated.
Also, you should use ``cm checkfilesets'' and not ``cm flush'' for the purpose
of getting the CM to know about new fileset information.  There's an existing
OT CR describing the fact that the CM doesn't refresh its knowledge of whether
root.dfs is replicated or not when you do ``cm checkfilesets''.
Does this help?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/20/93 public]
Well - yes and no. I changed the replication type to release and released
root.dfs and got:
 
root@dce12> fts release root.dfs -localauth
(Warning) DeleteVolume: 0,,2 doesn't exist.  Ignoring...
Released fileset root.dfs successfully
 
Then I did the cm checkfilesets and I can now see the replicated mount
points.
 
So ... the remaining questions are:
 - why did I NOT have to reboot the CM if there's an open CR on this?
 - why the DeleteVolume warning?
 - was it simply the cm checkfilesets or release replication that fixed things?
 
I'm reconfiguring with the 102a build available today - I should be able
to answer at least the last one. Apparently, it's not enough to create the
additional root.dfs mount point before the first addsite - you must do
it before the setrepinfo? Isn't the default mount point created by crmount
read-write or must I use the -rw option explicitly? 
 
Thanks.

[4/20/93 public]
Again, I'm not sure of the history of this particular CM; you haven't said.
Generally, once the CM evaluates the root fileset for a cell, it will remember
whether it should use root.dfs or root.dfs.readonly, but I thought that
there was no way, short of rebooting, to get it to check that information
again.  Perhaps I'm wrong, which would be great!
 
I'd bet that you could have just done ``cm checkfilesets'' and the CM would
have figured it out.  Changing to release replication is a red herring.
You could have done
	fts lsft -fileset root.dfs
	fts lsft -fileset root.dfs.readonly -server dce5
	fts lsft -fileset root.dfs.readonly -server valentine
to find out what the real status of the root.dfs fileset and its two replicas
was--in particular, to see if the fileset version numbers matched, so that
the replicas were up to date.  Certainly the one replica that seemed
to be OK on valentine should have been adeqate.
 
As to the specific questions, maybe you didn't have to reboot your CM; that's
great.  The DeleteVolume warning is just noise, since you're doing the
initial ``fts release'' for a fileset.  And, I believe that it was the
cm checkfilesets that fixed it up, as I claimed.  Yes, you have to use
the -rw option to ``fts crmount'' to force access to a read-write fileset
from a read-only one.  You don't have to do it before the setrepinfo; it
doesn't really matter.  It matters that you do it before the addsite,
though.

[4/21/93 public]
Well - I was about to cancel this bug because I DID get (forced) scheduled
replication of root.dfs (on the local aggregate) to work - once. However,
the second time I tried to update the read-only replica, it either doesn't
take OR the problem is in seeing the update.
 
/: = read-only root.dfs
/:/root_dfs = read-write mount point to root.dfs
create mount points dce12_u3, root_dfs_ro
update
cm check
ls /: and ls /:/root_dfs_ro and ls /:/root_dfs all agree
 
create a new fileset and mount point, epi.1 and /:/root_dfs/epi_1
update
cm check
ls /: and ls /:/root_dfs_ro agree with each other but not with /:/root_dfs
only /:/root_dfs has the new mount point

[4/21/93 public]
What evidence do you have that you waited long enough after whatever it is
you mean by ``update'' to let the changes propagate to the copies of
root.dfs.readonly?  Cancelling this still sounds like a good idea.
Even in release-style replication, where ``update'' might mean ``fts release'',
you have to wait for the non-local repservers to propagate changes.

[4/21/93 public]
By update I mean fts update root.dfs -all and I only have the fact that
I've waited more than 3 hours for the repserver to update a fileset
that only contains 3 mount points to a fileset that contains 4 mount points.
I'm using the fact that my read-only mount points (/: and the explicit
/:/root_dfs_ro) do NOT show the 4th mount point as evidence.
 
Is there some more definitive way for me to know?
 
This cell has been nearly idle during this - except for a few (3) fts renames
and the cm checks and ls's used to look at /:, /:/root_dfs and /:/root_dfs_ro.

[4/21/93 public]
The same old ways:
	fts statrep <servername> -long
		to find out what a repserver thinks it's doing, or
	fts lsrep root.dfs -all
 
To find out for sure, do
	fts lsft -fileset root.dfs
	fts lsft -fileset root.dfs.readonly -server valentine
	fts lsft -fileset root.dfs.readonly -server dce5
(or whatever) and look at the fileset version numbers for the r/w
and the readonly instances.  This will tell you for sure whether the
repserver has done the update.  These numbers are listed after
``Version'' on the line that also lists llBack and llFwd.
 
It's difficult to tell whether some three hour wait is ``enough'' without
knowing the replication parameters.  Furthermore, since you didn't do a
``fts release'' I have to assume that you're not using release replication.
Why make me guess?  Do ``fts lsfldb root.dfs'' and put the output here.
After all this time, it shouldn't be hard to know what evidence is good
evidence!

[4/21/93 public]
root@dce12> fts lsfldb root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  valid
        backup      ID 0,,3  valid
number of sites: 1
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner               
valentine.osf.org   RW,RO,BK lfs_aggr 0:30:00 hosts/valentine<nil>              
 
root@valentine> fts lsft -fileset root.dfs
_____________________________________________
root.dfs 0,,1 RW LFS     states 0x18010005 On-line
    valentine.osf.org, aggregate lfs_aggr (ID 1)
    Parent 0,,0       Clone 0,,2       Backup 0,,3
    llBack 0,,3       llFwd 0,,0       Version 0,,20     
    1048576 K alloc limit;       9 K alloc usage
       5000 K quota limit;       9 K quota usage
    Creation Wed Apr 21 08:41:08 1993
    Last Update Wed Apr 21 10:42:04 1993
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  valid
        backup      ID 0,,3  valid
number of sites: 1
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner               
valentine.osf.org   RW,RO,BK lfs_aggr 0:30:00 hosts/valentine<nil>               
_____________________________________________
 
root@valentine> fts lsft -fileset root.dfs.readonly -server valentine
_____________________________________________
root.dfs.readonly 0,,2 RO LFS Scheduled  states 0x10421206 On-line
    valentine.osf.org, aggregate lfs_aggr (ID 1)
    Parent 0,,1       Clone 0,,2       Backup 0,,3
    llBack 0,,0       llFwd 0,,0       Version 0,,22     
    1048576 K alloc limit;      17 K alloc usage
       5000 K quota limit;      17 K quota usage
    Creation Tue Apr 20 18:37:55 1993
    Last Update Tue Apr 20 18:38:18 1993
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  valid
        backup      ID 0,,3  valid
number of sites: 1
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner               
valentine.osf.org   RW,RO,BK lfs_aggr 0:30:00 hosts/valentine<nil>               
_____________________________________________
 
ot@valentine> fts statrep -server /.:/hosts/valentine -long
Status of rep server /.:/hosts/valentine (valentine.osf.org) at Wed Apr 21 15:45:49 1993
Replicas managed: 2; host/connection blocks: 1
IDs: 0 allocated, 0 in use, 0 re-used
Next forced keep-alive in 86381 secs, at Thu Apr 22 15:45:30 1993
Primary comm blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
Primary keep-alive blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
 
Active replicas on /.:/hosts/valentine:
 
epi.1, cell 134228757,,3182450524: src 0,,8 (lfs_aggr) (on valentine.osf.org) => valentine.osf.org 0,,9 (lfs_aggr2)
   flags 0x20001, volstates 0x10423206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,1; curVV: 0,,1; WVT ID = 735398615,,35
   Lost token 7794320 ago; token expires 5681 hence; new version published 735421550 ago
   vvCurr 727627230.854646 (7794320 ago); vvPingCurr 735412831.004041 (8719 ago)
   Last update attempt 735403232.784323 (18318 ago); next scheduled attempt 0.647868 (-735421550 hence)
   Status msg: GetToken returned 0x404 token, ID 735398615,,35.
 
root.dfs, cell 134228757,,3182450524: src 0,,1 (lfs_aggr) (on valentine.osf.org) => valentine.osf.org 0,,2 (lfs_aggr)
   flags 0x20081, volstates 0.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,20; curVV: 0,,0; WVT ID = 735398615,,36
   Lost token 7795408 ago; token expires 5819 hence; new version published 735421551 ago
   vvCurr 727626143.487095 (7795408 ago); vvPingCurr 735412970.004027 (8581 ago)
   Last update attempt 0.000000 (735421551 ago); next scheduled attempt 735421570.096823 (19 hence)
   Status msg: NewVolume: can't create new replica volume: File exists
 
root@valentine> fts lsheader -aggr lfs_aggr -server valentine
Total filesets on server valentine aggregate lfs_aggr (id 1): 4
epi.1                    0,,8 RW      9 K alloc      9 K quota On-line
root.dfs                 0,,1 RW      9 K alloc      9 K quota On-line
root.dfs.backup          0,,3 BK      9 K alloc      9 K quota On-line
root.dfs.readonly        0,,2 RO     17 K alloc     17 K quota On-line
Total filesets on-line 4; total off-line 0; total busy 0

[4/21/93 public]
Hey, that was really worthwhile!
I have to imagine that root.dfs in this cell has gone through some
changes, like having been re-created from scratch or something.  At the
moment, the fileset version for root.dfs.readonly is actually larger
than the fileset version for root.dfs (22 > 20), or else something went
haywire in the initial copy attempt.
 
In any event, the repserver on valentine is stuck unable to update the
root.dfs.readonly fileset, because it looks to repserver like something
besides itself created the fileset after repserver started up.  Is there
some peculiar history here?  Did the replication style for root.dfs change
once or twice?  What has happened to this root.dfs?   --Thanks!

[4/21/93 public]
History:
I first reported this CR based on a b23 build. I had gotten to the point
of doing funky things like changing types of replication in THAT cell
but that's not of interest now.
 
I then upgraded to a 102a build by reconfiguring the cell from scratch.
The aggregates were all newaggr'd. The configuration is slightly different
from the b23 one - mainly in that it is simpler (ie. only 1 replication site
for root.dfs, it has always been scehduled replication). I did, however,
clone this root.dfs in addition to replicating it. If you're looking for
reassurance that the b23 aggregates were really newaggr'd, (I would be),
root.dfs is on a different machine and aggregate in the 102a cell. The
lsheader for the "old" root.dfs aggregate shows it's been wiped clean of
any root.dfs reference.
 
From then on, you know the history from above - I was ready to cancel as
an operator error (because I USED to use cm flush when I should use cm check).
The first fts update root.dfs -all worked. The second didn't. I've been getting
tkm assertions in this cell, CR 7767, if that's of any interest - this has
made being CERTAIN the CM doesn't need to be rebooted to recognize a replicated
root a little difficult - and I needed to salvage the aggregate housing root.dfs
after one of these assertion crashes.
 
So ... nothing really interesting - outside chance the salvage messed things
up - otherwise the repserver's the only game in town.

[4/21/93 public]
Recovering from a 2nd tkm assert, fts lsft root.dfs and root.dfs.readonly
now agree on "Version", ie. 20. It looks like a reset more than a recovery
since the 2 filesets still do not match.
 
I've retried fts update root.dfs -all WITHOUT changing root.dfs and have
periodically done fts statrep -server valentine -long - the Status msg
has held steady at:
   Status msg: New volChanged object (0,,0 -> 0,,20) has 30 fids.
 
I then tried to create a new mount point and seem to have turned on an
every 20 seconds type error:
 
root@dce12> fts crmount -dir /:/root_dfs/epi_4 -fileset epi.4
dfs: fileset (0,,2)  version is older than our latest on server 130.105.5.27 in cell p102a_cell.qadce.osf.org.
dfs: fileset (0,,2)  version is older than our latest on server 130.105.5.27 in cell p102a_cell.qadce.osf.org.
dfs: fileset (0,,2)  version is older than our latest on server 130.105.5.27 in cell p102a_cell.qadce.osf.org.
 
I have to go now - feel free to login to the cell remotely if you want to
look at this before tomorrow - otherwise, we can look at this together tomorrow.

[4/22/93 public]
CR 7767 struck again so upon reboot, rc.dfs:
 
+ /opt/dcelocal/bin/salvage -rec -verify /dev/rz1a 
Verifying /dev/rz1a
Will run recovery on /dev/rz1a
recovery statistics:
        Elapsed time was 4465 ms
        9 log pages recovered consisting of 176 records
        Modified 8 data blocks
        104 redo-data records, 0 redo-fill records
        0 undo-data records, 0 undo-fill records
Ran recovery on dev 1/1
Volume 10 version 0.14 Lower than Anode 20 Version 0.16
In volume root.dfs.readonly 0,,2 (avl #10)
  Volume version 0,,20 smaller than largest anode version number seen 0,,22
Verify Zero Link Count List: Volume root.dfs.readonly file counts didn't match: claims 0 but should be 1.
Verify: Zero Link Count list in volume root.dfs.readonly needs updating.
unsafe <no name was available>: volume index: 10 anode index 32
zeroLinkCnt <no name was available>: volume index: 10 anode index 6
Processed 5 vols 26 anodes 5 dirs 25 files 1 acls
Done.  Some inconsistencies found verifying /dev/rrz1a

[4/23/93 public]
If there were no oddball methods of updating root.dfs here, like restoring
it from some other fileset or zapping it and recreating it, then what we
have is a repserver crash-proofing problem.  The only mechanism for
upping the fileset's version number is the fileset restoration process, so
something must have gone wrong with repserver's treatment of a fileset
while it was being restored, or in one of the crashes that interrupted
the restoration process.  Repserver creates filesets with ZAPME set in the
states field, then edits that states field to clear ZAPME and set
IS_COMPLETE once the dump is done.  My best guess as to what's happening
here is that the restoration completed but wasn't sync'ed to disk
before the repserver turned on its IS_COMPLETE flag, so that when the
machine crashed with the CM bug, repserver couldn't make that much sense
out of what was happening with the replica.  Thus, it thought that it
had to create a new root.dfs.readonly fileset, with the existing fileset
ID, but there was already one there that it hadn't been able to hook up
to.
 
The salvager error is something that Ted would ultimately need to check
out, but I believe that it's nothing substantially new: its problems are
covered by existing low-priority defects (not copying the ZLC list on
a clone, some handling for fileset versions smaller than data versions--
a case that will arise in the middle of restorations).
 
A failsafe recovery mechanism would be to zap the bogus root.dfs.readonly
fileset and restart the repserver.
 
I agree that repserver could be changed to be a little more aggressive in
its restart protocol, verifying fileset versions for inconsistencies
as well as verifying fileset states.  I don't think this needs to be an
A 0 bug report, though.

[4/23/93 public]
I had not done anything funky but yes, it sounds possible that the 7767
crashes could have caught the repserver mid-replication.
 
I newaggr'd and went from there - I haven't hit 7767 or this one since. I have
had an fts update command "ignored" but we haven't finished that discussion
yet ... I did succeed in with multiple fts updates of root.dfs.
 
Since this is no longer blocking me, it's a B 1 - the 102 and 102a docs say
that the system will gracefully recover from crashes so I don't think this
should go below a 1. I'm a little concerned that there are known problems
in salvager with recovery that are not 0's or 1's but that's another discussion
too ... Thanks for the analysis.

[5/11/93 public]
I think that this problem is ultimately addressed by many of the deltas
that were submitted.  Please re-test with them, in particular with the
fix for OT 7896.  Thanks.
Changed Responsible Engr. from `cfe' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[5/28/93 public]
Closing this one!
Many successful scheduled replications of root.dfs with dfs.carl. There
are still salvage/recovery issues with replication but these are covered
by other CRs.



CR Number                     : 7752
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : flserver
Short Description             : flservers core dumping in idle cell
Reported Date                 : 4/19/93
Found in Baseline             : 1.0.2
Found Date                    : 4/19/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-ot7752-ubik-more-bullet-proofing
Transarc Herder               : jaffe@transarc.com

[4/19/93 public]
CONFIG:	3 pmax cell, 3 flservers
BUILD:	weekly build 23
Unable to trace the cause but, in an idle cell this weekend, 2 of the 3
flservers core dumped - 1 on Saturday, 1 Monday morning. The cell was
completely idle the whole time. Both core files reveal the following:
OSF version 1.0 created Wed Feb  3 16:34:56 1993
..(no debugging symbols found)...
Core was generated by `flserver'.
Program terminated with signal 6, IOT/Abort trap.
#0  0x73055b48 in ubik_dbVersion ()
(gdb) where
#0  0x73055b48 in ubik_dbVersion ()
#1  0x73008610 in ubik_dbVersion ()
#2  0x7300d030 in ubik_dbVersion ()
#3  0x40e12c in AbortDumpHandler ()
Cannot access memory at address 0x101bb81c.
(gdb) break main
Breakpoint 1 at 0x40fcd0
(gdb) run
Starting program: /opt/dcelocal/bin/flserver 
(no debugging symbols found)...(no debugging symbols found)...
Breakpoint 1, 0x40fcd0 in main ()
(gdb) kill
Kill the inferior process? (y or n) y
In Local core dump file,
#0  0x73055b48 in kill ()
(gdb) where
#0  0x73055b48 in kill ()
Both flservers output a icl.ubik file 1-2 minutes before they core dumped.

[4/20/93 public]
Filled in Interest List CC with `pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/20/93 public]
Well, the stack trace of the core dump doesn't seem to make much sense.
ubik_dbVersion is a global variable and not a function. It may help to compile
the flserver with -g, I guess. Was there a icl.ubik present in the same 
directory as the core file? The stack trace refers to AbortDumpHandler() which
gets called on signals such as SIGIOT, SIGSEGV. The AbortDumpHandler should
dump the ICL log before killing the process. The time the flservers dumped
core, is this time close the usual bosserver server restart time? This might
provide clues, particularly on a quiescent system. The information in the
FlLog may also help. Thanks.

[4/20/93 public]

[4/20/93 public]
Yes - the stack traces from flserver core dumps have always been questionable
for me. Bill S. however used to ask me to save them anyway so I've saved a
compressed version of the core dump and an uncompressed copy of its corresponding
icl.ubik file in ~notuser/gmd/7752.
I don't believe either core dump was near the automatic restart (which was
left at the default - 4am Saturday is it?) - although the first one may have
been - I believe it was about 3 am Saturday morning - too late to find out now.
The *Logs didn't contain anything useful - again, too late now.
When I see this again, I will make sure I've heard from you before I reconfig.

[4/22/93 public]

[4/26/93 public]
I managed to convert the icl file full of raw ops to one that has all the
catalog messages. Looking at this ICL file, it looks like it came from host
130.105.5.27 that was the sync site at the time of failure. It was receiving
YES votes from 130.105.5.25 and 130.105.5.31 and was in recovery state 0x1f
which indicates that everything about the flservers was healthy. The last
line of the ICL log are meaningful. The beacon RPC sent by 130.105.5.27
failed with rpc_s_invalid_binding. This could have aborted the flserver. The
binding handles to other ubik servers is generated when the server starts
up and are used for every outgoing ubik rpc. This binding handle had somehow
got corrupted (either the handle or the runtime binding data structures),
which resulted in the error. Now the data corruption could have caused
a segmentation violation to be generated that killed the flserver process. 
We have seen this rpc_s_invalid_binding error come from ubik about a month
ago during ubik testing here, although we couldn't figure out at that time
what the bug was. It may be hard to track such errors unless we have a
reasonably usable core dump.
Gail, do you have the ICL dump and core dump file from the other flserver
that crashed. It may be useful to take a look at this to see if the two
flservers failed in similar ways. This may indicate if this failure is
reproducible. Let me know.
I will be checking in a fix for ubik where I refresh the ubik binding handles
each time I get an RPC error from an outgoing ubik RPC. I don't know if this
can solve this problem though.

[4/26/93 public]
Unfortunately, I don't have the other core or icl.ubik files. I've dropped
the priority to a 2 pending more data. Thanks for the analysis.

[5/24/93 public]
Another flserver core dump in an idle cell - this time on a rios. The core
file is not usable but an icl file was created at the time of the core dump.
The last line of the icl file contains:
time 611.575257, pid 10: received vote 738252303 from server 130.105.5.27, error
code=382312477
where that error code stands for :
382312477 (decimal), 16c9a01d (hex): invalid binding (dce / rpc)
I've placed the whole icl file in ~notuser/gmd/7752

[7/21/93 public]
This defect is actually fixed and the delta is already in 103 tree.
Vijay, please ot export it, then we are all set. 
Filled in Transarc Herder with `jaffe@transarc.com'

[7/26/93 public]

[7/26/93 public]
Defect Closure Form
-------------------
--Regression test program below--
The best way to test this fix would be to run multiple flservers in a cell for
a few days with fts and other tests (CHO) running. The flservers should not
dump core but be alive. Use the udebug program to check once in a while whether
ubik quorum continues to be maintained.
Associated information:
Tested on TA build:  
???
Tested with backing build:  
???
Filled in Transarc Deltas with `vijay-ot7752-ubik-more-bullet-proofing' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 7748
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : legal cache sizes
Reported Date                 : 4/16/93
Found in Baseline             : 1.0.2
Found Date                    : 4/16/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot-7748-round-up-dfsd-provided-cache-sizes
Transarc Herder               : jaffe@transarc.com

[4/16/93 public]
As part of system test, I'm to try out different cache sizes - there
is a safety catch for trying to specify "0" or "1" as the number of
kilobyte blocks in the CacheInfo file - but "10" is allowed.
When I then try to cd /: and ls, I hang in the ls.
The /usr/adm/messages file says:
Apr 16 17:56:23 valentine vmunix: dfs: starting dfs cache scan...
Apr 16 17:56:29 valentine vmunix: dfs: found 0 cache files.
Apr 16 18:02:13 valentine vmunix: dfs:can't free cacheblocks
I have 2 dfstrace dumps, one the time of the "can't free" message and one
from 20 minutes later if these are needed.

[4/19/93 public]
Filled in Interest List CC with `pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar' to `kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/4/93 public]
I've taken a look, and it seems that the number of blocks must be at least
as big as 2 chunks.  The default chunk size of 8192, so the minimum number
of blocks would need to be 17.  The code is rather convoluted, so I'll let 
Mike decide if this is truely the case.  Gale, can you try using a block
size of 17.  I'm pretty sure that this will work (albit with terrible
performance).

[7/20/93 public]
Assign it to me. 
Changed Subcomponent Name from `cm' to `dfsd' 
Changed Interest List CC from `pakhtar@transarc.com' to `pakhtar@transarc.com, 
 kazar@transarc.com' 
Changed Responsible Engr. from `kazar@transarc.com' to `tu@transarc.com'

[7/20/93 public]
Yes, it is indeed fixed the way we plan to. The fix is in 103 dfs-103-3.23
build. I will run the test and export it later when I have that build
in my machine. This defect is scheduled to complete by 9/6 for OSF.
The remaining work is to modify the document by saying that the CM will 
round dfsd's given block size to two chunks if the given size is less than it. 
                                                                            
This is tracked by OT#8320
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Changed Subcomponent Name from `dfsd' to `cm' 
Changed Interest List CC from `pakhtar@transarc.com, kazar@transarc.com' to 
 `pakhtar@transarc.com, kazar@transarc.com, jeff@transarc.com' 
Filled in Transarc Herder with `jaffe@transarc.com'

[7/23/93 public]
This defect can be avoided by following the documentation, and it does not
cause a loss of data.  Its a B2.
Changed Priority from `1' to `2'

[8/18/93 public]
Changed Responsible Engr. from `tu@transarc.com' to `kazar' 
Filled in Transarc Deltas with 
 `kazar-ot-7748-round-up-dfsd-provided-cache-sizes' 
Filled in Transarc Status with `export'

[8/18/93 public]
Changed Interest List CC from `pakhtar@transarc.com, kazar@transarc.com, 
 jeff@transarc.com' to `pakhtar@transarc.com, kazar@transarc.com, 
 jeff@transarc.com, kdu@transarc.com'

[12/17/93 public]
Closed.



CR Number                     : 7745
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : ticket expiration errors from fts commands
Reported Date                 : 4/16/93
Found in Baseline             : 1.0.2
Found Date                    : 4/16/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/{security/dfsauth/dfsauth_client,rep/rep_main}.c
Sensitivity                   : public
Transarc Deltas               : comer-ot7745-add-identity-autorefresh-to-localauth-context
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/16/93 public]
 
The fts commands periodically get errors relating to auth ticket
expirations (from dfs servers?) when the user issuing the command
has valid, unexpired credentials:
 
root@fire  # fts addsite -file m1.lfs4 -server /.:/hosts/alcatraz -aggr m1.aggr2 -maxsi 4m
Couldn't talk to repserver on alcatraz.osf.org: auth ticket expired (dce / rpc)
Couldn't talk to repserver on fire.osf.org: auth ticket expired (dce / rpc)
Couldn't talk to repserver on singsing.osf.org: auth ticket expired (dce / rpc)
Added replication site /.:/hosts/alcatraz m1.aggr2 for fileset m1.lfs4
root@fire  # date
Fri Apr 16 15:22:54 EDT 1993
root@fire  # klist 
root@fire  # klist
DCE Identity Information:
        Warning: Identity information is not certified
        Global Principal: /.../cho_cell/cell_admin
        Cell:      00661a52-ca3a-1bc9-9074-02608c2f6c55 /.../cho_cell
        Principal: 00000064-ca3a-2bc9-9000-02608c2f6c55 cell_admin
        Group:     0000000c-ca3e-2bc9-9001-02608c2f6c55 none
        Local Groups:
                0000000c-ca3e-2bc9-9001-02608c2f6c55 none
                00000064-ca62-2bc9-ab01-02608c2f6c55 acct-admin
                00000065-ca62-2bc9-ab01-02608c2f6c55 subsys/dce/sec-admin
                00000066-ca63-2bc9-ab01-02608c2f6c55 subsys/dce/cds-admin
                00000068-ca64-2bc9-ab01-02608c2f6c55 subsys/dce/dts-admin
                00000067-ca63-2bc9-ab01-02608c2f6c55 subsys/dce/dfs-admin
                00000069-ca64-2bc9-ab01-02608c2f6c55 subsys/dce/dskl-admin
Identity Info Expires: 93/04/18:10:36:55
Account Expires:       never
Passwd Expires:        never
....
 
root@fire  # fts addsite -file m1.lfs4 -server /.:/hosts/alcatraz -aggr m1.aggr2 -maxsi 4m -verbose
There is already a R/O replica on alcatraz.osf.org (m1.aggr2).
Error in addsite: status ffffffff (unknown facility) (unknown)
root@fire  # fts setrepinfo -file m1.lfs4 -change -release -verbose
Calling REP_AllCheckReplicationConfig(alcatraz.osf.org, {39881775,,1817561865}, {0,,2508}, 3, flg=0...)...
Couldn't talk to repserver on alcatraz.osf.org: auth ticket expired (dce / rpc)
Calling REP_AllCheckReplicationConfig(fire.osf.org, {39881775,,1817561865}, {0,,2508}, 3, flg=0...)...
Couldn't talk to repserver on fire.osf.org: auth ticket expired (dce / rpc)
Calling REP_AllCheckReplicationConfig(singsing.osf.org, {39881775,,1817561865}, {0,,2508}, 3, flg=0...)...
Couldn't talk to repserver on singsing.osf.org: auth ticket expired (dce / rpc)
 
I have also seen a similar thing with the fts tests during the cho runs:
 
Fileset 0,,2393 created on aggregate m1.aggr1 of /.:/hosts/alcatraz
Passed
fldb_InitUbikHandle: ubik client init for /.../cho_cell/fs failed (auth ticket expired (dce / rpc)).
test10: Test case, E11: Failed
 
fts: Missing required parameter '-server'
Passed
Passed
fldb_InitUbikHandle: ubik client init for /.../cho_cell/fs failed (auth ticket expired (dce / rpc)).
test13: Test case, E23: Failed
 
In all cases the cell admin had valid credentials (48 hour lifetime)
So I would guess that it's one of the servers having the problem?
 
Here is how the registry is set up for the cho runs:
 
domain account
change -p dce-ptgt -g none -o none -mcl 2h -mcr 1w
change -p dce-rgy -g none -o none -mcl 0 -mcr 0
change -p hosts/<<<M1>>>/self -g none -o none -mcl 10h -mcr 1w
change -p hosts/<<<M1>>>/cds-server -g subsys/dce/cds-server -o none -mcl 10h -mcr 1w
change -p hosts/<<<M1>>>/gda -g subsys/dce/cds-server -o none -mcl 10h -mcr 1w
change -p hosts/<<<M1>>>/dfs-server -g subsys/dce/dfs-admin -o none -mcl 10h -mcr 1w
change -p hosts/<<<M2>>>/dfs-server -g subsys/dce/dfs-admin -o none -mcl 10h -mcr 1w
change -p hosts/<<<M3>>>/dfs-server -g subsys/dce/dfs-admin -o none -mcl 10h -mcr 1w
change -p hosts/<<<M2>>>/self -g none -o none -mcl 10h -mcr 1w
change -p hosts/<<<M3>>>/self -g none -o none -mcl 10h -mcr 1w
change -p hosts/<<<M4>>>/self -g none -o none -mcl 10h -mcr 1w
change -p krbtgt/<<<CELL_NAME>>> -g none -o none -mcl 0 -mcr 0
 
u
  Authorization Policy:
    Max certificate lifetime:                 2d
    Max renewable lifetime:                   4w
Properties:
    Properties for Registry at:               /.../cho_cell
    Registry is NOT read-only
    Certificates to this server may be generated at any site.
    Encrypted passwords are hidden
    Unix IDs ARE embedded in PGO UUIDs
    Low UID for principal creation:           100
    Low UID for group creation:               100
    Low UID for org creation:                 100
    Maximum possible UID:                     32767
    Minimum certificate lifetime              5m
    Default certificate lifetime              2d
rgy_edit=> view -f cell_admin
cell_admin [none none]:*:100:12::/::
  created by: /.../cho_cell/cell_admin  1993/04/12.15:59
  changed by: /.../cho_cell/cell_admin  1993/04/12.15:59
  password is: NOT valid, was last changed: 1993/04/12.15:59
  Account expiration date: none
  Account MAY be a server principal
  Account MAY be a client principal
  Account is: valid
  Account CAN NOT get post-dated certificates
  Account CAN get forwardable certificates
  Certificates to this service account MAY be issued via TGT authentication
  Account CAN get renewable certificates
  Account CAN NOT get proxiable certificates
  Account CAN NOT have duplicate session keys
  Good since date: 1993/04/12.15:59
  Max certificate lifetime: default-policy
  Max renewable lifetime: default-policy
rgy_edit=> view -f hosts/fire/dfs-server
hosts/fire/dfs-server [subsys/dce/dfs-admin none]:*:110:103::/::
  created by: /.../cho_cell/cell_admin  1993/04/12.16:59
  changed by: /.../cho_cell/hosts/fire/dfs-server  1993/04/12.16:59
  password is: valid, was last changed: 1993/04/12.16:59
  Account expiration date: none
  Account MAY be a server principal
  Account MAY be a client principal
  Account is: valid
  Account CAN NOT get post-dated certificates
  Account CAN get forwardable certificates
  Certificates to this service account MAY be issued via TGT authentication
  Account CAN get renewable certificates
  Account CAN NOT get proxiable certificates
  Account CAN NOT have duplicate session keys
  Good since date: 1993/04/12.17:00
  Max certificate lifetime: 10h
  Max renewable lifetime: 1w

[4/16/93 public]
I can't say what's going on here, except that we're suddenly seeing this
as well (not real often).  It would take some time to reproduce this;
restarting a DFS server (e.g. repserver under a debugger) causes the problem
to go away.  I added Mike Comer to the CC list because he's the expert on
how DFS administrative servers use authentication.
 
The repserver never requires authentication for its incoming RPCs, yet
the call to the repserver's client stubs is returning the auth-ticket-expired
error code.  I only noticed that it was happening when I wasn't running
my client as root, but that's not to say that it couldn't happen in other
circumstances.  Any more information would be conjecture here, but this
didn't happen with build 19, but it does happen with build 21.
I'm tempted to call it a manifestation of a security defect.
Filled in Interest List CC with `cfe, comer, dce-ot-sec' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/19/93 public]
Changed Responsible Engr. from `jsk@transarc.com' to `jaffe@transarc.com'

[4/19/93 public]
I'm not sure what's going on here, either.  The auth ticket expired
error code (unless it's being passed back from within the bowels of
the repserver) should indicate a problem with the client's ticket.
This is independent of whether the RPC is authenticated or not.  If
you chose to send credentials and those credentials are out of date,
you'll get this message.  According to the klist, the identity in
this case is not out of date.  It would have been nice to see the
rest of the klist output to see if the tickets really were expired.

[4/19/93 public]
 
I've also seen this auth ticket expired errors - the messages accompanied
a problem I reported in 7735. I believe the problem is fairly easy to
reproduce with the steps I noted in 7735, ie:
 
fts create
fts setrepinfo ... -release
fts addsite
fts crmount
(write to read-write fileset created)
fts release
fts delete (read-write fileset created)
 
At least I'm 2 for 2 with this set of steps on b23.

[4/20/93 public]
I had just rebooted a b21-based cell with two servers; I tried Gail's
scenario but it worked fine for me.  Two observations:
(1) The repserver will indeed pass back error codes that it gets in
    many cases, including the REP_AllCheckReplicationConfig call that fts
    is making.  Repserver runs as localauth, and for the direct file
    exporter calls, as well as for the ftserver calls, it's careful to
    check for the auth-ticket-expired code and call the
    dfsauth_RefreshLocalAuth() procedure and retry.  It isn't doing this
    for two classes of calls, though:
	(1a) ubik_Call() calls to the flservers, which it doesn't control;
	(1b) calls to peer repservers, where I assumed that they couldn't
	     return this code--an assumption that is probably false.
(2) The message from fldb_InitUbikHandle is from fts when it calls
    ubik_ClientInit(); I have no idea why it would be failing if the
    relevant credentials are not expired.  Was -localauth or -noauth
    being used in the command line to fts?  (In the AUTH_ARG test option,
    presumably.)
 
If Mr. Comer thinks that expired localauth credentials could cause a call
from one repserver to another to return ticket-expired even though the
callee doesn't check authentication, then indeed we'll have to add suitable
code to refresh local authentication to repserver.  But it would also
be appropriate to add similar code to ubik_Call(), so that if it's trying
to run with local authentication, it can properly manage its bindings
and their authentication--both for the exported server procedures and
for the UBIKVOTE_GetSyncSite() call in ubikclient.c.
(As to why all this rigamarole is necessary for every possible localauth
RPC client, I again have no idea.)
 
I expect that Gail's repserver has expired localauth credentials, so that
it gets ticket-expired errors talking to the flservers.  If she were to
restart her repserver, these errors would vanish.
 
The flservers seem to keep their -localauth authentication refreshed in
their mutual communication, where ubik knows that it's running with
-localauth.

[4/21/93 public]
This fix addresses the fts/repserver problem, only.  It shouldn't have
any impact on the fldb_InitUbikHandle failures, which I haven't seen
here.  If the latter problem still exists, please file a new defect
and include all of the klist output.  Thanks.
Defect Closure Form
-------------------
The fix was to add a function to the dfsauth package that would
maintain the localauth context for the rep server.  This function
spawns a thread that sleeps until 2 minutes before the TGT is supposed 
to expire then refreshes the context.  
--Verification procedure below--
Gail's scenario is a good one to reproduce the problem.  Before
running the test, though, the dfs-server principal's (or the cell's)
default certificate lifetime should be changed to something small but
over 2 minutes.  After this time has expired, wait 5 more minutes for
the RPC auth grace period to expire then run Gail's test.  Voila, no
more auth-ticket-expired messages.
I did an addition test of setting a breakpoint in the thread and
making sure that the context update code was being properly called.
Tested on TA build:  
	dfs-102-2.9
Tested with backing build:  
	dce1.0.2b21
Filled in Subcomponent Name with `rep' 
Filled in Affected File with 
 `file/{security/dfsauth/dfsauth_client,rep/rep_main}.c' 
Filled in Transarc Deltas with 
 `comer-ot7745-add-identity-autorefresh-to-localauth-context' 
Changed Transarc Status from `open' to `export'

[4/23/93 public]
I'm concerned about the why/how the 2 minute window was chosen. 
I believe other DCE daemons update at the 1/2 way point. Also, I'm
curious as to what DCE principal the repserver runs as and what
other DFS daemons do about ticket expiration - is there some reason
the repserver is unique? Thanks.

Changed Interest List CC from `cfe, comer, dce-ot-sec' to `cfe, comer, jaffe' 
Changed Responsible Engr. from `jaffe@transarc.com' to `comer@transarc.com'

[4/25/93 public]
The two minutes is to give the repserver enough time to reestablish
its identity.  Sec_clientd uses the minimum of the time remaining
divided by 2 and 10 minutes.  I would really worry about the security
code if they thought they needed 5 hours to refresh an identity:^).
I think 2 minutes should be sufficient but I'd be happy to bump it up
to 10 if you think that would be better.
As for the other DFS servers, they run as the self identity, so they
don't have to worry about this.  I don't know the history behind why
the repserver is different from the other servers.  Can you comment on
this Craig?

[4/26/93 public]
10 minutes sounds like a good compromise to me.  Hope that suits everybody
else.  Thanks!

[4/26/93 public]
submitted today, and also in dfs-102-2.10
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7739
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 7398
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : Acl Test
Short Description             : User "nobody" with id 32767 causes ACL tests to fail on OSF/1
Reported Date                 : 4/16/93
Found in Baseline             : 1.0.2
Found Date                    : 4/16/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : test/file/acl/README.tmp, + others
Sensitivity                   : public
Transarc Deltas               : rajesh-ot7739-acl-test-setup-failure-on-pmax
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/16/93 public]
The elusive bug that prevented ACL tests from running on OSF/1 has
finally been found. The setup phase of ACL tests choose uids for the
test users it creates in the registry by finding the max. of the uids
in the local /etc/passwd file and registry database and using the next
5 succesive numbers.  By default the max user id allowed by the
registry is set to 32767 and the /etc/passwd file on PMAX have a user
nobody with id 32767. This causes ids 32768 to 32772 to be used as user
ids for the 5 tests users needed by ACL tests. The rgy_edit command to 
add a test user fails with the following messages:
(rgy_edit) Unable to add principal  "acluou" - Invalid data - record too long (
Registry Edit Kernel) (dce / sad)
Domain changed to: account
?(rgy_edit) Unable to add account "acluou" - Principal does not exist (Registry
Edit Kernel) (dce / sad)
But the ACL setup code does not check for errors from the rgy_edit
command and continues. (The messages are NOT even in the log as the
test redirects stdout and stderr of the rgy_edit command to
/dev/null).  And since these users do not exist in the registry,
subsequent tests fail.
This problem does not occur on the rios as there is no user in the passwd
file with a 32767 id.
This is almost certainly the problem that Ron Arbo experienced (OT 7398).
Instead of reopening that one, we just opening a new one.
A quick fix is in the pipeline.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/18/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Details of the QUICK fix: The test setup phase now does not consider
32767 while finding the max.  id in the local passwd file. It then
checks if the id of any test user to be created by the ACL tests would
exceed 32767 and if so, it generates an error. The max id allowed by
registry is a configurable parameter in the ACL tests.
The modified tests passed on a pmax client and a rios server with no
regressions.  
Associated information:
Tested on TA build:  dfs-102-2.9
Tested with backing build: dce1.0.2b21 
Filled in Transarc Deltas with `rajesh-ot7739-acl-test-setup-failure-on-pmax' 
Changed Transarc Status from `open' to `export'

[4/20/93 public]
in dfs-102-2.10
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `test/file/acl/README.tmp, + others' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7735
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 2890
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : rel_notes
Short Description             : fts delete not clean
Reported Date                 : 4/16/93
Found in Baseline             : 1.0.2b23
Found Date                    : 4/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2
Affected File(s)              : 1.0.2A Release Notes
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[4/16/93 public]
The Release Notes for 1.0.2 say that deleting a read-write fileset also
deletes any read-only replicas of the fileset. Using b23 in a 2 pmax
cell, fts lsfldb tells me the read-only replica is still there, it's
only when I create a mount point and try to access it that I find out
otherwise. Can we make this a little more clean? ie. have the fldb
reflect the true state (exists or doesn't) of the read-only replica?
 
root@dce12> fts delete -fileset epi.1 -server dce5 -aggr lfs_aggr -localauth
Fileset 0,,7 on aggregate lfs_aggr server dce5 deleted
 
root@dce5> fts lsfldb -fileset epi.1
        readWrite   ID 0,,7  invalid
        readOnly    ID 0,,8  valid
        backup      ID 0,,9  invalid
number of sites: 1
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner
dce5.osf.org        RO       lfs_aggr 0:00:00 hosts/dce5     <nil>
root@dce5> fts crmount -dir /:/epi_1_RO -fileset epi.1.readonly
root@dce5> cd /:/epi_1_RO
dfs: dce errors (code 648998915) from the replication server 130.105.202.25 in cell p102_cell.qadce.osf.org
 
648998915 (decimal), 26aef003 (hex): Given volume is not a replica (dfs / rep)

[4/16/93 public]
This is an essentially cosmetic message; as I understand what's going on
here, the CM then proceeded to allow access to the read-only fileset that
really was on dce5.osf.org.
 
The message should probably be changed to ``Fileset is not managed by
repserver'' or some such.  The release-style replica still exists even
after deleting the R/W fileset, but the cache manager can't find out anything
interesting about it from the repserver.  I'm a little surprised that the
CM thought it could.
 
I believe that the release note mentions that it's only repserver-managed
replicas (all replicas for scheduled replication, all but the staging
replica for release replication) that are deleted when the R/W is deleted.
The replica in question for this OT is in fact the staging replica for
release replication, so it wasn't deleted.
 
I understand that the problem alluded to in the release notes, that
repserver-managed R/O filesets are deleted when the R/W is deleted, still
exists as a problem.
Changed Responsible Engr. from `pakhtar' to `cfe' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'
[4/16/public]
Actually, the CM did NOT proceed to allow access to the read-only fileset -
but other than that, you're on the money.

[4/16/93 public]
OK, I'll take the bait!  What did the CM do instead?
[4/16/public]
 
root@dce5> fts lsfldb -fileset epi.1
        readWrite   ID 0,,7  invalid
        readOnly    ID 0,,8  valid
        backup      ID 0,,9  invalid
number of sites: 1
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner  
dce5.osf.org        RO       lfs_aggr 0:00:00 hosts/dce5     <nil> 
root@dce5> fts crmount -dir /:/epi_1_RO -fileset epi.1.readonly
root@dce5> cd /:/epi_1_RO
dfs: dce errors (code 648998915) from the replication server 130.105.202.25 in cell p102_cell.qadce.osf.org
 
(code is for "Given volume is not a replica")

[4/16/93 public]
I read that report about code 648998915 from the earlier part of the report.
What happened after that?  What error (if any) was reported?  Anything
on the machine console?
[4/16/public]
Nada - nothing on the console, nothing in the var/dfs/adm/*Logs. Unfortunately,
I didn't do an lsheader before cleaning up after this. I'll try the steps
again to see how reproducable this is.

[4/16/93 public]
So the CM just hung your shell, so it never gave a prompt?
Had you done ``fts release'' on this fileset?  That is, had there ever been
an epi.1.readonly fileset?  (Regardless of what the fldb says?)  Thanks.
[4/16/public]
The CM didn't hang my shell, I just wasn't allowed to cd to the mount point
for the read-only fileset. I'm afraid I did leave out some relevant details
though - upon retrying all this with epi.2 :
root@dce5> fts create epi.2 -aggr lfs_aggr -server dce5
        readWrite   ID 0,,13  valid
        readOnly    ID 0,,14  invalid
        backup      ID 0,,15  invalid
number of sites: 0    number of addresses: 1
   server           flags     aggr   siteAge principal      owner               
dce5.osf.org        RW       lfs_aggr 0:00:00                <nil>               
Fileset 0,,13 created on aggregate lfs_aggr of dce5
root@dce5> fts setrepinfo -fileset epi.2 -release
fts setrepinfo: Using default value for maxage of 2:00:00
fts setrepinfo: Using derived value for failage of 1d0:00:00
fts setrepinfo: Using default value for reclaimwait of 18:00:00
root@dce5> fts addsite epi.1 -server dce5 -aggr lfs_aggr
There is already a R/O replica on dce5.osf.org (lfs_aggr).
Error in addsite: status ffffffff (unknown facility) (unknown)
root@dce5> fts addsite epi.2 -server dce5 -aggr lfs_aggr
Added replication site dce5 lfs_aggr for fileset epi.2
root@dce5> fts crmount -dir /:/epi_2 -fileset epi.2
root@dce5> mkdir /:/epi_2/dir1
root@dce5> cd /:/epi_2/dir1
root@dce5> /dcetest/dcelocal/systest/file/filewnr
dce5_497_WNR : PASSED
root@dce5> fts release epi.2
(Warning) DeleteVolume: 0,,14 doesn't exist.  Ignoring...
Released fileset epi.2 successfully
root@dce5> fts lsheader -aggr lfs_aggr -server dce5
Total filesets on server dce5 aggregate lfs_aggr (id 1): 4
epi.1                    0,,10 RW    106 K alloc    106 K quota On-line
epi.2                    0,,13 RW      8 K alloc    106 K quota On-line
epi.2.readonly           0,,14 RO    106 K alloc    106 K quota On-line
root.dfs                 0,,1 RW     17 K alloc     17 K quota On-line
Total filesets on-line 4; total off-line 0; total busy 0
root@dce5> fts delete epi.2
fts: Missing required parameter '-server'
root@dce5> fts delete epi.2 -server dce5
fts: Missing required parameter '-aggregate'
root@dce5> fts delete epi.2 -server dce5 -aggr lfs_aggr
Couldn't talk to repserver on dce5.osf.org: auth ticket expired (dce / rpc)
Fileset 0,,13 on aggregate lfs_aggr server dce5 deleted
Note the DeleteVolume warning and the auth ticket expired messages - I saw
these exact same messages with the epi.1 scenario. I am logged in as
cell_admin and my credentials are NOT expired - your thoughts on this
Hobson?

[4/16/93 public]
CORRECTION: in both cases (original epi.1 case and current epi.2 case), I AM
able to cd to the read-only mount point. So, this request is a cosmetic one,
lowering the priority.
problem

[7/29/93 public]
After talking to Craig and Elliot about this issue, here is what we have 
decided: The heart of the problem, deleting a rw fileset should not 
delete its read-only replicas, will be fixed among other replicas-related 
works in ot2890. 
                                                                             
Until the enhancement is done, deleting a RW fileset will also deletes 
its read-only replicas as described in 102 Release Notes. However, the Release
Notes failed to mention that the status flag regarding the read-only 
replicas stored in FLDB is not valid any more. 
Change this defect to a DOC defect and change the 102 document to describe the 
situation. 
Changed CR in Code, Doc, or Test? from `code' to `doc' 
Filled in Inter-dependent CRs with `2890' 
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Filled in Subcomponent Name with `rep' 
Filled in Interest List CC with `tu@transarc.com, cfe@transarc.com, 
 khale@transarc.com' 
Changed Responsible Engr. from `cfe' to `jeff@transarc.com'

[8/9/93 public]
I included the following text in the DCE 1.0.2A Release Notes:
 
   "The DCE 1.0.2 Release Notes for the State of the DCE DFS Code report that
    if you remove the read/write version of a DCE LFS fileset, the appropriate
    Replication Servers automatically remove all read-only versions of the
    fileset. This information is still true. However, it should also be noted
    that the status flag for the read-only replicas of the fileset in the
    fileset's FLDB entry remains valid, even though all read-only replicas
    have been removed. The status flag continues to be valid because the
    replication sites are still defined, not because replicas exist at those
    sites."
 
The information in this note was verified by Tu here at Transarc.  Larry
Kaplan at OSF graciously agreed to incorporate the text in the 1.0.2A
Release Notes.  This one can now be closed.
Changed Subcomponent Name from `rep' to `rel_notes' 
Changed Interest List CC from `tu@transarc.com, cfe@transarc.com, 
 khale@transarc.com' to `tu@transarc.com, cfe@transarc.com, khale@transarc.com, 
 kdu@transarc.com' 
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `1.0.2A Release Notes' 
Changed Transarc Status from `open' to `closed'

[08/17/93 public]
Closed bug.



CR Number                     : 7732
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dce_config does not set up bak/bakserver
Reported Date                 : 4/15/93
Found in Baseline             : 1.0.2
Found Date                    : 4/15/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : dce_config
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[4/15/93 public]
This defect is open to track the change necessary to dce_config to support 
installation and configuration of the bakserver.  Bakserver is a ubik server
just like flserver, so some of the same gyrations must be used to make sure
that it gets started cleanly.
Ron, the file is in /afs/transarc.com/public/jaffe/dce_config.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/26/93 public]
Incorporated Elliot's changes to dce_config with minor changes.  Fixed.

[12/17/93 public]
Closed.



CR Number                     : 7720
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : got me
Short Description             : anonymous user id should be -3, not -2
Reported Date                 : 4/14/93
Found in Baseline             : 1.0.2
Found Date                    : 4/14/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/fshost/fshs_prutils.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7720-change-anon-to-minus-3
Transarc Herder               : jaffe@transarc.com

[4/14/93 public]
In the DFS section of the Admin Guide, in the chapter "Using 
ACLs and Groups", under the heading "ACL Entry Types for 
Unauthenticated Users, bullet 1 Note, the principal id for 
the anonymous user is shown as -3, it should be -2.
Also, the Note says "A system administrator must use the rgy_edit
command provided with the DCE Security Service to create the
principal anonymous in the Registry Database of the local cell."
This should be stated as a recommendation not a requirement. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/15/93 public]
This defect is reassigned as a code bug.  The code needs to be changed to
match the documentation.
Changed CR in Code, Doc, or Test? from `doc' to `code' 
Changed Subcomponent Name from `admin_gd' to `got me' 
Changed Short Description from `anonymous user id should be -2 not -3' to 
 `anonymous user id should be -3, not -2' 
Filled in Interest List CC with `jeff@transarc.com' 
Changed Responsible Engr. from `jeff@transarc.com' to `kazar@transarc.com' 
Filled in Transarc Status with `open'

[4/22/93 public]
Defect Closure Form
-------------------
Verified that files created by unathenticated requests are
owned by uid -3 and gid -3.  Also verified that connectathon
still ran fine as anonymous user.
Changed Responsible Engr. from `kazar@transarc.com' to `demail1!carl' 
Changed Resp. Engr's Company from `tarc' to `ibm' 
Filled in Transarc Deltas with `cburnett-ot7720-change-anon-to-minus-3' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/fshost/fshs_prutils.c' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7717
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : flserver
Short Description             : flserver logging prints zero values for delete site operation
Reported Date                 : 4/14/93
Found in Baseline             : 1.0.2
Found Date                    : 4/14/93
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/flserver/flprocs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-ot7717-flserver-correct-altersite-log-msg
Transarc Herder               : jaffe@transarc.com

[4/14/93 public]
Build: 23
Configuration: standard cho
The flserver seems to be looping infinitely during test13 of the fts tests.
Here's the flLog:
ta 0
93-Apr-14 09:45:30 flserver: created site fffffff0, principal 'trouble.240', quota 0
93-Apr-14 09:46:46 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 09:48:32 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 09:50:53 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 09:52:31 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:09:48 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:11:52 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:13:18 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:14:52 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:17:18 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:21:53 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:24:23 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:25:52 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:27:29 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:29:21 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:31:58 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:33:11 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:34:55 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:36:05 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:38:35 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:40:11 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:42:09 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:44:44 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:46:17 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:47:51 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:49:27 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:51:50 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:53:14 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:54:45 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:56:29 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 10:58:59 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:00:47 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:02:01 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:03:37 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:05:44 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:07:36 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:09:14 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:10:47 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:12:21 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:16:29 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:18:29 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:19:23 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:21:25 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:23:32 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:25:16 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:26:56 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:28:52 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:30:47 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:32:19 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:34:26 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:36:23 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:38:28 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:41:08 flserver: Deleted site 0, principal '', quota 0
93-Apr-14 11:43:04 flserver: Deleted site 0, principal '', quota 0
# ps -deaf | fgrep flserver             
    root 19512 19255   0   Apr 13      - 38:19 /opt/dcelocal/bin/flserver 
# 
# ps -deaf | fgrep fts
    root  5689 19255   0   Apr 13      -  1:18 /opt/dcelocal/bin/ftserver 
    root 17713 24084   0 11:51:37  pts/0  0:00 fts delserverentry -server 255.255.255.162 -verbose
Changed Responsible Engr. from `jsk@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/14/93 public]
Actually, I think this is a bug in the flserver logging.  It looks like
vl_AlterSite bzero's DescBuff and then tries to print its values out
to the log.

[4/19/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Do
1. fts crserver <server name> <server principal>
2. fts delserver <server name>
The FlLog should now contain two entries, one for the server entry created,
and one for the server entry deleted. The server deletion entry in the log
should contain meaningful values for the server address, principal name
and quota instead of all zeros. 
Associated information:
Tested on TA build:  
dfs-102-2.9
Tested with backing build:  
dce1.0.2b21
Filled in Transarc Deltas with 
 `vijay-ot7717-flserver-correct-altersite-log-msg' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/flserver/flprocs.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7716
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : util
Short Description             : relative dates in dump schedules sometimes handled incorrectly
Reported Date                 : 4/14/93
Found in Baseline             : 1.0.2
Found Date                    : 4/14/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot7716-relative-dates-in-dump-schedules-incorrect
Transarc Herder               : jaffe@transarc.com

[4/14/93 public]
   There is a problem with relative dates associated with dump schedules
such that if you don't include a day ("d") component with the relative date,
the expiration date put on the tape might be incorrect (+~17 yrs. on AIX),
due to the use of an uninitialized variable (reltime) in the routine:
util/kreltime.c:Add_RelDate_to_Time.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/20/93 public]
As part of the OT sweepting task, assign this to  Abhijit.
Changed Interest List CC from `vijay@transarc.com,khale@transarc.com' to 
 `vijay@transarc.com, davecarr@transarc.com' 
Changed Responsible Engr. from `davecarr@transarc.com' to `khale@transarc.com'

[7/23/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
Perform the suggested dump without the day component in the date and 
then check the date on the tape. 
Do the same test after the fix. 
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-103-3.20
Tested with backing build:  
 dce1.0.2b6
Filled in Transarc Deltas with 
 `khale-ot7716-relative-dates-in-dump-schedules-incorrect' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7715
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : cm sysname examples for AIX 3.2 incorrect
Reported Date                 : 4/13/93
Found in Baseline             : 1.0.2
Found Date                    : 4/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[4/13/93 public]
In both the DFS Admin Guide section and the Admin Ref man page
for the cm sysname command, the examples for AIX 3.2 are wrong.
The current examples show that when you run "cm sysname" on
an AIX 3.2 machine the output is "Current sysname is 'aix32_rs';
the output actually is 'rs_aix32'.  
All references to aix32_rs should be changed to rs_aix32.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/30/93 public]
Filled in Fix By Baseline with `1.0.3' 
Filled in Transarc Status with `open'

[9/17/93 public]
Filled in Affected File with `See description' 
Changed Responsible Engr. from `jeff@transarc.com' to `kdu@transarc.com'

[9/23/93 public]
The following files are affected:
	admin_gd/dfs/dfs/2_issues_dfs.gpsml
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_ref/man8dfs/bak_dump.8dfs
	admin_ref/man8dfs/cm_sysname.8dfs
	admin_ref/man8dfs/fts_addsite.8dfs
	admin_ref/man8dfs/fts_lsreplicas.8dfs
	admin_ref/man8dfs/fts_release.8dfs
	admin_ref/man8dfs/fts_rename.8dfs
	admin_ref/man8dfs/fts_rmsite.8dfs
	admin_ref/man8dfs/fts_setrepinfo.8dfs
	admin_ref/man8dfs/fts_update.8dfs
	admin_ref/man8dfs/upclient.8dfs
	admin_ref/man8dfs/upserver.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 7711
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ftserver
Short Description             : missing return in AclHashVal() causes core dump
Reported Date                 : 4/13/93
Found in Baseline             : 1.0.1
Found Date                    : 4/13/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : ftserver/ftserver_procs.c
Sensitivity                   : public
Transarc Deltas               : cburnett-ot7711-missing-aclhashval-return
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/13/93 public]
AclHashVal() in ftserver_dump.c is missing a return statement.  The
result returned can be random stuff on the stack and sometimes 
causes the ftserver to core dump with a segmentation violation
on fts moves and dumps.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/13/93 public]
Defect Closure Form
-------------------
Moves complete successfully now without ftserver dumping core.
Tested on a 2.8+ DFS.
Filled in Transarc Deltas with `cburnett-ot7711-missing-aclhashval-return' 
Changed Transarc Status from `open' to `export'

[4/15/93 public]
imported into dfs-102-2.9b22
Filled in Affected File with `ftserver/ftserver_procs.c' 
Changed Transarc Status from `export' to `import'

[4/15/93 public]
submitted on 4/15/93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7709
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : read permission denied
Reported Date                 : 4/13/93
Found in Baseline             : 1.0.2
Found Date                    : 4/13/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : unknown by Transarc
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[4/13/93 public]
BUILD:	weekly build 23
CONFIG:	2 machine cell, 1 pmax, 1 rios
	pmax = fl server, root.dfs in lfs
	rios = core server and fl server, epi.1 in lfs
TEST:	dfs.lock - multiple copies of filewnr process request exclusive
	write and shared read locks concurrently - all as same principal.
NOTE:	It is not possible that the credentials expired since credentials
	are new with each invocation of the filewnr process via
	dce_login -exec filewnr syntax, and new credentials are good for
	10 hours in this cell. Test failed in a window of 2 minutes.
DATA:	
MACHINES="valentine cobbler"
DFS_PATH="/:/epi_2"
PRINC=gmd
PW=blah
NUMFILEWRITES=1000
NUMPROCPERMACH=2
SLEEPRANGE=5
DETAILS:
Test was run on pmax, fileset resides on rios. Test ran successfully for
5 iterations when, inexplicably, it failed with 
cobbler_10609_WNR: open failed
cobbler_10609_WNR: Permission denied
cobbler_10609_WNR: using blocksize 8193.
cobbler_10609_WNR: using file /:/epi_2/lock_run1566/file.
cobbler_10609_WNR: using 1000 blocks.
cobbler_10609_WNR: Using pattern 6.
cobbler_10609_WNR: Using read_only mode.
cobbler_10609_WNR: Requesting SR lock.
cobbler_10609_WNR: DCE principal = gmd
cobbler_10609_WNR : 0 successful writes
cobbler_10609_WNR : 0 successful reads
cobbler_10609_WNR : 0 successful compares
cobbler_10609_WNR : 1 total error count
cobbler_10609_WNR : FAILED
while 3 concurrent processes (including one on cobbler, the rios) succeeded
with the exact same parameters:
cobbler_23661_WNR: using blocksize 8193.
cobbler_23661_WNR: using file /:/epi_2/lock_run1566/file.
cobbler_23661_WNR: using 1000 blocks.
cobbler_23661_WNR: Using pattern 6.
cobbler_23661_WNR: Using read_only mode.
cobbler_23661_WNR: Requesting SR lock.
cobbler_23661_WNR: DCE principal = gmd
cobbler_23661_WNR : 0 successful writes
cobbler_23661_WNR : 1001 successful reads
cobbler_23661_WNR : 1001 successful compares
cobbler_23661_WNR : 0 total error count
cobbler_23661_WNR : PASSED

[4/15/93 public]
Gail, I reassigned this to you so I could ask a question or two, which is
whether we could know the C code for this test case.  Is it in some part
of the tree where we should be able to find it?  If not, could you tell us
how to see a copy of it?  Thanks.
Filled in Interest List CC with `pakhtar, cfe' 
Changed Responsible Engr. from `pakhtar' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/15/93 public]
Craig - the dfs.lock script and filewnr program are in the src/test/systest/file
directory. As noted in the 102 release notes, the test is currently lacking in
concurrency control so the NUMFILEWRITES must be large enough to ensure that
one filewnr process can not complete before another starts. If this occurs,
the script will report false failures. In this case, however, the failure is
a permission denied error, not the false failure case that 2 exclusive
write locks appear to have been granted concurrently.

[4/19/93 public]
In an attempt to reproduce this, I hit 7475. I often hit time out problems
which may or may not account for this one. I bumped 7475 to 0 so I'm dropping
this to a 1 so that these get worked on in the right order.

[4/27/93 public]
I did manage to reproduce this - along with hitting 7808 - in a 2 pmax test.
The "fail" log above includes the DCE principal based on info returned by
sec_login_inquire_net_info() from within the filewnr program that is trying
to open() the file for read. I'm beginning to suspect that dfsbind is dying
in the midst of this test, causing the file access to be seen as "nobody"
and therefore being refused read permission - I'll watch for that.
This wouldn't explain the concurrent accesses on cobbler succeeding though.

[5/4/93 public]
Changed Responsible Engr. from `kazar' to `kazar@transarc.com'

[5/18/93 public]
Can you please retest this on the new dfs-carl drop.  There have been a
number of significant changes which should fix this problem.  We cannot
make any more progress given the current information.
Changed Responsible Engr. from `kazar@transarc.com' to `gmd'

[5/18/93 public]
Please include the CR #s so I can understand exactly what
you believed was going on and was fixed - it helps immensely.
Just curious - have you run the test successfully there? Thanks.

[6/1/93 public]
We were able to successfully reproduce this until now, however it seems to
have been fixed.  Please retest after picking up our latest submissions
(you should probably wait until we submit the acl changes to be sure the
fix is in).
Changed Responsible Engr. from `tu@transarc.com' to `gmd@osf.com' 
Changed Resp. Engr's Company from `tarc' to `osf'

[6/1/93 public]
Any idea which deltas are related? This was believed to be related to security
problems which are yet to be fixed so I'd like to know what the real problem/fix
turned out to be - Thanks.

[6/21/93 public]

Closing after a short but successful run. Delays in verifying this fix
were due to build problems and waiting for an enhancement to the dfs.lock
script which allows it to control concurrency much better.



CR Number                     : 7696
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd
Short Description             : Document how /: works
Reported Date                 : 4/13/93
Found in Baseline             : 1.0.2
Found Date                    : 4/13/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : admin_gd/dfs/dfs/1_overview_dfs.gpsml
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[4/13/93 public]
A note from Howard's FAQ that should be incorporated in the
DFS section of the Admin Gd (Jeff, if you think this belongs
somewhere else, feel free to change the CR).
From: weisman@osf.org
To: twd@cs.brown.edu (Tom Doeppner)
Cc: melman@osf.org, walt@osf.org, weisman@osf.org
Subject: DCE: What handles /:?
Date: Fri, 20 Nov 92 10:20:54 -0500
>I'm almost finished with the final version of the internals course,
>but one thing still bothers me.  Who sets up "/:"?  Presumably it is
>a symbolic link to /.../local_cell_name/fs.  Will this link be somehow
>created on all official, DCE-certified workstations?
This is done internally in the CDS pathname parser.  When it recognizes
"/:" it maps to "/.:/fs".  This enables commands like acl_edit and cdscp to
locate DFS.  To see this and much much more, take a look at
cds/library/dnscvtcdsfull.c.
There is also a symbolic link contained in the local file system.  This is
installed by dce_config:
	ln -s $cellname/fs /: >/dev/null 2>&1
This is how <ls> will locate the fs junction point.

[4/13/93 public]
Fixed my username so the defect was assigned to me.
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Changed Short Description from `how /: works' to `Document how /: works' 
Changed Responsible Engr. from `jeffk' to `jeff@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `open'

[6/22/93 public]
Changed "Fix By Baseline" from 1.0.3 to 1.1.

[7/30/93 public]
Changed Fix By Baseline from `1.1' to `1.0.3'

[10/15/93 public]
Additional text describing the functionality, definition, and restrictions of
the /: prefix (and, in some cases, the /.: prefix) was included in the chapter
of the DFS Administration Guide named 1_overview_dfs.gpsml.  Additional text
clarifies how the prefix is created and what commands rely on it; modified
text better describes restrictions associated with the prefix.  The text was
determined and verified by Beth Bottos, Mike Comer, and Pervaze Akhtar.  This
one, I believe, can be closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `admin_gd/dfs/dfs/1_overview_dfs.gpsml' 
Changed Transarc Status from `open' to `closed'

[11/15/93 public]

Verified change in latest doc build and closed this CR.



CR Number                     : 7691
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : DFS stale host --- code=572616705
Reported Date                 : 4/13/93
Found in Baseline             : 1.0.2
Found Date                    : 4/13/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : cm_server.c, + others
Sensitivity                   : public
Transarc Deltas               : tu-ot7691-gettime-gets-stalehost
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/13/93 public]
A DFS client(jolt) got stale return(FSHS_ERR_STALEHOST) on AFS_GetTime call, 
and then lost communication with its DFS server for 10 minutes.
The following "dfstrace dump -follow cmfx -sleep 5" log shows:
	1. There is no retry on stale host error.
	   "The cm should try to establish a new context with 
	   the DFS server." --- Tu's comment from OT 7475 
	2. The interval between two AFS_GetTime's is about 600 seconds. 
	 
	   Normally, the gap is about 90 seconds.
Test config:
	DCE : built over last weekend(B23 ???)
	santa/PMAX : DFS server, DCE client
	jolt/PMAX : DFS client, DCE server
*** the trace has been moved to ~notuser/dfstrace.stalehost.log ***

[4/13/93 public]
This is the defect that was originally reported in ot7475. 
Filled in Subcomponent Name with `cm' 
Changed Interest List CC from `delgado, rsarbo, hathaway, davel' to `delgado, 
 rsarbo, hathaway, davel, jsk, kazar' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/14/93 public]
So the idea is that the CM's connection to the file exporter grows stale
because it waited too long to ping the server?  (The interval from the
time 11.xxxx to time 611.xxxx above.)  One simple cause for this might be
that TKN_InitTokenState doesn't store the host lifetime in the cell
serverp->hostLifeTime, but rather the hostRPCGuarantee, and this might
have been set to a larger value in the file server being used here.
What were the parameters to the ``fxd'' command line?  Did fxd print any
warnings?
Changed Interest List CC from `delgado, rsarbo, hathaway, davel, jsk, kazar' to 
 `delgado, rsarbo, hathaway, davel, jsk, kazar, cfe' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `open'

[4/15/92 public]
There is no input for maxLife according to the following command line
inside /etc/rc.dfs:
$DCELOCAL/bin/fxd -mainprocs 7 -admingroup subsys/dce/dfs-admin
Normally, if idle, the pinging interval is ~90 seconds according to
my continuous dfstrace. 
I got an impression that this bug tend to happen in the first day after 
I re-configure/re-start the DFS server(santa) and/or the client(jolt).
Yesterday, I rebooted the client and haven't got any timeout yet for about
24 hours according to the continuous dfstrace. Now, I want to reboot the 
server and see if I can re-produce it.

[4/15/93 public]
Well, my last edit got a disk full error. As a result, some of the infomation 
got truncated. Here, I am trying to remedy this by putting the old info
back. 
Sorry for the inconvenience !
The delta is to make sure that the PingServer function will retry the 
AFS_GetTime call upon receiving a stale host error from the fx server rather
than mark the server down. 
However, the delta does fix the "10 minites gap" problem observed by Hsiao.
The "10 miniutes gap" problem was, if occurred,  that all CM's background 
daemons stop working for 10 mins and then resumed their operations. 
My only conjecture is that the TPQ package who schedules all deamons'
activities may have a problem to wake up. ALso discussed it with Craig and
Comer. 
Anyhow, this delta is available to submit to fix the obvious problem. 
The "10 minutes gap problem", if we have more information later, should be
addressed in a separate ot. 
Defect Closure Form
-------------------
--Regression procedure--
I ran low and fs test to make sure there is no new problem introduced. 
--Verification procedure below--
It is hard to produce the exact scenario. I did the test through kdbx. That
is, I used kdbx to create the situation and tested it. 
Associated information:
Tested on TA build:  dfs-102-2.8
Tested with backing build:  dce1.0.2b19
Filled in Transarc Deltas with `tu-ot7691-gettime-gets-stalehost`

[4/15/93 public]
Since my dfstrace log is too big, I decided to move it to a file called:
	~notuser/hsiao/dfstrace.stalehost.log

[4/16/93 public]
I got a trace at the server side when I got timeout on "ls" at a 
DFS client, dce10. The following is a little piece of it and a more
detailed on is ~notuser/hsiao/dfstrace.stale.trace.at.server.
time 852.296247, pid 324: fshs_HostCheckDaemon ..
time 852.296247, pid 324: fshs_Enumerate START
time 852.296247, pid 324: fshs_CheckHost START c2b3e000
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e088
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e110
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e198
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e220
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e2a8
time 852.296247, pid 324: CheckHost: stale princ c2b5f1b0 ref 0
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e330
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_CheckHost START c2b3e3b8
time 852.296247, pid 324: fshs_CheckHost END
time 852.296247, pid 324: fshs_UpdateHostList called
time 852.296247, pid 324: fshs_Enumerate END
time 924.745476, pid 0: GetTime
...
time 1004.737705, pid 0: GetTime
time 1004.741611, pid 0: fshs_GetPrincipal START
time 1004.741611, pid 0: fshs_GetHost, cookie c2b53200
time 1004.741611, pid 0: fshs_FindHost, cookie c2b53200
time 1004.741611, pid 0: find a prime host c2b3e3b8
time 1004.741611, pid 0: GetTime returns 572616705
time 1014.796298, pid 0: GetTime
time 1014.800204, pid 0: fshs_GetPrincipal START
time 1014.800204, pid 0: fshs_GetHost, cookie c2b53200
time 1014.800204, pid 0: fshs_FindHost, cookie c2b53200
time 1014.800204, pid 0: find a prime host c2b3e088
time 1014.800204, pid 0: find a host in fast path c2b3e088
time 1014.800204, pid 0: fshs_FindPrincipal ..
time 1014.800204, pid 0: found a princ c2b5f000 ref 1
time 1014.800204, pid 0: find a princ (fast path) c2b5f000, ref 1
time 1014.800204, pid 0: fshs_GetPrincipal END c2b5f000, ref 1
time 1014.800204, pid 0: fshs_PutPrincipal c2b5f000 ref 1
time 1014.800204, pid 0: GetTime returns 0
time

[4/16/93 public]
Not sure about the significance of this trace. Apparantly, GetTime call
from host c2b3e3b8 got a stale host error and the other GetTime call from 
host c2b3e088 was fine. Well, Maybe I should take a look at the whole portion 
of the dftrace.

[4/21/93 public]
imported into 2.9b23, testing in CHO and submitted on 4/21
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `cm_server.c, + others' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7678
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts move can't handle wrong boot time error
Reported Date                 : 4/8/93
Found in Baseline             : 1.0.2
Found Date                    : 4/8/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot7678-reset-connections-on-errors
Transarc Herder               : shl@transarc.com
Transarc Status               : submit

[4/8/93 public]
during fts move, the target server crashed and was rebooted.  The fts move
command on the client is looping infinitely with the following error:
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
FTSERVER_SetStatus for trans 46 (mask=40000040) failed
Error: wrong boot time (dce / rpc)
I don't think the correct behaviour for this is to loop infinitely
retrying and getting the same error.  It's not as if this error
situation can be corrected on the server side (eg correcting the boot time).

[4/8/93 public]
Changed Responsible Engr. from `jsk@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `shl@transarc.com' 
Added field Transarc Status with value `open'

[4/8/93 public]
How often are the messages occurring?  About every 30 seconds?

[4/9/93 public]
Altered all ftserver calls to go through the volc_volsint.c interface
module; most were, but some weren't.  In that module, whenever we get an
RPC error (including rpc_s_wrong_boot_time) we call rpc_binding_reset()
to allow further calls to get through.
 
Defect Closure Form
-------------------
--Verification procedure below--
To regression-check, run the fts tests.
Functional checks are difficult because it is hard to set up an environment
in which the ftserver connections will go bad with RPC errors, and most
such situations will leave orphaned work on the ftserver, since the
ftserver's ``transaction'' state is stored only in kernel memory.
Code inspection will show that the rpc_binding_reset() call will be made
on any RPC error from an ftserver call, though, so the error reported here
won't happen.
 
Associated information:
 
Tested on TA build:  dfs-102-2.8
Tested with backing build:  dce1.0.2b19
Filled in Transarc Deltas with `cfe-ot7678-reset-connections-on-errors' 
Changed Transarc Status from `open' to `export'

[4/12/93 public]
Changed Transarc Status from `export' to `import'

[4/15/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7676
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : assertion failure in efs_volops.c line 2530
Reported Date                 : 4/8/93
Found in Baseline             : 1.0.2
Found Date                    : 4/8/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : jdp-ot6363-increase-quota-before-clone-operation, jdp-db3331-better-clone-cleanup, ota-db3388-bad-cleanup-when-deleting-non-empty-volume
Transarc Herder               : shl@transarc.com
Transarc Status               : submit

[4/8/93 public]
Build B22 plus fix for 7608
Configuration:
m1 - rios, core server + fldb +dfs server + dfs client, exports lfs, jfs
m2 - rios, core client + fldb + dfs server + dfsclient
m3 - pmax, core client + fldb + dfs server + dfsclient, exports lfs, ufs
m4 - pmax, core client, dfs client.
I was moving a fileset back and forth between m1 and m3 while running
low tests in moderate mode from m1.  At the same time but on a the
aggregate I was doing 'fts zap' commands to get rid of some move-temp
filesets that were residuals from old failed moves (different fileset).
I hit an assertion failure in efs_volops while doing the fts zap on m3
and at the same time the moving fileset was being moved over to m3 from
m1 (but on the same aggreate).
Here's the code around the assert with line numbers (2530 is the failed
assert).
  2525                  code = vol_efsTwiddleInconBit(bvolp, 0);
  2526          }
  2527      }
  2528
  2529  out:
  2530      afsl_PAssert (ap == NULL, ("ap not 0; reclone = %d", doReClone));
  2531      afsl_PAssert (b == NULL, ("b not 0; reclone = %d", doReClone));
  2532      afsl_PAssert (bc == NULL, ("bc not 0; reclone = %d", doReClone));
  2533      afsl_PAssert (volB == NULL, ("volB not 0; reclone = %d", doReClone));
Changed Responsible Engr. from `jsk@transarc.com' to `bwl@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `shl@transarc.com' 
Added field Transarc Status with value `open'

[4/8/93 public]
Filled in Interest List CC with `cfe@transarc.com'

[4/8/93 public]
Craig claims that this is Ted's code, so putting Ted on Interest list.
The following code in SomeClone can fall through to the assertion that ap
is null, without clearing ap:
    if (ap) {
        /* found an anode so {re,un}clone it */
        code = epiv_OpenBackingAnode (ap, &volB, &bc);
        afsl_MBZ (code);
        /* For regular volume ops we require the following relationship to be
         * true.  If this volume pair is bogus we fail.  If the volumes are ok
         * but the index doesn't match we could have serious problems and so
         * panic. */
        if (volB != bvolh) {
            (void) epiv_CloseAnode (bc);
            bc = 0;
            (void) epiv_Close (buffer_nullTranId, volB);
            volB = 0;
            code = EINVAL;
            goto out;
        }
        .
        .
        .
    }
The comment says that, indeed, we are expected to panic.  Is it conceivable
that this is the cause of the panic?
Changed Interest List CC from `cfe@transarc.com' to `cfe@transarc.com, 
 ota@transarc.com'

[04/08/93 public]
It's happened again.  I'll leave it overnight like this.
Please tell me what info you'd like to see (structures, etc.).
Note that the stack trace isn't useful because the routines
are declared private and we can't tell where we are.
I'll append the ot with the octal dump of the process
stack later.

[4/12/93 public]
I didn't add these asserts but clearly they are there to prevent leaking
anode handles of the various participants.
The PAssert should generate some output before panicing, but I don't see
that info in the report above?  Do we know if this is an unclone?
If things were happening as Diane describes it shouldn't matter that the
zap and moves were concurrent.  I assume that by different filesets we
mean that the zap and move targets have different R/W fileset ids and
that they have *NO* relationship to one another.  Thus either the move
target was being restored into (no obvious reason to be in efsSomeClone)
or the zap failed.
It would be really useful to know the involved fileset ids.  It would
also be very handy to know the index of ap, (the local variable "index",
or voldata->vld_lastIndex, or ap->disk->index).  Also unless it is
really large it might be useful to save the offending aggregate.
If the code fragment Bruce includes is at fault it indicates a very
strange state of affairs.  Namely the backing anode isn't in the fileset
it is supposed to be in.
Changed Interest List CC from `cfe@transarc.com, ota@transarc.com' to `cfe, 
 ota, jdp'

[4/12/93 public]
I think that Jeff and I convinced ourselves that this ``very strange'' state
of affairs might have occurred if the aggregate had run out of space while
making a clone for the fileset move, where the ftserver's cleanup action
really hadn't destroyed the clone fileset (due to the problems discovered
last week).  If that R/W fileset had then been cloned again (successfully,
after space had been made available on the aggregate), the backing anode
might be in the not-really-destroyed fileset.  Seemed plausible at the time.

[04/13/93 public]
Here is a different instance where the same assertion failure occurred.  I have
an lfs fileset which somehow during the course of cho got in a weird state
such that I got EIO whenever I accessed some of the files in it (not all the
files in this fileset had this problem.) I did a dfsexport -detach, then ran
salvage which said it had repaired some inconsistencies, re-dfsexported it
and then deleted and zap'd filesets on this aggregate and got the assertion failure
during fts zap.
root@fire  # dfsexport -detach /dev/rz1d
dfsexport: Revoking tokens for filesets on aggregate 3...
root@fire  # dfsexport
dfsexport: /dev/rz1b, ufs, 2, 0,,1645
dfsexport: /dev/rz1f, lfs, 4, 0,,0
root@fire  # salvage /dev/rz1d
Salvaging /dev/rz1d
Will run recovery on /dev/rz1d
In 5,67 at m3.lfs1.backup:/test/low.rios/prog14
  Directory entry contains invalid fid
  -> Removing directory entry
  Directory in backing fileset unrepairable
  in anode (#2)
    Volume is damaged
    -> Marked as inconsistent
In volume m3.lfs1 0,,10 (avl #6)
  in anode (#2)
    Volume is marked as inconsistent, not walked
In volume m1.lfs2.move-temp 0,,1264 (avl #11)
  in anode (#2881)
    Volume is marked as inconsistent, not walked
oughtRestore m3.lfs1.backup:/test/low.rios/prog14 volume index: 5 anode index 67
Processed 6 vols 500 anodes 31 dirs 416 files 0 acls
Done.  Some inconsistencies repaired salvaging /dev/rrz1d
root@fire  # dfsexport /dev/rz1d
root@fire  # fts delete -file  m3.lfs1.backup -server /.:/hosts/fire -aggr m3.aggr1
Fileset 0,,12 on aggregate m3.aggr1 server /.:/hosts/fire deleted
root@fire  # fts delete -file m1.lfs2.move-temp -server /.:/hosts/fire -aggr m3.aggr1
Could not fetch FLDB entry (name='m1.lfs2.move-temp')
Error: FLDB: no such entry (dfs / vls)
root@fire  # fts zap -ft 0,,1264 -server /.:/hosts/fire -aggr m3.aggr1
Connection closed by foreign host.
 => This is where the panic occurred <=

[4/13/93 public]
(((adding text that Dawn Stokes' edit attempts accidentally deleted)))
root@fire  # fts zap -ft 0,,1264 -server /.:/hosts/fire -aggr m3.aggr1
Connection closed by foreign host.
 => This is where the panic occurred <=
 
Here's the relevant fldb entries:
 
m1.lfs2  
        readWrite   ID 0,,487  valid
        readOnly    ID 0,,1265  invalid
        backup      ID 0,,489  valid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
fire.osf.org        RW,BK    m3.aggr1 0:00:00 hosts/fire     <nil>
m3.lfs1  
        readWrite   ID 0,,10  valid
        readOnly    ID 0,,11  invalid
        backup      ID 0,,12  valid
number of sites: 1
   server           flags     aggr   siteAge principal      owner    
fire.osf.org        RW,BK    m3.aggr1 0:00:00 hosts/fire     <nil> 
 
I think the backup versions were created as a side-effect of running
the fts tests in this cell, and there were also numerious server disk quota
exceeded messages being logged during the cho run.
 
Here's Dawn's addition:

[4/13/93 public]
Could this be related to the panic I hit about a month ago, but
have been unable to reproduce?? Jeff and I exchanged mail about
it, but never came to a conclusion.  The scenario was that I
had run the full suite of fts tests several times, and was trying
to clean some things up:
 
# fts delete dfswitch.efs3.backup dfswitch efs1
 
panic: failed:line 2046, ...episode/vnops/efs_volops.c, assert
efsSomeClone
afscall_volser
kafs_syscall
 
The running proc was ftserver.
 
Note that this line number is pretty old, and our line numbers
are different since we don't carry history in the prologs.'

[4/13/93 public]
Yes, this looks like a similar panic.  The out-of-space condition is the
one that seems to be key here.

[5/3/93 public]
This problem is believed to be fixed by the listed deltas all of 
which have been included in dfs-carl 1.8.
Filled in Transarc Deltas with 
 `jdp-ot6363-increase-quota-before-clone-operation, 
 jdp-db3331-better-clone-cleanup, 
 ota-db3388-bad-cleanup-when-deleting-non-empty-volume' 
Changed Transarc Status from `open' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7669
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : dfsbind forks too late & loses timer thread
Reported Date                 : 4/7/93
Found in Baseline             : 1.0.1
Found Date                    : 4/7/93
Severity                      : B
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cburnett-ot7669-dfsbind-loses-tmr-thread
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[4/7/93 public]
The dfsbind deamon (main_helper.c) is calling for fork to late
in its processing chain.  Before it calls fork it calls do_AuthInit()
which causes the RPC to get initailized and several threads to get
started.  The forked child will not inheret the threads and thus
dfsbind will be left running without a timer thread.  As soon
as the first RPC packet gets dropped, the service thread in dfsbind
will hang indefinitly since there will be no timer thread to retransmit
the packet of timeout the call.  The fix is to perform the fork earlier
below is the corrected mainproc function in main_helper.c.
Look for ifdef/ifndef OT too see the change.  I will also send
the changed file to Transarc.
mainproc(as, arock)
     register struct cmd_syndesc *as;
     char *arock;
{
    long code, qsize;
    long temp;
    struct timespec ts;
    int i; 
    int syntaxError = 0;
    pthread_t thread;
    Debugging = 0;
    if (as->parms[DFSBIND_ARG_DEBUG].items) {
        /* -debug */
        Debugging = 1;
    }
    /* time-to-live value for sucessfull lookups */
    if (as->parms[DFSBIND_ARG_JUNCTION].items) {
        if((ttlhit = strtoul(as->parms[DFSBIND_ARG_JUNCTION].items->data, 
			     (char **) NULL, 10)) < 30) {
	    fprintf(stderr, "junctionlife must be at least 30 seconds, default is %d\n",
		    TTLHIT_DEFAULT);
	    syntaxError++;
	}
    } else
        ttlhit = TTLHIT_DEFAULT;
    /* time-to-live for prefix lookups */
    if (as->parms[DFSBIND_ARG_PREFIX].items) {
        if ((ttldir = strtoul(as->parms[DFSBIND_ARG_PREFIX].items->data, 
			      (char **) NULL, 10)) < 30) {
	    fprintf(stderr, "prefixlife must be at least 30 seconds, default is %d\n",
		    TTLDIR_DEFAULT);
	    syntaxError++;
	  }
    } else
        ttldir = TTLDIR_DEFAULT;
    /* time-to-live value for unsucessfull lookups */
    if (as->parms[DFSBIND_ARG_NOTFOUND].items){
        if ((ttlmiss = strtoul(as->parms[DFSBIND_ARG_NOTFOUND].items->data, 
			       (char **) NULL, 10)) < 30) {
	    fprintf(stderr, "notfoundlife must be at least 30 seconds, default is %d\n",
		    TTLMISS_DEFAULT);
	    syntaxError++;
	  }
    } else
        ttlmiss = TTLMISS_DEFAULT;
    if(as->parms[DFSBIND_ARG_EXPRESS].items){
        if((hpth = strtoul(as->parms[DFSBIND_ARG_EXPRESS].items->data, 
			   (char **) NULL, 10)) <= 0) {
	    fprintf(stderr, "must specify at least 1 express daemon, default is %d\n",
		    DEFAULT_EXPRESS_PROCS);
	    syntaxError++;
	}
    }
    if(as->parms[DFSBIND_ARG_REGULAR].items){
        if((lpth = strtoul(as->parms[DFSBIND_ARG_REGULAR].items->data, 
			   (char **) NULL, 10)) <= 0) {
	    fprintf(stderr, "must specify at least 1 regular daemon, default is %d\n",
		    DEFAULT_REGULAR_PROCS);
	    syntaxError++;
	}
    }
    if (syntaxError)
	exit(1);
#ifdef fork
#undef fork /* Turn off CMA wrapper for fork */
#endif /* fork */
#ifdef OT /* ORB: 5337 */
    /* fork if not debugging.  It is important to fork early before any
     * RPC activity occurrs which will start threads.
     */
    if (Debugging == 0) {
	temp = fork();
	if (temp > 0) {
	    _exit(0);		/* parent exits; child continues */
	} else if (temp < 0) {
	    fprintf(stderr, "Could not fork: result %d, error %d\n", 
		    temp, errno);
	    exit(1);
	}
	/* we are in the child here */
	/*
	 * get rid of our controlling terminal and get into a new
	 * process group
	 */
	if (setsid() == -1) {
	  perror("setsid");
	}
    }
#endif /* OT */
    /* start the real processing */
    do_InitAuth(Debugging);   
retry:
    if ((fd = KRPC_OPEN_HELPER()) < 0){
         code = errno;
   	if (code == EALREADY){
             fprintf(stderr,"helper is already running - exiting \n");
             exit(code);
	   }
        if (code == EINPROGRESS){
            /* subsystem shutting down - wait and retry */
  	    ts.tv_sec = 30;
	    ts.tv_nsec = 0;
 	    pthread_delay_np (&ts);
             goto retry;     
	   }
         perror("dfsbind failed to start ");
         exit(code);        
    }   
   qsize = (hpth+lpth)*2;
   /*
    * configure the kernel's helper queue size and
    * enable the the helper subsystem 
    */
   if (ioctl(fd, KRPCH_CONFIGURE, (char *)&qsize) < 0){
       fprintf(stderr,"can't configure krpc_helper - exiting %d", errno);
       exit(errno);
     }
   if (ioctl(fd, KRPCH_ENABLE, (char *)NULL) < 0){
       fprintf(stderr,"can't enable krpc_helper - exiting %d", errno);
       exit(errno);
     }
#ifndef OT /* ORB: 5337 */
    /* fork if not debugging */
    if (Debugging == 0) {
	temp = fork();
	if (temp > 0) {
	    _exit(0);		/* parent exits; child continues */
	} else if (temp < 0) {
	    fprintf(stderr, "Could not fork: result %d, error %d\n", 
		    temp, errno);
	    exit(1);
	}
	/* we are in the child here */
	/*
	 * get rid of our controlling terminal and get into a new
	 * process group
	 */
	if (setsid() == -1) {
	  perror("setsid");
	}
    }
#endif /* OT */
    /* set up ICL */
    code = icl_StartCatcher(0);
    if (code < 0)
	(void) fprintf(stderr, "could not start ICL signal watcher, code=%d, errno=%d\n", code, errno);
    else
	SetupLog();
/* Initialize queues and create worker threads */
   for (i=0; i < NQUEUES; i++){
       pthread_mutex_init(&work_queue[i].qlock, pthread_mutexattr_default);
       pthread_cond_init(&work_queue[i].wcond, pthread_condattr_default);
     }
   initialize_free_list(&free_list, qsize);
   work_queue[HIGHPRI].head = work_queue[HIGHPRI].tail = NULL;
   work_queue[NOPRI].head = work_queue[NOPRI].tail = NULL;
   for(i=0; i< hpth; i++){
       pthread_create(&thread, pthread_attr_default, 
                      (pthread_startroutine_t)service_thread,
                      (pthread_addr_t)&work_queue[HIGHPRI]);
   }  
   for(i=0; i< lpth; i++){
       pthread_create(&thread, pthread_attr_default, 
                      (pthread_startroutine_t)service_thread,
                      (pthread_addr_t)&work_queue[NOPRI]);
   }  
   dispatch_thread();
  
  }
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[4/7/93 public]
Defect Closure Form
-------------------
Validated that rpc timer thread was still active after start
of dfsbind and that an ls on /.:/fs worked.
Filled in Transarc Deltas with `cburnett-ot7669-dfsbind-loses-tmr-thread' 
Changed Transarc Status from `open' to `export'

[04/08/93 public]
I believe that this change, as currently proposed, undoes the change
submitted for OT 6952, which moved the fork() in dfsbind startup till
_after_ the KRPC device open.  If the device open must follow the
Do_InitAuth() call in mainproc(), then we would seem to have a circular
dependency here.  The current ordering is (without Carl's change):
	do_InitAuth()
	KRPC_OPEN_HELPER()
	fork()
There's no doubt this is wrong and must be fixed.  Carl's change would
make the order:
	fork()
	do_InitAuth()
	KRPC_OPEN_HELPER()
This loses the fix in 6952, which enabled the parent dfsbind process to
return a non-zero exit code successfully if dfsbind startup failed.
Diane believes that it would be possible to use this ordering of
events, which, if correct, would both fix this defect and preserve the
fix to 6952:
	KRPC_OPEN_HELPER()
	fork()
	do_InitAuth()
However, I would prefer to change dfsbind to start up like all other
DCE user-space daemons/servers now do, and have the parent explicitly
synchronize with the child via a pipe, then exit when the child sent it
a status word.  In this case, the status could be passed to the
parent's caller as an exit code.  This change would use the same
ordering proposed by Carl, but would add additional code to do the
synchronization.
Comments?

[4/8/93 public]
Thanks for the comment Don.  Transarc noticed this as well.
I have changed the code to the following order:
        KRPC_OPEN_HELPER()
        fork()
        do_InitAuth()
and it seems to work ok.  I am going to drop this additional change.
As for the parent child serialization method, I think that is great.
we should do this for 102.a.  Don, would you or Diane like to
make the change?

[4/12/93 public]
Changed Transarc Status from `export' to `import'

[4/15/93 public]
submitted on 4/14/93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7665
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Could not End Transaction, I/O error (dfs/fts)
Reported Date                 : 4/7/93
Found in Baseline             : 1.0.2
Found Date                    : 4/7/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot7665-no-spin-on-oldsite
Transarc Herder               : shl@transarc.com
Transarc Status               : submit

[4/7/93 public]
 
Attempt fts release without proper credentials, then retry with proper
credentials:
 
root@valentine> fts release epi.1
Could not lock FLDB entry (vol=0,,10, type=0, op=32)
Error: Requested access denied (dfs / dau)
Error in release: Requested access denied (dfs / dau)
 
root@valentine> dce_login cell_admin -dce-
Password must be changed!
root@valentine> fts release epi.1
Could not End Transaction, I/O error (dfs / fts)
The fileset 0,,10 could not be released: I/O error (dfs / fts)
Deleting the damaged or incomplete read-only 0,,11
Error in release: fileset release failed (dfs / fts)
ot@valentine> fts lsfldb -fileset epi.1
        readWrite   ID 0,,10  valid
        readOnly    ID 0,,11  valid
        backup      ID 0,,12  invalid
        releaseClone        0,,23
number of sites: 3
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner
valentine.osf.org   RW       lfs_aggr 0:00:00 hosts/valentine<nil>
valentine.osf.org   RO       lfs_aggr3 0:00:00 hosts/valentine<nil>
dce5.osf.org        RO       lfs_aggr2 0:00:00 hosts/dce5     <nil>
root@valentine> cd /:/epi_1_ro
root@valentine> ls
dfs: version of fileset 0,,11 (epi.1.readonly) is older than our latest
dfs: version of fileset 0,,11 (epi.1.readonly) is older than our latest
(ad nauseum - unable to interrupt continual output of this message)

[4/7/93 public]
Any chance we could see what's in the FtLog on valentine.osf.org?
The ``fts release'' command fails immediately if it fails to
lock the FLDB entry, so I have trouble imagining side effects of having
attempted a release command unauthenticated.  This leaves open the
possibility that there are nasty problems with an ordinary cross-aggregate
release command, but we haven't seen those here.
As to the infinite loop in the CM, I assume that these messages were
coming out every 20 seconds but they stopped after about 400 seconds.
Does that sound right?
 
Had this fileset been released before (successfully)?
What does ``fts lshead valentine.osf.org -long'' say?
Were there any non-DFS problems like disk errors?
Thanks!
Changed Responsible Engr. from `pakhtar' to `cfe' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `shl@transarc.com' 
Added field Transarc Status with value `open'

[4/7/93 public]
Ah, something else to check.  What does ``fts statftser valentine.osf.org''
say?  Thanks again.

[4/7/93 public]
I've been trying to reproduce this locally, but cannot, with any weird
combination of scenarios.  Does it still happen for you?

[4/7/93 public]
The FtLog on valentine:
93-Apr-07 10:23:38 Log file initialized as /opt/dcelocal/var/dfs/adm/FtLog
93-Apr-07 10:23:44 Ftserver starting
93-Apr-07 12:08:27 Restoring fileset 0,,22/3
93-Apr-07 12:08:32 vols_Restore: returning 0
93-Apr-07 12:08:32 Restored fileset 0,,22/3: returned code 0
93-Apr-07 13:47:53 Restoring fileset 0,,11/3
93-Apr-07 13:47:53 vols_Restore: returning 572825648
93-Apr-07 13:47:53 Restored fileset 0,,11/3: returned code 572825648
93-Apr-07 13:47:54 Errors from both dumper (0,,23/1: 5) and restorer (572825648
)
 
From the /usr/adm/messages file on valentine:
Apr  7 13:49:42 valentine vmunix: dfs: version of fileset 0,,11 (epi.1.readonly
) is older than our latest
Apr  7 13:50:13 valentine last message repeated 529 times
Apr  7 13:52:14 valentine last message repeated 2348 times
 
Yes, epi.1 had been released before successfully - I had been playing
with shutting down repserver and detaching aggregates yesterday in this
cell - but rebooted cleanly today and had done nothing destructive.
 
I don't see any evidence of non-DFS problems like disk errors - I believe
these would show up in /usr/adm/messages as well.
 
root@valentine> fts statftser valentine.osf.org
No active transactions on valentine.osf.org
 
root@valentine> fts lshead valentine.osf.org -long
Total filesets on server valentine.osf.org aggregate lfs_aggr (id 1): 3
epi.1 0,,10 RW LFS     states 0x18010005 On-line
    valentine.osf.org, aggregate lfs_aggr (ID 1)
    Parent 0,,0       Clone 0,,23      Backup 0,,12
    llBack 0,,0       llFwd 0,,0       Version 0,,188    
    1048576 K alloc limit;     216 K alloc usage
       5000 K quota limit;     216 K quota usage
    Creation Wed Apr  7 13:47:22 1993
    Last Update Wed Apr  7 13:47:57 1993
 
root.dfs 0,,15 RW LFS     states 0x10010005 On-line
    valentine.osf.org, aggregate lfs_aggr (ID 1)
    Parent 0,,0       Clone 0,,16      Backup 0,,17
    llBack 0,,0       llFwd 0,,0       Version 0,,18     
    1048576 K alloc limit;       9 K alloc usage
       5000 K quota limit;       9 K quota usage
    Creation Tue Apr  6 17:43:29 1993
    Last Update Tue Apr  6 17:45:25 1993
 
root.dfs 0,,1 RW LFS     states 0x10010005 On-line
    valentine.osf.org, aggregate lfs_aggr (ID 1)
    Parent 0,,0       Clone 0,,2       Backup 0,,3
    llBack 0,,0       llFwd 0,,0       Version 0,,29     
    1048576 K alloc limit;       9 K alloc usage
       5000 K quota limit;       9 K quota usage
    Creation Mon Apr  5 15:23:07 1993
    Last Update Tue Apr  6 11:17:18 1993
 
Total filesets on-line 3; total off-line 0; total busy 0
 
NOTE: The 2 root.dfs filesets! I had tried to replicate root.dfs and then
detach the aggregate and lost access to 0,,1. I created the second
root.dfs and appeared to proceed with my testing okay (after OT 7662 and
rebooting). From yesterday:
 
root@valentine> dfsexport /dev/rz1a -detach
dfsexport: Revoking tokens for filesets on aggregate 1...
Can't get 0xd0c token on 0,,1, server valentine.osf.org: fileset already deleted/moved (dfs / xvl)
dfsexport: Could not revoke tokens for fileset 0,,1: fileset already deleted/moved (dfs / xvl)
dfsexport: Failed to detach /dev/rz1a:lfs_aggr (fileset already deleted/moved (dfs / xvl))
root@valentine> fts lsheader -aggr lfs_aggr -server valentine
Total filesets on server valentine aggregate lfs_aggr (id 1): 2
epi.1                    0,,10 RW     14 K alloc     14 K quota On-line
**** Fileset 0,,1 is busy: No such device ****
Total filesets on-line 1; total off-line 0; total busy 1
In other words, I lost my root.dfs and had to recreate it. 
I can fts lsfldb okay on dce5 - cm flushfileset /.:/fs and cm
statservers appear to work.

[4/7/93 public]
Hm; you should probably use fts zap to get rid of the old root.dfs that you
no longer want.  Leaving it around could easily cause problems when you
reboot, in that dfsexport will attempt to export both of them.
 
When you did the failing fts release command, the dump/restore operation
that shipped the changes over got an I/O error dumping the fileset, but
that's all the information that's logged.  What happens if you try the
fts release again?  How would I reproduce this?  (How did you make it
happen, in some detail?)
 
I admit that I wasn't expecting the ``is older than our latest'' messages
2300 times in two minutes.  Sigh.

[4/7/93 public]
I believe I have rebooted at least once since creating the 2nd root.dfs -
dfsexport didn't complain at all ... 
 
I get the same results when I try fts release epi.1 again. Note that
the releaseClone fileset # is upped since the last time.
 
root@valentine> fts release epi.1
Could not End Transaction, I/O error (dfs / fts)
The fileset 0,,10 could not be released: I/O error (dfs / fts)
Deleting the damaged or incomplete read-only 0,,11
Error in release: fileset release failed (dfs / fts)
root@valentine> fts lsfldb -fileset epi.1
        readWrite   ID 0,,10  valid
        readOnly    ID 0,,11  valid
        backup      ID 0,,12  invalid
        releaseClone        0,,26
number of sites: 3
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner
valentine.osf.org   RW       lfs_aggr 0:00:00 hosts/valentine<nil>
valentine.osf.org   RO       lfs_aggr3 0:00:00 hosts/valentine<nil>
dce5.osf.org        RO       lfs_aggr2 0:00:00 hosts/dce5     <nil>
 
I'll try to retrace my steps to this point:
	- config 3 pmax cell, 1 fl server, 1 file server, 1 client only,
		all 3 dts servers, no ntp running
	- create epi.1 on same aggregate as root.dfs on fl server
	- create 2 release replication sites for epi.1, 1st is on
		a second aggregate on fl server, 2nd is remote on plain fs
	- write to epi.1 and release successfully
	- attempt to replicate root.dfs w/o fix
	- attempt to detach aggregate housing epi.1 and root.dfs
	- write to epi.1 again
	- detach aggregate housing local epi.1.readonly on fl server
	- attempt to release epi.1
	- fails - says it's going to delete fileset 0,,11 (epi.1.readonly)
		but doesn't (2nd/remote replica on fs server was available
		and fldb was not changed)
	- reattach aggregate housing local epi.1.readonly on fl server
	- unauthenticated, attempt to release epi.1 again
	- authenticated, attempt to release epi.1 again
 
I also had an epi.2 on the plain file server with a scheduled replica on
the same aggregate on the fl server as one of epi.1's. I was also writing
to epi.2 and doing fts updates - could this have anything to do with it?
 
I want to upgrade to today's build - do you need more info from this config?

[4/7/93 public]
Oh - the lsreplicas output looks interesting too:
root@dce5> fts lsreplicas epi.1 -all
On dce5.osf.org:
epi.1, cell 134228756,,3011985792: src 0,,10 (lfs_aggr) (on valentine.osf.org) => dce5.osf.org 0,,11 (lfs_aggr2)
   flags 0x60081, volstates 0x10421106.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,188; curVV: 0,,33; WVT ID = 0,,0
   Lost token 6413 ago; token expires 79988 hence; new version published 734224474 ago
   vvCurr 734218061.525464 (6413 ago); vvPingCurr 734218062.283204 (6412 ago)
   Last update attempt 734224303.766477 (171 ago); next scheduled attempt 734224504.257770 (30 hence)
   Status msg: GetIncrementalDump: can't open remote source ft: No such device

[4/8/93 public]
I'll try the steps you took in detaching aggregates; that could explain why
the release is failing.  Meanwhile, what I'd like to do is submit a fix
for the endless fast stream of error messages about backwards-moving
fileset versions.  I'm not sure which particular problem you think needs to
be addressed in the near term.

[4/8/93 public]
OK, here's what I've done.  I tried your sequence of operations and was
unable to get the dfsexport -detach to fail; I suspect that the fix for
OT 7629 (which I'm running with), which repairs the way in which dfsexport
obtains whole-fileset tokens at detach time, is allowing the detach to
succeed correctly where it failed before.  At the same time, though, I
worked through the spin-loop generating messages of this sort, which
is a bug in my fix to the old 7342(?)--where CMs print so many messages
that they drop clock interrupts.  This bug fix should prevent any such
messages from being sent at high volume--they should appear at most every
20 seconds (per server).
 
I also repaired the problem of the new stage replica being visible with
a high fileset version number even when it's damaged.  This version of
fts will hold it off-line until the update results are analyzed.
 
My understanding is that the failed aggregate detach left the server in a
state that prevented subsequent releases from working, because of I/O errors
reading the R/W fileset with ftserver's dump command.  (Everything else
worked OK: the damaged distinguished replica was deleted, and therefore
the repserver couldn't use it to do subsequent updates.)  Everything, that
is, except that a CM had spotted an instance of some R/O fileset with
a higher version number and had advanced its saved VV, going into a tight
loop.
 
Defect Closure Form
-------------------
--Other explanation below--
Couldn't reproduce the problem, probably because of other fixes,
but this repairs a problem that caused unbounded error messages to be
generated.  Verify it by checking that cm_CheckVolSync() no longer
prints an error message, but returns VOLERR_TRANS_VVOLD that cm_Analyze()
prints a message for once every 20 seconds.  Also verify in userInt/fts/
volc_vprocs.c that ForwardVolume() doesn't close the totid (destination
transaction ID), but that its caller VC_ReleaseVolume does that after
checking whether the result was good.
Associated information:
Tested on TA build:  dfs-102-2.8
Tested with backing build:  dce1.0.2b19
Filled in Transarc Deltas with `cfe-ot7665-no-spin-on-oldsite' 
Changed Transarc Status from `open' to `export'

[4/8/93 public]
Changed Transarc Status from `export' to `import'

[4/15/93 public]
submitted on 4/14/93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7662
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : ftu
Short Description             : dfsexport -detach of lfs aggr failed
Reported Date                 : 4/6/93
Found in Baseline             : 1.0.2b21
Found Date                    : 4/6/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : unknown
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : shl@transarc.com
Transarc Status               : open

[4/6/93 public]
BUILD:	bl 21
CONFIG:	3 pmax cell, 1 fl server
Detach reported failure, APPEARED to succeed, prevented further access
to /: on that server.
root@dce5> dfsexport /dev/rz3c -detach
dfsexport: Revoking tokens for filesets on aggregate 2...
ftu_DetachAggr: Can't unmount the aggregate: Invalid argument
root@dce5> cat /opt/dcelocal/var/dfs/dfstab
# blkdev aggname aggtype aggid 
/dev/rz3c       lfs_aggr2       lfs     2
root@dce5> dfsexport
root@dce5> cd /:
/:: No such device
DETAILS:
	valentine = fl server AND home of lfs_aggr (root.dfs, epi.1)
	dce5 = home of lfs_aggr2 (epi.2, epi.1.readonly)
Other problems seen in this cell MAY be related, so I list them here:
	had previously tried to replicate root.dfs, scheduled to lfs_aggr2
	(known bug)
	had previously tried to detach lfs_aggr:
root@valentine> dfsexport /dev/rz1a -detach
dfsexport: Revoking tokens for filesets on aggregate 1...
Can't get 0xd0c token on 0,,1, server valentine.osf.org: fileset already deleted/moved (dfs / xvl)
dfsexport: Could not revoke tokens for fileset 0,,1: fileset already deleted/moved (dfs / xvl)
dfsexport: Failed to detach /dev/rz1a:lfs_aggr (fileset already deleted/moved (dfs / xvl))
root@valentine> fts lsheader -aggr lfs_aggr -server valentine
Total filesets on server valentine aggregate lfs_aggr (id 1): 2
epi.1                    0,,10 RW     14 K alloc     14 K quota On-line
**** Fileset 0,,1 is busy: No such device ****
Total filesets on-line 1; total off-line 0; total busy 1
In other words, I lost my root.dfs and had to recreate it. 
I can fts lsfldb okay on dce5 - cm flushfileset /.:/fs and cm
statservers appear to work.

[4/6/93 public]
Changed Responsible Engr. from `pakhtar' to `jdp@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `shl@transarc.com' 
Added field Transarc Status with value `open'

[4/7/93 public]
Downgrading to a 2 since, after speaking with Jeff Prem, it appears that
the 2 symptoms are unrelated. The detach error reported is merely a warning
that the agfs (?) had been removed by a previous failed attempt at
detaching (due to other problems, I am not able to doublecheck whether this
was my first or 2nd attempt at detaching).
The inability to access /: is PROBABLY due to the failed attempt at 
replicating root.dfs.
I am leaving this open since Jeff will be submitting a code that will 
make dfsexport detach more robust and less likely to report this error.

[7/14/93 public]
Gail, I would like to assign this to you for now. I cannot reproduce it
and we have not seen this for a long time. Jeff has also made several fixes 
in this area and may have already fixed it. 
  
If you cannot reproduce it, could you like to consider cancel it ? 
  
Thanks,
Tu
  
Changed Responsible Engr. from `jdp@transarc.com' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[7/30/93 public]
Closing - I have not had any problems detaching lfs aggregates since
Jeff's submission.



CR Number                     : 7660
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : Typo in cm/cm_init.c\cm_DoInitDLUnInit()
Reported Date                 : 4/6/93
Found in Baseline             : 1.0.2
Found Date                    : 4/6/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : kazar-ot7660-fix-init-typo
Transarc Herder               : 

[4/6/93 public]
In looking at osi_Alloc()/osi_Free() calls in the CM, I noticed
a typo in this loop:
    for (i=0; cm_dlinfo->nAddrs; i++) {
        osi_Free((opaque)cm_dlinfo->principal[i],
         (long)(strlen(cm_dlinfo->principal[i])+1));
        osi_Free((opaque)cm_dlinfo->serverAddr[i],
         (long)(sizeof(struct sockaddr_in)));
    }
That predicate should be "i < cm_dlinfo->nAddrs", of course.
This function is called only from diskless, which is why I'm
making the defect P3.

[4/6/93 public]
Changed Responsible Engr. from `jsk@transarc.com' to `kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[7/7/93 public]
Fixed in 103 3.16.
Filled in Transarc Deltas with `kazar-ot7660-fix-init-typo' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 7657
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7568
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Decrease stack usage of DFS functions with biggest frame sizes
Reported Date                 : 4/6/93
Found in Baseline             : 1.0.2
Found Date                    : 4/6/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/episode/vnops/efs_init.c, file/cm/cm_vnodeops.c, file/cm/cm_volume.c
Sensitivity                   : public

[4/6/93 public]
The exact scope of this defect is still open to debate, but the idea is
to reduce the size of the stack frames currently allocated by
DFS-related functions that appear in the kernel, for some set of
functions that both have large frames and can appear in long call
chains.  The motivation is to lessen the risk of overflowing per-thread
kernel stacks.  As has been pointed out, only the worst of these
functions can be guessed at a priori, but the worst are bad enough to
warrant fixing in 1.0.2.

The "fix" is to move large objects off the stack and allocate them
dynamically.  This has performance implications that are more or less
acceptable according to which entry point is involved.  The best
available way to allocate objects dynamically seems to be to use
the osi_AllocBufferSpace() interface.

Due to the lack of hard data implicating these functions in stack
overflow problems (with the exception noted below), I'm recommending
addressing only the very largest stack frames for 1.0.2 -- specifically,
the entry points with stack frames over 1.5KB.  Of these, I believe
the following should be addressed:

afscall_episode                         cc8     3272
cm_CheckVolumeInfo                      9c8     2504
cm_symlink                              808     2056
cm_rename                               630     1584

And I believe the following need _not_ be addressed, due to the
apparent nature of the call chains they may appear in:

agfs_Mount                              6c8     1736
vol_efsBulkSetStatus                    660     1632

Corrections are welcome, of course.  I've put myself as responsible
code engineer, but other volunteers are also quite welcome (:-)).

As a final note, I should say that I do have some evidence (gathered
from a PMAX kernel crash dump) that a call chain involving cm_rename()
and cm_CheckVolumeInfo() was responsible for an occurrence of OT 7568
(see that report for more info).  I think it's equally possible for
cm_symlink() to cause an overflow with the same call chain.

[04/09/93 public]
Sigh.  Have to assign this one to Ron -- I believe I've made
correct fixes to all of the entry points cited above for
changing, but more testing (especially on RIOS) is needed to
be sure of that.

So the changes can't be submitted till next week.

[04/15/93 public]
Don submitted these changes to the 1.0.2a tree yesterday.  Fixed.

[12/17/93 public]
Closed.



CR Number                     : 7656
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7773
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : user space spin in rep tests
Reported Date                 : 4/6/93
Found in Baseline             : 1.0.2
Found Date                    : 4/6/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/ftserver/ftserver_dump.c,file/ftserver/ftserver_vprocs.c,file/rep/rep_main.c
Sensitivity                   : public
Transarc Deltas               : jdp-ot7656-catch-exceptions-while-forwarding
Transarc Herder               : shl@transarc.com
Transarc Status               : submit

[4/6/93 public]
I started the rep tests last night on a 2 machine PMAX cell (shotz
and digital5, FLDB on digital5).  This morning I found "fts rmsite 11
/.../digital5_cell/hosts/shotz lfs_aggr2" spinning (i.e accumulating CPU
time) in user space. It's been doing this most of the night. The ftserver
on shotz appears to have picked up a suspicious amount of CPU time as well:
 
 487 ??       S    192:01.71 /opt/dcelocal/bin/ftserver
 94  pts/0    S      4:30.30 fts rmsite 11 /.../digital5_cell/hosts/shotz lfs_
 
The contents of the FLDB are:
 
> # fts lsfldb
>
> Lemieux.699
>         readWrite   ID 0,,11  valid
>         readOnly    ID 0,,12  invalid
>         backup      ID 0,,13  invalid
> number of sites: 1
>   Sched repl: maxAge=0:03:00; failAge=0:05:00; reclaimWait=1:30:00; minRepDelay0
>    server           flags     aggr   siteAge principal      owner
> shotz.osf.org       RW       lfs_aggr2 0:02:00 hosts/shotz    <nil>
>
> lfs_set
>         readWrite   ID 0,,4  valid
>         readOnly    ID 0,,5  invalid
>         backup      ID 0,,6  invalid
> number of sites: 1
>    server           flags     aggr   siteAge principal      owner
> shotz.osf.org       RW       lfs_aggr2 0:00:00 hosts/shotz    <nil>
>
> root.dfs
>         readWrite   ID 0,,1  valid
>         readOnly    ID 0,,2  invalid
>         backup      ID 0,,3  invalid
> number of sites: 1
>    server           flags     aggr   siteAge principal      owner
> digital5.osf.org    RW       lfs_aggr 0:00:00 hosts/digital5 <nil>
> ----------------------
> Total FLDB entries that were successfully enumerated: 3 (0 failed)
 
fts lsheader reveals (looks fine to me):
 
> # fts lsheader -server /.:/hosts/shotz
> Total filesets on server /.:/hosts/shotz aggregate lfs_aggr2 (id 11): 4
> Lemieux.699              0,,11 RW     28 K alloc     28 K quota On-line
> Lemieux.699.clone        0,,14 TEMP     10 K alloc     22 K quota **Off-line
> Lemieux.699.readonly     0,,12 RO     22 K alloc     22 K quota On-line
> lfs_set                  0,,4 RW      9 K alloc      9 K quota On-line
> Total filesets on-line 3; total off-line 1; total busy 0
>
> Total number of filesets on server /.:/hosts/shotz: 4
> # ^shotz^digital5
> fts lsheader -server /.:/hosts/digital5
> Total filesets on server /.:/hosts/digital5 aggregate lfs_aggr (id 1): 1
> root.dfs                 0,,1 RW    164 K alloc    164 K quota On-line
> Total filesets on-line 1; total off-line 0; total busy 0
>
> Total number of filesets on server /.:/hosts/digital5: 1
 
When I initially configured the two machines, I chose the FLDB config
for shotz after configing digital5 as the FLDB.  When I realized my
mistake (i.e. it complained about quorum on cd /:), I attempted to
recover by removing the shotz member from the RPC group /.:/fs and
restarting the flserver on digital5.  I also did a "bos delete" on the
flserver for shotz.  I was able to access /: fine then and udebug
was happy.  However, the CM apparently had cached the shotz flserver
info somewhere because cm statservers complained about "servers still
down on shotz.osf.org" or some such thing.  "bos status" on shotz
showed ftserver and repserver were happy.  This is when I started the
rep tests because access to /: was fine and I knew of no way to fix
the complaint from cm statservers (cm checkfilesets didn't help).  Might
this be related to the above "spin"?  What other info can I extract
from the existing config?  I'm thinking of re-trying the fts rmsite
under the debugger to find the loop.

[4/6/93 public]
Changed Interest List CC from `cfe@transarc.com' to `' 
Changed Responsible Engr. from `jsk@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `shl@transarc.com' 
Added field Transarc Status with value `open'

[4/6/93 public]
This looks far more like some sort of RPC communication glitch between fts
and the ftserver (or repserver) than much of anything else.  You can get
the ftserver and repserver to say what they think they're doing with:
	fts statft shotz
	fts statft digital5
	fts statrep shotz -long
	fts statrep digital5 -long
That information would be pretty important in deciphering where things are
stuck.  Other interesting information would include:
	- contents of the FtLog and RepLog files in /opt/dcelocal/var/dfs/adm
	- whether the bosserver has been having to restart the ftserver
	  and/or the repserver recently (bos status <server> -long)
 
So to let me understand what the flserver config issue is:
	- for a while both digital5 and shotz were in the /.:/fs RPC group,
	  but only digital5 is there now
	- all running flservers know that only digital5 is an flserver
	- digital5 is the only running flserver
	- a CM might have shotz configured as an flserver, but you're
	  running with the old fix for 7372 that falls over from a failing
	  flserver to a working one, so that should be OK.
Is this true?  If so, indeed everything should work OK.
 
I'll look forward to the answers to these questions.
Filled in Interest List CC with `jdp'

[4/6/93 public]
I forgot to mention.  ``fts lsheader <server> -long'' is probably more useful
in these cases, just to see the detailed fileset header reports.  Thanks.

[4/6/93 public]
As for the flserver config issue:
 
(1) I've verified (again) there's only one flserver currently running 
in the cell (i.e. on digital5)
(2) rpccp show group -u /.:/fs shows only one group member (digital5)
(3) udebug says:
root@digital5  # /opt/dcelocal/bin/udebug  -group  /.:/fs
Host 130.105.5.26, his time is 0
Vote: Last yes vote for 255.255.255.255 at -66094 (not sync site); Last vote started at -734107826
Local db version is 734041769.85
I am sync site until 1413375821 (1 servers)
Recovery state 1f
Sync site's db version is 0.0
0 locked pages, 0 of them for write
This server started at -66057
 
cm statservers still complains about shotz.osf.org being down.
 
I know of no reason why 7372 would NOT be in this configuration
if it was submitted.  I can check into it further if the raw data 
below doesn't yield anything useful.
 
Here's the other raw data you asked for (my comments indented and prefixed with ##).  Let me know if I missed something important, I cut and pasted this
pretty quickly:
 
% dce_login cell_admin -dce-
Password must be changed!
%  fts statft shotz
Total transactions: 1
--------------------------------------
trans: 34  created: Mon Apr  5 17:14:12 1993
descriptor: 0; ref count 1;  last call: Mon Apr  5 17:14:13 1993
fileset: 0,,11  aggregate Id: 11   aggrtype: 2/1
action: repserver forward fileset (dfs / xvl); ops: Dump, GetStatus
Fileset status 0x18014005: R/W, busy, know dally, type-RW
--------------------------------------
 
% fts statft digital5
No active transactions on digital5
% fts statrep shotz -long
Status of rep server shotz (shotz.osf.org) at Tue Apr  6 10:26:30 1993
Replicas managed: 1; host/connection blocks: 1
IDs: 1 allocated, 4294967295 in use, 2 re-used
Next forced keep-alive in 24356 secs, at Tue Apr  6 17:12:26 1993
Primary comm blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
Primary keep-alive blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
 
Active replicas on shotz:
 
Lemieux.699, cell 134228756,,3012024386: src 0,,11 (lfs_aggr2) (on shotz.osf.org) => shotz.osf.org 0,,12 (lfs_aggr2)
   flags 0x20001, volstates 0x10421206.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
   srcVV: 0,,214; curVV: 0,,153; WVT ID = 734038149,,310
   Lost token 62044 ago; token expires -47551 hence; new version published 734106390 ago
   vvCurr 734044346.182371 (62044 ago); vvPingCurr 734044439.010581 (61951 ago)
   Last update attempt 734044440.815217 (61950 ago); next scheduled attempt 0.182371 (-734106390 hence)
   Status msg: Calling FTSERVER_Forward() on primary
% fts statrep digital5 -long
Status of rep server digital5 (digital5.osf.org) at Tue Apr  6 10:26:50 1993
Replicas managed: 0; host/connection blocks: 1
IDs: 1 allocated, 4294967293 in use, 3 re-used
Next forced keep-alive in 24821 secs, at Tue Apr  6 17:20:31 1993
Primary comm blocks: 2 done, 0 oversize; 0 tot overage, (0:0) squared.
Primary keep-alive blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
fts statrepserver: No replicas on server digital5.osf.org.
% bos status /.:/hosts/shotz -long
Instance ftserver, (type is simple) currently running normally.
    Process last started at Mon Apr  5 16:07:11 1993 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/ftserver'
 
Instance repserver, (type is simple) currently running normally.
    Process last started at Mon Apr  5 16:07:25 1993 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/repserver'
 
% bos status /.:/hosts/digital5 -long   
Instance flserver, (type is simple) currently running normally.
    Process last started at Mon Apr  5 16:28:38 1993 (5 proc starts)
    Last exit at Mon Apr  5 16:28:38 1993
    Parameter 1 is '/opt/dcelocal/bin/flserver'
 
Instance ftserver, (type is simple) currently running normally.
    Process last started at Mon Apr  5 14:04:02 1993 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/ftserver'
 
Instance repserver, (type is simple) currently running normally.
    Process last started at Mon Apr  5 14:04:19 1993 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/repserver'
 
% fts lsheader /.:/hosts/shotz -long
Total filesets on server /.:/hosts/shotz aggregate lfs_aggr2 (id 11): 4
Lemieux.699 0,,11 RW LFS     states 0x18014005 On-line
    shotz.osf.org, aggregate lfs_aggr2 (ID 11)
    Parent 0,,0       Clone 0,,12      Backup 0,,13
    llBack 0,,0       llFwd 0,,0       Version 0,,214    
    1048576 K alloc limit;      28 K alloc usage
       5000 K quota limit;      28 K quota usage
    Creation Mon Apr  5 16:58:22 1993
    Last Update Mon Apr  5 18:46:40 1993
 
Lemieux.699.clone 0,,14 TEMP LFS Scheduled  states 0x1044023e **Off-line
    shotz.osf.org, aggregate lfs_aggr2 (ID 11)
    Parent 0,,11      Clone 0,,12      Backup 0,,13
    llBack 0,,14      llFwd 0,,0       Version 0,,218    
    1048576 K alloc limit;      10 K alloc usage
       5000 K quota limit;      22 K quota usage
    Creation Mon Apr  5 17:14:00 1993
    Last Update Mon Apr  5 17:14:24 1993
 
Lemieux.699.readonly 0,,12 RO LFS Scheduled  states 0x10421206 On-line
    shotz.osf.org, aggregate lfs_aggr2 (ID 11)
    Parent 0,,11      Clone 0,,12      Backup 0,,13
    llBack 0,,0       llFwd 0,,14      Version 0,,153    
    1048576 K alloc limit;      22 K alloc usage
       5000 K quota limit;      22 K quota usage
    Creation Mon Apr  5 17:11:41 1993
% fts lsheader /.:/hosts/digital5 -long
Total filesets on server /.:/hosts/digital5 aggregate lfs_aggr (id 1): 1
root.dfs 0,,1 RW LFS     states 0x10010005 On-line
    digital5.osf.org, aggregate lfs_aggr (ID 1)
    Parent 0,,0       Clone 0,,2       Backup 0,,3
    llBack 0,,0       llFwd 0,,0       Version 0,,197    
    1048576 K alloc limit;     164 K alloc usage
       5000 K quota limit;     164 K quota usage
    Creation Mon Apr  5 14:09:05 1993
    Last Update Tue Apr  6 09:18:54 1993
 
Total filesets on-line 1; total off-line 0; total busy 0
 
Total number of filesets on server /.:/hosts/digital5: 1
 
	## Contents of FtLog on shotz is too long to 
	## include in full.  This info appears interesting, 
	## though.  Here's the last screen full from the log:
 
93-Apr-06 10:31:55 (trans 106dec10: desc=0, time=734044453, ctime=734044452, states=0x18014005, accs=0x82, acce=691089536)
93-Apr-06 10:32:25 trans 106dec10 (Id=34, 0,,11/11) is 62292 seconds old (ref count 1)
93-Apr-06 10:32:25 (trans 106dec10: desc=0, time=734044453, ctime=734044452, states=0x18014005, accs=0x82, acce=691089536)
93-Apr-06 10:32:55 trans 106dec10 (Id=34, 0,,11/11) is 62322 seconds old (ref count 1)
93-Apr-06 10:32:55 (trans 106dec10: desc=0, time=734044453, ctime=734044452, states=0x18014005, accs=0x82, acce=691089536)
93-Apr-06 10:33:25 trans 106dec10 (Id=34, 0,,11/11) is 62352 seconds old (ref count 1)
93-Apr-06 10:33:25 (trans 106dec10: desc=0, time=734044453, ctime=734044452, states=0x18014005, accs=0x82, acce=691089536)
93-Apr-06 10:33:55 trans 106dec10 (Id=34, 0,,11/11) is 62382 seconds old (ref count 1)
93-Apr-06 10:33:55 (trans 106dec10: desc=0, time=734044453, ctime=734044452, states=0x18014005, accs=0x82, acce=691089536)
93-Apr-06 10:34:25 trans 106dec10 (Id=34, 0,,11/11) is 62412 seconds old (ref count 1)
93-Apr-06 10:34:25 (trans 106dec10: desc=0, time=734044453, ctime=734044452, states=0x18014005, accs=0x82, acce=691089536)
	## Here's contents of FlLog on shotz
	## There's no flserver currently running on shotz
 
93-Apr-05 16:01:42 flserver: log initialized to /opt/dcelocal/var/dfs/adm/FlLog
93-Apr-05 16:01:43 flserver: using default group of /.../digital5_cell/fs
93-Apr-05 16:02:14 flserver: cell digital5_cell, trial ID 134228756,,3012024386, 2 servers.
93-Apr-05 16:02:14 flserver: ready to service requests.
	## Here's contents of BosLog on shotz
 
Mon Apr  5 16:00:05 1993: /opt/dcelocal/bin/bosserver: beginning logging
Mon Apr  5 16:00:05 1993: Server directory access is okay
repserver: no aggregates on this server
 
	## Here's contents of RepLog on shotz
 
93-Apr-05 16:07:27 repserver: log initialized to /opt/dcelocal/var/dfs/adm/RepLog
93-Apr-05 16:07:27 Replication server started.  Mainprocs=1; tokenprocs=4; verbose=0.
93-Apr-05 16:08:28 repserver: no aggregates on this server
93-Apr-05 16:57:59 Release-style replicated fileset Barasso.699, rw=0,,7, ro=0,,8, with no R/O on primary!
 
	## Here's the contents of the logs on digital5
 
root@digital5  # ls -l
total 12
-rw-r--r--   1 root     bin                 448 Apr  5 14:05 BosLog
-rw-r--r--   1 root     bin                 462 Apr  5 16:47 FlLog
-rw-r--r--   1 root     bin                 302 Apr  5 16:21 FlLog.old
-rw-r--r--   1 root     bin                1313 Apr  5 17:14 FtLog
-rw-r--r--   1 root     bin                 360 Apr  5 14:05 RepLog
root@digital5  # cat BosLog
Mon Apr  5 13:45:58 1993: /opt/dcelocal/bin/bosserver: beginning logging
Mon Apr  5 13:45:59 1993: Server directory access is okay
Initializing fldb header...
repserver: FL server not yet up for VL_GetCellInfo: FLDB: cannot create FLDB with read-only operation (dfs / vls)
repserver: FLDB: cannot create FLDB with read-only operation (dfs / vls) from EnumerateVLDB().  Retrying.
Initializing fldb header...
repserver: no aggregates on this server
root@digital5  # cat FlLog 
93-Apr-05 16:28:42 flserver: log initialized to /opt/dcelocal/var/dfs/adm/FlLog
93-Apr-05 16:28:45 flserver: using default group of /.../digital5_cell/fs
93-Apr-05 16:29:25 flserver: cell digital5_cell, trial ID 134228756,,3012024386, 1 servers.
93-Apr-05 16:29:25 flserver: ready to service requests.
93-Apr-05 16:47:02 flserver: filling in the principal names for the 1 DB servers.
93-Apr-05 16:47:02 flserver: site 0 (1a056982) has principal 'hosts/digital5'
root@digital5  # cat FtLog
93-Apr-05 14:04:08 Log file initialized as /opt/dcelocal/var/dfs/adm/FtLog
93-Apr-05 14:04:11 Ftserver starting
93-Apr-05 14:09:05 ftserver_CreateVolume: created root.dfs as 0,,1 on aggr 1
93-Apr-05 16:49:39 SFTSERVER_ListVolumes: can't open 0,,8: 691089536
93-Apr-05 16:49:41 Restoring fileset 0,,8/1
93-Apr-05 16:49:42 vols_Restore: returning 0
93-Apr-05 16:49:42 Restored fileset 0,,8/1: returned code 0
93-Apr-05 16:51:08 Restoring fileset 0,,10/1
93-Apr-05 16:51:08 vols_Restore: returning 0
93-Apr-05 16:51:08 Restored fileset 0,,10/1: returned code 0
93-Apr-05 16:51:27 Restoring fileset 0,,10/1
93-Apr-05 16:51:28 vols_Restore: returning 0
93-Apr-05 16:51:28 Restored fileset 0,,10/1: returned code 0
93-Apr-05 17:00:09 Restoring fileset 0,,12/1
93-Apr-05 17:00:11 vols_Restore: returning 0
93-Apr-05 17:00:11 Restored fileset 0,,12/1: returned code 0
93-Apr-05 17:03:15 Restoring fileset 0,,10/1
93-Apr-05 17:03:16 vols_Restore: returning 0
93-Apr-05 17:03:16 Restored fileset 0,,10/1: returned code 0
93-Apr-05 17:11:43 Restoring fileset 0,,12/1
93-Apr-05 17:11:46 vols_Restore: returning 0
93-Apr-05 17:11:46 Restored fileset 0,,12/1: returned code 0
93-Apr-05 17:14:03 Restoring fileset 0,,10/1
93-Apr-05 17:14:04 vols_Restore: returning 0
93-Apr-05 17:14:04 Restored fileset 0,,10/1: returned code 0
root@digital5  # cat RepLog
93-Apr-05 14:04:22 repserver: log initialized to /opt/dcelocal/var/dfs/adm/RepLog
93-Apr-05 14:04:22 Replication server started.  Mainprocs=1; tokenprocs=4; verbose=0.
93-Apr-05 14:05:09 repserver: FL server not yet up for VL_GetCellInfo: FLDB: cannot create FLDB with read-only operation (dfs / vls)
93-Apr-05 14:05:43 repserver: no aggregates on this server

[4/7/93 public]
This configuration is about to go away.  Do we have all the info we 
need?

[4/7/93 public]
This information is great.  I can probably make good progress with it as-is.
The only little additional information I could use, if possible, is the FtLog
on shotz, back when the repeating information started to come out.  There
should have been two ftserver transactions going on, one to read fileset
11 (which was continuing until the present) and the other to restore into
fileset 14 (on the same aggregate on shotz).  What would be useful would be
what happened to the ftserver transaction that was restoring into fileset 14.
 
For that matter, on the off chance that this config is still around, it would
be useful to see where the ftserver is spending its time.  Presumably it's in
some RPC executor thread (in SFTSERVER_Forward(), I'd guess), but it would be
truly great to know where that thread is stuck.
 
Basically, the repserver is asking the ftserver to forward an incremental
dump of fileset 11 (from volversion 153) into a restoration onto fileset 14,
which is a copy-on-write clone of fileset 12--all on the same aggregate.
There's some silly locking going on in the repserver that causes a high-
level lock to be held--in read mode--over the FTSERVER_Forward() call,
and the REP_AllCheckReplicationConfig() call that fts makes will block
waiting for that lock (in write mode).  Thus, there's a broad circle of
problems, but everything would have finished if SFTSERVER_Forward() would
either complete or fail; this is why it would be useful to know what the
first problem was.
 
If this information is now unavailable, I'll try to make some guesses, though
it would be most useful to get a stack trace of the offending ftserver
thread, or some hints from the FtLog (or maybe FtLog.old by now) on shotz.
Thanks for your help.

[4/7/93 public]
Unfortunately, we can't attach to runnable processes on the PMAX.
Here's the start of the Ftlog on shotz:
93-Apr-05 16:07:16 Log file initialized as /opt/dcelocal/var/dfs/adm/FtLog
93-Apr-05 16:07:18 Ftserver starting
93-Apr-05 16:11:15 ftserver_CreateVolume: created lfs_set as 0,,4 on aggr 11
93-Apr-05 16:45:49 ftserver_CreateVolume: created Barasso.699 as 0,,7 on aggr 11
93-Apr-05 16:58:22 ftserver_CreateVolume: created Lemieux.699 as 0,,11 on aggr 1
1
93-Apr-05 17:01:04 Restoring fileset 0,,12/11
93-Apr-05 17:01:07 vols_Restore: returning 0
93-Apr-05 17:01:07 Restored fileset 0,,12/11: returned code 0
93-Apr-05 17:08:33 Restoring fileset 0,,14/11
93-Apr-05 17:08:34 vols_Restore: returning 0
93-Apr-05 17:08:34 Restored fileset 0,,14/11: returned code 0
93-Apr-05 17:12:05 Restoring fileset 0,,12/11
93-Apr-05 17:12:07 vols_Restore: returning 0
93-Apr-05 17:12:07 Restored fileset 0,,12/11: returned code 0
93-Apr-05 17:14:23 Restoring fileset 0,,14/11
93-Apr-05 17:14:24 vols_RestoreDir: Dumped name is longer than allowed on this s
ystem (26144 bytes vs. 255 bytes allowed)
93-Apr-05 17:17:27 trans 10005308 (Id=35, 0,,14/11) is 184 seconds old (ref coun
t 1)
93-Apr-05 17:17:27 (trans 10005308: desc=1, time=734044463, ctime=734044452, sta
tes=0, accs=0x4, acce=691089536)
93-Apr-05 17:17:27 trans 106dec10 (Id=34, 0,,11/11) is 194 seconds old (ref coun
t 1)
93-Apr-05 17:17:27 (trans 106dec10: desc=0, time=734044453, ctime=734044452, sta
tes=0x18014005, accs=0x82, acce=691089536)
93-Apr-05 17:17:58 trans 10005308 (Id=35, 0,,14/11) is 215 seconds old (ref coun
t 1)
93-Apr-05 17:17:58 (trans 10005308: desc=1, time=734044463, ctime=734044452, sta
tes=0, accs=0x4, acce=691089536)
93-Apr-05 17:17:58 trans 106dec10 (Id=34, 0,,11/11) is 225 seconds old (ref coun
t 1)
93-Apr-05 17:17:58 (trans 106dec10: desc=0, time=734044453, ctime=734044452, sta
tes=0x18014005, accs=0x82, acce=691089536)
93-Apr-05 17:18:28 trans 10005308 (Id=35, 0,,14/11) is 245 seconds old (ref coun
t 1)
93-Apr-05 17:18:28 (trans 10005308: desc=1, time=734044463, ctime=734044452, sta
tes=0, accs=0x4, acce=691089536)
93-Apr-05 17:18:28 trans 106dec10 (Id=34, 0,,11/11) is 255 seconds old (ref coun
t 1)
93-Apr-05 17:18:28 (trans 106dec10: desc=0, time=734044453, ctime=734044452, sta
tes=0x18014005, accs=0x82, acce=691089536)
93-Apr-05 17:19:00 trans 10005308 (Id=35, 0,,14/11) is 277 seconds old (ref coun
t 1)
Here's the last reference I can find to that transaction in the log:
93-Apr-05 20:46:01 trans 10005308 (Id=35, 0,,14/11) is 324 seconds old (ref coun
t 0): GCing
In between, it looks like it got very old with reference count of 1,
then its reference count went to 0 and it wasn't very old anymore (??).
Here's the first reference to that transaction:
This is where the references to that transaction stop:
93-Apr-05 20:46:01 trans 10005308 (Id=35, 0,,14/11) is 324 seconds old (ref coun
t 0): GCing
There are references to transaction 10005308 up until Apr-05 at
20:46 at which point

[4/8/93 public]
The plan is to address the problems that made the failure get out of hand.
In particular, we'll catch exceptions while forwarding volumes in both the
ftserver and repserver.  We'll also add a sanity check on file name length in
vols_DumpDir to match the one in vols_RestoreDir.  From the trace above, it
appears that the restore dir check may have started the ball rolling on
this particular failure.  An inspection of the code hasn't revealed any
obvious problems, so it can only help to try to detect the problem sooner
(at dump time).
Changed Responsible Engr. from `cfe@transarc.com' to `jdp@transarc.com'

[4/8/93 public]
--Other explanation below--
Just eyeballed and compiled these changes to get them into today's build.
Tested on TA build:  
dfs-102-2.8
Tested with backing build:  
dce1.0.2b19
Filled in Transarc Deltas with `jdp-ot7656-catch-exceptions-while-forwarding' 
Changed Transarc Status from `open' to `export'

[4/8/93 public]
Changed Transarc Status from `export' to `import'

[4/15/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with 
 `file/ftserver/ftserver_dump.c,file/ftserver/ftserver_vprocs.c,file/rep/rep_mai
 n.c' 
Changed Transarc Status from `import' to `submit'

[4/15/93 public]
We've seen it again with the above described fixes included.  Re-opened.

[4/21/93 public]
Apparently, none of these fixes had made it into 1.0.2.  Elliot re-submitted
them in connection with OT 7773.  I'm re-marking this as ``fix''.
I indeed stared at cobbler&shotz for several hours, but given this situation,
I have no need of their current configuration now.  Thanks.
Filled in Inter-dependent CRs with `7773' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7652
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Install of DFS tests needs fixing
Reported Date                 : 4/5/93
Found in Baseline             : 1.0.2
Found Date                    : 4/5/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : test/file/Makefile
Sensitivity                   : public

[4/5/93 public]
I noticed a couple of problems with the DFS test install:

(1) The rep tests apparently aren't being installed at all.

(2) The test/file directory contains the scripts runone and
runavg.  They are essentially uncommented and undocumented.  
They appear to be master scripts of some sort for running
low and fs tests.  We need to figure out whether they should
be there, and what documentation (README, in-line comments, etc.)
should be included with them.

(3) The README files in many of the test directories are not
being installed (fts, cm, maybe others).

[4/5/93 public]
Re-assigned to Gail.

[4/12/93 public]
Adjusted Makefiles to solve (1) and (3) above have been submitted. No
consensus has been reached on (2). Downgrading severity/priority since majority
of the problems has been addressed.

[9/28/93 public]
Defuncted runavg, runone and updated test/file/Makefile - status = fix.

[10/1/93 public]
Verified nightly build install tree does not contain either - closing.



CR Number                     : 7643
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : xvnode glue
Short Description             : xglue_rename typo
Reported Date                 : 4/2/93
Found in Baseline             : 1.0.2
Found Date                    : 4/2/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : file/xvnode/xvfs_aixglue.c
Sensitivity                   : public
Transarc Deltas               : jaffe-ot7643-fix-typo-in-glue
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/2/93 public]
xglue_rename() has a typo where it calls
xvfs_GetVolume(dp, &volp1) and then xvfs_GetVolume(dp, &volp2).
The second call should probably be xvfs_GetVolume(tdp, &volp2).

[5/17/93 public]
just fixing Elliot's name for sorting purposes.
Changed Responsible Engr. from `Elliot_Jaffe@transarc.com' to 
 `jaffe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/1/93 public]

[6/1/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Tested by running low test in moderate mode on jfs through glue.
Associated information:
Tested on TA build:  dfs-carl-1.11
Tested with backing build:  dce1.0.2ab2
Changed Affected File from `xvfs_aixglue.c' to `file/xvnode/xvfs_aixglue.c' 
Filled in Transarc Deltas with `jaffe-ot7643-fix-typo-in-glue' 
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `export'

[6/1/93 public]
Changed Transarc Status from `export' to `import'

[6/2/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7635
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : volume
Short Description             : fileset move underload gets VOLERR_PERS_DAMAGED
Reported Date                 : 4/2/93
Found in Baseline             : 1.0.2
Found Date                    : 4/2/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2a
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot7635-reserve-offline-outofservice
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[4/2/93 public]
During a run of moving a fileset back and forth under load the fileset
eventually became unavailable with the following error message:
 
# ls
dfs: fileset (id = 0,,487) error (code 691089415) from server 130.105.5.78 in cell dfs_cho_cell.
dfs: fileset (0,,487) is not available!
ls: The file . does not exist.
 
vol_errs.h:#define VOLERR_PERS_DAMAGED                      (691089415L)
 
I will try to get more information if possible.  I'd appreciate it
if someone at transarc could comment on what information might
be interesting to look at.
 
Configuration: Standard cho
m1: Rios core server + dfs server + fldb + dfs client exports lfs, jfs
m2: Rios core client + dfs server + dfs client +fldb
m3: Pmax core client + dfs server + dfs client + fldb exports lfs ufs
m4: Pmax core client + dfs client
 
There is a fileset moving back and forth between m1 and m3 every half
hour.  m4 runs low.moderate to this fileset and m2 runs connectathon.
There were also some files deleted which were open by other processes
at the time they were deleted.  The fts move command is issued from m2.
During the course of the run the fileset becomes unavailable with the
aforementioned error message.

[4/2/93 public]
Changed Responsible Engr. from `jsk@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[04/2/93 public]
I don't know if this helps but here's the relevant in-core volume
structure for the volume with the problem.  This is a hex dump and
each value represents 32 bits.
 
0          0          0          0xc27a7700
8041bfc8   0          1          0xc29d0ae0
0          1e7        0          0
0          0          0          0
0          0          1          0
0          10021      0          0
0          0          0          0
0          0          0          0
6c2e316d   327366     0          0
 
+ 24 more zeros

[4/2/93 public]
This error condition persisited for about 11-12 hours.  I then did a
bos restart on the server where this fileset was residing.  This
indirectly? caused the fileset to get moved (or finish moving) to the other server
and then the VOL_PERS_DAMAGE errors vanished.
 
Can someone explain what is happending here?

[4/2/93 public]
OK, here's what seems to be happening.
 
The struct volume you quote is for fileset ID 487, named m1.lfs2, and it
has (or had) VOL_OUTOFSERVICE set in the states field.  That's why it
was returning the VOLERR_PERS_DAMAGED (in vol_misc.c, vol_StartVnodeOp()).
Now, VOL_OUTOFSERVICE is set in the target copy of a fileset when a fileset
move is started, but it's cleared well before the FLDB is changed to point to
the target.  Another time that VOL_OUTOFSERVICE is set is if a fileset is
opened for some sequence of operations by ftserver or repserver, but then
there are no fileset operations performed on that fileset for some interval
of several minutes; the exporter (in xvolume/vol_desc.c) will set the
VOL_OUTOFSERVICE flag.
 
There are two things that would be useful: one is some indication of what the 
fts move command was saying (and it would be even more helpful if it were
being run in -verbose mode) when the failure occurred.  The other would be
some clue as to why doing a bosserver restart would have shut down the
VOL_OUTOFSERVICE flag.  If the fts-moves were in a script that didn't shut
down, then it's quite possible that one of the successive events would have
shut off VOL_OUTOFSERVICE.  If that was the case (that fts move was still
being run), then maybe the bosserver restart repaired a broken ftserver
or flserver and let fts move do its repair work.

[4/6/93 public]
Did I not get back to this with questions soon enough?  I've tried going
through our copy of the code that you have, and can't decide which of many
alternatives would cover what you're seeing.  If there's no more information
to be had here, I'd suggest that we cancel or downgrade this report.

[4/6/93 public]
Downgraded pending more information.
Changed Priority from `1' to `2'

[04/06/93 public]
I'd like to run the scenario more before we decide to
cancel this.  I have not had the opportunity to rerun the tests
since I've been trying to upgrade the cell to the new
build with the new flserver.

[04/07/93 public]
I was setting up the cho tests and wanted to move a fileset back to the rios
to run pmax client rios server tests.  I tried 3 times to unsucessfully move
the fileset (comm failure) now I the volume is not available and I am getting
periodic error messages for 691089513 and then 691089415.  It has been at least
20 minutes and the volume is still not available. I didn't run verbose because
I naievely did not expect it to fail.
root@fire  # fts move -file m1.lfs2 -fromserver fire -toserver alcatraz -fromaggr m3.aggr1 -toaggr m1.aggr2
Error in move: communications failure (dce / rpc)
root@fire  # 
root@fire  # cd /project/dce/build/dce1.0.2/export/pmax/usr/include/dcedfs
root@fire  # fgrep 691089513 *
vol_errs.h:#define VOLERR_TRANS_CLONE                       (691089513L)
root@fire  # cd /.:/fs
root@fire  # cd m1.lfs2
dfs: fileset (0,,487) is busy with code 691089513
dfs: fileset (0,,487) is busy with code 691089513
dfs: fileset (0,,487) is busy with code 691089513
dfs: fileset (id = 0,,487) error (code 691089415) from server 130.105.5.78 in cell cho_cell.
m1.lfs2: No such file or directory
root@fire  # fts statftserver -server /.:/hosts/fire
No active transactions on /.:/hosts/fire
root@fire  # 
# fts statftserver -server /.:/hosts/alcatraz
dfs: fileset (id = 0,,487) error (code 691089415) from server 130.105.5.78 in cell cho_cell.
dfs: fileset (id = 0,,487) error (code 691089415) from server 130.105.5.78 in cell cho_cell.
dfs: fileset (id = 0,,487) error (code 691089415) from server 130.105.5.78 in cell cho_cell.
Total transactions: 1
--------------------------------------
trans: 14  created: Wed Apr  7 14:07:07 1993
descriptor: 0; ref count 0;  last call: Wed Apr  7 14:07:09 1993
fileset: 0,,526  aggregate Id: 2   aggrtype: 2/1
action: create new fileset (dfs / xvl); ops: Delete, SetStatus
Fileset status 0x10001: R/W, type-RW
--------------------------------------
I'll retry the fts move with verbose to see if anything interesting comes out.

[04/07/93 public]
Note that this is bl22 (today's nightly).
here's fts move -verbose:
root@fire  # fts statftserver -server /.:/hosts/fire
No active transactions on /.:/hosts/fire
root@fire  # fts move -verbose -file m1.lfs2 -fromserver fire -toserver alcatraz -fromaggr m3.aggr1 -toaggr m1.aggr2
Cloning fileset 0,,487 to 0,,562 (name m1.lfs2.move-temp, Reclone=0)
Creating token keep-alive thread.
Revocation socket is: inet/130.105.5.78/36612
Creating RPC listener thread.
About to get token 404 for fileset 0,,487 from fire.osf.org.
Listening for net calls (calling rpc_server_listen)
Connecting to file server on fire.osf.org...
STKN_InitTokenState called with flags of 0
...connection done (result 0)
Token 734203501,,71356, type 0x404, obtained on fileset 0,,487.
FTSERVER_Clone failed to clone fileset (trans=11, name=m1.lfs2.move-temp)
Error: communications failure (dce / rpc)
Pinging server fire.osf.org....done (returned 0)
Releasing token (1) for fileset 0,,487 from fire.osf.org.
Releasing token on file server...done.
FTSERVER_DeleteVolume failed for fileset associated with trans 14
Error: communications failure (dce / rpc)
FTSERVER_AbortTrans failed to End trans 14

[04/07/93 public]
just adding the rest of the verbose stuff.

[4/7/93 public]
This is exactly the symptom I'd expect out of OT 7608 (comm failure doing
possibly-lengthy fileset operations on large-ish filesets).  If the fileset
that you were trying to move was running the low tests (for example), then
this is what you'd expect as a consequence of 7608.  Indeed, I produced the
traces for 7608 from a fileset that had had the low tests running in it more
than once.  Is this just a dup?

[04/07/93 public]
7608 only addresses certain timeout problems.  This might be part of
what we're seeing here, although I'm not running low moderate while
moving this fileset.  What I think is not correct is that fts move
cannot clean up after itself and leaves the fileset in a state
where it cannot be accessed by anyone.
(it's been over an hour and I still can't access this fileset.)

[4/7/93 public]
7608 addresses the problem of RPC timeouts while doing clone/reclone/unclone/
destroy operations on filesets with lots of files, or lots of file data,
in them.  Running the low tests will produce such filesets, particularly
if those tests are interrupted in the middle by failed fts move commands.
That's exactly what happened here, I believe.  The FTSERVER_Clone() RPC
call fails with comm_failure.  Further attempts to use that binding
handle to the ftserver then fail, also with comm_failure; thus, fts cannot
clean up after itself.  Even if fts were to mint a new binding handle for
RPCs, it would catch the ftserver in a very funny state, trying to delete
a transaction that was still in progress (continuing the clone).
 
We have in progress a more general design for cleaning up after failed
fileset moves, but anticipated that fixing it would fall off the end of
the 1.0.2 frame into 1.0.2a, which I believe is along the lines of what
Dave L. had said yesterday.  I believe that the redesign of the failed
move cleanups will handle what's required here (including bringing the
fileset back on line), but we should adjust priorities to reflect the
expectations on fileset moves.

[4/8/93 public]
I figured it would be a good idea to extract part of what I've been working
on with respect to cleanups, and to submit that to address the problem that
filesets get odd status bits due to timeouts, but those bits are never
cleaned up.  The bits in question are the VOL_OUTOFSERVICE bit, which
is set when a fileset descriptor times out, and VOL_OFFLINE, which is set
at attach time when a fileset is marked inconsistent (VOL_DELONSALVAGE).
Those bits are redundant anyway, since the file system already does a fine
job of maintaining VOL_DELONSALVAGE all by itself.
 
Thus, even after a failed clone (as was seen above), any good fileset will
return to service within a few minutes, and any other problems can be
addressed by re-executing the command (such as if the clone really was
stopped half-way through).  There are still cases where a fileset move will
not be cleaned up completely properly, but this change will allow the
administrator to return the system to service (by re-executing the fileset
move command).
 
Defect Closure Form
-------------------
--Verification procedure below--
Clone a fileset but abort fts or ftserver midway through.  If you're lucky,
the fileset will be left busy.  After the busy fileset descriptor times out
(in 10 minutes or so), you will be able to recover the fileset by re-starting
the clone, or the fileset will be on-line again all by itself.  (Note the
exception above on fileset move cleanups.)
 
Associated information:
 
Tested on TA build:  dfs-102-2.8
Tested with backing build:  dce1.0.2b19
Filled in Transarc Deltas with `cfe-ot7635-reserve-offline-outofservice' 
Changed Transarc Status from `open' to `export'

[4/9/93 public]
Changed Transarc Status from `export' to `import'

[4/15/93 public]
submitted on 4/14/93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7607
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : dfsbind,cm
Short Description             : Can't run DFS on cell with X.500 Name
X.500 Name
Reported Date                 : 3/29/93
Found in Baseline             : 1.0.2b20
Found Date                    : 3/29/93
Severity                      : A
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/dfsbind/dfs_helper.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[3/29/93 public]
 
After configuring a single-machine pMax cell-cum-filesystem, using a valid
X.500 cell name (from the XDS switch test script, no less) the following
unfortunate occurance, uhhh, occurred:
cdscp> show dir /.:
 
                        SHOW
                   DIRECTORY   /.../c=ie/o=digital
                          AT   1993-03-29-10:42:24
            RPC_ClassVersion = 0100
                     CDS_CTS =
1993-03-29-19:31:10.772635100/08-12-34-5f-ff-ff
                     CDS_UTS =
1993-03-29-20:32:15.880761100/08-12-34-5f-ff-ff
              CDS_ObjectUUID = 0075e50f-4e7e-1bb7-a7bb-0812345fffff
                CDS_Replicas = :
          Clearinghouse UUID = 001be66c-4e77-1bb7-a7bb-0812345fffff
                       Tower = ncacn_ip_tcp:130.105.5.52[]
                       Tower = ncadg_ip_udp:130.105.5.52[]
                Replica type = master
          Clearinghouse Name = /.../c=ie/o=digital/prelude_ch
                 CDS_AllUpTo = 0
             CDS_Convergence = medium
                CDS_InCHName = allowed
        CDS_DirectoryVersion = 3.0
            CDS_ReplicaState = on
             CDS_ReplicaType = master
               CDS_LastSkulk =
1993-03-29-19:31:10.772635100/08-12-34-5f-ff-ff
              CDS_LastUpdate =
1993-03-29-20:32:15.880761100/08-12-34-5f-ff-ff
             CDS_RingPointer = 001be66c-4e77-1bb7-a7bb-0812345fffff
                   CDS_Epoch = 0075e50e-4e7e-1bb7-a7bb-0812345fffff
          CDS_ReplicaVersion = 3.0
             CDS_GDAPointers = :
                     Timeout = :
                  Expiration = 1993-03-30-15:32:08.021-05:
                   Extension = +1-00:00:00.000I0.000
          Clearinghouse UUID = 000cfb48-4eae-1bb7-b52e-0812345fffff
                       Tower = ncacn_ip_tcp:130.105.5.52[]
                       Tower = ncadg_ip_udp:130.105.5.52[]
                Replica type = gda
          Clearinghouse Name = /.../c=ie/o=digital/prelude_gda
cdscp> quit
 
#
#  Here's the error:  we try to cd into DFS . . .
#
 
root@prelude  # cd /: 
/:: Invalid argument
root@prelude  # cd /.:/fs
/.:/fs: Invalid argument
root@prelude  # cd /.../c=ie/o=digital/fs
/.../c=ie/o=digital/fs: Invalid argument
root@prelude  # cd /.../c=ie/o=digital
/.../c=ie/o=digital: Invalid argument
root@prelude  # cd /...
root@prelude  # cd c=ie/o=digital/fs
c=ie/o=digital/fs: No such file or directory
root@prelude  # mkdir /tmp/c=ie
 
# note:  '=' is a valid filename char, even on a pMax . . .
root@prelude  # fts lsfldb -server prelude
 
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner
   
prelude.osf.org     RW       /export 0:00:00 hosts/prelude  <nil>
   
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
root@prelude  # lsaggr -server prelude
There is 1 aggregate on the server prelude (prelude.osf.org):
              /export (/dev/rz1c): id=1     (non-LFS)
Anyway, I have to make this a priority one.  We're supposed to be able to 
do this.

[3/29/93 public]
Forgive me, but it's not a simple job to do the X.500 configuration,
particularly given that we have essentially no local expertise.  Thus,
I'd like to ask you (Dave T) to run a couple of experiments, if you would.
 
(a) After trying doing the ``cd'' commands, give dfsbind the ICL signal and
    tell us what's in the dumped ICL file.
(b) Try running the dfsbind_test executable that you'll find as
    obj/pmax/file/dfsbind/dfsbind_test, and hand it some X.500 names,
    partial or full (like c=ie or c=ie/o=digital ) and tell us what
    it says.
 
For that matter, ideally you could use dfsbind_test as an X.500 test driver
to tell if the underlying problem is in the X.500 support or in DFS.
Feel free to reassign this OT to me once you've tried these commands, if
you indeed believe that the problem continues to be in DFS.  Thanks!
Changed Short Description from `Can't run DFS on cell with' to `Can't run DFS 
 on cell with X.500 Name' 
Changed Interest List CC from `Teodor.Dumitrescu@ap.mchp.sni.de' to 
 `Teodor.Dumitrescu@ap.mchp.sni.de, cfe' 
Changed Responsible Engr. from `Elliot_Jaffe@transarc.com' to `treff' 
Changed Resp. Engr's Company from `tarc' to `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/30/93 public]

[4/1/93 public]
Dfsbind handles the pathname a component at a time.  The basic problem
is that there's no way to tell that the first component is valid:
	root@prelude  # cdscp show dir /.../c=ie
	
	                        SHOW
	                   DIRECTORY   /.../c=ie
	                          AT   1993-04-01-04:28:49
	Error on entity: /.../c=ie
	Software error detected in server (dce / cds)
	Function: dnsEnumAttr
Dfsbind first calls sec_id_parse_name, to try to get a cell name.
When called on c=ie, it returns sec_rgy_object_not_found.  It would be
nice if this could return some kind of 'partial results' error code.
If not, though, dfsbind could be changed to call
rpc_ns_entry_inq_resolution().  Currently, when this function is
called on c=ie, it returns CDS_NAMESERVERBUG (Software error detected
in server).  Dfsbind has to have something better than this to look
for. 
Changed Component Name from `dfs' to `cds' 
Changed Interest List CC from `Teodor.Dumitrescu@ap.mchp.sni.de, cfe' to 
 `Teodor.Dumitrescu@ap.mchp.sni.de, cfe, comer@transarc.com' 
Changed Priority from `1' to `0'

[04/01/93 public]
At Mark Fox' request, during the conference call today Mark Hickey
ran the gda_child process directly, passing it the name shown
above, and observed the error code returned.  Specifically, Mark H.
tried the commands:
 
	% gda_child /.../c=ie
 
	% gda_child /c=ie
 
	% gda_child c=ie
 
In all three cases the status returned by the command was 3.

[4/1/93 public]
Here are the definitions of the status codes returned by gda_child (ref.
export/*/usr/include/dce/gda_x500.h) to the gdad:
 
/*
 * Child process exit codes
 */
#define CHILD_OK                0       /* Cell name looked up successfully */
#define CHILD_UKNOWNENTRY       1       /* Name not in X.500 */
#define CHILD_UNDERSPECIFIED    2       /* Name in X.500, but not a cellname */
#define CHILD_ERROR             3       /* Miscellaneous runtime error -- */
                                        /* see syslog. */
 
I believe the gdad maps CHILD_ERROR to CDS_NAMESERVERBUG.

[4/1/93 public]
So is this now a GDA bug that Siemens should look into?
Should we look at what was recorded in syslog by the attempts to
evaluate /.../c=ie ?

[4/1/93 public]
Letting DFS bug-admin track this bug too.
Changed Interest List CC from `Teodor.Dumitrescu@ap.mchp.sni.de, cfe, 
 comer@transarc.com, bolinger, Helmut' to `Teodor.Dumitrescu@ap.mchp.sni.de, 
 cfe, comer@transarc.com, bolinger, Helmut, dce-ot-dfs'

[4/2/93 public]
Sorry - forgot to change component.

[4/2/93 public]
 
Ok, I've got bad news, and bad news.  The bad news is:
 
	1.  CHILD_ERROR is the default error code returned by gda_child.
	It means all kinds of things.	In this case, it's 
	(src/directory/gds/gda/gda_child.c):
 
	/*
	 * Bind to a DSA, read the cell data and unbind
	 */
	if (x500_bind(&bind_id) == X500_ERROR) {
		exit_code = CHILD_ERROR;
		goto rtnhere;
	}
	and what it means is:  gds isn't running.  No kidding.  You don't
	need gds running to have a cell named with an X.500 name.  So as
	far as the gda is concerned, the error is that it can't help
	CDS out.
 
        2.  The reason that CDS calls the gda in the first place
	is that all it got as input (as Mike Comer points out, above)
	from DFS was /.../c=ie.  CDS __knows__ its own name is
	c=ie/o=digital, but it's not it's job to do partial string matching
	on cell, or any other, names.  In fact, it's quite possible that
	there's a c=ie/o=guinness, and a c=ie/o=waterford, etc.  So it
	kicks it over to gds, which __could__ return a binding to
	c=ie, since gds knows what that is.  That is, if gds were running
	and the DSA could find an entry for it.
 
	So, the only way for DFS to get to the local-cell-with-an-X.500-name,
	as things are currently implemented, would be to call CDS with
	the __entire__ cell name broken out of the pathname, which is
	pretty tough, given the fact that "fs" is no longer a well-known
	name, etc.  The idea of doing repetetive calls out with more and
	more segments of the pathname when errors are returned, until 
	either CDS returns success, or the end of the pathname is reached,
	is truly disgusting, but it __could__ work.  A simple readlink(2)
	call on /... could get it, too, but . . ..  
 
	Now, if a gds were running in the cell, _on_ the DFS client machine,
	and if everything was set up right in a DSA somewhere in the cell,
	dfsbind could end up doing a __cross__cell__ binding to the
	DFS server in it's own cell.  Now, that's unbelievably gross.
 
	3.  But wait, there's more.  In looking at bind_helper.c, I
	help but notice the following in 
 
int do_GetBind(inStr, inSize, outStr, outSizeP)
     char *inStr;
     long inSize;
     char *outStr;
     long *outSizeP;
{
//	inStr has the partial cell name string . . .
// 	stuff deleteed . . .
 
    static char preFix[] = "/.../";
//
//	more stuff deleted, until:
 
    inStr -= strlen(preFix); 
    inSize += strlen(preFix);
    bcopy (preFix, inStr, strlen(preFix));
 
//  and then 
 
    sec_id_parse_name (sec_rgy_default_handle, inStr, cellName,
                       (uuid_t *)NULL, prinName,
                       (uuid_t *)NULL, &code);
	Does it look like dfsbind puts a "/..." in front of every
	presumed cell name, partial or otherwise?  Looks like a bug to
	me.  So, if I'm correct, it looks like _any_ 2-or-more-"dir"
	X.500 cell name would never be matched in the piecewise fashion, 
	even if it could be found in gds.  
     So:  I'm kicking this back to Transarc, in the form of Mike Comer,
as he was unlucky enough to be this bug's last respondent from Pittsburgh.
I agree that a little more granularity in error returns from gda_child
and cds are indeed in order here, BUT:  it's a DFS bug.  CDS and the gda
are doing the right things, and DFS is going to have get a little smarter
about identifying the cell name before calling out into the cell.  Somehow.

[4/3/93 public]
Not related to the real problem, but germane to a previous note...
 
In the shower this morning I realized that the gda_child experiment above
was flawed. gda_child was correct to return a 3 when Don ran it from the
shell. Had he translated the attribute type (c in this case) to an object
id in the form that the gda_child expects it (85.4.6=ie I believe), and if the
gds were running, he probably would have gotten an exit status of 2
(underspecified name).

[4/5/93 public]
OK, in answer to Dave Treff's observations with which he reassigned the
problem to DFS.
 
(1) By the very architecture of the thing, the naming-helper part of
dfsbind (as well as most all of the CM itself) doesn't treat the
``local cell'' any differently than it does any other cell.  If gds has
to be running in order to get at cells named in X.500 space that aren't
the local cell, and if Dave wasn't running gds, then his experiment was
flawed; he should fix that and try again.
 
(2) Again, a far deeper architecture prevents DFS from accumulating things
that look like a cell name until they look like a complete cell name, and
then trying that: it's the vnode layer, where DFS, in the form of
cm_lookup(), gets handed the name components one at a time.  DFS *must*
do something with these partial components.  It asks dfsbind, which asks
CDS, which presumably would ask GDS.  Once dfsbind (and CDS) were given
a complete X.500-registered cell name that happened to be the local cell
name, I don't know why CDS wouldn't recognize it as its own name and build
local bindings.
 
I have no idea why Dave T. is wondering about special-casing access to
the local cell.
 
Now, dfsbind could cheat and claim that any name is legitimate; it could
even limit always-legitimate names to those that have equal-signs in them.
This seems like a really bad idea, since people could construct paths like
/.../c=xx/o=Antarctic_Forestry_Service/dogs=bears/2+2=5/man:woman=fish:bicycle
or some other sillyness, with no reality check.  I really believe that
dfsbind must acquire information about the intermediate nodes (RDNs, I guess)
and whether they could possibly be the prefix to an X.500-registered cell
name.
 
(3a) In the course of solving this OT report, we'll probably learn how to
make a more defensive set of calls in dfsbind.  There could be multiple
problems here, but I believe that the first order of business is to get the
lower levels' house in order before figuring out what has to change in
DFS to accommodate it.  It should take essentially no time to adapt dfsbind
to do the right thing, once we discover what that right thing is.
 
(3b) I have no idea what Dave T.'s complaint about do_GetBind.  The name
that comes in to do_GetBind has the ``/.../'' stripped off it, so it has to
be re-instated before calling any DCE function.  This means that yes,
indeed, we'll be asking about /.../c=ie and /.../c=ie/o=waterford and
/.../c=ie/o=waterford/ou=usa_sales and all that kind of stuff, and if
we can't get decent analyses of those names from *some* CDS or security
function, we have no alternative but to ask either that those functions be
made to work, or to ask what those functions should be.
 
Meta-comment.  It seems to me that Dave T's expectations about what should
be happening here are flawed--that there are hard constraints of incremental
cell name resolution, that I believe are well-known within the DCE project.
A priori, I have no idea what kinds of gds software, or X.500 GDA software,
one has to run to have one's cell be usefully registered in X.500-land;
I would expect that this is an area of expertise for Siemens, the CDS folks,
and the OSF.  I would hope that we could ask the OSF to shoulder the
burden of running an X.500-configured cell on which to carry out the
dfsbind (or gda_child) test.  It appears that Dave T's initial test was
incomplete in that it couldn't possibly allow for the incremental
resolution of cell names (which has been part of the design since its
inception), and I'm going to have to ask the OSF to set up a more appropriate
test environment.  I don't know Dave T's availability, so in his possible
absence I'm assigning this to John Dugas, who may want to reassign it.
 
At present, I believe that dfsbind (the bind_helper.c part) will have to
be changed to tolerate no-registry-found errors from sec_id_parse_name(),
and then to handle codes (from rpc_ns_entry_inq_resolution()) of the form
DNS_UNDERSPECIFIEDNAME or whatever else comes recommended.
Changed Subcomponent Name from `dfsbind, cm' to `' 
Changed Interest List CC from `Teodor.Dumitrescu@ap.mchp.sni.de, cfe, 
 comer@transarc.com, bolinger, Helmut, dce-ot-dfs' to 
 `Teodor.Dumitrescu@ap.mchp.sni.de, cfe, comer@transarc.com, bolinger, Helmut, 
 dce-ot-dfs, mfox' 
Changed Responsible Engr. from `comer@transarc.com' to `jd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[4/5/93 public]
forgot to tag this as a GDS problem, or at least a GDS question.
Changed Component Name from `dfs' to `gds' 
Filled in Subcomponent Name with `testing?'

[03/07/93 public]
Ok:  Per our conference call yesterday, I'm knocking this down to a 2,
and re-assigning to me, with the idea of deferring it to 1.0.3
unless someone gets a bright idea by the end of the week or so.
I'm still fooling around with it.  I've found out
	1. dfsbind_test  using the FULL name (c=ie/o=digital), with or 
	without GDS running, causes ds_DoBind to return EISDIR rather than the 
	EINVAL returned by c=ie, alone.  This means that CDS is doing the
	proper full match of the pathname-as-cellname, but somehow
	either it or the helper disagree as to whether it's the local cell
	or not.  This tells us nothing about the short-term problem, but
	gives us clues to our problems in the long-term.
	2.  By yesterday afternoon I still hadn't had success doing a 
	"cross-cell" access of /.: via gds, but it's (I'm sure) some kind
	of pilot error which I can correct when I pick this up again on
	Friday.

[4/7/93 public]
Changed to an enhancement, as agreed during yesterday's conference call.
Changed Defect or Enhancement? from `def' to `enh'

[4/8/93 public]
Dave -- I have versions of dfsbind and dfsbind_test in a 
/afs/transarc.com/public/comer/dfsbind.tar that should handle 
multi-level cell names and that also do partial (string) matches
against the local cell name.  Once you nail down the gds problems,
you might want to give them a try or give me a pointer to the
environment and let me do it.

[10/14/93 public]
Re-assigning to bmw@ibm austin since he will submit the tested fix.
Hope this mail gets through..

[10/14/93 public]
Dropped change that was provided from DFS team at IBM.
Verified that change did indeed fix problem.

[12/17/93 public]
Closed.



CR Number                     : 7604
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : move udebug back to the src tree
Reported Date                 : 3/26/93
Found in Baseline             : 1.0.2
Found Date                    : 3/26/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : vijay-db3346-udebug-now-back-to-source-tree
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[3/26/93 public]
udebug was moved to the test tree to live with the ubik tests, but it
seems there might be problems using udebug if it lives in the test tree.
Most notably, it won't be able to install it in /opt/dcelocal/bin. This
would be too bad because this program is useful in determining the state
of ubik servers. So, it now comes back to the source tree.
Added field Transarc Deltas with value 
 `vijay-db3346-udebug-now-back-to-source-tree' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/26/93 public]
Changed Transarc Status from `open' to `import'

[4/1/93 public]
this was submitted as part of 7590
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7574
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ncsubik
Short Description             : ubik should use its own thread pool
Reported Date                 : 3/23/93
Found in Baseline             : 1.0.2
Found Date                    : 3/23/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[3/23/93 public]
Ubik interfaces should use their own thread pool instead of sharing execution
threads with that of the server itself. This would prevent the server 
interface from starving out the ubik interface of threads. This could result
in loss of quorum and other problems.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[4/1/93 public]
submitted as part of 7590 changes
Changed Priority from `2' to `1' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7570
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : insufficient quota in shutdown.test script
Reported Date                 : 3/23/93
Found in Baseline             : 1.0.2
Found Date                    : 3/23/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : file/episode/anode/{mp.int.test,mp.test,mp_repeat.test,mt.test,shutdown.test}
Sensitivity                   : public
Transarc Deltas               : ota-ot7570-measure-mp.int.test-quota-usage
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[3/23/93 public]
I found still another case where I didn't allow enough quota on a volume
in a script using mp.int.test for the worst case.  Make definitive
measurement and fix all affected scripts.  Update comment in
mp.int.test.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/23/93 public]
Defect Closure Form
-------------------
--Regression test program below--
The all_tests scripts should verify that this problem is fixed.  To test
this specific change I ran all the test_anode scripts that invoke
mp.int.test without trouble.
Associated information:
Tested on TA build:  
dfs-102-2.8
Tested with backing build:  
dce1.0.2b19
Filled in Transarc Deltas with `ota-ot7570-measure-mp.int.test-quota-usage' 
Changed Transarc Status from `open' to `export'

[3/26/93 public]
Changed Transarc Status from `export' to `import'

[3/31/93 public]
submitted on March 31 1993
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with 
 `file/episode/anode/{mp.int.test,mp.test,mp_repeat.test,mt.test,shutdown.test}'
Changed Transarc Status from `import' to `submit'

[4/7/93 public]
This is ota using shl's console.
Defect Closure Form
-------------------
A variant of this problem cropped up again and I have made another
change to this delta.  This delta is revision 1.2.
--Verification procedure below--
I ran mt.test and it succeeded.  Bruce Eppinger is running the whole
suite of test_anode tests (all_tests) concurrently.
Associated information:
Tested on TA build:  
dfs-102-2.9 (unreleased)
Tested with backing build:  
dce1.0.2b21
Changed Interest List CC from `jaffe, mason' to `jaffe, mason, shl, bruce' 
Changed Status from `fix' to `open' 
Changed Fixed In Baseline from `1.0.2' to `' 
Changed Transarc Status from `submit' to `export'

[4/7/93 public]
Imported into dfs-102-2.9.
Changed Transarc Status from `export' to `import'

[4/15/93 public]
submitted on 4/07/93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7568
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7657
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : kernel stack overflow
Reported Date                 : 3/23/93
Found in Baseline             : 1.0.2
Found Date                    : 3/23/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : file/kutils/krpc_dbg.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[3/23/93 public]
On a number of occasions we've seen the PMAX'es become "catatonic"
during DFS testing.  The symptoms of this catatonic state include:
- can't login
- won't respond to ping
- console is dark (screen saver) and won't wake up when you hit
  a key
- no response to the do key  (can't get into kdb)
The theory is a kernel stack overflow is causing these symptoms.
The current plan of attack for addressing this problem is:
1) try to get a crash dump by pushing the reset button.  We have
been unable to get this to work and are going back to recheck
the instructions we were given for this procedure.
2) bump the kernel stack size
3) add some sanity checking at context switch time to ensure
the stack pointer isn't "close" to overflow.
We're still verifying the feasibility of each of these approaches.
Feel free to suggest others :^>.

[3/25/93 public]
One thing to note is that dfs_icl_log allocates a somewhat large (for the
kernel) automatic buffer:
dfs_icl_printf(va_alist)
{
    va_list ap;
    char *fmt;
    char buf[1024];
    va_start(ap);
    fmt = va_arg(ap, char *);
    vsprintf(buf, fmt, ap);
    va_end(ap);    
    icl_Trace1(krpc_iclSetp, DFS_TRACE_RPC, ICL_TYPE_STRING, buf);
}
If you have rpc tracing turned on, you are thus somewhat likely to blow the
stack.  It might make sense to make `buf' either smaller or static.

[4/1/93 public]
Don has made the change suggested above.  Re-assigned to him.

[04/02/93 public]
Well, I made the change, but then Diane saw a similar un-kdbable hang
in the CHO cell _without_ KRPC ICL logging enabled.  I've spent some
time trying to figure out what's happening, but have only a limited
amount of data.  I have found two CM functions that currently allocate
very large stack frames, however, and I'd like to suggest changing at
least one of them to reduce the size of the frame.
First, some background:  I've been debugging this using a PMAX kernel
crash dump.  Unfortunately, this dump contains no registers for the
active process at the time of the dump, so, though I have the kernel
stack for the active thread, I don't know its current stack or frame
pointer.  The only "interesting" (i.e., DCE/DFS-related, non-blocked)
threads on the system are three threads running low/prog8 in moderate
mode.  Two of these threads are blocked in the CM, with the stacks:
task 0xc14601f0 [prog8               ] proc 0xc14bb360 pid 5511 map 0xc14bcc08 ss I: thr 0xc14e2ba0 thread 0xc14e2ba0, pcb 0xc3a22c08, W_EVENT 0
    0xc3a213b4 thread_block() from mpsleep+3ac
    0xc3a213ec mpsleep() from sleep+7c
    0xc3a2144c sleep() from pthread_cond_wait+b4
    0xc3a2147c pthread_cond_wait() from ccall_binding_serialize+2dc
    0xc3a214a4 ccall_binding_serialize() from rpc__dg_call_start+2d0
    0xc3a214fc rpc__dg_call_start() from rpc_call_start+2e8
    0xc3a21584 rpc_call_start() from VL_GetEntryByID+140
    0xc3a215bc VL_GetEntryByID() from cm_CheckVolumeInfo+e8
    0xc3a21774 cm_CheckVolumeInfo() from cm_Analyze+13f4
    0xc3a2213c cm_Analyze() from cm_rename+554
    0xc3a2220c cm_rename() from nosf_rename+ac
    0xc3a2283c nosf_rename() from xglue_rename+150
    0xc3a2286c xglue_rename() from rename+208
    0xc3a228fc rename() from syscall+27c
    0xc3a22adc syscall() from VEC_syscall+44
    0xc3a22b64 VEC_syscall()
         <Exception frame> Cause=0x20 Badvaddr=0x10005ffc SR=0xff3c EPC=0x73056500
    User stack:
task 0xc145fe18 [prog8               ] proc 0xc14bb078 pid 5512 map 0xc14bcdac ss I: thr 0xc14e2744 thread 0xc14e2744, pcb 0xc3a26c08, W_EVENT edata+478c
    0xc3a25bb4 thread_block() from osi_Wait+c4
    0xc3a25bec osi_Wait() from cm_CheckVolumeInfo+3d0
    0xc3a25c1c cm_CheckVolumeInfo() from cm_Analyze+13f4
    0xc3a265e4 cm_Analyze() from cm_remove+398
    0xc3a266b4 cm_remove() from nosf_remove+60
    0xc3a26a0c nosf_remove() from xglue_remove+188
    0xc3a26a34 xglue_remove() from vn_remove+13c
    0xc3a26a94 vn_remove() from unlink+4c
    0xc3a26abc unlink() from syscall+27c
    0xc3a26adc syscall() from VEC_syscall+44                  
    0xc3a26b64 VEC_syscall()                                  
         <Exception frame> Cause=0x20 Badvaddr=0x73060cb0 SR=0xff3c EPC=0x730572dc
    User stack:
The third thread is the active thread for which I have no registers and
for which kdb can't produce a backtrace.  The stack contents are
tantalizing but inconclusive.  At some point (currently or in the
past), this thread has completely filled its stack.  The topmost frame,
just below the end of a memory region belonging to another thread, is a
call from hardclock() to TIMER_WRITE_LOCK() to acquire a spin lock that
must be locked at splhigh() before reading/writing the kernel's
time-of-day clock value.  At the time of the dump, this lock was locked
and the current thread was the locker.  These facts may indicate that
the thread has self-deadlocked in trying to acquire the lock a second
time, or that it's blocked while holding the lock.  I've been unable to
find any circumstances in which either could happen.
However, one thing I did do was to examine the size of the stack frames
allocated by all routines mentioned in the active thread's kernel
stack.  Some subset of these, of course, represents the current call
chain through the stack, though I haven't figured out what the chain
is.  It seems plausible that the active thread has followed a call
chain through the CM similar to that of the other threads; it's
possible that this thread was also executing in KRPC "beneath" the CM
portion of the chain, based on the stack contents.  Anyway, two
routines that have been executed on this stack at some point are
cm_CheckVolumeInfo, with a frame size of 0x9c8 (2504) bytes, and
cm_rename, with a frame size of 0x630 (1584) bytes.  And of course
cm_rename calls cm_CheckVolumeInfo (see the stacks above).
In looking at cm_rename, I don't see any trivial way to reduce its
frame size (lots of smallish structs).  But cm_CheckVolumeInfo seems to
owe its frame size primarily to a struct vldbentry -- could this please
be dynamically allocated, or perhaps a pool of them maintained?  Given
the frequency with which this function seems to be called, it seems
dangerous for it to have such large stack frame requirements.
In short, though I can't explain what was happening on this machine at
the time of the crash, I think the failure can be avoided by lessening
kernel stack demands.  I'll be checking more globally for large stack
frames, to try to pre-empt this problem in the future.

[4/5/93 public]
Indeed, it should be possible to change cm_CheckVolumeInfo to allocate one
of the 8k buffers kept for just such purposes, just as does
cm_GetVolumeByNameWithHint(): you'd do something like
	struct vldbentry *vep;
	vep = (struct vldbentry *) osi_AllocBufferSpace();
	(use the buffer, then:)
	osi_FreeBufferSpace((struct osi_buffer *)vep);
I'm at work on another problem or else I could do this.  Clearly, you have
to free the osi_buffer pointer exactly once, on all exit paths.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[04/09/93 public]
The change described just above has been made under OT 7657.  I'll
reserve this defect to track the change made to dfs_icl_printf() to
reserve a buffer statically rather than on its stack.  This change has
been submitted now for a week or so.  However, rather than moving this
defect to fix now, I'd like to leave it open until the changes for 7657
are submitted, and until Transarc and OSF both have run for some period
of time without seeing the un-kdbable hang problem.

[04/15/93 public]
In accordance with our discussion at the tech conference call
this morning I'm marking this defect as fixed (i.e. the change 
to dfs_icl_printf() has been submitted by Don to 1.0.2).
If the unkdbable hang re-appears, we'll open another bug.

[4/16/93 public]
we hit it again, I'll open a new bug

[12/17/93 public]
Closed.



CR Number                     : 7552
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dce_config
Short Description             : growaggr not installed
Reported Date                 : 3/19/93
Found in Baseline             : 1.0.2
Found Date                    : 3/19/93
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : src/config/dce_config
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[3/19/93 public]
Going through admin_checklist - wanted to try out growaggr - not on
either pmax or rios.

[3/22/93 public]
Filled in Interest List CC with `jdp@transarc.com, pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar' to `jaffe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[3/22/93 public]
The problem here is that dce_config does not install it.  I'm assigning it
to Ron since the OSF traditionally handles dce_config problems.
Filled in Subcomponent Name with `dce_config' 
Changed Responsible Engr. from `jaffe@transarc.com' to `rsarbo' 
Changed Resp. Engr's Company from `tarc' to `osf'

[6/7/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 7526
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 8618
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Document restrictions on intercell fileset moves
Reported Date                 : 3/17/93
Found in Baseline             : 1.0.2
Found Date                    : 3/17/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[3/17/93 public]
BUILD:	nb available 3/11/93 (aka dfs 2.8)
CONFIG:	3 pmax cell, 2 flservers
TEST:	admin_checklist
Before syncfldb, ufs partition dfs exported with fileset id 0,,4.
After syncfldb, fldbentry for above ufs partition gone.
Expected behavior (which I believe I have seen - but perhaps only
when conflict was between 2 lfs filesets?) is to have new fldbentry
renumbered to next available fileset id.
BEFORE:
root@dce12> fts lsfldb
dce12.u3  
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
dce12.osf.org       RW       /u3     0:00:00 hosts/dce12    <nil>           
epi.1  
        readWrite   ID 0,,22  valid
        readOnly    ID 0,,23  invalid
        backup      ID 0,,24  invalid
number of sites: 1
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner           
valentine.osf.org   RW       val_u4_aggr 0:00:00 hosts/valentine<nil>       
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
valentine.osf.org   RW       val_u4_aggr 0:00:00 hosts/valentine<nil>           
valentine.u2  
        readWrite   ID 0,,7  valid
        readOnly    ID 0,,8  invalid
        backup      ID 0,,9  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
valentine.osf.org   RW       /u2     0:00:00 hosts/valentine<nil>           
----------------------
Total FLDB entries that were successfully enumerated: 4 (0 failed)
LSHEADER AND SYNFLDB:
root@dce5> fts lsheader -server dce5 -aggregate lfs_aggr2
Total filesets on server dce5 aggregate lfs_aggr2 (id 3): 4
Barasso.663              0,,10 RW      8 K alloc     22 K quota On-line
Barasso.663.readonly     0,,11 RO     22 K alloc     22 K quota On-line
dce5.u3                  0,,25 RW      9 K alloc      9 K quota On-line
dce5_fset                0,,4 RW      9 K alloc      9 K quota On-line
Total filesets on-line 4; total off-line 0; total busy 0
root@dce5> fts syncfldb -server dce5
-- done processing entry 1 of total 3 --
Could not fetch FLDB entry (fs=0,,10, type=0)
Error: FLDB: no such entry (dfs / vls)
        readWrite   ID 0,,10  valid
        readOnly    ID 0,,11  valid
        backup      ID 0,,12  invalid
number of sites: 0    number of addresses: 1
   server           flags     aggr   siteAge principal      owner           
dce5.osf.org        RW,RO    lfs_aggr2 0:00:00                <nil>           
-- done processing entry 2 of total 3 --
-- done processing entry 3 of total 3 --
FLDB synchronized with server dce5
AFTER:
root@dce12> fts lsfldb  
Barasso.663  
        readWrite   ID 0,,10  valid
        readOnly    ID 0,,11  valid
        backup      ID 0,,12  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
dce5.osf.org        RW,RO    lfs_aggr2 0:00:00 hosts/dce5     <nil>           
dce5.u3  
        readWrite   ID 0,,25  valid
        readOnly    ID 0,,26  invalid
        backup      ID 0,,27  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
dce5.osf.org        RW       lfs_aggr2 0:00:00 hosts/dce5     <nil>           
dce5_fset  
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
dce5.osf.org        RW       lfs_aggr2 0:00:00 hosts/dce5     <nil>           
epi.1  
        readWrite   ID 0,,22  valid
        readOnly    ID 0,,23  invalid
        backup      ID 0,,24  invalid
number of sites: 1
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner           
Error: communications failure (dce / rpc)
valentine.osf.org   RW       1       0:00:00 hosts/valentine<nil>           
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
Error: communications failure (dce / rpc)
valentine.osf.org   RW       1       0:00:00 hosts/valentine<nil>           
valentine.u2  
        readWrite   ID 0,,7  valid
        readOnly    ID 0,,8  invalid
        backup      ID 0,,9  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner           
Error: communications failure (dce / rpc)
valentine.osf.org   RW       4       0:00:00 hosts/valentine<nil>           
----------------------
Total FLDB entries that were successfully enumerated: 6 (0 failed)
NOTE: the communications failures are due to the fact that fts moves
of the filesets appeared to have worked and I disabled the ftserver
process on valentine, and successfully cd'd into /: (root.dfs now
on dce5) - but the fldb never reflected this. Entering this as
as separate CR, data here for reference only.
root@dce12> cd /:
root@dce12> ls
dce12_u3  epi_1
root@dce12> cm whereis /:/epi_1
dfs: fileset (0,,22) is not available!
cm: '/:/epi_1': No such device
root@dce12> cm whereis /:
The file '/:' resides in the cell 'dce12_cell', in fileset 'root.dfs',

[3/18/93 public]
I believe that the expectations are a little far-fetched.  In particular,
there was already a fileset with ID 0,,4 in the FLDB, and ``syncfldb'' by
definition says to change what's in the FLDB to agree with what's on the
disk.  In this case, what was already on the disk (albeit in the dfstab
file and not in the fileset itself) was a fileset on dce5 with ID 0,,4.
 
Thus, I don't believe that this is a failure, but that it's pilot error:
if there's a fileset with id X on a disk, then invoking syncfldb on that
disk should (and must) update or overwrite the fldb entry for id X.
I don't know of any case in which a free fileset ID will be selected by
syncfldb.  Could we please cancel this one?
Filled in Subcomponent Name with `fts' 
Filled in Interest List CC with `cfe' 
Filled in Responsible Engr. with `gmd' 
Filled in Resp. Engr's Company with `osf' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[3/18/93 public]
I believe the (albeit uninspired) machine names may have caused some 
confusion here. DCE5 has an lfs aggregate with a fileset id from a 
previous fldb. My current fldb has the same fileset id for a ufs aggregate
on DCE12. If I ask that my current fldb be sync'd to include the fileset
info on DCE5's lfs aggregate, shouldn't I get a warning at least that I'm
tromping out/tromped out an existing fldb entry ( in this case, one for
a fileset on a different machine, DCE12)? I could've sworn I'd
seen this type of conflict handled more gracefully by syncfldb before -
ie. the fileset being added whose fileset id is currently in use gets
the next available fileset id. In any event - if syncfldb is documented
to be a "USE AT YOUR OWN RISK - COULD CAUSE DATABASE CORRUPTION" function then,
yes, what I'm asking for is an enhancement - but if not, I think we
should document this OR change the destructive behavior ...
 
NOTE: this happened in a cell whose flservers (dce5 and dce12)
BOTH thought they were the sync site.

[3/18/93 public]
One of the intended uses of this command is to allow you to remove a disk
from a hard-crashed machine and attach it to a working machine, and yet
bring all the data from the disk back on-line as being served by the new
machine.  In doing this, you want to overwrite all the FLDB entries that
pointed to the disk's old location and now point them to this new location
(new server, possibly a new aggregate ID).  That's the kind of repair
that ``fts syncfldb'' is intended to handle, and that's exactly why the
server/aggr information in the FLDB entries is overwritten so freely.
Certainly the doc doesn't say that syncfldb issues warnings and requires
confirmation.  From the 7.11 section of the DFS admin guide:
``The fts syncfldb command....checks the FLDB entry associated with
the fileset to verify that the FLDB correctly records the fileset's status
at the site.  If the FLDB is incorrect, fts syncfldb alters it; if no
entry exists for an online fileset, it creates one.''
 
Apparently, folks have gotten used to re-populating their FLDBs after
a re-configuration by using fts syncfldb, and this should work pretty much
OK (except that any replication information is lost).  It works out
because all the filesets on-line in a cell should have been mutually
consistent.  But, unfortunately, ``fts syncfldb'' isn't a general tool
for gently incorporating aggregates of any source into a DFS installation;
it will in fact overwrite existing data.  It's *meant* to overwrite
existing data.  Perhaps there's a need for a tool to do this kind of
gentle aggregate incorporation, but ``fts syncfldb'' isn't that tool, and
trying to make it into that tool will only keep it from being the kind of
tool that you need to make more gross configuration repairs.
 
I believe that a gentler tool could be built, and could even have been
partially simulated by hand, in that ``fts lshead'' on the newly-attached
aggregate on dce5 would have shown you the filesets that existed there,
and ``fts zap'' would have let you destroy the filesets that you didn't
want before incorporating them into the cell.  What's not built is a tool
to re-number filesets on a disk in preparation for syncfldb, or any kind
of automatic collision-detector that would check that all the filesets
on an aggregate have IDs different from any that are currently in the FLDB.
 
Now, fts syncfldb does have some marginally-friendly behavior, in that if
you're incorporating a fileset that has a fileset ID larger than any
so far allocated in the FLDB, then syncfldb will ensure that the FLDB's
high-water-mark for fileset IDs is increased to accommodate the filesets
to be incorporated.  However, in no case has syncfldb ever re-numbered an
existing fileset (LFS or otherwise) to give it an unused fileset ID.
 
Again, I'm going to ask Jeff K. to take a crack at whether the fts syncfldb
(and fts syncserv) documentation is sufficiently scary that this sort of
behavior should have been acceptable.
Changed Interest List CC from `cfe' to `cfe, jeff@transarc.com'

[3/18/93 public]
You're right Craig - the friendly behavior I thought I'd seen was NOT
renumbering but accepting new/higher fileset id numbers. 
 
I would like to see syncfldb be the single tool for both the "moved" aggregate
case you mention and the "reconfig" or "added" aggregate case I hit most often -
a sysadmin could easily have to move an aggregate from a machine in one
cell to a machine in another cell and not want to delete or renumber by
hand filesets with duplicate fileset ids OR remember to use another tool ...
 
SO ... we can mark this as a doc bug to warn unsuspecting sysadmin's of the
damage they can do - if it's already well doc'd, can we make this a post-1.0.2
enhancement request to make syncfldb friendlier and smarter?

[3/18/93 public]
I vote with Gail.  At the very least, syncfldb should be better 
documented (i.e. with clear warnings about its limited intelligence)
for 1.0.2.  Craig may remember OT4467 which describes problems
I had with syncfldb in the 1.0.1 timeframe that sound an awful lot 
like the problems Gail was seeing except in my case syncfldb blew 
away my root.dfs FLDB entry.  2 folks at OSF have used syncfldb now
and both ran into the same "pilot error".  Sounds conclusive enough
to justify a doc change to me and probably an open OT defect/enhancement 
for 1.0.3.

[3/19/93 public]
Well, I understand that both Ron and Gail have stumbled into similar
disasters with syncfldb overwriting existing fldb entries when importing
essentially-foreign aggregates, and I agree that this is a situation that
should bear some specific warning.  I spoke with Jeff Kaminski, who claims
that the documentation for 1.0.2 froze a while ago and thus it wouldn't
be possible for a warning there, but we can probably all come up with a
release note that addresses the issue, and some kind of suggestion for
work to be addressed in a later release (e.g. 1.0.3).
 
All that granted, though, I don't know how syncfldb could change to handle
both points of view, and I'm afraid I keep wondering how syncfldb came
to be viewed as the tool to use to import foreign aggregates.  As I say,
one could come up with a tool to import foreign aggregates--possibly an
fts sub-function--but its purpose honestly seems to be to be at odds with
the kinds of repairs that syncfldb is supposed to be authorized to do.
Can the OSF offer any enlightenment here, as to how these two functions
are viewed as being such close relatives?  For concrete examples, consider
two cases:
	(a) an LFS fileset with an ID already in the FLDB for a different
	    server/aggregate.
	    (a1) syncfldb overwrites the existing FLDB entry, and has to;
	    (a2) aggregate-import ideally wants to create a new FLDB entry
		 and renumber the LFS fileset with the new ID number, but
		 this might not be possible if the fileset is locally mounted.
	(b) a UFS fileset exported with an ID that's already in the FLDB
	    for a different server/aggregate.
	    (a1) syncfldb, again, overwrites the existing FLDB entry, and
		 has to do that to do its job;
	    (a2) aggregate-import ideally will create a new FLDB entry and
		 request that the administrator edit the server's
		 /opt/dcelocal/var/dfs/dfstab file to match, after first
		 detaching (un-exporting) the UFS aggregate and then
		 re-attaching (re-exporting) it.
 
The whole rationale behind ``syncfldb'' is to say that, for the filesets
on the server/aggregate(s) named in the fts syncfldb command, the
information in the FLDB is wrong and the information on the aggregate(s)
is right.  This is fundamentally different from the foreign-aggregate-import
problem, where an aggregate was created using IDs from some other numbering
system and you now want to import it into a DCE DFS cell.  Is this helping
to clarify my point of view here?  Can the OSF help by telling me how these
two functions are now viewed as so similar?  (Thanks!)

[3/19/93 public]
In general, I feel anything that has a destructive side effect should either
be well documented and/or have a warning and/or require acknowledgement
of the destruction to occur. 
 
I view the two functions as similar because, in both cases, there is
header information that needs to be recorded/reflected in the FLDB. 
I think there'll be plenty of cases where the FLDB is half of what
the sysadmin wants and the aggregate header is the other half ...
 
I believe most system administators will think in terms of all the filesets
they manage, whether in one cell or many, by name and NOT in terms of
cell-specific fileset id's.
 
I believe the 2 functions could be handled if the software DID note
fileset names AND fileset ids and allowed the system admin the option
to alter or overlay name and fileset id conflicts.
 
For instance:
aggregate has:	fileset id 4 for fileset pmax.bin
		fileset id 7 for fileset usr.gmd
		fileset id 10 for fileset user.gail
fldb has:	fileset id 7 for fileset user.gail
		fileset id 4 for fileset pmax.bin on dead server
 
fts syncfldb says:
	id conflict: fileset id 0,,7
	overlay user.gail entry with usr.gmd? (y/n) n
	add new fileset usr.gmd? (y/n) y
fileset 0,,13 created ...
	id conflict: fileset id 0,,4
	overlay pmax.bin entry with pmax.bin? (y/n) y
	name conflict: user.gail
	overlay user.gail id 0,,7 with user.gail id 0,,10? (y/n) n
	add fileset 0,,10 with new name? (y/n) y
	name for fileset id 0,,10 : user.marie
 
You could have an "-y" switch to turn off interactive portion and just
answer "yes" to everything. If it's easier for this to be a separate
tool from synfldb, that's fine with me. Let me know what you think
Craig - you and Tony and Elliot probably have more DFS sys admin experience
than I do.

[3/22/93 public]
Craig and I discussed the fts syncfldb command as it relates to the problem
encountered at OSF.  The documentation can potentially be updated to better
document the somewhat confusing effects of using the command to "move" a
fileset from one cell to another.  For now, the best I can do is a release
note that provides the following admonitory information:
   The fts syncfldb command is used to synchronize the filesets that reside on
   an aggregate with the FLDB of the cell in which they reside.  The command
   creates FLDB entries for filesets that do not have them; it also updates
   FLDB entries to record the current locations of filesets whose entries
   do not accurately record their present sites.  However, the fts syncfldb
   command is inappropriate for creating FLDB entries for existing filesets
   from one cell in the FLDB of another cell, a situation that can occur if
   a disk is physically moved from a server machine in one cell to a server
   machine in another cell.
   For example, suppose fileset Foo has fileset ID number 0,,8 in the FLDB of
   cell A.  If the disk that houses fileset Foo is removed from its server
   machine in cell A and installed on a server machine in cell B, the fts
   syncfldb command should not be used to create an entry for the fileset in
   the FLDB of cell B:  If fileset Bar with ID number 0,,8 already exists in
   cell B, the command updates its FLDB entry to reflect the location and
   information associated with fileset Foo, making it impossible to access
   fileset Bar.  Issuing the command on the aggregate that houses fileset Bar
   corrects the entry for that fileset, but fileset Foo is then no longer
   accessible.  Because of potential fileset ID number conflicts of this type,
   there is no convenient way to move a disk partition from a machine in one
   cell to a machine in a second cell and update the FLDB of the second cell.
This is a first draft of a possible release note.  Please feel free to provide
any corrections or clarifications to this information, as necessary.  I would
like to include information about this problem in the release notes sometime
in the next day or two.  Sorry for any confusion; thanks for any help.

[3/22/93 public]
Jeff - The Release Note looks good - it's a confusing topic. Could you add
a short paragraph about what you SHOULD do when there is a fileset id
number conflict and syncfldb is not appropriate? Ie. how does one change
the cell A fileset id of Foo to add it to cell B? Thanks.
 
Craig - Are we changing this CR to a doc one and adding a separate CR for
the code enhancement request? Or do you feel the manual steps Jeff will
describe above are sufficient to cover this?

[3/22/93 public]
I believe that I understand the suggestion that Gail is making; I even think
that I understand its motivation.  I still feel strongly that syncfldb
isn't a general aggregate-import tool, and that it's weakened by adding
that requirement to it.  I have nothing against the creation of a tool
that would do this aggregate-import function, and am happy to deal with
an enhancement request to that effect.
 
We haven't even discussed all the problems with importing filesets into
a new cell.  For instance, if there are any ACLs at all, even ones that
are created implicitly, in the fileset, you can't naively import such
filesets into a cell, because the UUID of the cell will be different from
the cell UUIDs stored in the ACLs.  Now, it's possible to re-create a cell
with the same UUID as that of the previous cell (which it must therefore
be replacing), in which case the operation makes sense.  But the naive
importing of live filesets will simply fail, for well-understood reasons.
It continues to make sense to detach a disk from one server and attach
it to another server in the same cell, then running syncfldb; it
continues to make sense to use syncfldb to repair some classes of damage
that might be encountered if fileset operations are interrupted at
inopportune moments; and it continues to make sense to use syncfldb to
recreate an FLDB after installing a new cell (with the same UUID as before).
But, by definition, all concurrently-active cells must have different UUIDs,
so it doesn't make sense to import filesets across cell boundaries, since
the UUIDs will no longer match up.  Interestingly, fileset dump and restore
will preserve complete ACLs, with cell UUIDs, so dump and restore can't be
used naively to move fileset contents.  One thing that will work today is
to tar together the contents of the filesets on a disk, move the disk,
clean it completely with ``newaggr'', re-create the filesets with
``fts create'', and restore the non-ACL contents of the filesets via tar.
This isn't ideal, but to do the job right would require the translation
of arbitrary ACLs, which is quite a tough problem.
 
So, in direct answer to Gail's questions, yes, in my opinion, this has
turned into a doc defect report, and there might be a code enhancement
request yet to be created, also.  So far we've resisted the temptation
to make any of the fts commands interactive (going so far as to remove
a question that ``fts restore'' used to ask), in hopes of making this
command suite more useful for script-writing--if you were looking for
any kind of existing command suite style guide.  I'm hoping that I've
clarified some of the difficulties to be faced by an aggregate-import
function, or even a fileset-import function, as motivation for keeping
the syncfldb/import functions distinct (as I thought they were in this
first round of tools).

[3/25/93 public]
Okay, this CR is now a doc one assigned to Jeff at Transarc. Up'd the
priority to 1 to reflect the fact that the Release Note has to be done
in order to ship.
The enhancement request is CR 7594.

[4/10/93 public]
I included the following release note, with some slight modifications, in the
DCE 1.0.2 Release Notes:
*** Begin Release Note ********************************************************
The documentation needs to state that moving a fileset from one cell to another
is not supported with DFS.  Neither of the following fileset operations is
supported:
 o Dumping a fileset in one cell and restoring it in another cell.
 o Moving the physical disk on which a fileset resides from a machine in one
   cell to a machine in another cell.
Moving a fileset from one cell to another causes the following types of
problems:
 1. The ACLs on file and directory objects in the fileset prohibit access to
    the objects because they are essentially "foreign" ACLs in the new cell.
    Because all ACLs contain the UUID of the cell in which they are are
    created, objects that have ACLs cannot be moved between cells.
 2. The fileset's ID number may conflict with the ID number of a fileset in
    the new cell.  A fileset's ID number is unique within the cell in which
    it is created; it may not be unique in another cell because a different
    fileset may already have that ID number in the other cell.
Note that if a physical disk is moved from a machine in one cell to a machine
in a different cell, the fts syncfldb command is inappropriate for creating
FLDB entries for existing filesets on the disk in the FLDB of the new cell. 
The fts syncfldb command synchronizes the filesets that reside on an aggregate
with the FLDB of the cell in which they reside.  It creates FLDB entries for
filesets that do not have them; it also updates FLDB entries to record the
current locations of filesets whose entries do not accurately record their
present sites.
For example, suppose fileset foo has fileset ID number 0,,8 in the FLDB of cell
A.  Suppose also that fileset bar has fileset ID number 0,,8 in the FLDB of
cell B.  If the disk that houses fileset foo is removed from its server machine
in cell A and installed on a server machine in cell B, the fts syncfldb command
should not be used to create an entry for the fileset in the FLDB of cell B:
The command updates the entry for fileset bar to reflect the location and
information associated with fileset foo, making it impossible to access
fileset bar.  Issuing the command on the aggregate that houses fileset bar
corrects the entry for that fileset, but fileset foo is then no longer
accessible.  Such fileset ID conflicts prohibit the moving of a disk partition
from a machine in one cell to a machine in a second cell.
*** End Release Note **********************************************************
Because the crux of this defect has evolved into the need for better
documentation of intercell fileset moves, I changed the one-line description
accordingly.  I also lowered both the priority and the severity of the defect
because it has been documented in the DCE 1.0.2 Release Notes.
Changed H/W Ref Platform from `pmax' to `all' 
Changed S/W Ref Platform from `osf1' to `all' 
Changed Subcomponent Name from `fts' to `admin_gd, admin_ref' 
Changed Short Description from `syncfldb overlays existing entry' to `Document 
 restrictions on intercell fileset moves' 
Changed Interest List CC from `cfe, jeff@transarc.com' to `cfe@transarc.com, 
 jeff@transarc.com' 
Changed Severity from `B' to `C' 
Changed Priority from `1' to `2' 
Changed Fix By Baseline from `1.0.2' to `' 
Filled in Transarc Status with `open'

[7/30/93 public]
Filled in Fix By Baseline with `1.0.3'

[9/17/93 public]
Filled in Affected File with `See description' 
Changed Responsible Engr. from `jeff' to `kdu@transarc.com'
Filled in Inter-dependent CRs with `8618'

[9/23/93 public]
The following files are affected:
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
	admin_ref/man8dfs/fts_dump.8dfs
	admin_ref/man8dfs/fts_restore.8dfs
	admin_ref/man8dfs/fts_syncfldb.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 7474
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : permanent data structures doc shows ACLs wrong
Reported Date                 : 3/11/93
Found in Baseline             : 1.0.2
Found Date                    : 3/11/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : tech_suppl/tech.suppl102/part10_lfs/perm_data.ps
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[3/11/93 public]
I tried to use the permanent data structures document to understand a
disk ACL that was causing us some trouble.  In the process I found two
problems.  The first is that the document describes a 32-bit class field
as being the first word of every entry.  This field doesn't really exist.
Also the description of ACL Variant "B" refers to "foreign_user" where
"foreign_other" is meant.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/15/93 public]
The text that needs to be edited occurs in Section 2.11 of the technical
supplement document entitled "DCE Local File System: Permanent Data
Structures" (perm_data.ps).  Ted knows what changes need to be made and
will try to get to them by DCE 1.0.3; the necessary changes are relatively
small and are restricted to the indicated section.
Filled in Affected File with `tech_suppl/tech.suppl102/part10_lfs/perm_data.ps'

[7/21/93 public]
Ted corrected the mistakes in the file perm_data.ps.  Doug Weir has graciously
consented to take care of submitting the file into ODE.  This one can be
closed.
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[08/17/93 public]
Closed bug.



CR Number                     : 7432
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dacl
Short Description             : dacl code forgets allocated size of complex entries
Reported Date                 : 3/5/93
Found in Baseline             : 1.0.2
Found Date                    : 3/5/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/security/dacl/dacl{.p.h,_default.c,_modify.c,_parse.c,_sec_acl.c}
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7432-fix-dacl-complex-entry-memory-handling
Transarc Herder               : jaffe@transarc.com

[3/5/93 public]
The dacl code allocates conservatively for acl entries but doesn't
remember how much it actually allocates.  This means that in some
cases, the memory is never freed and in others it thinks it is
returning a different number of bytes than it really is.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/8/93 public]
Defect Closure Form
-------------------
The fix was to add a member to the dacl_t structure to keep track of
the number of entries allocated.
--Verification procedure below--
Ran a 'bos *admin' tests by hand.  Ran ACL test suite.
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Changed Fix By Baseline from `1.0.2' to `1.0.3' 
Filled in Affected File with 
 `file/security/dacl/dacl{.p.h,_default.c,_modify.c,_parse.c,_sec_acl.c}' 
Filled in Transarc Deltas with 
 `comer-ot7432-fix-dacl-complex-entry-memory-handling' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7362
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts dump/restore doesn't support device vnodes
Reported Date                 : 2/25/93
Found in Baseline             : 1.0.2
Found Date                    : 2/25/93
Severity                      : B
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : dimitris-dump-vnode-device
Transarc Herder               : 

[2/25/93 public]
This defect was discovered during OSF/1 volops testing.  Currently, the
dump file format used by fts dump and restore doesn't record the device
number associated with an vnode that describes a block or
character-special device.  Hence device vnodes are incorrectly restored
from a dump.  Though this doesn't matter much at the moment, one can
imagine scenarios (like if we actually supported DCE's diskless
service) in which it would be important.
Craig agreed that this should be treated as a defect for resolution
after 1.0.2.  When this defect is addressed, attention should be paid
to inodes for FIFO's and local sockets, as well, though they may not
present a problem.  (On OSF/1, at least, the correct initialization of
a FIFO or socket doesn't depend on data that vary from one inode to
another, so there's no need to save anything in the dump beyond the
vnode type.)

[3/15/93 public]
Filled in Interest List CC with `jdp@transarc.com, pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/14/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
Create a character special file, and a block special file in an episode
fileset. Then copy the fileset using 'fts dump | fts restore'
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-102-2.8
Tested with backing build:  
dce1.0.2b19
Changed Interest List CC from `jdp@transarc.com, pakhtar@transarc.com' to 
 `jdp@transarc.com, pakhtar@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `dimitris@transarc.com' 
Filled in Transarc Deltas with `dimitris-dump-vnode-device' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 7346
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : krpc
Short Description             : add kprc debugging to icl
Reported Date                 : 2/24/93
Found in Baseline             : 1.0.2
Found Date                    : 2/24/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.2a
Affected File(s)              : kutils/syscall.c, kruntime
Sensitivity                   : public

[2/24/93 public]

We need the krpc debugging output to work with the dfs icl logs.
This will better help us trace problems such as ot 6959 where
it is not clear whether we have dfs or krpc problems.

[3/12/93 public]

This work has been done and submitted for the pmax.
Since Don is currently trying to gather information about the
AIX implementation (specifically what's the equivalent
of vsprintf on AIX), I'm going to pass the responsiblity
on to him.

[04/09/93 public]
Well, I guess I haven't pushed hard enough for this.  After
some initial discussion with Carl on whether this change was
desirable and doable under AIX, I haven't heard anything more.
I probably should assign this to Carl for resolution, but
I'll wimp out and pass it back to Diane.

Please update the CR, though, Carl, with your current thoughts
on doing this.

[09/01/93 public]

Closing this. This was done for OSF1 and will
not be done for AIX.



CR Number                     : 7337
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : growaggr
Short Description             : growaggr -noaction ignores -aggrsize values.
Reported Date                 : 2/23/93
Found in Baseline             : 1.0.2
Found Date                    : 2/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efs_growaggr.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : mason-ot7337-error-exit-for-growaggr
Transarc Herder               : jaffe@transarc.com

[2/23/93 public]
If I do the following command:
# growaggr -aggregate /dev/lv02 -aggrsize -2 -noaction
aggregate /dev/lv02 holds 12288 (1024B) blocks, 12288 assigned
A zero is returned. I expected a non-zero return code since -aggrsize is
an invalid size.  
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `mason@transarc.com' 
Added field #Transarc Status with value `open'

[2/23/93 public]
The problem appears to be that atol is used to assign into an unsigned
number, so the logic to check for this condition never gets triggered.
The program interprets the above to be "-aggrsize 4294967294".
I'll fix it.
Filled in Interest List CC with `comer,bwl' 
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[2/24/93 public]
Updated short header to describe the real problem.
Changed Defect or Enhancement? from `def' to `enh' 
Changed Short Description from `growaggr with negative aggrsize returns zero' 
 to `growaggr -noaction ignores -aggrsize values.'

[2/24/93 public]
Growaggr now works as documented.  The -noaction and -aggrsize switches
together verify if the -aggrsize argument is valid.
Changed this code to be aware that it really is handling unsigned quantities.
Defect Closure Form
-------------------
--Regression test program below--
Growaggr continues to increase the size of aggregates as necessary (hand
test)
--Verification procedure below--
To verify that the bug is gone, try running growaggr with invalid arguments.
It now exits with a non-zero status code and identifies the error.  I even
checked bad range errors (a "feature" which was added while doing all of this
bullet-proofing work) to verify that they are now properly identified.
Associated information:
Tested on TA build:  
dfs-102-2.7
Tested with backing build:  
dce1.0.2b15
Filled in Transarc Deltas with `mason-ot7337-error-exit-for-growaggr' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/vnops/efs_growaggr.c' 
Changed Transarc Status from `export' to `import' 
Added field Transarc Herder with value `jaffe@transarc.com'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7336
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : config
Short Description             : rc.dfs not updated for file server config
Reported Date                 : 2/23/93
Found in Baseline             : 1.0.2
Found Date                    : 2/23/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.2a
Affected File(s)              : src/config/dce_config
Sensitivity                   : public

[2/23/93 public]
The appropriate entries are not uncommented in rc.dfs upon config
of a dfs (private) file server machine.  The result is a dfs 
fileserver machine won't restart properly after a reboot.

A couple of new calls to enable_in_dfs_rc() need to be inserted in 
the right places in config_dfsfs() in dce_config.

[6/7/93 public]
Fixed.

[12/17/93 public]
Closed.



CR Number                     : 7331
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7312
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : fts restore -overwrite makes LFS destination unusable
Reported Date                 : 2/23/93
Found in Baseline             : 1.0.2
Found Date                    : 2/23/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : dimitris-ot7331-bad-ctime-can-cause-infinite-loop
Transarc Herder               : jaffe@transarc.com

[2/23/93 public]
This testing was done on a single-machine PMAX cell using the OSF NB
of 2/22, which is based on DFS 2.6.  I've found that one cannot
reliably overwrite non-empty LFS filesets with the fts restore
command.  It seems hard to believe that a problem this easy to
reproduce exists at this juncture, but I can make it happen with
this simple procedure:
	Create an LFS or UFS fileset containing a few files. (I
	created two empty files with touch(1).)
	Dump it.
	Create an empty LFS fileset to receive the dump, and
	create its mount point.
	Put a few files in this new fileset.  (Once again, I
	created two empty files there.)
	Restore the dump into the new fileset.  Again, the
	dump can come either from a UFS or an LFS fileset.
	Watch any access (cd, ls, ...) to the newly-restored
	fileset hang, with a stack trace like this one:
		cm_GetReturnOpenToken
		AFS_FetchStatus
		rpc__call_transceive
		rpc__dg_call_transceive
		rpc__dg_call_receive_int
		rpc__dg_call_wait
		pthread_cond_wait
		[...]
	(Only the stack through rpc__call_transceive is
	invariant.  The thread may hang at other places within
	RPC.)
Other notes:
	I found this problem while testing the OSF/1 volops
	code, so the kernel contains the new code.  However,
	OSF/1 UFS (and the new code) are not implicated in the
	failure, as far as I can see.
	The problem also existed with DFS 2.4, but I didn't
	want to report till I'd tested with 2.6.
	The problem disappears with a reboot -- that is, the
	restored fileset is usable with no problem once the
	cell comes back up.
I'd greatly appreciate it if someone at Transarc could try out
this scenario, so we can see whether you can reproduce the
failure as easily as I can.  Thanks.

[02/23/93 public]
Just FYI, I tried the failure scenario under the stock kernel
from the OSF 2/22 NB, with identical results.  Also, one
point I forgot to mention:
	If you use the same fileset as the source and destination
	for the fts operations (i.e., restore the dump into the
	same fileset where you did the dump), the failure doesn't
	occur.

[2/24/93 public]
Filled in Interest List CC with `kazar@transarc.com, cfe@transarc.com, 
 mason@transarc.com, pakhtar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[2/24/93 public]
Once again, we have a possible usage error in overwriting existing, exported
filesets (the companion bug for this is 7312).  An uncontrolled fileset
restoration into an existing fileset will work insofar as the data on the
disk is correctly updated, but the operation is not well-defined with
respect to exported access to the fileset.  Thus, the fact that a CM has
cached files from a fileset that has since undergone major, uncontrolled
surgery is a big hit for it to try to take.
.
I wind up thinking of this more as a caution that administrators have to
face when doing restorations into existing filesets.  Thus, I'd argue that
this should be something less than a B1 bug at the moment.
Filled in Inter-dependent CRs with `7312'

[02/24/93 public]
Per phone conversation with Pervaze:  Current thinking at Transarc
is that the scenario described by this bug (restoring atop an exported
LFS fileset) should be explicitly disallowed by the relevant utilities.
This change will not be made for 1.0.2, but will be made later, so
this defect is staying open at lower priority.
For 1.0.2, this restriction on the use of fts restore should be stated
in the release notes.  Jeff Kaminski has been added to the CC: line
as the person who should see to that.

[2/26/93 public]
I just figured I'd try to fill in what's happening here.
The CM in this situation is stuck in the loop in GetAccessBits, in cm_subr.c.
It repeatedly makes the AFS_FetchStatus call, but then discovers that it
doesn't have the correct access information cached (cm_GetACLCache returns
a failure), so it does it again.  Normally the ACL cache is built up in the
call to cm_MergeStatus (in cm_vnodeops.c), but what's happening here is that
the file's fetchStatusp->changeTime (for the restored copy of the file) is
earlier than what the CM has cached in scp->m.ServerChangeTime for the
file with the same FID (which was for the second fileset).  The loop is
trying to get the access bits for the root directory of the fileset, which
for now is always at the same FID (with (vnode,unique) of (1,1)).
Thus, the CM has already cached information about the file with that FID
(which it cached when it manipulated the second fileset), and one of the
key properties is the ServerChangeTime of the file.  The dump/restore
combination will make the fileset's root directory have a different, in this
case older, ServerChangeTime.  When the CM now sees this new ServerChangeTime,
it refuses to update its cached information, since the cached ServerChangeTime
is newer than the incoming ServerChangeTime, and what the CM could be doing is
analyzing the response to an older RPC.
One approach is to turn off SC_STATD in the restored-over case, but I haven't
yet found a way to do that.  It seems wrong to connect it up to the simple
revocation of tokens.  Perhaps there's some more aggressive use of the
afsVolSync structure that will do the job when the fileset really is
completely different.  But in any case, the CM is pretty thoroughly
confused by the restored identity of these files.
Filled in Transarc Herder with `jaffe@transarc.com' 
Filled in Transarc Status with `open'

[4/28/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
Repeat the procedure involving 'fts restore -overwrite' described at the
beginning of this report.
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-102-2.9
Tested with backing build:  
dce1.0.2b19
Filled in Subcomponent Name with `CM' 
Changed Interest List CC from `kazar@transarc.com, cfe@transarc.com, 
 mason@transarc.com, pakhtar@transarc.com, jeff@transarc.com' to 
 `kazar@transarc.com, cfe@transarc.com, mason@transarc.com, 
 pakhtar@transarc.com, jeff@transarc.com, tu@transarc.com' 
Changed Responsible Engr. from `tu@transarc.com' to `dimitris@transarc.com' 
Filled in Transarc Deltas with 
 `dimitris-ot7331-bad-ctime-can-cause-infinite-loop' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7321
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : flserver
Short Description             : flserver leaking memory
Reported Date                 : 2/22/93
Found in Baseline             : 1.0.1
Found Date                    : 2/22/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ncscompat/compat_rpcbind.c, file/security/dfsauth/dfsauth_server.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7321-fix-memory-leaks
Transarc Herder               : mason@transarc.com

[2/22/93 public]
I've had a cron job running which does a ps -el every hour.  flserver appears
to be leaking about 40K an hour.  During the course of about 36 hours, flserverwent from 1300K to 2844K in size.  This was under idle cell conditions, but in
a two flserver cell, so there was periodic voting going on.  I am running dfs
2.6. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/23/93 public]
pass the hot potato
Filled in Interest List CC with `cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `comer@transarc.com'

[3/8/93 public]
Defect Closure Form
-------------------
rpcx_binding_to_sockaddr() was calling rpc_binding_to_string_binding
but not freeing the string; dfsauth was calling dacl_ReadAclFromDisk
but not freeing the ACL entries.  See also ot7432, which fixes a third
leak (in the dacl package). 
--Verification procedure below--
Run the flserver for a while in a multi-flserver cell.  The flserver
process size now stabilizes at around 1590. 
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Filled in Affected File with `file/ncscompat/compat_rpcbind.c, 
 file/security/dfsauth/dfsauth_server.c' 
Filled in Transarc Deltas with `comer-ot7321-fix-memory-leaks' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7310
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : streamline cm_setattr()
Reported Date                 : 2/22/93
Found in Baseline             : 1.0.1
Found Date                    : 2/22/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_vnodeops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7310-faster-cm_setattr
Transarc Herder               : mason@transarc.com

[2/22/93 public]
Return a more efficient cm_setattr() implementation which is
about 1/2 the pathlength of the original.  Transarc has reviewed
and approved the code.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/22/93 public]
Defect Closure Form
-------------------
Validated that code still compiled and ran.
The fix is based on DFS 2.6.
Filled in Transarc Deltas with `cburnett-ot7310-faster-cm_setattr' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/cm/cm_vnodeops.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7302
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm_vnodeops.c does not need to include xcred.h
Reported Date                 : 2/20/93
Found in Baseline             : 1.0.1
Found Date                    : 2/20/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_vnodeops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7302-no-xcred-include-in-cm_vnodeops
Transarc Herder               : mason@transarc.com

[2/20/93 public]
cm_vnodeops.c does not need to include xcred.h since the cache
manager does not use any of the xcred functionality.  However
it needs to include direntry.h which is used to get as a byproduct
of including xcred.h.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/20/93 public]
Closur form:
	It still compiles and runs ok.
Filled in Transarc Deltas with 
 `cburnett-ot7302-no-xcred-include-in-cm_vnodeops' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/cm/cm_vnodeops.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7292
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Salvager fails on regular files
Reported Date                 : 2/19/93
Found in Baseline             : 1.0.1
Found Date                    : 2/19/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/newaggr.c, file/episode/salvage/salvage_main.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-ot7292-allow-salvage-of-regular-files
Transarc Herder               : jaffe@transarc.com

[2/19/93 public]
Due to the recent aggregate interlocking changes, the salvager now fails
on regular files.  The problem is that ftu_LockAggrDevice() checks to see
if the specified file is a character device and fails if it isn't.
This only affects salvager tests, since they are the only clients of the
salvager that expect to salvage regular files.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/19/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Try running both newaggr and the salvager on regular files.  A copy
of /unix in /tmp works well for this sort of thing.
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Changed Interest List CC from `ota' to `ota, jaffe' 
Filled in Transarc Deltas with `jdp-ot7292-allow-salvage-of-regular-files' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/newaggr.c, 
 file/episode/salvage/salvage_main.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7278
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : mknod syscall does not work on rios.
Reported Date                 : 2/18/93
Found in Baseline             : 1.0.2
Found Date                    : 2/18/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/xvnode/RIOS/xvfs_vfs2aix.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-ot7278-mknod-doesnt-work
Transarc Herder               : jaffe@transarc.com

[2/18/93 public]
This problem was found by running the ITL script "create.scr" which
issurs mknod calls to create BLK and CHR special files. The error
code returned is EPERM. In fact, the uid of the running process does not
have the root privilege to create such kind of files.
The UNIX allows only the super user to make special files.
syscall check point:
        Check creating char and block devices, and FIFOs.  Check mtime and
        atime fields on newly created objects.  Creating FIFOs should fail
        cleanly in DFS, since we don't support FIFIOs today. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/19/93 public]
This is not really a critical/must fix for 1.0.2 defect.
Downgrading to B2.
Changed Severity from `A' to `B' 
Changed Priority from `1' to `2'

[2/25/93 public]
This is an easy fix. The ufs glue layer at the fx server should check and
make sure that only a user a) logining as cell_admin AND b) s/he is also able 
to create a regular file before s/he is able to create a special device. 
The fix is to let the caller assume the local root's identity if and 
only if the caller is mmeber of the cell_admin.
Defect Closure Form
-------------------
--Verification procedure below--
1) login as a cell_admin,
2) mkdir a tdir
3.a) 
	- chmod 555 .
	- mknod dev0 b 10 11
	The mknod should fail since the mode bit is 555.)
3.b)
	
	- chmod 755 .
	- mknod dev0 b 10 11
	Now it is able to create a special device.
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Transarc Deltas with `tu-ot7278-mknod-doesnt-work' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/xvnode/RIOS/xvfs_vfs2aix.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7241
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsbind
Short Description             : Print statement outside of debug.
Reported Date                 : 2/17/93
Found in Baseline             : 1.0.1
Found Date                    : 2/17/93
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : fred-ot7241-print-in-debug-only
Transarc Herder               : mason@transarc.com

[2/17/93 public]
The dfsbind main_helper.c program has a printf() statement that says
when the freelist is empty.  This statement should only be issued when
in debug mode, and should print to stderr, like other debug statements.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/17/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
The printf() was changed to an fprintf to stderr. Also, it is now only
issued if debugging is turned on.  See the code.
Filled in Transarc Deltas with `fred-ot7241-print-in-debug-only' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7237
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bos tests
Short Description             : org undefined in bos test0
Reported Date                 : 2/16/93
Found in Baseline             : 1.0.1
Found Date                    : 2/16/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7237-define-org-in-test0
Transarc Herder               : mason@transarc.com

[2/16/93 public]
Bos test0 attempts to create an account for usera in the
registry, but is failing because org is never set ."
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'
M

[2/26/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
run bos test0.  Check to see that usera is no longer in the registry
after the test is done.
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Filled in Transarc Deltas with `comer-ot7237-define-org-in-test0' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7231
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : CM shutdown code invalidates good chunks
Reported Date                 : 2/15/93
Found in Baseline             : 1.0.1
Found Date                    : 2/15/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/cm/cm_dcache.c, file/cm/cm_shutdown.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cburnett-ot7231-cm-shutdown-loses-chunks
Transarc Herder               : mason@transarc.com

[2/15/93 public]
The cache manager shutdown logic causes online chunks to be
improperly marked in the Cacheitems file, so that when the
system is rebooted, valid chunks would be discarded in the
disk cache. The problem centers around updating the modTime
for the chunk record in the cacheItems file.  Transarc has
reviewed and approved the fix.  I will provide the changes.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/15/93 public]
Defect Closure Form
-------------------
I have been running with this for a couple of weeks with
no regression.  Also I have validated that after reboot
my cache has all the chunks from its previous life.
Filled in Transarc Deltas with `cburnett-ot7231-cm-shutdown-loses-chunks' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/cm/cm_dcache.c, file/cm/cm_shutdown.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7221
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : the database tape name is bak_db_dump.1
Reported Date                 : 2/13/93
Found in Baseline             : 1.0.2
Found Date                    : 2/13/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bubasics/tcdata.p.h, file/butc/tcudbprocs.c:$
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-ot7221-bak-database-tape-name-change
Transarc Herder               : mason@transarc.com

[2/13/93 public]
Another one of those merge conflicts, damn. The database tape name was 
changed from Ubik_db_dump.1 to bak_db_dump.1 to be reflect the database
being dumped to tape (and make room for dumps of other databases). This was
added as part of a major delta and is now missing. This fix will put it 
back.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/19/93 public]
Another merge conflict.
Defect Closure Form
-------------------
--Regression test program below--
Run bak savedb and then bak readlabel. The tape label should be "bak_db_dump.1"
and not "Ubik_db_dump.1".
Associated information:
Tested on TA build:  
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Filled in Transarc Deltas with `vijay-ot7221-bak-database-tape-name-change' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
Filled in Affected File with `file/bubasics/tcdata.p.h, 
 file/butc/tcudbprocs.c:$' 
Changed Transarc Status from `export' to `import'

[5/3/93 public]
imported into dfs-carl 1.8

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7220
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bakserver
Short Description             : spurios messages in BakLog
Reported Date                 : 2/13/93
Found in Baseline             : 1.0.2
Found Date                    : 2/13/93
Severity                      : B
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ncscompat/compat_handleCancel.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : vijay-ot7220-bakserver-remove-spurious-errors
Transarc Herder               : mason@transarc.com

[2/13/93 public]
A merge conflict prevented a fix from getting through. I'm redoing this
fix because the last time this was done, it was along with a different but
major fix. Spurios error messages get generated in BakLog and tends to 
clutter the BakLog quite a bit. These are not error conditions and so no
error messages should be generated. The bug is, the cancel handling macros
in bakserver RPCs do not zero out their return value upon success.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/19/93 public]
A merge conflict  caused this change to be reimplemented.
Defect Closure Form
-------------------
--Regression test program below--
Executing bak commands should not cause spurious DFS_DISABLE_CANCEL error
messages to accumulate in the BakLog.
Associated information:
Tested on TA build:  
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Filled in Transarc Deltas with `vijay-ot7220-bakserver-remove-spurious-errors' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ncscompat/compat_handleCancel.h' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7206
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : upclient
Short Description             : upclient -help returns non-zero
Reported Date                 : 2/11/93
Found in Baseline             : 1.0.3
Found Date                    : 2/11/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/update/client.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7206-fix-upclient-help
Transarc Herder               : jaffe@transarc.com

[2/11/93 public]
Doing upclient -help returns 2.  Asking for help should be a valid request from
a user and should return 0.  This is a one-line fix in client.c
      for(i=0;i<argc;i++) {
        if (strncmp(argv[i], "-help", strlen(argv[i])) == 0 ||
  	  strncmp(argv[i], "help", strlen(argv[i])) == 0)
! 	  exit(0);
      }
  
      if ( (objectListP == 0)
--- 1061,1067 ----
      for(i=0;i<argc;i++) {
        if (strncmp(argv[i], "-help", strlen(argv[i])) == 0 ||
  	  strncmp(argv[i], "help", strlen(argv[i])) == 0)
! 	goto exit;
      }
  
      if ( (objectListP == 0)
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `mason@transarc.com' 
Added field #Transarc Status with value `open'

[2/11/93 public]
Changed Responsible Engr. from `mason@transarc.com' to `comer@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[2/22/93 public]
Defect Closure Form
-------------------
Did as Jean suggested.
  [update] ./upclient -help
  Usage: ./upclient  -server <machine> -path <filename | directory_name>... [-time <frequency>] [-file <log_file>] [-verbose] [-help]
  [update] echo $status
  0
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Changed H/W Ref Platform from `rs6000' to `all' 
Changed S/W Ref Platform from `aix' to `all' 
Changed Found in Baseline from `1.0.2' to `1.0.3' 
Changed Fix By Baseline from `1.0.2' to `1.0.3' 
Filled in Affected File with `file/update/client.c' 
Filled in Transarc Deltas with `comer-ot7206-fix-upclient-help'

[3/2/93 public]
Filled in Transarc Status with `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Transarc Herder with `jaffe@transarc.com' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7190
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bos tests
Short Description             : bos test10 should clean up registry entries
Reported Date                 : 2/9/93
Found in Baseline             : 1.0.1
Found Date                    : 2/9/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/file/bos/tests/test10
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7190-bos-test10-cleanup
Transarc Herder               : mason@transarc.com

[2/9/93 public]
Bos test10 creates users (usera-userf, userz) in the registry
for testcase use.  It should clean these up when the testcase
completes so that the registry doesn't get cluttered up with
extra principals.  This is particularly a problem for bos test0
which also tries to create some of these entries and exits if
unable to (even if it was because they already exist).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/27/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Run test10.  It should pass and there should not be user[a-g,z]
principals in the registry and there should not be keys for 
userz in the keytable.
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Filled in Affected File with `test/file/bos/tests/test10' 
Filled in Transarc Deltas with `comer-ot7190-bos-test10-cleanup' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7189
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bos tests
Short Description             : DetermineLocalAuth never sets localauth if long hostname
Reported Date                 : 2/9/93
Found in Baseline             : 1.0.1
Found Date                    : 2/9/93
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : test/file/util/gen_FunctionsScript
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7189-fix-test-util
Transarc Herder               : mason@transarc.com

[2/9/93 public]
Attempted to verify our orbit equivalent of OT 5015, and determined
that the fix dropped for test9 does not work in all cases.  The 
original problem was that test9 did not run on a client because it
tried to use -localauth with some commands.  The fix that was dropped
with 5015 was to add a routine in util/gen_FunctionsScript called
DetermineLocalAuth which only sets the localAuth flag if it can 
determine from the registry that the machine is a server.  The problem
is that is assumes short hostname in all cases when in fact the
registry entry has whatever the hostname command on the machine
returns.  So in the case of a machine with a long hostname like
mine:
# hostname
dfswitch.austin.ibm.com
localAuth is never set (server or client) and test9 isn't doing any
-localauth testing.
I am opening this as a new bug instead of re-opening 5015 because
there were also serveral other bos testcase fixes dropped under 5015
which are fine.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/9/93 public]
Changing from enh to def.
Changed Defect or Enhancement? from `enh' to `def'

[3/2/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
** Under ksh on power8:
	# hostname
	power8
	# . gen_FunctionsScript
	# echo $localAuth
	<nothing>
** Try this host
	# DetermineLocalAuth
	# echo $localAuth
	-localauth
** Try non-existent machine
	# DetermineLocalAuth foo
	?(rgy_edit) Cannot retrieve any matching entries for hosts/foo/dfs-server   "" - Entry not found (Registry Edit Kernel) (dce / sad)
	# echo $localAuth
	<nothing>
** Try another server in cell
	# DetermineLocalAuth olorin
	# echo $localAuth
	-localauth
** Set host's name to full name
	# hostname power8.transarc.com
	power8.transarc.com
	# hostname
	power8.transarc.com
	# DetermineLocalAuth
	?(rgy_edit) Cannot retrieve any matching entries for hosts/power8.transarc.com/dfs-server   "" - Entry not found (Registry Edit Kernel) (dce / sad)
** Host is not found in registry but it is using the whole name
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Filled in Affected File with `test/file/util/gen_FunctionsScript' 
Filled in Transarc Deltas with `comer-ot7189-fix-test-util' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7147
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : xvolume
Short Description             : Increase frequency of fileset descriptor garbage collection
Reported Date                 : 2/5/93
Found in Baseline             : 1.0.2b6
Found Date                    : 2/5/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : rajesh-ot7147-gc-fileset-desc
Transarc Herder               : jaffe@transarc.com

[2/5/93 public]
DFS has a devoted PX thread that garbage collects dormant fileset
descriptors every 5 mins but for standalone EFS there is no such
thread. Hence if a fileset op, on fileset F1 say, is interrupted on
SEFS, the corresponding fileset descriptor will not be recycled, until
all the currently allocated fileset descriptor table entries are all
used up. This causes further fileset ops on the F1 to fail till then.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/5/93 public]
An enhancement actually
Changed Defect or Enhancement? from `def' to `enh'

[2/26/93 public]
Defect Closure Form
-------------------
After a fileset op has been interrupted on a fileset, this solution allows
any further fileset ops on a fileset issued 5 mins later to succeed. Fileset
ops issued withing the 5 min period will fail as before.
Changes to allow success of fileset ops issued immediately after the
interrupted one was deemed unnecessary at this time given that
interrupted fileset ops are v. unusual in nature. If you seriously
disagree with this, please open another bug instead of reopening this
one.
--Verification procedure below--
1. Create an episode aggregate
2. Create a fileset with some id F1
3. Mount the fileset
4. Run some tests to create some files/dirs e.g. you could run cthon/test1 
   without the -f switch.
5. Issue an clone command e.g.
   efts clone <aggr> <F1> -destid <dest_fileset_id>
   and
   Interrupt it immediately e.g. issue a CTRL-C
6. Issue another fileset command on that fileset e.g.
   efts lsquota aggr F1
   If it fails with "efts: fileset being cloned (dfs / xvl)", you are all ok
   so far, else you need to go back to step5 and try to be faster in
   issuing the CTRL-C.
7. Wait for > 5 minutes. Good time for a coffee break!
8. Reissue the lsquota command
   efts lsquota aggr F1
   and you should get the correct lsquota output. 
   If you get "efts: fileset being cloned (dfs / xvl)", the bug probably
   has not been fixed.
Associated information:
Tested on TA build:  dfs-102-2.6
Tested with backing build:  dce1.0.2b15
Filled in Transarc Deltas with `rajesh-ot7147-gc-fileset-desc' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7079
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : Add cmdebug command to get CM info.
Reported Date                 : 2/1/93
Found in Baseline             : 1.0.2
Found Date                    : 2/1/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jess-ot7079-cm-add-cmdebug
Transarc Herder               : jaffe@transarc.com

[2/1/93 public]
Generally, this command will be used to get interesting information
about Cache Manager. It will print out Cache entries especially those
with non-zero ref count and the lock information.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/2/93 public]
This is not required for 1.0.2.
Changed Severity from `B' to `C' 
Changed Priority from `1' to `2'

[2/2/93 public]
Changed Defect or Enhancement? from `def' to `enh'

[3/15/93 public]
This delta is has been tested on dfs 2.6.
A problem left is that we will register TKN server ep in rpc runtime database
so that we will get rid of command argument for the port number.
Associated information:
Tested on TA build:  
dfs-102-2.5.
Tested with backing build:  
Filled in Transarc Deltas with `jess-ot7079-cm-add-cmdebug' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7069
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 5000
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : improve salt tests cow1, cow6
Reported Date                 : 1/29/93
Found in Baseline             : 1.0.2
Found Date                    : 1/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : blake-ot7069-fix-salt-COW-tests
Transarc Herder               : jaffe@transarc.com

[1/29/93 public]
Ted and I agreed that two tests in the salt salvager test program
should be improved:

"cow1" checks for COW anodes that are not of "regular" type, or that
are not backed by regular anodes.  The current repair rule is that the
COW anode should alway be cleared.  It makes more sense to convert the
COW anode to non-COW if its type is regular.  Also, an off-by-one error
in subtest cow1b makes it do something other than what I intended.

"cow6" checks for COW anodes whose (bBlock, bOffset) information does
not point to a valid anode.  We decided to add a few more variations
to the existing test.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/1/93 public]
I used these tests to verify the fixes in OT 5000.  A cursory review of
the complaints produced by this test suggest the fixes for OT 5000 are
correct.  There are other problems these tests still show.  For
instance, see OT 7057.
Changed CR in Code, Doc, or Test? from `code' to `test' 
Filled in Inter-dependent CRs with `5000' 
Changed Subcomponent Name from `test' to `lfs' 
Filled in Interest List CC with `ota'

[2/2/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Run the test programs as follows (on power11):

	% salt -run cow1,cow6 /dev/epi-blake

The program output reflects the modifications.

Associated information:

Tested on TA build:  
dfs-102-2.4

Tested with backing build:  

Filled in Transarc Deltas with `blake-ot7069-fix-salt-COW-tests' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 7058
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : missing file not subscribed
/file/episode/scavenge/Makefile
is unable to find test_aggr.
rsarbo, bolinger
Reported Date                 : 1/29/93
Found in Baseline             : 1.0.2
Found Date                    : 1/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/salvage/Makefile
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : blake-ot7058-script-to-generate-test-aggregate
Transarc Herder               : 

[1/29/93 public]
The /file/episode/scavenge/Makefile makes reference to 
OTHERS		=test_aggr
which does not appear in the directory.

[1/29/93 public]
The problem is that this file did not get dropped.  It is a binary file that
should be in the tree.  Can you tell me how to get this into ODE without
it modifying any of the file contents (for copyrights and such).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[2/1/93 public]
I've set it up by submitting the file as a uuencoded file, and having the 
Makefile unpack it.
Filled in Short Description with `missing file not subscribed' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `file/episode/salvage/Makefile'

[2/1/93 public]
Here are some issues based upon my phone call with Elliot earlier today:
test_aggr is a datafile but it is not clear whether it is machine dependent
or not.  Also, if it's a datafile then it should be available in formats
decipherable on platforms other than Unix.  (I.e., does MVS have a uudecode
facility.)  Therefore, this defect will not be closed until the following
issues are resolved:
1.  The file can be managed under source control in some format other than
compressed uuencoded.  Rich Zeliff @ OSF will own finding a solution for
this.  After this has been fixed the defect will be reassigned to Elliot
Jaffe to resolution of point 2:
2.  If this file is machine specific then suitable versions of each will be
provided in machine specific directories.

[2/1/93 public]
I've checked, and the file is machine specific.  I've talked to the developer
(blake@transarc.com), and he will try and write a script which can create
this aggregate automagically on each platform.  Please assign this defect
to him once we solve step one above.
Changed Interest List CC from `jaffe@transarc.com, delgado,' to 
 `jaffe@transarc.com, delgado, blake@transarc.com'

[2/4/93 public]
I have replaced test_aggr with a mkaggr script that generates the
a suitable test aggregate.
Defect Closure Form
-------------------
--Verification procedure below--
1) Note that "build" in the scavenge directory correctly installs mkaggr.
2) Run mkaggr and observe by inspection it has created a suitable test
   aggregate.
Associated information:
Tested on TA build:  
dfs-102-2.4
Tested with backing build:  
Filled in Transarc Deltas with `blake-ot7058-script-to-generate-test-aggregate'

[8/18/93 public]
Changed Responsible Engr. from `zeliff' to `blake' 
Changed Resp. Engr's Company from `osf' to `tarc' 
Filled in Transarc Status with `export'

[12/17/93 public]
Closed.



CR Number                     : 7057
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 4867
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvager cannot fix dirs if out of quota/space
Reported Date                 : 1/29/93
Found in Baseline             : 1.0.2
Found Date                    : 1/29/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/sal_errors.h, file/episode/anode/volume.c, file/episode/anode/volume.h, file/episode/salvage/paths.c, file/episode/salvage/salvage_dhops.c, file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot7057-handle-full-errors
Transarc Herder               : jaffe@transarc.com

[1/29/93 public]
This problem developed in salt test cow1a.  It bashed a file which was
removed.  The salvager tried to remove the dir entry that pointed to the
removed file.  This required allocating a fragment becasue the dir was
COW (and unmodified since clone).  This failed because the fileset was
out of quota.  This error code was silently discarded as explained in OT
4867.  The result was repeated attempts to repair resulted in the same
error being reported over and over.
Even if the error had been reported back the salvager does have any code
to do the right thing.  It probably should override quota checking.  In
the case of AGGREGATEFULL errors it should attempt to delete the
directory itself.  This shouldn't be too hard.  I guess I should just
set a global variable to override quota errors during salvage.
Returning an error code from Enumerate (which is attempting to remove
the dir entry) should result in the dir itself being removed.
Ultimately the fileset can just be made INCONSISTENT.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/10/93 public]
I suppressed quota errors during salvage by addition of a new epiv
interface procedure.  This routine may be of use to the volops,
specifically the clone operations which go through some effort to
accomplish the same result.  See the delta for additional details.
Defect Closure Form
-------------------
--Regression test program below--
The script scavenge/simple_test when run on epi-blake (which contains a
fileset which is over quota) tests the VOLUMEFULL case.  Also as
mentioned above the "cow" subtest of the salt script exercises this
case.
--Other explanation below--
I tested the AGGREGATEFULL part of the fix by hand.  At present I do not
have a test aggregate which is out of space so I can not test this
regressibly.
Associated information:
Tested on TA build:  
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Changed Interest List CC from `bwl,mason,blake' to `bwl,mason,blake,jdp' 
Filled in Transarc Deltas with `ota-ot7057-handle-full-errors' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/sal_errors.h, 
 file/episode/anode/volume.c, file/episode/anode/volume.h, 
 file/episode/salvage/paths.c, file/episode/salvage/salvage_dhops.c, 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7046
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Keep raw dev. utilities from touching live aggrs.
Reported Date                 : 1/28/93
Found in Baseline             : 1.0.1
Found Date                    : 1/28/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : jdp-db3072-aggregate-interlocking
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[1/28/93 public]
We've been talking for some time about providing interlocking to
prevent careless administrators from doing things like running newaggr
and salvage on "live" aggregates.  To this end, I plan to add two new
functions to the ftutil library, ftu_LockAggrDev() and
ftu_UnlockAggrDev().  These two operations, combined with a modified
ftu_AttachAggr() will provide interlocking as follows.
ftu_AttachAggr()
        1. Lock the aggregate's device (fail if can't obtain lock)
        2. Attach the aggregate
        3. Unlock the device
ftu_LockAggrDev()
        1. Lock the aggregate's device (fail if can't obtain lock)
        2. Do a lookup to see if this device is attached (fail if it is) 
ftu_UnlockAggrDev()
        1. Unlock the device    
So, both the salvager and newaggr would be coded as:
        ftu_LockAggrDev(...)
        <Do normal processing>
        ftu_UnlockAggrDev(...)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/28/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Run the ftutil lock tests by invoking "fset_test -I scripts/lock.itl".
Tested on TA build:  
dfs-102-2.4
Tested with backing build:  
dce1.0.2b11
Filled in Transarc Deltas with `jdp-db3072-aggregate-interlocking' 
Changed Transarc Status from `open' to `export'

[2/16/93 public]
imported into dfs-102-2.6, and submitted 02-16-93
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 7038
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : flserver
Short Description             : VL_CreateEntry does not free allocated block if not auth
Reported Date                 : 1/27/93
Found in Baseline             : 1.0.2
Found Date                    : 1/27/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : flserver/flprocs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot7038-vl_createentry-free-block-on-not-auth
Transarc Herder               : jaffe@transarc.com

[1/27/93 public]
In VL_CreateEntry, a block is allocated at one point.  Later
auth checking will cause the create to bomb out, but the block
is not freed.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/27/93 public]
Defect Closure Form
-------------------
One-line change: Verified by inspection.
Associated information:
Tested on TA build:  
	dfs-102-2.3
Tested with backing build:  
	dce1.0.2b10
Filled in Affected File with `flserver/flprocs.c' 
Filled in Transarc Deltas with 
 `comer-ot7038-vl_createentry-free-block-on-not-auth' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 7000
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Cleanup anode layer source comments
Reported Date                 : 1/25/93
Found in Baseline             : 1.0.2
Found Date                    : 1/25/93
Severity                      : C
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : many
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot7000-improve-comments
Transarc Herder               : jaffe@transarc.com

[1/25/93 public]
While updating interface specs found several problems with comments
(like some functions didn't have any).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/23/93 public]
The source comments for all function documented in the Episode design
specification now match.  This affects all EXPORTed and most SHAREd
functions in the episode/anode directory.
Defect Closure Form
-------------------
--Other explanation below--
Commentary changes only.  No functionality change.
Basic test_anode scripts run.
Associated information:
Tested on TA build:  
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Filled in Interest List CC with `jaffe,mason' 
Changed Priority from `2' to `1' 
Filled in Transarc Deltas with `ota-ot7000-improve-comments' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `many' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 6996
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : Aggregate name is not recognized
Reported Date                 : 1/25/93
Found in Baseline             : 1.0.1
Found Date                    : 1/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/xaggr/export.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : dimitris-ot6996-have-dfsexport-use-aggrName
Transarc Herder               : jaffe@transarc.com

[1/25/93 public]
dfsexport does not use the aggregate name (2nd field in dfstab) for anything,
even though the documentation says that you can attach or detach based on
aggregate name (as opposed to device name).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/6/93 public]
Changed Responsible Engr. from `jdp@transarc.com' to `dimitris@transarc.com'

[4/9/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Run dfsexport and provide the aggregate name instead of the device name.
Test for -detach case also.
Associated information:
Tested on TA build:  
dfs-102-2.8
Tested with backing build:  
dce1.0.2b19
Filled in Transarc Deltas with `dimitris-ot6996-have-dfsexport-use-aggrName' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/xaggr/export.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 6995
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Make use of us_InitDevice
Reported Date                 : 1/25/93
Found in Baseline             : 1.0.2
Found Date                    : 1/25/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot6995-use-us-initdevice
Transarc Herder               : jaffe@transarc.com

[1/25/93 public]
The function us_InitDevice, added to us_io.c as part of the changes for
OT CR 4179, never gets called.  Various callers of the user-space device
simulation could be cleaned up by rewriting them to call this function.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/27/93 public]
Defect Closure Form
-------------------
--Other explanation below--
Appropriate regression tests were run.

Associated information:

Tested on TA build:  
dfs-102-2.3-b11

Tested with backing build:  
dce1.0.2b11

Filled in Transarc Deltas with `bwl-ot6995-use-us-initdevice' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6982
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Hide platform-dependent access to rdev field
Reported Date                 : 1/22/93
Found in Baseline             : 1.0.2
Found Date                    : 1/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot6982-hide-rdev-differences
Transarc Herder               : jaffe@transarc.com

[1/22/93 public]
Create and use macros so that access to v_rdev doesn't require platform
#ifdef's in the Episode code.  This goes especially for OSI_VN_INIT the whole
purpose of which is to avoid #ifdef's.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/27/93 public]
Defect Closure Form
-------------------
--Other explanation below--
Appropriate regression tests were run.

Associated information:

Tested on TA build:  
dfs-102-2.3-b11

Tested with backing build:  
dce1.0.2b11

Filled in Transarc Deltas with `bwl-ot6982-hide-rdev-differences' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6981
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : allow >1 user of test_vnodeops on a machine
Reported Date                 : 1/22/93
Found in Baseline             : 1.0.2
Found Date                    : 1/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot6981-better-temp-file-name
Transarc Herder               : jaffe@transarc.com

[1/22/93 public]
The names of the temp files created by test_vnodeops should include the
user name, e.g. /tmp/dev-bwl-two instead of /tmp/dev-two, so that different
people can run test_vnodeops at the same time on the same machine.  (Similar
to what test_anode is already doing.)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/27/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Two different users can run test_vnodeops on the same machine at the same
time.  If they can both run, the bug is fixed.

Associated information:

Tested on TA build:  
dfs-102-2.3-b11

Tested with backing build:  
dce1.0.2b11

Filled in Transarc Deltas with `bwl-ot6981-better-temp-file-name' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6978
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : vnva_FileConstruct should bzero the uuid
Reported Date                 : 1/22/93
Found in Baseline             : 1.0.2
Found Date                    : 1/22/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot6978-init-anode-uuid
Transarc Herder               : jaffe@transarc.com

[1/22/93 public]
vnva_FileConstruct, used to create files during a volume op, should bzero
the uuid in the file status before calling epif_Create.  Since we are not
currently using the uuid, and who knows if we ever will, zeroing it makes it
a handy source of spares.  (Note that vnva_FileCreate, used to create files
during a vnode op, is already zeroing the uuid.)
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/27/93 public]
Defect Closure Form
-------------------
--Other explanation below--
This can only be verified using the debugger.
Appropriate regression tests were run.

Associated information:

Tested on TA build:  
dfs-102-2.3-b11

Tested with backing build:  
dce1.0.2b11

Filled in Transarc Deltas with `bwl-ot6978-init-anode-uuid' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6876
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ITL
Short Description             : File_test needs to link pioctl lib.
Reported Date                 : 1/18/93
Found in Baseline             : 1.0.2
Found Date                    : 1/18/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jess-ot6876-itl-link-pioctl-file_test
Transarc Herder               : jaffe@transarc.com

[1/18/93 public]
System call test (file_test) needs to link with pioctl library so that
pioctl functions can be called directly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/15/93 public]
Defect Closure Form
This delta check in a pioctl call interface for file-test component.
There are eight pioctl function calls in the interface. There will be
more pioctl calls exported in this interface.
The delta has been tested on two-machine cell (pmax and rios).
Associated information:
Tested on TA build:
dfs-102-2.7.
Tested with backing build:
dce1.0.2b17.
Filled in Transarc Deltas with `jess-ot6876-itl-link-pioctl-file_test' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6873
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fshost
Short Description             : fshost should use a different ICL event set than PX
Reported Date                 : 1/18/93
Found in Baseline             : 1.0.2
Found Date                    : 1/18/93
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : fshost: fshs.h, fshs_{host,hostops,princ,prutils,subr}.c, px.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot6873-make-fshost-use-own-event-set, cburnett-ot6873-missing-iclset-decl-in-px
Transarc Herder               : jaffe@transarc.com

[1/18/93 public]
The ICL tracing messages in the fshost package use the event set 
structure from the PX; they should use their own.  While this 
won't really cause problems, it is wrong and should be fixed, 
preferably for 1.0.2.  
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/18/93 public]
just visiting

[1/27/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
** See that the new set, fshost, is present
   $ dfstrace lsset
   Available sets:
   cm active
   fshost active
   fx active
** Disable cm and fx, clear, and dump
   $ dfstrace setset cm fx -inactive
   $ dfstrace clear
   $ dfstrace dump | more
   DFS Trace Dump -
   
      Date: Wed Jan 27 15:02:52 1993
   
   Found 1 logs.
   
   Contents of log cmfx:
   time 609.754276, pid 20459: fshs_FindHost, cookie 6591bc0
   time 609.754332, pid 20459: find a prime host 6591bc0
   time 609.754404, pid 20459: PutHost ref = 0
   time 609.754460, pid 20459: find a prime host 6591bc0
   time 609.754507, pid 20459: allocate a sec host 0
   time 609.755824, pid 20459: fshs:TKN_TokenRevoke code = 106754184
   time 609.769648, pid 20459: fshs_GetPrincipal END 0, ref 0
   time 609.769705, pid 20459: fshs_FindHost, cookie 6591bc0
   time 609.769756, pid 20459: find a prime host 6591bc0
   time 609.769827, pid 20459: find a sec host : 65cf088, its prime host : 0
   time 609.769897, pid 20459: cannot find a host in fast path
   time 609.769945, pid 20459: found a princ 0 ref 0
   time 609.769991, pid 20459: found a princ 0 ref 0
   time 609.770033, pid 20459: fshs_AllocPrincipal END..
   
	.
	.
	.
** See only fshost messages.  Voila.
Associated information:
Tested on TA build:  
	dfs-102-2.3
Tested with backing build:  
	dce1.0.2b10
Filled in Affected File with `fshost: fshs.h, 
 fshs_{host,hostops,princ,prutils,subr}.c' 
Filled in Transarc Deltas with `comer-ot6873-make-fshost-use-own-event-set' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Changed Affected File from `fshost: fshs.h, 
 fshs_{host,hostops,princ,prutils,subr}.c' to `fshost: fshs.h, 
 fshs_{host,hostops,princ,prutils,subr}.c, px.h' 
Changed Transarc Deltas from `comer-ot6873-make-fshost-use-own-event-set' to 
 `comer-ot6873-make-fshost-use-own-event-set, 
 cburnett-ot6873-missing-iclset-decl-in-px' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 6848
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ITL test library
Short Description             : Incorporate local enhancements.
Reported Date                 : 1/14/93
Found in Baseline             : 1.0.1
Found Date                    : 1/14/93
Severity                      : E
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : fred-ot6848-add-pack-unpack-capability
Transarc Herder               : mason@transarc.com

[1/14/93 public]
Time to incorporate enhancements made by Ofer.  This will include the
pack/unpack capability for data types.  This is needed for better 
distributed testing.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/25/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
I added the pack/unpack capability, but currently no data type is using it.
This will be added in the future, after additional changes are made to
ITL.  Look at the itl_value.c file and note the extension of the variable
function array.  Also, all of our user defined data types were extended,
e.g. all the .c files in the common_data directory.
Tested on TA build:  dfs-102-2.6
Filled in Transarc Deltas with `fred-ot6848-add-pack-unpack-capability' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6785
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts test20 fails because it restores incrementals in the wrong order
Reported Date                 : 1/10/93
Found in Baseline             : 1.0.2
Found Date                    : 1/10/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : vijay-ot6785-fts-tests-fix-incremental-restore-errors
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[1/10/93 public]

In fts/test20, a full dump is restored followed by a restore of a second
incremental with the first incremental not being restored. This causes 
problems with episode. This test segment should be backed out until this
issue is resolved.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[1/12/93 public]

Defect Closure Form
-------------------

--Regression test program below--

Just run test20 using the runtests driver and it should pass.

Associated information:

Tested on TA build:  
dfs-102-2.2

Tested with backing build:  
dce1.0.2b6

Filled in Transarc Deltas with 
 `vijay-ot6785-fts-tests-fix-incremental-restore-errors' 
Changed Transarc Status from `open' to `export'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6784
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts tests have to be updated to include the new fts command argument changes
Reported Date                 : 1/10/93
Found in Baseline             : 1.0.2
Found Date                    : 1/10/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : vijay-ot6784-fts-test-dump-lsft-should-handle-server-switch
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[1/10/93 public]

The "fts dump" and "fts lsft" commands now have a -server option. The fts
tests have to be fixed to handle this change. At least test12 seems to fail
due to this reason. fts commands in the tests usually don't use the 
command line switches, but just the arguments. This is the reason for this
problem. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[1/10/93 public]

Changed CR in Code, Doc, or Test? from `code' to `test' 
Changed Interest List CC from `davecarr@transarc.com' to `comer@transarc.com'

[1/11/93 public]

Defect Closure Form
-------------------

--Regression test program below--
Run the fts test12. test12 will pass with this fix, but will not otherwise.

Associated information:

Tested on TA build:  
dfs-102-2.2

Tested with backing build:  
dce1.0.2b6

Filled in Transarc Deltas with 
 `vijay-ot6784-fts-test-dump-lsft-should-handle-server-switch' 
Changed Transarc Status from `open' to `export'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6757
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvage dir walk bashed dirb fid uniques to -1
Reported Date                 : 1/7/93
Found in Baseline             : 1.0.2
Found Date                    : 1/7/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3513-do-not-hold-dirbs-on-recurse
Transarc Herder               : jaffe@transarc.com

[1/7/93 public]
The salvager directory walk code that checks for fid->unique == 0 and
sets it to DONT_CARE is actually writing on the *dbP.  If we are doing
any repairs these changes may actually get written back (but not
logged?).  A little unclear, but is seems like a bad idea.
Further, since we hold the dbP refCount high during the recurse, we
limit the directory depth to the number of dir buffers we have.  Also
not really a great idea.  With a little extra copying it looks like it
would be pretty easy to call dirb_Release much earlier and thus avoid
both problems.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[5/26/93 public]
This bug is identical to db 3513, for which Ted has exported a delta.
I do not know what release(s) include Ted's delta, if any.
Filled in Reported by Company's Ref. Number with `Transarc (Sybase) 3513' 
Filled in Transarc Deltas with `ota-db3513-do-not-hold-dirbs-on-recurse' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6745
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Missing VN_RELE in vol_efsAppenddir
Reported Date                 : 1/6/93
Found in Baseline             : 1.0.2
Found Date                    : 1/6/93
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot6745-need-vn_rele-in-appenddir
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[1/6/93 public]
The reason that the vnode ref count on directories is high after doing
restorations is that vol_efsAppenddir calls VFtoEV to get an LFS vnode,
but it doesn't call VN_RELE to drop its reference.  A simple fix.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/6/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Run test/file/rep/rtest1, which may not work, but it will complete without
hanging up doing successive replica propagations.

Associated information:

Tested on TA build:  
	dfs-102-2.2

Tested with backing build:  
	dce1.0.2b6
Filled in Transarc Deltas with `cfe-ot6745-need-vn_rele-in-appenddir' 
Changed Transarc Status from `open' to `export'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6718
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : xvnode/OSF1
Short Description             : kernel crash in nosf_page_read
Reported Date                 : 1/4/93
Found in Baseline             : 1.0.2
Found Date                    : 1/4/93
Severity                      : A
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2b12
Affected File(s)              : xvnode/OSF1/xvfs_vfs2osf.c
Sensitivity                   : public

[1/4/93 public]

PMAX server panic'd with the following stack trace.

vm_fault
vm_fault_page
vnode_pager_data_request_direct
vs_cluster_read
xglue_pgrd
nosf_page_read+1e4

The problem appears to be with the OSF1 specific nosf_page_read.
It seems that on error VOPX_READ could possibly return a null 
buffer pointer (which is what caused the crash).  The 
if (error) return(EIO) statement should probably be moved to
occur before we attempt to do the MIN(n, bsize-bp->b_resid)
calculation.

[1/11/92 public]


fix submitted.

[12/17/93 public]
Closed.



CR Number                     : 6717
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : tsr
Short Description             : add some automated tsr tests for fts move recovery
Reported Date                 : 1/4/93
Found in Baseline             : 1.0.2
Found Date                    : 1/4/93
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot6717-add-tsr-tests-for-fts-move-recovery
Transarc Herder               : jaffe@transarc.com

[1/4/93 public]
Add some automated TET-based tests for tsr recovery on fts move
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[6/29/93 public]
Tests run but do not yet pass.
Filled in Transarc Deltas with 
 `comer-ot6717-add-tsr-tests-for-fts-move-recovery' 
Changed Transarc Status from `open' to `export'

[6/29/93 public]
Changed Defect or Enhancement? from `def' to `enh'

[12/17/93 public]
Closed.



CR Number                     : 6714
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Need to cache ftserver death
Reported Date                 : 12/31/92
Found in Baseline             : 1.0.2
Found Date                    : 12/31/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-ot6714-cache-ftserver-unavailability
Transarc Herder               : jaffe@transarc.com

[12/31/92 public]
At the moment, using something like ``fts lsfldb'' when an ftserver is down
or unreachable is quite painful, as every time fts wants to translate an
aggregate ID to an aggregate name, it makes an RPC call that times out.
Seems it wouldn't be difficult to get fts to cache the fact that an ftserver
isn't responding, at least for the duration of a single fts command, so
that fts wouldn't keep trying to do this translation over and over, but
would simply revert to printing numeric aggregate IDs.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/5/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Create several filesets on a server.  Shut down the ftserver for that
file server.  Issue the ``fts lsfldb'' command.
Without the fix, the fts command hangs for two minutes doing the translation
of aggregate-id to aggregate-name for each fileset on that server.
With the fix, only the first fileset on that server encounters the two-minute
delay.
Associated information:
Tested on TA build:  dfs-103-3.24
Filled in Transarc Deltas with `cfe-ot6714-cache-ftserver-unavailability' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6713
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Defend better against successive restorations
Reported Date                 : 12/31/92
Found in Baseline             : 1.0.2
Found Date                    : 12/31/92
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : cfe-ot6713-rephantomize-when-needed
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/31/92 public]
The process of doing a fileset move does two successive restorations onto
the same destination fileset, one of a full dump and one of an incremental
dump.  The same open (busy) fileset handle is used for both restorations.
Unfortunately, the first restoration leaves the Episode fileset in a state
that is incompatible with the second restoration.  (In brief, all the Episode
vnodes were phantomized at VOLOP_OPEN time, and restoration expects them to
be phantomized at the outset but un-phantomizes them in the course of the
restoration.)  The fix is to phantomize the vnodes again when beginning
a restoration, regardless of whether VOLOP_OPEN might have done so already.

This averts a panic in the new VOLOP_DELETE code, which now correctly uses
the episode/anode file layer, keeping track of zero-link-count files properly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/31/92 public]
Defect Closure Form
-------------------
--Verification procedure below--
Ensure that ``fts move'' works without panic()ing.  fts/test5 will test this.

Associated information:

Tested on TA build:  
	dfs-102-2.2

Tested with backing build:  
	dce1.0.2b6
Changed Interest List CC from `bwl, jdp' to `bwl@transarc.com, 
 jdp@transarc.com, jaffe@transarc.com' 
Filled in Transarc Deltas with `cfe-ot6713-rephantomize-when-needed' 
Changed Transarc Status from `open' to `export'

[1/5/93 public]
Changed Transarc Status from `export' to `import'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6681
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 6663
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bakserver
Short Description             : budb_client.p.h is obsolete and should be removed
Reported Date                 : 12/29/92
Found in Baseline             : 1.0.2
Found Date                    : 12/29/92
Severity                      : B
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : vijay-ot6681-budb_client.p.h-is-obsolete
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[12/29/92 public]

I forgot to remove budb_client.p.h in the delta for OT 6663. This file
is not being used anywhere and should go.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[12/30/92 public]

Defect Closure Form
--Other explanation below--

Nothing to test here. Just deleted a obsolete file. Make sure this file
is gone.

Associated information:

Tested on TA build:  
dfs-102-2.2

Tested with backing build:  
dce1.0.2b6

Filled in Transarc Deltas with `vijay-ot6681-budb_client.p.h-is-obsolete' 
Changed Transarc Status from `open' to `export'

[1/5/93 public]
Changed Transarc Status from `export' to `import'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6677
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : acl test
Short Description             : chmodtest incorrectly determines group mode bits from an ACL
Reported Date                 : 12/29/92
Found in Baseline             : 1.0.2
Found Date                    : 12/29/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : rajesh-ot6677-determine-group-mode-bits-from-an-ACL-correctly
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/29/92 public]

The get_mode_from_acl subroutine does not initialize mask_obj to an
empty string as it does user_obj, group_obj and other_obj variables.
If the present ACL does not have a mask_obj but the ACL analysed by
the previous call to this function did, then the present ACL is
acquires incorrectly the mask_obj entry of the previously analysed
ACL. This causes incorrect determination of the group_mode (UNIX group
mode bits) for this ACL. 

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/29/92 public]
Defect Closure Form
-------------------
--Other explanation below--
The problem was found while investigating failure of acl_edit due
to unavailable registry server. The problem is not easily reproducible
without much special setup. And since it is not a major change
but just filling in what was probably an oversight, its best verified
by inspection.

Associated information:
Tested on TA build:  dfs-102-2.2
Tested with backing build:  dce1.0.2b6

Filled in Transarc Deltas with 
 `rajesh-ot6677-determine-group-mode-bits-from-an-ACL-correctly' 
Changed Transarc Status from `open' to `export'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6669
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 6354, 6307
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : px
Short Description             : the fx server should also return the valid output
Reported Date                 : 12/28/92
Found in Baseline             : 1.0.2
Found Date                    : 12/28/92
Severity                      : B
Priority                      : 1
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : tu-ot6669-tsr-px-output-status
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/28/92 public]

The fx server should also return a valid output status during the TSR work even
if the client does not have a READ_STATUS token when  renewing a token with 
the SAME_TOKEN_ID.

Also, the CM, during TSR, should use the "ServerChangeTime" field recorded in 
the scache to do the comparison. 

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/30/92 public]

[1/11/93 public]
This defect describes specific changes required to fix TSR. This basic problem,
TSR failure, is tracked by 6307.
Filled in Inter-dependent CRs with `6354' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `6307' 
Changed Transarc Status from `open' to `closed'

[1/12/93 public]

[1/12/93 public]

Prevaze is right about this. However, I created a delta,
 tu-ot6669-tsr-px-output-status, based on this OT. This delta fixes, among 
other minors, the TSR problems tracked by 6307.  

I re-open this OT so that the delta could be picked up by our integration 
team and be integrated into dfs 2.4.

In the meantime, OT6307 could be used to make sure that all the tsr tests
will pass against dfs 2.4.

Defect Closure Form
-------------------
--Regression test program below--

I have run the low test/moderate mode in a two-machines cell for one
iteration ie., 12 hours. 

--Verification procedure below--
Also run the tsr test to make sure that the fix is correct.

Associated information:

Tested on TA build:  

Tested with backing build:  

Changed Inter-dependent CRs from `6354' to `6354, 6307' 
Changed Status from `dup' to `open' 
Changed Duplicate Of from `6307' to `' 
Filled in Transarc Deltas with `tu-ot6669-tsr-px-output-status' 
Changed Transarc Status from `closed' to `export'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `export' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6663
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : remove obsolete files in backup
Reported Date                 : 12/28/92
Found in Baseline             : 1.0.2
Found Date                    : 12/28/92
Severity                      : B
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : vijay-ot6663-bak-remove-obsolete-files
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[12/28/92 public]

There are quite a few files in backup that are obsolete and should be removed.
butm/test_ftm.c is one of them. There might be others. These files haven't
been kept pace with changes in backup and would take substantial work to
get them updated. Compilation problems occur when changes in backup are not
propagated to these obsolete files.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[12/29/92 public]

Defect Closure Form
-------------------
--Other explanation below--

There is nothing to test here, as this delta just removes a bunch of files
that were never used. I just ran some of the bak commands to make sure nothing
broke.

Associated information:

Tested on TA build:  
dfs-102-2.2

Tested with backing build:  
dce1.0.2b6

Filled in Transarc Deltas with `vijay-ot6663-bak-remove-obsolete-files' 
Changed Transarc Status from `open' to `export'

[1/5/93 public]
Changed Transarc Status from `export' to `import'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6659
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : acl tests
Short Description             : acl_edit failures are not handled correctly
Reported Date                 : 12/28/92
Found in Baseline             : 1.0.1
Found Date                    : 12/28/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot6659-acl_edit-should-return-error-when-fail
Transarc Herder               : jaffe@transarc.com

[12/28/92 public]
The ACL tests do not check for failure (the return value) after
executing acl_edit at places e.g.syscall/scripts/chmodtest.sh
chacl_test subroutine and at other places, on failure of acl_edit
simply exit without logging an error message e.g.
syscalls/scripts/chmodtest.sh get_mode_from_acl subroutine.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/16/93 public]
In addition to misc/chmodtest.sh, misc/sizetest.sh also did not log the error
on failure of any acl_edit operation. 
                                                                               
Defect Closure Form
-------------------
--Regression test program below--
Run both misc/chmodtest.sh and misc/sizetest.sh to make sure there is no
regression problem. 
Associated information:
Tested on TA build:  
Tested with backing build:  
Changed Interest List CC from `mason@transarc.com' to `comer@transarc.com' 
Changed Responsible Engr. from `rajesh@transarc.com' to `tu@transarc.com' 
Filled in Transarc Deltas with 
 `tu-o-ot6659-acl_edit-should-return-error-when-fail' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6658
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : acl test
Short Description             : ACL tests exits on error without logging error message.
Reported Date                 : 12/28/92
Found in Baseline             : 1.0.1
Found Date                    : 12/28/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot6658-acl_edit-chmodtest-dont-report-error
Transarc Herder               : jaffe@transarc.com

[12/28/92 public]
The syscalls/chmodtest.sh ( chacl_test subroutine ) exits if
the ACLs and UNIX mode bits do not correspond without logging an error
message.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/13/93 public]
Assign this to me as part of sweeping old OTs plan.
Changed Interest List CC from `mason@transarc.com' to `comer@transarc.com' 
Changed Responsible Engr. from `rajesh@transarc.com' to `tu@transarc.com'

[8/13/93 public]
The enhancements made by IBM Austin may have already addressed this issue.

[8/16/93 public]
                                                                               
I got a copy of IBM's work from Rajesh and took a closer look at what their
enhancement was all about. They made a lot of changes on the user interface
of acl_edit by adding a set of new different options to this command. In 
addition, they also incoporated a DBG Trace utility for debuging purpose. 
The level of trace information to be collected can be specified at the new
user interface. Their work is very useful. 
                                                                               
However, IBM's work did not address any of defects reported in ot6658 and 
ot6659. 
                                                                               
I would recommend that IBM's work should be integrated into our code
based as part of the enhance work described in ot6276.   
Defect Closure Form
-------------------
In addition to acl/scripts/misc/chmodtest.sh ( chacl_test subroutine ), there 
are also other places in acl/scripts/misc/getset.sh, 
acl/scripts/misc/inherit.sh and acl/scripts/misc/sizetest.sh that do not
log the error to the status file when comparing ACL and Unix mode bits fails. 
                                                                              
This delta fixed the above problems. 
                                                                              
--Regression test program below--
Run the above acl tests to make sure there is no regression problem. 
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Transarc Deltas with 
 `tu-o-ot6658-acl_edit-chmodtest-dont-report-error' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6625
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Efts create broken by VOLCHECK
Reported Date                 : 12/23/92
Found in Baseline             : 1.0.1
Found Date                    : 12/23/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.2
Affected File(s)              : some
Sensitivity                   : public
Transarc Deltas               : jdp-ot6625-do-open-for-getstatus-in-efts-CreateFT
Transarc Herder               : jaffe@transarc.com
Transarc Status               : submit

[12/23/92 public]
When attempting to bring a new fileset online, the CreateFT needs to
specify FTU_SYS_GET_STATUS when opening the fileset.  The VOLCHECK macro
is now working and causes the subsequent call to getstatus to fail.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/23/92 public]
Defect Closure Form
-------------------
--Verification procedure below--
Just create a fileset with efts.

Tested on TA build:  
dfs-102-2.2

Tested with backing build:  
dce1.0.2b6

Filled in Interest List CC with `ota' 
Filled in Transarc Deltas with 
 `jdp-ot6625-do-open-for-getstatus-in-efts-CreateFT' 
Changed Transarc Status from `open' to `export'

[1/5/93 public]
Changed Transarc Status from `export' to `import'

[1/14/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Changed Affected File from `file/episode/sautils/newft.c' to `some' 
Changed Transarc Status from `import' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 6607
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : writes to mapped memory cause quota under-res'v't'n.
Reported Date                 : 12/22/92
Found in Baseline             : 1.0.2
Found Date                    : 12/22/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-ot6607-mapped-writes
Transarc Herder               : jaffe@transarc.com

[12/22/92 public]
A write to mapped memory that fills in a hole in a file will eventually
(asynchronously) cause a disk block to be allocated for the file.  When
allocating the disk block, epia_Strategy will decrement the file's
quota/space reservation.  But this is inappropriate because there was no
corresponding VOP_RDWR operation that would have incremented the reservation.
The solution is that the reservation should not be decremented unless the
block is within the file's firstReserved-lastReserved range.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/12/93 public]
Filled in Interest List CC with `demail1!carl'

[2/23/93 public]
I am planning a better solution than the one outlined above.  I do not have
time to do it right now, but I am describing it in general terms so that Carl
and I do not end up implementing incompatible solutions.
The storage should be reserved (call efs_Reserve) at the time of the store,
i.e. when efs_strategy gets a buffer that's B_READ but has B_PFSTORE.  This
way, if the reservation fails, the page fault will signal an exception, and
the process can handle it; this is much better than having the exception go
to whatever random process happens to write the page.
Also, we should make sure that we always get a PFSTORE when we need one.
Specifically, I think we are getting them now when the store extends the file,
but not when it fills a hole in the file.  epia_Strategy detects holes in the
file.  In the AIX 3 implementation, epia_Strategy could call vm_protectp when
it detects the hole.  On other platforms, information about holes will have
to be propagated all the way up to the caller of efs_strategy.  This could
be done by designating a new bit in the vnode, VD_HASHOLE, and passing an
out-parameter to epia_Strategy in which it could record whether it found a
hole.  Also a new out-parameter would be needed for epia_Read.  Pretty ugly!
Another method would be to write a new function to test for holes over a
given range, and call that function separately from the call to epia_Strategy
(or even separately from the call to efs_strategy).
Changed Interest List CC from `demail1!carl' to `demail1!carl, 
 ota@transarc.com'

[4/2/93 public]
Defect Closure Form
-------------------
--Regression test program below--
#include <fcntl.h>
#include <sys/types.h>
#include <sys/statfs.h>
/* #include <sys/mman.h> */
#include <sys/shm.h>
main(){
struct statfs fsstat;
char *fbuf;
char buf[8192];
int f = open("test_file",O_RDWR|O_CREAT|O_TRUNC,0777);
write (f, buf, 8192);
lseek (f, 8192*2, 0);
write (f, buf, 8192);
fsync (f);
/* fbuf = mmap (0, 8192*3, PROT_READ|PROT_WRITE, MAP_SHARED, f, 0); */
fbuf = shmat (f, 0, SHM_MAP);
printf("fbuf: %x\n", fbuf);
if (fbuf == -1) perror("test6607");
fstatfs (f, &fsstat);
printf ("free blocks before store: %d\n", fsstat.f_bfree);
printf ("fbuf [8192 + 100]: %d\n", fbuf [8192 + 100]);
fbuf [100] = 'a';
printf ("stored to 100\n");
fbuf [8192 + 100] = 'a';
printf ("stored to 8192 + 100\n");
fstatfs (f, &fsstat);
printf ("free blocks after store: %d\n", fsstat.f_bfree);
/* munmap (fbuf, 8192*3); */
shmdt (fbuf);
close (f);
}
--Other explanation below--
The above program creates a file with a hole in it, maps the file, and
then stores to a byte in mapped memory corresponding to the hole.  It prints
out the free blocks in the file system before and after the store.  If the
store causes a block to be allocated, the bug has been fixed.  Note that the
units in which free blocks are printed out are equal to the fragment size.
So if the store causes one whole block to be allocated, the difference in
the two printed values will be the number of fragments per block.
Note that the file mapping/unmapping is done with shmat/shmdt.  When I tried
to use mmap/munmap (see commented-out code), the store always caused a 
segmentation fault.  This seems to be a problem in the AIX VMM.
Associated information:
Tested on TA build:  
dfs-102-2.7
Tested with backing build:  
I don't know and can't find out--the 2.7 tree has been reaped.
Filled in Transarc Deltas with `bwl-ot6607-mapped-writes' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6445
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvage does not verify ACLs
Reported Date                 : 12/14/92
Found in Baseline             : 1.0.2
Found Date                    : 12/14/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot6445-verify-acls
Transarc Herder               : jaffe@transarc.com

[12/14/92 public]
Tony mentioned during the last DFS course that the contents of ACLs were
not being verified.  I was surprised to here this but I checked the code
and indeed the body of epis_VerifyAcl (in episode/salvage/aux_verify.c)
is ifdef'd out.  Does anyone know why this was done or why it can't be
remedied by just turning it back on?  The code there looks basically
plausible.
Since ACLs are bona fide meta-data and since they are clearly security
relevant we really much check their contents.  That is why this is SevB.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/8/93 public]
Changed Interest List CC from `mason, bwl, ota, bab, rajesh' to `mason, bwl, 
 ota, bab, rajesh, blake'

[2/12/93 public]
As the earlier comments state ACL's are important meta-data.
This should be fixed in 1.0.2.
Changed Interest List CC from `mason, bwl, ota, bab, rajesh, blake' to `mason, 
 bwl, ota, bab, rajesh, blake,demail1!carl' 
Changed Priority from `2' to `1'

[2/12/93 public]
After discussion with Transarc I agree this is not pri 1
material.
Changed Priority from `1' to `2'

[4/27/93 public]
Fixed by calling same verification function used by setacl.
Defect Closure Form
-------------------
--Regression test program below--
The aggregate power8.epi1.920918.attach-panic.Z in ~ota/de/salvage-data
has many interesting ACL problems.  It shows both good and bad ACLs so
seems to be a reasonable test.  This is run with the all_salvage script
which is in turn called by the regression_test script (both in the same
dir).
I also ran it on the aggregates used in out self-host cell and it
reported no (unexplained) ACL errors.  Unfortunately the salt tests do
not yet do much with ACLs.
Associated information:
Tested on TA build:  
dfs-102-2.9
Tested with backing build:  
dce1.0.2b21
Changed Interest List CC from `mason, bwl, ota, bab, rajesh, 
 blake,demail1!carl' to `mason, bwl, ota, bab, rajesh, 
 blake,demail1!carl,SSHI@AUSVM1.VNET.IBM.COM' 
Filled in Transarc Deltas with `ota-ot6445-verify-acls' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6431
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ITL
Short Description             : pioctl calls need to be exported in ITL interface.
Reported Date                 : 12/11/92
Found in Baseline             : 1.0.2
Found Date                    : 12/11/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jess-ot6431-itl-add-pioctl-calls
Transarc Herder               : jaffe@transarc.com

[12/11/92 public]
Some file system call tests and pioctl test itself require that 
the pioctl call to be able to be called from ITL interface.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/11/92 public]
Changed Defect or Enhancement? from `def' to `enh'

[3/15/93 public]
Defect Closure Form
This delta exports all pioctl function call to ITL.
The delta has been tested on two-machine cell (pmax and rios) and 
a basic pioctl function call script.
Associated information:
Tested on TA build:  
dfs-102-2.7.
Tested with backing build:  
dce1.0.2b17.
Filled in Transarc Deltas with `jess-ot6431-itl-add-pioctl-calls' 
Changed Transarc Status from `open' to `export'

[3/15/93 public]

[12/17/93 public]
Closed.



CR Number                     : 6363
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Quota mods for FTSERVER clone, reclone, and destroy
Reported Date                 : 12/7/92
Found in Baseline             : 1.0.1
Found Date                    : 12/7/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/ftserver/ftserver_vprocs.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jdp-ot6363-increase-quota-before-clone-operation
Transarc Herder               : jaffe@transarc.com

[12/7/92 public]
Due to the dual quotas now associated with filesets, the operations associated
with the following functions need to be modified:
	FTSERVER_Clone
	FTSERVER_Reclone
	FTSERVER_Destroy
Basically, these operations require code changes to ensure that quota
restrictions don't prevent their completion.  The visible and allocated quotas
(see defect 6358) of the fileset(s) in question may need to be combined to
make sure that the operations don't violate quota limits.  The quotas would
be changed temporarily to allow the operations to complete, after which they
would be reset to their proper values.  This would enable the operations to
reliably circumvent quota checking practices that could conceivably prevent
their completion.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/5/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `6360' 
Filled in Fixed In Baseline with `1.0.2' 
Changed Transarc Status from `open' to `closed'

[1/5/93 public]
Jeff's initial description seems to have been targeted to a code change,
not to a doc change.  In any case, the ops he describes, along with the
corresponding ones in the repserver, need to be modified to ensure that
the dual-quota limitations do not interfere with the high-level operations.
Changed CR in Code, Doc, or Test? from `doc' to `code' 
Changed Subcomponent Name from `app_gd,app_ref' to `fts' 
Changed Priority from `2' to `3' 
Changed Status from `dup' to `open' 
Changed Duplicate Of from `6360' to `' 
Changed Fixed In Baseline from `1.0.2' to `' 
Changed Responsible Engr. from `mjc@transarc.com' to `jdp@transarc.com' 
Changed Transarc Status from `closed' to `open'

[01/12/93 public]
Marked this bug a duplicate of #6360, as requested by Monica Cellio
at Transarc.  #6360 was closed on 01/12/93.

[1/12/93 public]
I believe that this OT was mistakenly entered as a doc defect.  I've again
moved it to be a code defect and assigned it to Jeff Prem.  Hope it sticks
this time.
Changed Interest List CC from `jeff@transarc.com, cfe@transarc.com' to 
 `jeff@transarc.com, cfe@transarc.com, mjc@transarc.com' 
Changed Status from `dup' to `open' 
Changed Duplicate Of from `6360' to `' 
Changed Fixed In Baseline from `1.0.2' to `' 
Changed Affected File from `4' to `'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/ftserver/ftserver_vprocs.c' 
Filled in Transarc Deltas with 
 `jdp-ot6363-increase-quota-before-clone-operation' 
Changed Transarc Status from `open' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 6358
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 6360, 5926, 6399
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Make fileset quota interface changes on fts cmds.
Reported Date                 : 12/7/92
Found in Baseline             : 1.0.1
Found Date                    : 12/7/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[12/7/92 public]
Output of the fts lsheader and, by implication, fts lsft commands needs to be
updated to reflect the additional information each will now provide about the
actual, allocated size of a DCE LFS read/write fileset.  Due to the nature of
read/write and backup filesets, each read/write fileset has both a "visible,"
or effective, quota and an "allocated," or actual, quota.  The visible quota
is that which can be changed directly via the fts interface; the allocated
quota is the true disk usage of the read/write fileset.  The output of these
two fts commands will now include an additional "alloc" field to apprise the
administrator of a fileset's allocated quota.
Users will be concerned only with the visible quota, as that dictates the
amount of data they can store in a fileset.  When they deal with users,
administrators also will be concerned with the visible quota.  However, when
they are concerned with the actual, physical disk usage of a read/write
fileset, administrators will need to be aware of the allocated quota.
Because backup filesets are implemented with copy-on-write anodes, it is the
allocated quota of the backup version of a fileset that has the effective disk
usage of the newly-cloned read/write fileset on which it is based.  As data in
the read/write fileset changes, the read/write fileset's allocated quota grows,
while the backup fileset's quota remains unchanged until the next backup
operation.
The administrator who needs to move a fileset to make room on an aggregate
needs to be aware of the allocated quotas of both the read/write and backup
version of the fileset, as the combined allocated quotas of the two types of
filesets represent the amount of disk space that will be freed by the move. 
The combined visible quotas may or may not reflect the true sizes of the two
filesets.
Backup and replication operations will be adjusted to achieve logical, expected
results.  In other words, the fts interface will handle the two types of quotas
in such a way that their implementation is transparent to both the user and the
administrator.  (An additional defect will be created to reflect the necessary
documentation changes of this code change on the DFS API documentation.)
Craig, please correct or expand the above description as you feel necessary;
thanks.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `closed'

[12/7/92 public]
Filled in Inter-dependent CRs with `6360' 
Changed Transarc Status from `closed' to `open'

[12/21/92 public]
The code OTs are: 5926 (for Episode parts) and 6399 (for fileset parts).
Changed Inter-dependent CRs from `6360' to `6360, 5926, 6399'

[4/10/93 public]
I lowered the priority of this defect because it is fully documented in the
DCE 1.0.2 Release Notes.
Changed Priority from `1' to `2'

[9/2/93 public]
Filled in Affected File with `See description' 
Changed Responsible Engr. from `jeff@transarc.com' to `kdu@transarc.com'

[9/16/93 public]
The following files are affected:
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
	admin_ref/man8dfs/fts_lsheader.8dfs
	admin_ref/man8dfs/fts_lsft.8dfs
	admin_ref/man8dfs/fts_setquota.8dfs
	users_gdref/dfs/u1_intro_dfs.gpsml
	users_gdref/man1dfs/fts_lsquota.1dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build and
closed this bug.



CR Number                     : 6344
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : Syscall tests.
Short Description             : Have problems to run on PMAX.
Reported Date                 : 12/4/92
Found in Baseline             : 1.0.2
Found Date                    : 12/4/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : jess-ot6344-fix-syscall-test-on-pmax
Transarc Herder               : jaffe@transarc.com

[12/4/92 public]
This delta is opened for fixes on pmax running of the syscall tests.
Some problems are because the different system call definitions
and some other minor adjustments.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/14/92 public]
The delta for this OT was imported into TA build dfs-102 2.3, but will
be backed out by hand.

[12/15/92 public]

[3/15/93 public]
Defect Closure Form
-------------------
This delta has been tested on a two-machine cell configured by dfs 2.7.
Refer to ot 6277 for possible problems.
This delta must go with "jess-ot6277-add-syscall-tests-1".
Associated information:
Tested on TA build:  
dfs 2.7
Tested with backing build:  
dce1.0.2bi17
Filled in Transarc Deltas with `jess-ot6344-fix-syscall-test-on-pmax' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6284
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts crmount and delmount do not validate input mount dir correctly
Reported Date                 : 12/2/92
Found in Baseline             : 1.0.1
Found Date                    : 12/2/92
Severity                      : E
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot6284-fts-crm-use-wrong-mnt
Transarc Herder               : mason@transarc.com

[12/2/92 public]
Erroneously, I executed a fts crmount with a local (non-DFS) mount point - a 
subdir of /. Instead of failing the fts crmount created the mount symlink.
And subsequently, fts delmount failed to remove this symlink. 
$ fts lsfldb
fset.acltest.101
        readWrite   ID 0,,101  valid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
power11.transarc.co RW       epi1    0:00:00 hosts/power11  <nil>          <nil>
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
power11.transarc.co RW       /usr    0:00:00 hosts/power11  <nil>          <nil>
----------------------
Total FLDB entries that were successfully enumerated: 2 (0 failed)
$ fts crmount /acltest -fileset 101
fts: DFS junction for cell '/.../' not found: not registered in endpoint map (dc
e / rpc).
fts crmount: continuing anyway.
$ ls -l /acltest
lrwxrwxrwx   1 root     transarc       5 Dec 02 10:12 /acltest@ -> #101.
continuing anyway.
$ fts delmount /acltest
fts delmount: error removing mount point for /acltest: No such file or director
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[8/10/93 public]
This delta only fixes the problem associated with the fts crmount. The 
problem associated with the fts delmount has been addressed in ot5929.
Defect Closure Form
-------------------
--Verification procedure below--
# fts crm /foo fooo
fts crmount: mount points must be created within the DFS file system
# fts crm / foooo
fts crmount: mount points must be created within the DFS file system
# fts crm /tmp/foo foooo
fts crmount: mount points must be created within the DFS file system
#
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Interest List CC with `vijay' 
Changed Responsible Engr. from `vijay@transarc.com' to `tu@transarc.com' 
Filled in Transarc Deltas with `tu-o-ot6284-fts-crm-use-wrong-mnt' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6259
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : Errors when "dump"ing to mislabeled tape
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot6259-bak-errors-when-dumping
Transarc Herder               : jaffe@transarc.com

[12/1/92 public]
   If an attempt is made to "dump" to a mislabeled tape and the job is
aborted, information is added to the database (as shown by "dumpinfo") as if
the dump had succeeded.
   Also, butc aborts properly but outputs the message:
        "file ../../../../src/file/butc/dump.c, line 917, code = 546545671".
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/26/93 public]
Defect Closure Form
-------------------
--Regression test program below--
Run bak in interactive mode
Do a dump of a partition
do a labeltape with a bogus label
Do another dump, and abort before this
starts dumping. Verify that the dump doesnt appear in the dumpinfo
--Verification procedure below--
See above
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-102-2.9 
Tested with backing build:  
dfs-102-2.9
Filled in Transarc Deltas with `khale-ot6259-bak-errors-when-dumping' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6255
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : -tcid option range checking handled incorectly
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot6255-bak-option-range-not-checked
Transarc Herder               : jaffe@transarc.com

[12/1/92 public]
   Some of the bak commands that have a -tcid option do not check the upper
range limit (currently 7).  bak/commands.c should probably be perused to
find all such occurrences.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/27/93 public]
   While working on another bug, I had occasion to look at all the commands
that take a -tcid option and found that six of the eleven do not do upper
range limit checking.  They are: labeltape, readlabel, restoredb, savedb,
scantape and status.

[4/12/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
Try doing a 
bak labeltape -tcid 1025
This should fail  with a message about how tcid's greater than 1023 are
not allowed. Prior to the fix it would simply say unable to connect to
port 1025
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-102-2.2
Tested with backing build:  
dfs-102-2.2
Filled in Transarc Deltas with `khale-ot6255-bak-option-range-not-checked' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6250
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : "dumpinfo" sometimes has confusing output
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot6250-bak-dumpinfo-has-confusing-output
Transarc Herder               : jaffe@transarc.com

[12/1/92 public]
   When "dumpinfo" is called with no dump information in the database the
following message is output:
        dumpInfo: index to iterator function is out of range (dfs / bdb)
	          Can't get dump information
The command should output a more informative and accurate message.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/30/93 public]
Defect Closure Form
-------------------
--Regression test program below--

[12/17/93 public]
Closed.



CR Number                     : 6245
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ACL test
Short Description             : The utility programs in acl/util generate error
Reported Date                 : 11/30/92
Found in Baseline             : 1.0.1
Found Date                    : 11/30/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot6245-acl_edit-generates-errors
Transarc Herder               : jaffe@transarc.com

[11/30/92 public]
The utility program generate error messages even when errors are
expected without generating any "expected error" messages. This can be
confusing.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[8/13/93 public]
Assign this to me as part of sweeping old OTs plan.
Filled in Interest List CC with `comer' 
Changed Responsible Engr. from `rajesh@transarc.com' to `tu@transarc.com' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com'

[8/16/93 public]
The fix is very straightforwar: just removed all error messages in the 
utility code and let the utility code return the status code to the caller.
The caller (the test driver) will print out (it already did) the appropriate
error messages depending on whether it is testing an error case or not. 
                                                                              
Defect Closure Form
-------------------
--Regression test program below--
Ran the whole set of scripts/syscalls test suite to make sure that there is
no regression problem. 
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Transarc Deltas with `tu-o-ot6245-acl_edit-generates-errors' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6236
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : "bak help" still lists abort
Reported Date                 : 11/30/92
Found in Baseline             : 1.0.2
Found Date                    : 11/30/92
Severity                      : E
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot6236-backup-bak-help-still-lists-abort
Transarc Herder               : jaffe@transarc.com

[11/30/92 public]
    abort is now obsolete but is still in "bak help" and, when invoked, prints
out the message:
	 bak: Abort no longer supported, use kill 
   All references to abort should be eliminated.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/30/93 public]
Defect Closure Form
-------------------
--Regression test program below--
The bug can be verified very easily by  typing bak help. abort will
show up in the old menu, but not in the new menu
--Verification procedure below--
Type 'bak  help'. abort should not show up in the help menu. Also, typing
'bak abort' should fail  with "unrecognized operation"
--Other explanation below--
Associated information:
Tested on TA build:

 [ dfs-102-2.8] 
Tested with backing build:

 [ dfs-102-2.8] 
Filled in Transarc Deltas with `khale-ot6236-backup-bak-help-still-lists-abort' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6232
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : butc
Short Description             : tabs in TapeConfig file cause parsing problems
Reported Date                 : 11/30/92
Found in Baseline             : 1.0.2
Found Date                    : 11/30/92
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : khale-ot6232-tabs-in-TapeConfig-file-cause-parsing-problems
Transarc Herder               : jaffe@transarc.com

[11/30/92 public]
   The procedure butc/tcmain.c:atocl is used in the parsing of some of the
fields in the TapeConfig file but it doesn't recognize tabs as valid
whitespace.  This is unintuitive and debugging TapeConfig file to find this
error would be unnecessarily difficult.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/1/92 public]
Filled in Interest List CC with `vijay@transarc.com'

[7/20/93 public]
   
As part of the OT sweeping plan, assign this to Abhijit. 
Changed Interest List CC from `vijay@transarc.com' to `vijay@transarc.com, 
 davecarr@transarc.com, tu' 
Changed Responsible Engr. from `davecarr@transarc.com' to `khale@transarc.com'

[7/25/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
Simply insert some tabs in the TapeConfig file and run butc. It should
run cleanly, without any problems. 
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-103-3.23
Tested with backing build:  
dce103
Filled in Transarc Deltas with 
 `khale-ot6232-tabs-in-TapeConfig-file-cause-parsing-problems' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6222
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Document dfstrace facility
Reported Date                 : 11/29/92
Found in Baseline             : 1.0.2
Found Date                    : 11/29/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : closed

[11/29/92 public]
The icldump utility needs to be documented. A man page explaining its
syntax and use should be provided. 
This isn't required for 1.0.2, but it certainly would be useful, if time
permits.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[12/14/92 public]
The icldump command has mutated to become the dfstrace suite.  It is a somewhat
large suite that accordingly requires a somewhat large amount of documentation.
Filled in Subcomponent Name with `admin_gd, admin_ref' 
Changed Short Description from `icldump needs a man page' to `Document dfstrace 
 facility'

[2/11/93 public]
When is the target date for this to be done.  Is it early in 1.03?
I hope so.
Changed Interest List CC from `kazar@transarc.com, pakhtar@transarc.com' to 
 `kazar@transarc.com, pakhtar@transarc.com,demail1!carl'

[2/11/93 public]
Right now, we're just concentrating on making 1.0.2, so no explicit plans
exist for 1.0.3 at this time.  As the dfstrace facility is important, a
technical specification will be available for 1.0.2.  I'll keep you posted
on any future progress toward this defect.

[6/22/93 public]
Changed "Fix By Baseline" from 1.0.3 to 1.1.

[7/30/93 public]
Changed Defect or Enhancement? from `enh' to `def'

[8/30/93 public]
Changed "Fix By Baseline back to 1.0.3.  Since this is a defect (and
Transarc has stated that it will fix it in 1.0.3).

[9/23/93 public]
Filled in Affected File with `See description' 
Changed Responsible Engr. from `jeff@transarc.com' to `kdu@transarc.com'

[10/12/93 public]

[10/13/93 public]
The following files are affected:
	admin_gd/dfs/dfs/11_scout_dfs.gpsml
	admin_ref/man8dfs/dfstrace.8dfs
	admin_ref/man8dfs/dfstrace_apropos.8dfs
	admin_ref/man8dfs/dfstrace_clear.8dfs
	admin_ref/man8dfs/dfstrace_dump.8dfs
	admin_ref/man8dfs/dfstrace_help.8dfs
	admin_ref/man8dfs/dfstrace_lslog.8dfs
	admin_ref/man8dfs/dfstrace_lsset.8dfs
	admin_ref/man8dfs/dfstrace_setlog.8dfs
	admin_ref/man8dfs/dfstrace_setset.8dfs
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Changed Transarc Status from `open' to `closed'

[11/11/93 public]

Verified changes in latest doc build and closed this bug.



CR Number                     : 6131
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 4557
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Writing restart block has slight vulnerability
Reported Date                 : 11/17/92
Found in Baseline             : 1.0.2
Found Date                    : 11/17/92
Severity                      : B
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot6131-write-checkpoint-block
Transarc Herder               : jaffe@transarc.com

[11/17/92 public]
Some discussions about sync in connection with OT 4557 revealed that
there is a slight period of vulnerability when writing a restart block.
This happens when a system is shutdown and when recovery finishes.  The
problem is that restart block is always written at block zero and will
obliterate some log block.  If this is part of the active log and if the
restart block isn't completely written then recovery will fail.  Clearly
this window is small and the only downside is to run salvage (which
should find no errors, otherwise it wouldn't have been correct to write
a restart block in the first place).
This fix is pretty easy.  If we write a checkpoint block as described in
OT 4557 then there would be no single point of failure.  If the restart
block is incomplete then the checkpoint block will govern.  If the
restart block and the checkpoint block are both block zero then the
previous active region will govern.  Both of these are safe, although a
longer recovery would be necessary in the latter case.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/8/93 public]
Rajesh has been handling stuff like this recently.
Changed Responsible Engr. from `ota@transarc.com' to `rajesh@transarc.com'

[9/24/93 public]
Changed Responsible Engr. from `rajesh@transarc.com' to `bwl@transarc.com'

[9/30/93 public]
Easy as this fix is the affected code is quite delicate and the risk of
serious corruption is to too great to justify closing this *TINY*
loophole.
Changed to enhancement.  We will continue to track this item as DB4411.
Changed Defect or Enhancement? from `def' to `enh' 
Filled in Reported by Company's Ref. Number with `4411'

[10/4/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
The test plan was as follows:
(1) Create an aggregate that has a checkpoint block and a restart block,
    but the restart block is bogus (i.e. is rejected by elbr_GetCleanLogInfo).
(2) Run an old (pre-checkpoint) version of salvage -recoveronly on the
    aggregate.  It should reject the aggregate because it doesn't like the
    log.
(3) Run a new version of salvage -recoveronly on the aggregate.  It salvages
    the aggregate OK.
This tests recovery using checkpoint blocks.  It doesn't test the "window
of vulnerability" case (where the bogus restart block overwrote part of the
log), at least not unless the aggregate's log just happened to wrap around,
but I verified using dbx that no part of the log except the checkpoint block
was being looked at, so there is no reason to believe that this particular
case would cause us to fail.
Step (1) was tricky.  Rather than try to inject an I/O error during writing
of the restart block, I did the following:
- Modify elbl_Close to write the wrong value into the restart record's
  recType (LOGREC_RSTRT+1 instead of LOGREC_RSTRT);
- build, install, and load a kernel extension using the modified elbl_Close;
- attach and detach a normal aggregate.
Of course, the modification to elbl_Close is not part of the fix, and was
immediately removed!
Associated information:
Tested on TA build:  
dfs-osf 1.4 (in directory dfs-osf-1.2)
Tested with backing build:  
(in directory dce1.0.3-0831)
Changed Defect or Enhancement? from `enh' to `def' 
Filled in Transarc Deltas with `bwl-o-ot6131-write-checkpoint-block' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 6027
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsexport
Short Description             : dfsatab is a dumb idea!
Reported Date                 : 11/10/92
Found in Baseline             : 1.0.2
Found Date                    : 11/10/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/xaggr/export.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : dimitris-do-not-use-dfsatab
Transarc Herder               : jaffe@transarc.com

[11/10/92 public]
The very idea is silly.  dfsexport should look in the aggregate registry. The
BSD guys learned this with mount points.  Frequently I run "dfsexport" and it
tells me one thing (wrong) while "efts lsaggr" tells me another thing
("right").  Since we've already got a utility which does it, let's do it
right.
Marked as an enhancement, but I think this one's important.  A bad dfsatab
means that dfsexport won't export your fileset!
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/7/93 public]
Changed Interest List CC from `cfe,ota' to `cfe,ota,jdp' 
Changed Responsible Engr. from `jdp' to `dimitris'

[4/7/93 public]
This looks like one in whose outcome I may potentially have an interest....
Changed Interest List CC from `cfe,ota,jdp' to `cfe,ota,jdp,jeff'

[4/7/93 public]
Yes.

[4/9/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
Export some aggregates and then run dfsexport with no arguments to ensure
that they all show up. 
Associated information:
Tested on TA build:  
dfs-102-2.8
Tested with backing build:  
dce1.0.2b19
Filled in Transarc Deltas with `dimitris-do-not-use-dfsatab' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/xaggr/export.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 5972
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bos command
Short Description             : bos create returns zero on invalid create
Reported Date                 : 11/6/92
Found in Baseline             : 1.0.1
Found Date                    : 11/6/92
Severity                      : D
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/bosserver/bbos_bnode.c, file/bosserver/bbos_bnode.h, file/bosserver/bbos_err.et, file/bosserver/bossvr_bnode_cronops.c, file/bosserver/bossvr_bnode_ezops.c, file/bosserver/bossvr_ncs_procs.
c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : comer-ot5972-return-error-on-create-for-bad-executable
Transarc Herder               : mason@transarc.com

[11/6/92 public]
When bos create is issued with a nonexistent cmd, it returns zero,
and puts the entry in BosConfig:
# bos create /.:/hosts/ecell2.austin.ibm.com foo -type simple -cmd /tmp/foo
# echo $?
# bos status ecell2 -long
bos: WARNING: short name for server used; no authentication information will be 
sent to the bosserver
Bosserver reports inappropriate access on server directories.
Instance flserver, (type is simple) currently running normally.
    Process last started at Fri Nov  6 15:22:57 1992 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/flserver'
Instance ftserver, (type is simple) currently running normally.
    Process last started at Fri Nov  6 15:22:58 1992 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/ftserver'
Instance repserver, (type is simple) currently running normally.
    Process last started at Fri Nov  6 15:23:02 1992 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/repserver'
Instance upserver, (type is simple) currently running normally.
    Process last started at Fri Nov  6 15:23:05 1992 (1 proc starts)
    Parameter 1 is '/opt/dcelocal/bin/upserver -path /opt/dcelocal/var/dfs/admin
.bos /opt/dcelocal/var/dfs/admin.fl /opt/dcelocal/var/dfs/admin.ft /opt/dcelocal
/var/dfs/admin.up /opt/dcelocal/var/dfs/admin.bak'
Instance foo, (type is simple) temporarily disabled, stopped for too many errors
, currently shutdown.
    Process last started at Fri Nov  6 17:19:43 1992 (13 proc starts)
    Last exit at Fri Nov  6 17:19:43 1992
    Last error exit at Fri Nov  6 17:19:43 1992, by exiting with code 2
    Parameter 1 is '/tmp/foo'
# pg BosConfig
restarttime 11 0 4 0 0
checkbintime 3 0 5 0 0
bnode simple flserver 1
parm /opt/dcelocal/bin/flserver
end
bnode simple ftserver 1
parm /opt/dcelocal/bin/ftserver
end
bnode simple repserver 1
parm /opt/dcelocal/bin/repserver
end
bnode simple upserver 1
parm /opt/dcelocal/bin/upserver -path /opt/dcelocal/var/dfs/admin.bos /opt/dcelo
cal/var/dfs/admin.fl /opt/dcelocal/var/dfs/admin.ft /opt/dcelocal/var/dfs/admin.
up /opt/dcelocal/var/dfs/admin.bak
end
bnode simple foo 1
parm /tmp/foo
end
This is on 1.53 level code. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[11/16/92 public]
Reassigned to Comer.
Changed Responsible Engr. from `mason@transarc.com' to `comer@transarc.com'

[2/23/93 public]
Inadvertantly opened this and enh instead of def originally.
Changed Defect or Enhancement? from `enh' to `def'

[2/26/93 public]
Defect Closure Form
-------------------
I added another bnode op that will validate the arguments prior to a
create.  THis guy now sees if the executable specified exists before
creating the bnode.  If it fails, the bosserver will now return
"specified executable not found".
--Verification procedure below--
   [dce] bos stat olorin -noa
   Instance ftserver, currently running normally.
   Instance repserver, currently running normally.
   Instance upclient, currently running normally.
   Instance bakserver, has core file, currently running normally.
** Normal create for type 'simple'
   [dce] bos create /.:/hosts/olorin test1 simple -cmd "/bin/sleep 3600" -local
   [dce] bos stat olorin -noa
   Instance ftserver, currently running normally.
   Instance repserver, currently running normally.
   Instance upclient, currently running normally.
   Instance bakserver, has core file, currently running normally.
   Instance test1, currently running normally.
   [dce] bos stop /.:/hosts/olorin test1 -local
   [dce] bos delete /.:/hosts/olorin test1 -local
   [dce] bos stat olorin -noa
   Instance ftserver, currently running normally.
   Instance repserver, currently running normally.
   Instance upclient, currently running normally.
   Instance bakserver, has core file, currently running normally.
** create of bogus bnode of type simple
   [dce] bos create /.:/hosts/olorin test1 simple -cmd "/bin/bogus 3600" -local
   bos: failed to create new server instance test1 of type 'simple' (specified executable not found (dfs / bbs))
   [dce] bos stat olorin -noa
   Instance ftserver, currently running normally.
   Instance repserver, currently running normally.
   Instance upclient, currently running normally.
   Instance bakserver, has core file, currently running normally.
** Now create a cron bnode
   [dce] bos create /.:/hosts/olorin test1 cron -cmd "/bin/sleep 3600" now -local
   [dce] bos stat olorin -noa -long
   Instance ftserver, (type is simple) currently running normally.
       Process last started at Fri Feb 26 14:50:00 1993 (1 proc starts)
       Parameter 1 is '/opt/dcelocal/bin/ftserver'
   
   Instance repserver, (type is simple) currently running normally.
       Process last started at Fri Feb 26 14:50:00 1993 (1 proc starts)
       Parameter 1 is '/opt/dcelocal/bin/repserver'
   
   Instance upclient, (type is simple) currently running normally.
       Process last started at Fri Feb 26 14:50:01 1993 (1 proc starts)
       Parameter 1 is '/opt/dcelocal/bin/upclient -server /.:/hosts/power8 -path /opt/dcelocal/var/dfs/admin.bos /opt/dcelocal/var/dfs/admin.ft'
   
   Instance bakserver, (type is simple) has core file, currently running normally.
       Process last started at Fri Feb 26 14:58:56 1993 (5 proc starts)
       Last exit at Fri Feb 26 14:58:56 1993
       Last error exit at Fri Feb 26 14:58:56 1993, due to signal 6
       Parameter 1 is '/opt/dcelocal/bin/bakserver.new'
   
   Instance test1, (type is cron) currently running normally.
       Auxiliary status is: running now.
       Process last started at Fri Feb 26 15:00:08 1993 (1 proc starts)
       Parameter 1 is '/bin/sleep 3600'
       Parameter 2 is 'now'
   
   [dce] bos stop /.:/hosts/olorin test1 -local
   [dce] bos delete /.:/hosts/olorin test1 -local
** Create bogus cron bnode
   [dce] bos create /.:/hosts/olorin test1 cron -cmd "/bin/bogus 3600" now -local
   bos: failed to create new server instance test1 of type 'cron' (specified executable not found (dfs / bbs))
   [dce] bos create /.:/hosts/olorin test1 cron -cmd "/bin/bogus 3600" sunday -local
   bos: failed to create new server instance test1 of type 'cron' (specified executable not found (dfs / bbs))
   
Associated information:
Tested on TA build:  
	dfs-102-2.6
Tested with backing build:  
	dce1.0.2b15
Filled in Interest List CC with `jeff@transarc.com' 
Filled in Transarc Deltas with 
 `comer-ot5972-return-error-on-create-for-bad-executable' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/bosserver/bbos_bnode.c, 
 file/bosserver/bbos_bnode.h, file/bosserver/bbos_err.et, 
 file/bosserver/bossvr_bnode_cronops.c, file/bosserver/bossvr_bnode_ezops.c, 
 file/bosserver/bossvr_ncs_procs.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 5952
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm loops making file server RPCs
Reported Date                 : 11/5/92
Found in Baseline             : 1.0.2
Found Date                    : 11/5/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[11/5/92 public]
The fts test suite was run on a pmax on UFS, with a second DFS server in the
cell being a rios exporting a UFS aggregate. Two commands in the fts suite,
"fts crmount", and "chmod" loop making calls to AFS_FetchStatus. The calls
to these two commands never completed, and have accumulated lots of CPU time.
The build is dfs-102-1.55 with the test version test-102-1.14. Here's a 
stack trace that I got from kdb. I have the stack trace of all threads. Let
me know if you would like them. The format below is, ps output of the 
process, and a kdb stack trace.
  942 p0  U     85:09.02 chmod 777 /.../power2.dce.transarc.com/fs/test
task 0x0c1564624 [chmod               ] proc 806502e8 pid 942 map 0x0c13a9090: thread 0x0c157f910  R  N pri = 13, 0u 5113s87140c
0x0c157f910$K
thread 0x0c157f910,  pcb 0x0c263a000, W_EVENT 0
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from rpc__dg_call_wait+1c8
rpc__dg_call_wait() from rpc__dg_call_receive_int+18c
rpc__dg_call_receive_int() from rpc__dg_call_transceive+440
rpc__dg_call_transceive() from rpc_call_transceive+78
rpc_call_transceive() from AFS_FetchStatus+3b0
AFS_FetchStatus() from GetAccessBits+1e8
GetAccessBits() from cm_AccessOK+34
cm_AccessOK() from cm_setattr+548
cm_setattr() from nosf_setattr+78
nosf_setattr() from xglue_setattr+118
xglue_setattr() from chmod+0a4
chmod() from syscall+288
syscall() from VEC_syscall+94
2$0000000001( FAILED
  925 p0  U    133:37.22 fts crmount -dir /.../power2.dce.transarc.com/fs/test/tmp.ufs -fileset power2.tmp
task 0x0c1564948 [fts                 ] proc 8064f9d8 pid 925 map 0x0c13a9240: thread 0x0c157fd84   W N 0c1553c68
0x0c157fd84$K
thread 0x0c157fd84,  pcb 0x0c2634000, W_EVENT 0c1553c68
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from rpc__dg_call_wait+1c8
rpc__dg_call_wait() from rpc__dg_call_receive_int+18c
rpc__dg_call_receive_int() from rpc__dg_call_transceive+440
rpc__dg_call_transceive() from rpc_call_transceive+78
rpc_call_transceive() from AFS_FetchStatus+3b0
AFS_FetchStatus() from GetAccessBits+1e8
GetAccessBits() from cm_AccessOK+34
cm_AccessOK() from cm_lookup+6a0
cm_lookup() from nosf_lookup+84
nosf_lookup() from xglue_lookup+124
xglue_lookup() from namei+6b8
namei() from symlink+0d8
symlink() from syscall+288
syscall() from VEC_syscall+94
2$0000000001( FAILED
Stack traces of some of the other threads
0x0c150a618$K
thread 0x0c150a618,  pcb 0x0c25c1000, W_EVENT 0
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c150a794$K
thread 0x0c150a794,  pcb 0x0c25bf000, W_EVENT 0c150a834
thread_block() from mpsleep+2a4
mpsleep() from event_wait+84
event_wait() from rpc__socket_select+210
rpc__socket_select() from lthread+158
lthread() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c150a49c$K
thread 0x0c150a49c,  pcb 0x0c25c3000, W_EVENT 0c1509a34
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c150a320$K
thread 0x0c150a320,  pcb 0x0c25c5000, W_EVENT 0c1509a5c
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c150a1a4$K
thread 0x0c150a1a4,  pcb 0x0c25c7000, W_EVENT 0c1509a84
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c150a028$K
thread 0x0c150a028,  pcb 0x0c25c9000, W_EVENT 0c1509aac
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c155cd84$K
thread 0x0c155cd84,  pcb 0x0c25cb000, W_EVENT 0c1509ad4
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c155cc08$K
thread 0x0c155cc08,  pcb 0x0c25cd000, W_EVENT 0c1509afc
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c155ca8c$K
thread 0x0c155cc08,  pcb 0x0c25cd000, W_EVENT 0c1509afc
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c155c910$K
thread 0x0c155cc08,  pcb 0x0c25cd000, W_EVENT 0c1509afc
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
0x0c155c49c$K
thread 0x0c155cc08,  pcb 0x0c25cd000, W_EVENT 0c1509afc
thread_block() from mpsleep+2a4
mpsleep() from sleep+7c
sleep() from pthread_cond_wait+98
pthread_cond_wait() from cthread_call_executor+1cc
cthread_call_executor() from base_routine+104
base_routine() from find_thread_args+0b0
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[7/20/93 public]
This defect was actually fixed and also reported in ot7331. 
The delta dimitris-ot7331-bad-ctime-can-cause-infinite-loop has already
been integrated in 1.0.3 tree.
Mark it cancelled. 
Changed Status from `open' to `cancel' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com'

[7/20/93 public]
Since it is only in 103 tree, I have to mark it as "transarc export".
Changed Status from `cancel' to `open' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5929
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : delmount blows away non-mount point files
Reported Date                 : 11/4/92
Found in Baseline             : 1.0.2
Found Date                    : 11/4/92
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot5929-cm-check-mnt-before-delete
Transarc Herder               : mason@transarc.com

[11/4/92 public]
fts delmount does not check to see if the argument passed is a valid DFS
mount point. Ordinary files are removed by this command. This cannot be right.
When the argument is a directory, fts delmount rightly fails to delete it
but gives out the following error message.
# mkdir test
# ls
test/
# fts delmount test
fts delmount: error removing mount point for test: Not owner
The error message does not seem right. I'm logged in as cell_admin and the
directory is in LFS. This is seen in dfs-102-1.53 on a rios.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[8/10/93 public]
This delta fixes two problems: 1) fts delm should not delete a regular file,
2) fts delm deleting a dir should NOT return 'not owner', it should EINVAL
instead. 
Defect Closure Form
-------------------
--Regression test program below--
The fixes are in CM and FTS but they are very isolated. 
--Verification procedure below--
# ls -l pp
-rw-r--r--   1 100      12             0 Aug 10 10:14 pp
# fts delm pp
fts delmount: error removing mount point for pp: Invalid argument
#
# ls -l pdir
total 0
# ls -ld pdir
drwxr-xr-x   2 100      12           256 Aug 10 18:25 pdir
# fts delm pdir
fts delmount: error removing mount point for pdir: Invalid argument
Associated information:
Tested on TA build:  
Tested with backing build:  
Changed Responsible Engr. from `vijay@transarc.com' to `tu@transarc.com' 
Filled in Transarc Deltas with `tu-o-ot5929-cm-check-mnt-before-delete' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5900
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : repserver
Short Description             : error messages should go to stderr only as a last resort
Reported Date                 : 11/3/92
Found in Baseline             : 1.0.2
Found Date                    : 11/3/92
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-ot5900-keep-stderr-quiet
Transarc Herder               : jaffe@transarc.com

[11/3/92 public]
The repserver is currently sending some reasonably normal messages to its
stderr, even if the relevant conditions are also being logged successfully.
For example:
	repserver: FL server not yet up for VL_ListByAttributes: no quorum
		elected (dfs / ubk)
	repserver: no quorum elected (dfs / ubk) from EnumerateVLDB().
		Retrying.
	repserver: no aggregates on this server.
These should be logged, but not also normally printed on stderr.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/5/93 public]
Defect Closure Form
-------------------
--Other explanation below--
Removed uses of stderr except when Debugging is set or when the repserver
is about to exit because of an error.
Tested on TA build:  dfs-103-3.24
Filled in Transarc Deltas with `cfe-ot5900-keep-stderr-quiet' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5893
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fshost,px
Short Description             : tokenint_TokenRevoke looks like it can be removed
Reported Date                 : 11/2/92
Found in Baseline             : 1.0.1
Found Date                    : 11/2/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/fshost/fshs_host.h, file/fshost/fshs_hostops.c, file/px/px_intops.c, file/px/px_repops.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-ot5893-remove-tokenint_TokenRevoke
Transarc Herder               : mason@transarc.com

[11/2/92 public]
The function tokenint_TokenRevoke (fshs_hostops.c) looks like it is 
never called and can be removed.  Also the comments preceeding the 
function indicate that it is bogus!!!.  There is also a comment in 
px_repops.c which mentions this function.
Here is the function:
/*
 * NOTE: This routine is bogus ???????
 */
int tokenint_TokenRevoke(hostp, tokenFids, tokens)
    register struct fshs_host *hostp;
    afsRevokes *tokenFids;
{
    int errorCode;
    AFSLOG(HS_DEBUG, 2, ("tokenint_TokenRevoke(hostp=%x)\n", hostp));
    if (hostp->flags & FSHS_HOST_SIMPLEKEEP)
        errorCode = pxint_TokenRevoke(tokenFids, tokens);
    else if (hostp->flags & FSHS_HOST_LOCALHOST)
        errorCode = afstoken_TokenRevoke(tokenFids, tokens);
    else    /* ???? */
        errorCode = (int)TKN_TokenRevoke(hostp->cbBinding, tokenFids);
    return errorCode;
}
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[11/11/92 public]
Reassign to Tu.
Filled in Interest List CC with `kazar,mason' 
Changed Responsible Engr. from `mason@transarc.com' to `tu@transarc.com'

[3/12/93 public]
Removed those obsolete functions and flag FSHS_HOST_LOCAL. They are no
longer used.
Defect Closure Form
-------------------
--Regression test program below--
I have run two copies of cho tests in a two-machine cell to test it and 
also 'kinit' to make sure there is no side-effect introduced. 
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Transarc Deltas with `tu-ot5893-remove-tokenint_TokenRevoke' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/fshost/fshs_host.h, 
 file/fshost/fshs_hostops.c, file/px/px_intops.c, file/px/px_repops.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 5778
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : admin_gd, admin_ref
Short Description             : Document use of short form machine names
Reported Date                 : 10/23/92
Found in Baseline             : 1.0.1
Found Date                    : 10/23/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : closed

[10/23/92 public]
The use of short forms of server machine names is discouraged in DFS.  This
information has appeared only as a release note; it needs to be documented
in appropriate places in the Admin Guide and Admin Ref documents.  The text
of the original release note on this topic follows:
"Do not use the short form of a machine name (for example, do not use <foo>
instead of /.../<cellname>/hosts/<foo> or /.:/hosts/<foo>) when issuing DFS
commands.  The results of using a short form name are unpredictable."
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[10/27/92 public]
Changed Severity from `D' to `C'

[10/27/92 public]
Changed Defect or Enhancement? from `enh' to `def'

[6/19/93 public]
The following release note changes the documentation that needs to be provided
for this defect:
 
   The documentation currently states that a server machine to be used with
   a command must be specified by its full DCE pathname (for example,
   /.../abc.com/hosts/<hostname>).  However, DFS commands that require a
   machine name also accept the short form of the machine's name (for example,
   <hostname>.abc.com or just <hostname>) of the machine's IP address (for
   example, 11.22.33.44).  Commands in the bos suite are exceptions to the
   use of short form names and IP addresses; if you use either of these
   methods to indicate a server machine with a bos command, the bos program
   displays the following message and executes with the same privilege as if
   you included the -noauth option:
 
      bos: WARNING: short form for server used; no authentication
      information will be sent to the bosserver
This information needs to be included across the documentation.

[6/21/93 public]
Filled in Interest List CC with `comer@transarc.com'

[9/9/93 public]
Changed Responsible Engr. from `jeff@transarc.com' to `kdu@transarc.com'

[9/16/93 public]
The following files are affected:
	admin_gd/dfs/dfs/4_adminkey_dfs.gpsml
	admin_gd/dfs/dfs/5_processes_dfs.gpsml
	admin_gd/dfs/dfs/6_ftavail_dfs.gpsml
	admin_gd/dfs/dfs/7_ftmgmt_dfs.gpsml
	admin_gd/dfs/dfs/9_backup_dfs.gpsml
	admin_gd/dfs/dfs/10_backrest_dfs.gpsml
	admin_ref/man8dfs/bak.8dfs
	admin_ref/man8dfs/bak_addftentry.8dfs
	admin_ref/man8dfs/bak_restoredisk.8dfs
	admin_ref/man8dfs/bak_restoreft.8dfs
	admin_ref/man8dfs/bos.8dfs
	admin_ref/man8dfs/bos_addadmin.8dfs
	admin_ref/man8dfs/bos_addkey.8dfs

	admin_ref/man8dfs/bos_create.8dfs
	admin_ref/man8dfs/bos_delete.8dfs
	admin_ref/man8dfs/bos_gckeys.8dfs
	admin_ref/man8dfs/bos_genkey.8dfs
	admin_ref/man8dfs/bos_getdates.8dfs
	admin_ref/man8dfs/bos_getlog.8dfs
	admin_ref/man8dfs/bos_getrestart.8dfs
	admin_ref/man8dfs/bos_install.8dfs
	admin_ref/man8dfs/bos_lsadmin.8dfs
	admin_ref/man8dfs/bos_lscell.8dfs
	admin_ref/man8dfs/bos_lskeys.8dfs
	admin_ref/man8dfs/bos_prune.8dfs
	admin_ref/man8dfs/bos_restart.8dfs
	admin_ref/man8dfs/bos_.rmadmin8dfs
	admin_ref/man8dfs/bos_rmkey.8dfs
	admin_ref/man8dfs/bos_setauth.8dfs
	admin_ref/man8dfs/bos_setrestart.8dfs
	admin_ref/man8dfs/bos_shutdown.8dfs
	admin_ref/man8dfs/bos_start.8dfs
	admin_ref/man8dfs/bos_status.8dfs
	admin_ref/man8dfs/bos_stop.8dfs
	admin_ref/man8dfs/bos_uninstall.8dfs
	admin_ref/man8dfs/fts.8dfs
	admin_ref/man8dfs/fts_addsite.8dfs
	admin_ref/man8dfs/fts_aggrinfo.8dfs
	admin_ref/man8dfs/fts_clonesys.8dfs
	admin_ref/man8dfs/fts_crfldbentry.8dfs
	admin_ref/man8dfs/fts_crserverentry.8dfs
	admin_ref/man8dfs/fts_delete.8dfs
	admin_ref/man8dfs/fts_delfldbentry.8dfs
	admin_ref/man8dfs/fts_delserverentry.8dfs
	admin_ref/man8dfs/fts_dump.8dfs
	admin_ref/man8dfs/fts_edserverentry.8dfs
	admin_ref/man8dfs/fts_lsaggr.8dfs
	admin_ref/man8dfs/fts_lsfldb.8dfs
	admin_ref/man8dfs/fts_lsft.8dfs
	admin_ref/man8dfs/fts_lsheader.8dfs
	admin_ref/man8dfs/fts_lsreplicas.8dfs
	admin_ref/man8dfs/fts_lsserverentry.8dfs
	admin_ref/man8dfs/fts_move.8dfs
	admin_ref/man8dfs/fts_restore.8dfs
	admin_ref/man8dfs/fts_rmsite.8dfs
	admin_ref/man8dfs/fts_statftserver.8dfs
	admin_ref/man8dfs/fts_statrepserver.8dfs
	admin_ref/man8dfs/fts_syncfldb.8dfs
	admin_ref/man8dfs/fts_syncserv.8dfs
	admin_ref/man8dfs/fts_unlockfldb.8dfs
	admin_ref/man8dfs/fts_update.8dfs
	admin_ref/man8dfs/fts_zap.8dfs
	users_gdref/dfs/u4_share_dfs.gpsm
Changed Status from `open' to `verified' 
Filled in Fixed In Baseline with `1.0.3' 
Filled in Affected File with `See description' 
Changed Transarc Status from `open' to `closed'

[11/10/93 public]

Verified changes in latest doc build (against all files
listed) and closed this bug.



CR Number                     : 5719
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : No prototype declaration for ubik_ClientInit().
Reported Date                 : 10/20/92
Found in Baseline             : 1.0.1
Found Date                    : 10/20/92
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot5719-add-prototype-dcl-ubik
Transarc Herder               : jaffe@transarc.com

[10/20/92 public]
The nubik.h file does not contain a prototype declaration for the exported
function ubik_ClientInit().
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[8/25/93 public]
Defect Closure Form
-------------------
--Regression test program below--
After adding the 'prototypes' for ubik interface, I have compiled flserver,
fts on RIOS to make sure that there is no rgressions problems. The prototype
does take effect when Vijay later comipled his stuff on Solaris. (He opend a SB
defect to fix that compilation complaining.)
Associated information:
Tested on TA build:  
Tested with backing build:  
Filled in Transarc Deltas with `tu-o-ot5719-add-prototype-dcl-ubik' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com' 
Changed Transarc Status from `open' to `export'

[8/26/93 public]
The delta to fix compilation problems on solaris is vijay-db4171-ubik-correct-warnings-on-dependent-modules 1.1 and this is exported.

[12/17/93 public]
Closed.



CR Number                     : 5694
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts returns 0 on failures for several subcommands
Reported Date                 : 10/19/92
Found in Baseline             : 1.0.2
Found Date                    : 10/19/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-ot5694-scanfldb-error-propagation
Transarc Herder               : jaffe@transarc.com

[10/19/92 public]
clonesys return code is not correct for failures
 
These should all return non-zero, the commands failed.
 
  # this clonesys command should fail because of the non-LFS support
  # and/or because of the CreateTrans failures.
# fts clonesys
Backup is supported only on LFS filesets (e.g. not on this non-LFS one).
Backup is supported only on LFS filesets (e.g. not on this non-LFS one).
JEP:Entered sec_krb_register_server
FTSERVER_CreateTrans of vol=0,,13, aggrId=12, flag=67108868 failed
Device busy
FTSERVER_CreateTrans of vol=0,,16, aggrId=12, flag=67108868 failed
Device busy
FTSERVER_CreateTrans of vol=0,,19, aggrId=13, flag=67108868 failed
Device busy
FTSERVER_CreateTrans of vol=0,,22, aggrId=13, flag=67108868 failed
Device busy
Backup is supported only on LFS filesets (e.g. not on this non-LFS one).
Backup is supported only on LFS filesets (e.g. not on this non-LFS one).
----------------------
Total FLDB entries that were successfully backed up: 0 (8 failed)
FAILED: Expected Invalid, Returned 0 - clonesys.000
 
   # new does not match any existing filesets, so it should fail
   # the message 0 (0 failed) is correct but the return code should
   # still be non-zero because no match occurred.
# fts clonesys -prefix new
----------------------
Total FLDB entries that were successfully backed up: 0 (0 failed)
FAILED: Expected Invalid, Returned 0 - clonesys.001
 
  # this clonesys command should fail because of the non-LFS support
TC: fts clonesys -server ${server[1]} -aggregate ${aggregate[1]}
EP: fts clonesys -server hercules.austin.ibm.com -aggregate /jfs2
Backup is supported only on LFS filesets (e.g. not on this non-LFS one).
----------------------
Total FLDB entries that were successfully backed up: 0 (1 failed)
FAILED: Expected Invalid, Returned 0 - clonesys.019
 
==================================================================
fts delfldbentry return codes are incorrect
 
  # The following command should return non-zero since the prefix provided
  # did not match any of the fileset names.
# fts delfldbentry -prefix I_string
----------------------
Total FLDB entries that were successfully deleted: 0 (0 failed)
FAILED: Expected Invalid, Returned 0 - delfldb.016
 
==================================================================
No admin.fl auth but unlockfldb returns 0 anyway
 
If you run fts unlockfldb without permission in the admin.fl adminlist
the command acknowledges "no permission access" and states 0 unlocked 1
failed but still returns zero.
 
I : fts unlockfldb -server ${server[1]} -aggregate ${aggregate[2]} -v
EP: fts unlockfldb -server /.:/hosts/haunted.austin.ibm.com -aggregate /jfs2 -v
Could not release lock on FLDB entry (vol= 0,,4, type=0)
FLDB: no permission access (dfs / vls)
----------------------
Total FLDB entries that were successfully unlocked: 0 (1 failed)
FAILED: Expected Invalid(I), Returned 0 - adm.fl.0365
 
==================================================================
No admin.fl auth but syncfldb returns 0 anyway
 
Commands outputs "no permission access", then outputs "synchronized with state
of server" and returns 0 when it should return non-zero.
 
I : fts syncfldb -server ${server[1]} -aggregate ${aggregate[2]} -v
EP: fts syncfldb -server /.:/hosts/haunted.austin.ibm.com -aggregate /jfs2 -v
Grouping 1 fileset entries by their parent IDs on aggregate ID 2
traversing the internal queue:
---------------------------
/dev/lv03 readWriteID 0,,4 valid readOnlyID 0,,0 invalid backupID 0,,0 invalid
---------------------------
Processing queue entries for server haunted.austin.ibm.com aggregate Id 2 ...
Could not get the highest allocated fileset id from FLDB
FLDB: no permission access (dfs / vls)
FLDB synchronized with state of server /.:/hosts/haunted.austin.ibm.com aggregate /jfs2
FAILED: Expected Invalid(I), Returned 0 - adm.fl.0380
 
==================================================================
No admin.f[lt] auth but syncserv returns 0 anyway
 
The syncserv command returns zero when it does not have permission to access
the fldb.  This occurs for no permission in admin.fl and admin.ft.
 
admin.fl
 
I : fts syncserv -server ${server[1]} -aggregate ${aggregate[2]} -v
EP: fts syncserv -server /.:/hosts/haunted.austin.ibm.com -aggregate /jfs2 -v
Processing FLDB entries on aggregate Id 2 ...
Processing FLDB entry 1 of total 1 in run 1...
Could not lock FLDB entry (vol=0,,4, type=0, op=128)
...locked for delete on Thu Jun  4 09:38:33 1992
FLDB: no permission access (dfs / vls)
^MCould not process FLDB entry for fileset jfs2_fs (676372507)
FLDB: no permission access (dfs / vls)
^M.. done.
Total entries : 1, Failed to process 1
 
jfs2_fs status before
  readWrite   ID 0,,4  valid
  readOnly    ID 0,,5  invalid
  backup      ID 0,,6  invalid
number of sites: 1
  Locked for delete on Thu Jun  4 09:38:33 1992
   server           flags     aggr   siteAge principal      owner          objid
haunted.austin.ibm. RW       /jfs2   0:00:00 hosts/haunted.austin.ibm.com<nil>
        <nil>
Server /.:/hosts/haunted.austin.ibm.com aggregate /jfs2 synchronized with FLDB
FAILED: Expected Invalid(I), Returned 0 - adm.fl.0385
 
===============================================================================
 
admin.ft
 
I : fts syncserv -server ${server[1]} -aggregate ${aggregate[2]} -v
EP: fts syncserv -server /.:/hosts/haunted.austin.ibm.com -aggregate /jfs2 -v
Processing FLDB entries on aggregate Id 2 ...
Processing FLDB entry 1 of total 1 in run 1...
FTSERVER_CreateTrans of vol=0,,4, aggrId=2, flag=2 failed
Requested access denied (dfs / dau)
^MCould not process FLDB entry for fileset jfs2_fs (556613638)
Requested access denied (dfs / dau)
^M.. done.
Total entries : 1, Failed to process 1
 
jfs2_fs status before
  readWrite   ID 0,,4  valid
  readOnly    ID 0,,5  invalid
  backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
haunted.austin.ibm. RW       /jfs2   0:00:00 hosts/haunted.austin.ibm.com<nil>
        <nil>
Server /.:/hosts/haunted.austin.ibm.com aggregate /jfs2 synchronized with FLDB
FAILED: Expected Invalid(I), Returned 0 - adm.fl.0385
 
==================================================================
statftserver returns 0 on permission failure
 
This was on 1.50 with LFS as root.dfs in a single system cell.
dce_logged in as user1 which is a member of test_group, test_group was
included in the admin.fl list but not the admin.ft list, so the permission
failure is valid but the return status is incorrect.
 
I : fts statftserver -server ${server[1]}
EP: fts statftserver -server /.:/hosts/ecell2
Could not lock FLDB entry (vol=0,,25, type=-1, op=256)
Error: FLDB: no permission access (dfs / vls)
Error in dump: FLDB: no permission access (dfs / vls)
No active transactions on /.:/hosts/ecell2
FAILED: Expected Invalid(I), Returned 0 - flft.0530
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[10/19/92 public]
Assigned to Vijay.  (Hope this is OK with Mike C.)  Most of these
failures are due to the ProcessEntries() routine (that scans several
FLDB entries) not accumulating any notion of failure of the sub-
tasks and passing them back via any error exit.  The statftserver
problem is different, but is simple.
Filled in Interest List CC with `cfe@transarc.com, comer@transarc.com' 
Changed Found in Baseline from `1.0.1' to `1.0.2' 
Changed Fix By Baseline from `1.0.1' to `1.0.2' 
Filled in Responsible Engr. with `vijay@transarc.com'

[11/9/92 public]
The edserverentry and lsserverentry return 0 when an invalid server is used.
# host I_host
host: 0827-801 Host name I_host does not exist.
 
# fts edserverentry -server I_Host
# echo $?
 
# host I_host
host: 0827-801 Host name I_host does not exist.
 
# fts lsserverentry -server I_machine
Description for site 'I_machine':
0.0.0.0 (2:0)
FLDB quota: 0;  uses: 0;  principal=`hosts/cellhost.austin.ibm.com';  owner=<nil
>;  objid=<nil>
# echo $?

[5/12/93 public]
I'm about to import Dawn's changes to fix these and adjust the test cases.
Changed Interest List CC from `cfe@transarc.com, comer@transarc.com' to 
 `vijay@transarc.com, comer@transarc.com, dawn%liz.austin.ibm.com@transarc.com' 
Changed Responsible Engr. from `vijay@transarc.com' to `cfe@transarc.com' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com'

[5/12/93 public]
I could not reproduce the problems with fts lsserverentry or fts
edserverentry.  However, this delta fixes all the problems described above.
Thanks to Dawn Stokes for doing most of the leg-work.
 
Defect Closure Form
-------------------
--Verification procedure below--
Notice the above messages.  Do ``fts clonesys JUNKNAME'' and look at the
program's exit status, which should be 1.
I ran the three changed fts tests (test3, test6, test8) and they worked
fine.
 
Associated information:
Tested on TA build:  dfs-carl 1.10
Tested with backing build:  dce1.0.2a-0427
Filled in Transarc Deltas with `cfe-ot5694-scanfldb-error-propagation' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5677
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : cm checkfileset command may have to perform the
Reported Date                 : 10/16/92
Found in Baseline             : 1.0.2
Found Date                    : 10/16/92
Severity                      : A
Priority                      : 0
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 1.0.3a
Affected File(s)              : many
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : cfe-ot5677-fts-move-redux, jaffe-ot5677-cleanup-merge, tu-ot5677-recover-fts-move-failure
Transarc Herder               : jaffe@transarc.com

[10/16/92 public]
cm checkfileset command may have to perform the 
move token state recovery, if the fileset has been moved to a different
server.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[10/16/92 public]
I'm saying this for completeness, even though I think that Tu knows it:
the move-fileset TSR needs to happen whenever the CM learns that the R/W
location of a fileset changed, whether it learns that via the
``cm checkfileset'' command, via the periodic re-checking of the FLDB that the
CM does, or via getting a VOLERR_PERS_DELETED error code back from a file
server (which causes an FLDB check).

[2/12/93 public]
I am raising this to a higher priority to get fixed in 1.0.2.  From
the description it seems that without this a CM will not recognize
to fileset movement.  Is this true?  If there are other ways that
the CM will recognize the movement and use the moved fileset then
maybe this one is not so important.  Would someone comment on this
defect explaining what the lack of this fix means to the cache
manager and what the behavior of the cache manager should be
when a fileset moves.
Changed Interest List CC from `cfe@transarc.com,kazar@transarc.com, 
 pakhtar@transarc.com' to `cfe@transarc.com,kazar@transarc.com, 
 pakhtar@transarc.com,demail1!carl' 
Changed Priority from `2' to `1' 
Changed Fixed In Baseline from `1.0.2' to `'

[2/12/93 public]
There's a deficiency in how the CM does fileset-move TSR, or at least there
has been.  (Tu can comment on whether he's fixed this yet.)  Fileset-move
TSR is triggered in the CM by the loss of the TKN_SPOT_HERE token; if that's
how the CM learns of a new fileset location, then it can correctly move its
token state to the new location for the R/W fileset.
The deficiency is (or was) that the CM can learn of a new fileset location
in a variety of ways, and there's a CM design/implementation error in the
move-TSR code in that those other ways of learning of a new fileset location
do NOT invoke move-TSR, so the CM will NOT move its token state to the
new R/W server.  The CM can learn of the new location if it happens to probe
the FLDB between the time that the FLDB changes and the time that it receives
the revocation request for its TKN_SPOT_HERE token.  This time interval can be
large if lots of CMs have TKN_SPOT_HERE tokens on a moving fileset and the
CM in question isn't the first on the revocation list.  Events that can
cause the CM to check the FLDB include accessing the volume after
a ``cm checkfilesets'' command has marked its volume knowledge as stale,
or after the periodic (hourly?) volume info sweep has done the same thing
asynchronously, or indeed if the old location for the fileset is accessed
in any way after fts has brought the source back on-line but exposed the
VOL_MOVE_SOURCE state bit, causing the file exporter to return
VOLERR_PERS_DELETED because the old location shouldn't be used any more.
As I say, Tu can and should comment on the implementation state of any
new code necessary to track fileset moves in these conditions.

[2/24/93 public]
If it is a "must fix" for 1.0.2, then it is a defect, not an enhancement. I've
altered the state accordingly. i.e. we intend to fix it.
Changed Defect or Enhancement? from `enh' to `def'

[3/4/93 public]
First, the its a "known" problem, rather than one thats been observed.
After some considerable discussion on the nature of the problem, how to
reproduce it, and the nature of the fixes, we've concluded the following:
- reproducing the problem is non-trivial. We currently have no means of
	reliably reproducing this.
- without the above, meaningful verification testing will not be possible. 
- the fixes do not require any protocol changes, but do affect more than
	a trivial (i.e. few lines) of code.
It therefore seems inappropriate to fix a problem that isn't observed in
practice, esp. at this stage of the 1.0.2 release.
I've therefore downgraded the problem, and it'll be addressed post 1.0.2.
Changed Priority from `1' to `2'

[4/23/93 public]
This defect fixes a number of problems including ot 7683.  Upgrade to match
that status.
Changed Interest List CC from `cfe@transarc.com,kazar@transarc.com, 
 pakhtar@transarc.com,demail1!carl' to `cfe@transarc.com,kazar@transarc.com, 
 pakhtar@transarc.com,demail1!carl, delgado@osf.org' 
Changed Severity from `B' to `A' 
Changed Priority from `2' to `1' 
Changed Fix By Baseline from `1.0.2' to `1.0.2a' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com'

[4/23/93 public]
The defect described in the "Short Description" field is overly
simplified. 
                                                                           
The fixes is to allow the CM to be more bullet-proof even though the fileset 
is moved abnormally. That is, during the fileset move, if the network is
partitioned, the ftserver is crashed, or even the flserver or fldb is in a 
strange state, etc., the CM should be able to recover itself. The CM would
not get tokens from both the source server and the target server for a 
partitcular file.
                                              
The CM follows a rigid rule to get tokens:
  a) It only gets the HERE token once for each fileset as opposed to getting a
     HERE token for each file before.
                   
  b) It always checks that it has a HERE token for a fileset first before it
     gets the types of tokens for files in that fileset.  
                                                                     
Upon receiving a HERE token revocation request, the CM is informed the
fact that the fileset is moved to a new location. The CM then starts the 
TSR-move procedure to renew tokens from the appropriate fx server.
However, at any time when requesting a token, if the CM notices that it does 
not have a HERE token, or the fx server that houses the fileset is different 
than the one from which it got the HERE token, the TSR-move procedure is also 
triggered, on the spot, to renew tokens from an appropriate fx server.
                                               
This is actually considered an abnormal case -- the CM apparently missed the
signal at the time the fileset is actually being moved. Most of the work in 
this fix is to remedy this situation.

[04/28/93 public]
This is now showing up during regular cho (the cm_Rele panic) and
it's really getting in the way - bumped up to 0.

[5/4/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `many' 
Filled in Transarc Deltas with `cfe-ot5677-fts-move-redux, 
 jaffe-ot5677-cleanup-merge, tu-ot5677-recover-fts-move-failure' 
Changed Transarc Status from `open' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 5575
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : epimount default volume id of 37 prevents epimount commands that only specify volume name to fail
Reported Date                 : 10/7/92
Found in Baseline             : 1.0.1
Found Date                    : 10/7/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : rajesh-ot5575-epimount-remove-bogus-default-values
Transarc Herder               : mason@transarc.com

[10/7/92 public]

epimount assumes a default value of 37 for volume_id if volume id is
not specified on the command line. This causes a epimount command that
only specifies a volume name to fail with EINVAL as the epimount code
does not care for the volume name if the volume id is non-zero. The
error EINVAL is generated in InitEM in efs_vfsops.c.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[10/7/92 public]
Defect Closure Form
-------------------

--Other explanation below--
Issue a valid epimount command with the fileset name specified but
without a fileset id and verify that it succeeds.

Associated information:

Tested on TA build:dfs-102-1.51

Tested with backing build:dce1.0.2-0911

Filled in Transarc Deltas with 
 `rajesh-ot5575-epimount-remove-bogus-default-values' 
Changed Transarc Status from `open' to `export'

[11/6/92 public]
Changed Transarc Status from `export' to `import'

[11/11/92 public]
I've tested this on RIOS and PMAX.  When trying to export a fileset with
fileset id 0, the proposed syntax works.  i.e.:

# epimount root.dfs /dev/epi0 /root.dfs
Mounting volume "root.dfs" (device /dev/epi0, id 0) at /root.dfs...
building vmount data
doing vmount
   did mount, return value = 0
# df /root.dfs
Filesystem    Total KB    free %used   iused %iused Mounted on
/dev/epi0        20000   13220   33%   -       -    /root.dfs

But if the fileset id is not 0, then the epimount fails.

this defect is still open.
Changed Transarc Status from `import' to `open'

[2/3/93 public]
I tested this again on a RIOS and epimount specifying only the fileset name
and not the fileset id succeeds irrespective of the fileset id value. e.g.

 [salvage] hostname
power11

 [salvage] efts lsaggr
DEVICE NAME              ID TYPE          TOTAL       FREE
/dev/epi1            786438 episode        4095       3599
/dev/epil1          1572868 episode        8191       7980

 [salvage] efts lsft epi1
NAME                       ID
epi1-0.101                0,,101

 [salvage]

 [salvage] efts create epi1 -name testfset -id 401

 [salvage] epimount /dev/epi1 testfset /mnt3
Mounting volume "testfset" (device /dev/epi1, id 0) at /mnt3...
building vmount data
doing vmount
   did mount, return value = 0

Hence I am changing this defect to export
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5242
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvage panics updating link count in backing fileset
Reported Date                 : 8/28/92
Found in Baseline             : 1.0.2
Found Date                    : 8/28/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : salvage/paths.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db3230-avoid-repairing-backing-fileset
Transarc Herder               : jaffe@transarc.com

[8/28/92 public]
This panic is illustrated by salvaging the aggregate in:
  ~/de/salvage-data/power1.920827.panic-updating-RO-link-counts.Z
The output looks like this:
Will run recovery on /dev/epi2
recover 3 pages, 261 records in 430 ms, touched 10 data blocks
Ran recovery on dev 1/1
Salvaging /dev/epi2
Will run recovery on /dev/epi2
In volume x.1.clone (avl #5)
  Volume is marked as inconsistent, not walked
In volume x.1.readonly (avl #6)
  in anode (#133)
    File's link count is 2 but should be 0
    -> adjusting link count
  in anode (#161)
    File's link count is 1 but should be 0
    -> adjusting link count
(global package/paths.c rev. 4.7 #83 assertion failed) alfs_MBZ: code(565575718) was not zero
Abort process (core dumped)
This error code translates as:
Message for 565575718 is 'other containers using these blocks for copy-on-write purposes (dfs / epi)'
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/27/92 public]
Reassigned to Blake.
Changed Interest List CC from `cfe@transarc.com' to `cfe' 
Changed Responsible Engr. from `ota@transarc.com' to `blake@transarc.com'

[2/8/93 public]
Ted's re-inheriting all of the salvager bugs.  Does this even still happen?
I know Ted's made considerable changes here.
Changed Reported by from `ota@transarc.com' to `blake' 
Changed Responsible Engr. from `blake@transarc.com' to `ota@transarc.com'

[3/9/93 public]
The above mentioned saved 4K/4K aggregate is garbaged probably because
it was saved with metamucil that thought it was a 8K/1K aggregate.  So I
am unable to verify this specific report.
However, the general problem was also reported by Shepherd Shi at IBM
who provided the following description:
    Assertion error:
    ===============
    assertion failed: line 88, file ../../../../../src/file/episode/salvage/paths.c
    /etc/rc.dfs: 10213 Abort: A memory image file is created as "core".
    Stack Trace:
    ===========
    (dbx) where
    raise.raise(0x0) at 0xd000693c
    abort.abort(0x0) at 0xd000684c
    SalvageLinkCount(0x0, 0xdeadbeef) at 0x10066f04
    epis_DirWalk(0x0, 0xdeadbeef) at 0x1006773c
    epis_VerifyAggrPaths(0x0, 0xdeadbeef) at 0x10067b94
    epig_VerifyExistingAggregate(0x0, 0xdeadbeef) at 0x10008e24
    VerifyEpisode(0x0, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef) at 0x10000970
    salvage(0x0, 0xdeadbeef) at 0x10000e5c
    cmd_Dispatch(0x0, 0xdeadbeef) at 0x10073b2c
    main(0x0, 0xdeadbeef) at 0x10001134
    ggregate(0x0, 0xdeadbeef) at 0x10008e24
    VerifyEpisode(0x0, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef, 0xdeadbeef) at 0x10000970
    salvage(0x0, 0xdeadbeef) at 0x10000e5c
    cmd_Dispatch(0x0, 0xdeadbeef) at 0x10073b2c
    main(0x0, 0xdeadbeef) at 0x10001134
This is "clearly" the same problem.
Due to some confusion on my part I fixed this under a Transarc defect
number (3230) with delta ota-db3230-avoid-repairing-backing-fileset.
I verified the problem using the scavenge test "simple_test" running on
the aggregate ~/de/salvage-data/epi-blake.Z by bashing 2 bytes at offset
13206168 to 0.
Defect Closure Form
-------------------
--Regression test program below--
See above comments.  But generally scavenge/simple_test.pl tries to bash
linkCounts and in an aggregate like epi-blake with a backing fileset, it
will trigger this bug.
Associated information:
Tested on TA build:  
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Changed Interest List CC from `cfe' to `cfe, SSHI@AUSVM1.VNET.IBM.COM' 
Filled in Reported by Company's Ref. Number with `3230' 
Filled in Transarc Deltas with `ota-db3230-avoid-repairing-backing-fileset' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5004
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvager should ignore VV errors on INCON filesets
Reported Date                 : 8/11/92
Found in Baseline             : 1.0.2
Found Date                    : 8/11/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-db2355-file-status-VV
Transarc Herder               : jaffe@transarc.com

[8/11/92 public]
The salvager detects and will repair problems with file VVs being ahead
of the fileset VV.  This is done on the first pass without checking the
fileset INCONSISTENT bit.  Since the VV consistency cannot be guaranteed
by the transaction mechanism, we need to supress these errors during
restore, clone, etc by examining the INCONSISTENT bit.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/27/92 public]
Reassigned to Blake.
Changed Interest List CC from `cfe@transarc.com' to `cfe' 
Changed Responsible Engr. from `ota@transarc.com' to `blake@transarc.com'

[5/5/93 public]
The implementation of Transarc DB 2355 also solves this problem.
Note that I've changed the Transarc status to "export" even though this
delta will probably be incorporated in a build under TR 2355.
Filled in Reported by Company's Ref. Number with `2355' 
Filled in Transarc Deltas with `ota-db2355-file-status-VV' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 5000
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 5076, 5000, 7057, 7069
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvager crashes if backing anode garbaged
Reported Date                 : 8/11/92
Found in Baseline             : 1.0.2
Found Date                    : 8/11/92
Severity                      : B
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/anode/anode.c file/episode/anode/epi_errs.et file/episode/anode/fixed_anode.c file/episode/anode/sal_errors.h file/episode/anode/salvage.c file/episode/anode/test_anode.c file/episo
de/salvage/paths.c file/episode/salvage/walk.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot5000-bad-backing-robustness
Transarc Herder               : jaffe@transarc.com

[8/11/92 public]
In epix_GetBackingAnode it doesn't check code from Open and instead
references through the backingP which is null.  If the backing anode is
trashed this can happen.  Add more bullet proofing here.  But return
code instead of panicing so salvager can do the right thing.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/17/92 public]
This but can bre reproduced by applying basic_test.pl to
~ota/de/salvage-data/clones.Z.  See OT 5076 for details of this perl
script.
Filled in Inter-dependent CRs with `5076' 
Changed Interest List CC from `cfe@transarc.com' to `cfe@transarc.com, 
 jdp@transarc.com'

[10/27/92 public]
Assigned to Blake.
Changed Interest List CC from `cfe@transarc.com, jdp@transarc.com' to 
 `cfe,jdp,ota' 
Changed Responsible Engr. from `ota@transarc.com' to `blake@transarc.com'

[1/28/93 public]
I've run across this again, so I plan to fix it now.
Changed Interest List CC from `cfe,jdp,ota' to `cfe,jdp,ota,blake' 
Changed Responsible Engr. from `blake@transarc.com' to `ota@transarc.com'

[2/1/93 public]
The delta addresses a variety of problems the salvage had with with
damaged backing anodes.  Most of these have been fixed.  The
verification test for these problems still reports errors (OT 7057) but
they have other causes.  Also these tests needed some improvements (see
OT 7069) which have been made but are not part of this delta.
Defect Closure Form
-------------------
--Regression test program below--
Run the salvage test program:
    salt -run cow1,cow6 /tmp/salvage-data/epi-blake
The test cow1a exhibits problems attributable to OT 7057 but the other
tests should succeed.
Associated information:
Tested on TA build:  
dfs-102-2.4
Tested with backing build:  
dce1.0.2b11
Changed Inter-dependent CRs from `5076' to `5076, 5000, 7057, 7069' 
Filled in Transarc Deltas with `ota-ot5000-bad-backing-robustness' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/anode/anode.c 
 file/episode/anode/epi_errs.et file/episode/anode/fixed_anode.c 
 file/episode/anode/sal_errors.h file/episode/anode/salvage.c 
 file/episode/anode/test_anode.c file/episode/salvage/paths.c 
 file/episode/salvage/walk.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 4952
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : test
Short Description             : test programs need to reset errno
Reported Date                 : 8/6/92
Found in Baseline             : 1.0.1
Found Date                    : 8/6/92
Severity                      : D
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot4952-low-not-to-return-confused-errors
Transarc Herder               : jaffe@transarc.com

[8/6/92 public]
There are cases where the test programs should reset errno to 0 prior to
making a system call.  This causes cases where the system call succeeds
when it should fail to print out an incorrect value for the system error
code in test case's error message. The test code is picking up the errno 
value from previous system calls since errno is not set when a system call
is successful.

[08/06/92 public]
I forgot to mention that I have seen this problem in low/prog8.c and low/prog9.c
I don't know if this is a problem with the other tests.

[8/25/93 public]
Defect Closure Form
-------------------
Oen of the major problems is that the low test programs call one
common function to print the error message. This function also prints out
the errno. 
This could be misleading if this function is called not because
of a failed unix system call. This problem is actually there in almost all 
of the low test C programs. 
I also ran it on RIOS/1.0.3 2.25 to make sure that the delta does not introduce
any new problem.
Tested on TA build:  
Tested with backing build:  
Changed Responsible Engr. from `fred@transarc.com' to `tu@transarc.com' 
Added field Transarc Deltas with value 
 `tu-o-ot4952-low-not-to-return-confused-errors' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `export'

[12/17/93 public]
Closed.



CR Number                     : 4867
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 2368
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Absent error handling on DHOP_WRITE calls
Reported Date                 : 7/29/92
Found in Baseline             : 1.0.2
Found Date                    : 7/29/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : dir/dir_main.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : bwl-o-ot4867-dhop-write-errors
Transarc Herder               : jaffe@transarc.com

[7/29/92 public]
Most of the calls to DHOP_WRITE in dir_main.c discard the error code.
In some of these cases there is a legitimate possibility of getting
EDQUOT or ENOSPC errors and these must be handled correctly.  In others
there isn't any reasonable error code an so a non-zero error code should
panic (otherwise it will lead to file system inconsistency).
Also note that some internal routines propagate errors out and their
callers do not check.  For example see the call to dir_MakeDir at the
beginning of dirs_Salvage.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/31/92 public]
As per my discussion with Ted, this is reassigned to Bruce.

[2/12/93 public]
What is the status of this defect?  This should be addressed
for 1.0.2 since there is a potential for filesystem
inconsistency!
Filled in Interest List CC with `demail1!carl' 
Changed Severity from `C' to `B' 
Changed Priority from `2' to `1'

[2/17/93 public]
After analysis, we really do believe this isn't a critical 1.0.2 bug.  The
circumstances in which this will be seen are restricted to an over-quota
directory write on a COW directory.  This will leave the directory entry in
question inaccessible, but does not cause a system panic/crash.
This condition can be corrected by running the salvager.
Changed Severity from `B' to `C' 
Changed Priority from `1' to `2'

[10/1/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
I did the following:
- Create a volume and epimount it.
- Set the volume's quota higher than the total space in the aggregate.
  This ensures that we will hit out-of-space errors before we get
  out-of-quota errors.
- In the volume, create a subdirectory with two entries, which are both
  hard links to the same file.
- Fill the aggregate nearly full, i.e. within a few fragments.
- Clone the volume.
- If the aggregate isn't completely full, completely fill it by creating some
  number of 1-fragment files.  The test for "completely full" is, of course,
  to do fts lsq (or efts lsq) and look at the aggregate figures.  If you
  can't get them identical (i.e. new file creations fail but fts lsq still
  thinks there's space), you're somewhat hosed by fragmentation.  Try some
  random measures like deleting the clone followed by another fts clone, etc.
- cd to the subdirectory and delete one of the two entries.
The deletion causes the COW directory to be copied, which is impossible
because you're out of space.  (Note that the COW copy doesn't affect your
visible quota, which is why it's necessary to set the test up to get space
errors rather than quota errors.)
If you have a "fixed" Episode, the rm command will get an error message about
no space left in the file system.  Now I ask you:  what other file system
can give you messages about no space left when you invoke rm?  Is this cool
or what?
If your Episode isn't fixed, here's what will happen:  The directory package
will think that it's doing the delete just fine.  All the calls to DHOP_WRITE
will return ENOSPC but it will blithely throw these codes away.  So the
directory buffer will reflect a successful deletion, while the disk will
reflect no change.  If you do "ls", you'll see that the entry is gone.  But
if you unmount, re-mount, and do "ls" again, you'll see that it has come back!
Associated information:
Tested on TA build:  
dfs-osf-1.2
Tested with backing build:  
dce1.0.3-0831
Filled in Transarc Deltas with `bwl-o-ot4867-dhop-write-errors' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 4777
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 4552
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : some CMOP_xxx calls cause panics
Reported Date                 : 7/20/92
Found in Baseline             : 1.0.1
Found Date                    : 7/20/92
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b20
Affected File(s)              : cm_init.c, afsd.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[7/20/92 public]
As noted in OT report 4552, some options given to dfsd will cause the CM
to panic.  The fix currently applied to the code base is to have the dfsd
program check its arguments before invoking the CMOP_xxx calls that
initialize the cache manager.  However, other programs that make CMOP_xxx
calls could well cause the same kernel panic() results.

Since OT 4552 is closed, this bug report is created to inherit the request
that the CMOP_xxx calls themselves be bullet-proofed against bad arguments.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/14/92 public]
Changed Interest List CC from `cfe@transarc.com' to 
 `cfe@transarc.com,demail1!carl'

[3/21/94 public]
Assigned to Diane to modify.

[04/08/94 public]

Modifications for this enhancement entail:

   CacheInit function checks the values of astatSize, afiles, and
   anamecachesize and returns EINVAL if the value is inappropriate.

   The CMOP_GO call checks that the expected number of cache files
   have been inititalized by InitCacheFile (cm_cacheCounter != 
   cm_cacheFiles). It will return an error if not; so app. programs 
   will need to check for an error return for this operation.
   We've modifed dfsd to check for an error in this case.

   If there is anything we've missed please let me know.

[09/27/94 public]
Closed.



CR Number                     : 4412
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : ubik
Short Description             : flserver generates cyptic error message when the
Reported Date                 : 6/23/92
Found in Baseline             : 1.0.1
Found Date                    : 6/23/92
Severity                      : E
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : tu-o-ot4412-ubik-better-handle-errors
Transarc Herder               : jaffe@transarc.com

[6/23/92 public]
The error occurred when configuring a rios server using dce_config.
This was analysed by Vijay and I have taken the liberty of pasting
below parts of mail messages from Vijay describing the situation
"I noticed a problem last week with the flserver when the disk space
for the /opt tree ran out. The flserver started fine and created the
fldb.* files in /opt/dcelocal/var/dfs. I noticed that the fldb header is
not created until a VL_ rpc comes through. The dce_config script does a
fts crserverentry followed by a fts crfldbentry and a fts lsfldb.
Because the ubik writes failed during the first two fts commands, the
fldb header was never initialized properly. These commands printed out
UIOERROR messages. The fts lsfldb (read transaction) also tries to
initialize the header and fails with UBADTYPE."
It would be helpful if the error messages carry more information.
Vijay in another mail message suggests:
"The way things can be improved in flserver/ncsubik is to make the read
transaction understand that the FLDB header that should have been
written by a fts (write type) command had failed and an appropriate
message should be printed; something like
         fts: FLDB header malformed; redo FLDB initialization
"
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/24/92 public]
Assigned to Vijay.
As per correspondence with Vijay and Rajesh, suggested that the UIOERROR
code be split into several different codes reflecting the particular disk
error that was returned at the Ubik server site (ENOSPC, EDQUOT, other).
Changed Subcomponent Name from `flserver' to `ubik' 
Changed Responsible Engr. from `cfe@transarc.com' to `vijay@transarc.com'

[11/20/92 public]
Apparently Vijay never "cml export'ed" delta
vijay-ot4412-ubik-handle-disk-errors, so we never picked it up (it was
never marked as released).  Now this delta is quite old and likely to
cause a "merge from hell".
Vijay is examining the state of this delta to determine if it's worth
salvaging.
In the meantime, I'm re-(tr-)opening this OT so that it shows up on
Vijay's OT TODO list as a reminder (and so it stops showing up as
inconsistent in our OT DB checks).
Changed Transarc Status from `export' to `open'

[8/31/93 public]
Re-implemented the fix since the previouos delta provided by Vijay might be
too old to be compatibile with the existing code base. 
Defect Closure Form
-------------------
--Regression test program below--
I ran the ubik test program utst to make sure that there is no regerssion
occurred. In the meantime, Vijay also imported this delta in his code
base along with other his ubik changes and ran the test to verify it. 
Tested on TA build:  
Filled in Interest List CC with `vijay@transarc.com' 
Changed Responsible Engr. from `vijay@transarc.com' to `tu@transarc.com' 
Changed Transarc Deltas from `vijay-ot4412-ubik-handle-disk-errors' to 
 `tu-o-ot4412-ubik-better-handle-errors' 
Changed Transarc Herder from `mason@transarc.com' to `jaffe@transarc.com' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 4378
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : newaggr -overwrite can lead to inconsistencies
Reported Date                 : 6/22/92
Found in Baseline             : 1.0.1b18
Found Date                    : 6/22/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.2
Affected File(s)              : file/{ftutil/ftutil.c,episode/{anode/newaggr.c,salvage/salvage_main.c}}
Sensitivity                   : public
Transarc Deltas               : jdp-db3072-aggregate-interlocking
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[6/22/92 public]
After one mount/umount cycle of a fileset, I decided to newaggr the 
partition on which the fileset resided.  However, after doing so I still 
see the old fileset information when I do an efts lsft.
Even after completely overwriting the partition using dd, then doinga
a newaggr, I still see the old fileset information.  I get an assertion
failure when I attempt to mount one of the old filesets.  The failure
occurs in volume_table.c (line 650, I believe).  The mount returns errno 
22 after continuing through the assertion failures.
It appears that some volume information in kernel is not getting 
flushed when an episode partition is umounted.  When the partition
itself is newaggr'ed, the information it contains about its filesets
is out of sync with the in-kernel information.

[7/2/92 public]
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[7/13/92 public]
Filled in Responsible Engr. with `jdp@transarc.com'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[10/14/92 public]
Filled in Interest List CC with `demail1!carl'

[11/18/92 public]
This is an enhancement. The ability to newaggr `live' partitions isn't 
supported. Post 1.0.2, the implementation could be changed to prevent
user errors.
Changed Defect or Enhancement? from `def' to `enh'

[3/9/93 public]
This has been addressed by the locking code that prevents newaggr or salvage
from modifying an attached aggregate (note that aggregates with
locally-mounted filesets are attached).
The problem described above where an unmount doesn't shutdown the associated
aggregate is almost certainly due to the unhealthy practice of using epimount
to mount a fileset while using unmount to unmount it.  Besides mounting
the fileset epimount also attaches the aggregate, so using unmount instead
of epiunmount leaves the aggregate attached.
Filled in Subcomponent Name with `lfs' 
Changed Interest List CC from `demail1!carl' to `demail1!carl, jaffe' 
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2' 
Filled in Affected File with 
 `file/{ftutil/ftutil.c,episode/{anode/newaggr.c,salvage/salvage_main.c}}' 
Filled in Reported by Company's Ref. Number with `3072' 
Filled in Transarc Deltas with `jdp-db3072-aggregate-interlocking' 
Changed Transarc Status from `open' to `submit'

[12/17/93 public]
Closed.



CR Number                     : 4066
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : auth
Short Description             : fts crserverentry does not check principal for existence in CDS
Reported Date                 : 6/4/92
Found in Baseline             : 1.0.1b17
Found Date                    : 6/4/92
Severity                      : D
Priority                      : 4
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1ab2
Affected File(s)              : flserver/flprocs.c, flserver/flutils.c, test/file/fts/test13
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[6/4/92 public]
Scenario:
========
This is in a one machine cell.  One of the fts tests [test 13] does
not remove all the server entries that it creates in the fldb. Hence I
used the bos shutdown command to bring down the flserver, ftserver and
repserver and then removed the /opt/dcelocal/var/dfs/fldb* files. Then
brought the flserver, ftserver and repserver back up using the bos
start command. I followed this by:
  fts crserverentry dewitt.transarc.com -principal hosts/dewitt.transarc.com -localauth
The contents of dfstab file at this stage were 
  # blkdev aggname aggtype aggid (fsid-for-UFS)
  /dev/hd2        /usr    ufs     1       0,,1   
Then,
  fts crfldbentry root.dfs dewitt.transarc.com 1 -localauth
The command succeeded but also complained -
connToServer(dewitt.transarc.com): error setting auth info (0, 1, /.../dewitt.com/host
s/dewitt.transarc.com/dfs-server) for binding: Error returned from rpc service (dfs / dau)
Following this, fts lsft, fts lsfldb commands also succeed but
generate the same error message. I killed secd and sec_clientd and
started them again by hand. In this case too fts lsft and lsfldb
commands succeed but still generate this error message. On rebooting
the machine and starting DCE and DFS by the rc scripts, fts lsft and
lsfldb still succeeded with this error.
Authenticating to DCE as cell_admin succeeded, but with the following error :
dfs: set authen binding failed (code 12)
dfs: Warning: create an unauthenticated binding
The "set authen binding failed" error did not appear when I
authenticated as rajesh to DCE and subsequently as cell_admin.
The log after machine rebooted is as follows:
 [(rootl)etc] cd /:
 [(rootl):] fts lsfldb
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
connToServer(dewitt.transarc.com): error setting auth info (0, 0, /.../dewitt.co
m/hosts/dewitt.transarc.com/dfs-server) for binding: Error returned from rpc ser
vice (dfs / dau)
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>
     <nil>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
2.37u 1.4s 0:07 45% 36+138k 0+0io 80pf+0w
 [(rootl):] $TESTCELL_SCRIPTS/klog cell_admin -dce-
/afs/tr/usr/fred/dfs/test/scripts/testcell/klog: password reset required.
User space cache: FILE:/tmp/dcecred_41000004
dfs: set authen binding failed (code 12)
dfs: Warning: create an unauthenticated binding
 [(rootl)fs] klist
DCE Identity Information:
        Warning: Identity information is not certified
        Global Principal: /.../dewitt.com/cell_admin
        Cell:      001EBA90-B04C-1A26-B491-02608C2F2C1D /.../dewitt.com
        Principal: 00000064-B04C-2A26-B400-02608C2F2C1D cell_admin
        Group:     0000000C-B057-2A26-B401-02608C2F2C1D none
        Local Groups:
                00000064-B077-2A26-8801-02608C2F2C1D acct-admin
                00000065-B077-2A26-8801-02608C2F2C1D subsys/dce/sec-admin
                00000066-B077-2A26-8801-02608C2F2C1D subsys/dce/cds-admin
                00000068-B078-2A26-8801-02608C2F2C1D subsys/dce/dts-admin
                00000067-B078-2A26-8801-02608C2F2C1D subsys/dce/dfs-admin
                00000069-B079-2A26-8801-02608C2F2C1D subsys/dce/dskl-admin
Identity Info Expires: 92/06/08:16:38:09
Account Expires:       never
Passwd Expires:        never
Kerberos Ticket Information:
Ticket cache: /tmp/dcecred_41000004
Default principal: cell_admin@dewitt.com
Server: krbtgt/dewitt.com@dewitt.com
        valid 92/06/04:16:38:09 to 92/06/08:16:38:09
Server: dce-rgy@dewitt.com
        valid 92/06/04:16:38:10 to 92/06/08:16:38:09
Server: dce-ptgt@dewitt.com
        valid 92/06/04:16:38:12 to 92/06/08:16:38:09
Client: dce-ptgt@dewitt.com     Server: krbtgt/dewitt.com@dewitt.com
        valid 92/06/04:16:38:12 to 92/06/08:16:38:09
Client: dce-ptgt@dewitt.com     Server: dce-rgy@dewitt.com
        valid 92/06/04:16:38:13 to 92/06/08:16:38:09
 [(rootl)fs]  fts lsfldb
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
connToServer(dewitt.transarc.com): error setting auth info (0, 0, /.../dewitt.com/hosts/d
ewitt.transarc.com/dfs-server) for binding: Error returned from rpc service (dfs / dau)
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>          <nil
>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
2.25u 1.14s 0:10 32% 56+132k 0+0io 0pf+0w
 [(rootl)fs] fts help lsfldb
fts lsfldb: list filesets in FLDB
Usage: fts lsfldb  [-fileset {<name> | <ID>}] [-server <machine>] [-aggregate <name>] [-l
ocked] [-cell <cellname>] [{-noauth | -localauth}] [-verbose] [-help]
 [(rootl)fs] fts lsfldb -noauth
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>          <nil
>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
 [(rootl)fs] ^no^local
fts lsfldb -localauth
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
connToServer(dewitt.transarc.com): error setting auth info (0, 1, /.../dewitt.com/hosts/d
ewitt.transarc.com/dfs-server) for binding: Error returned from rpc service (dfs / dau)
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>          <nil
>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
3.18u 1.85s 0:09 53% 54+144k 0+0io 0pf+0w
 [(rootl)fs] exit
 [(rootl)fs] 8.90u 5.99s 1:48 13% 40+104k 0+0io 124pf+0w
 [(rootl):]  $TESTCELL_SCRIPTS/klog rajesh rajesh
User space cache: FILE:/tmp/dcecred_41000007
 [(rootl)fs] fts lsfldb
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
connToServer(dewitt.transarc.com): error setting auth info (0, 0, /.../dewitt.com/hosts/d
ewitt.transarc.com/dfs-server) for binding: Error returned from rpc service (dfs / dau)
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>          <nil
>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
2.0u 1.13s 0:10 29% 54+128k 0+0io 0pf+0w
 [(rootl)fs] fts lsfldb -localauth
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
connToServer(dewitt.transarc.com): error setting auth info (0, 1, /.../dewitt.com/hosts/d
ewitt.transarc.com/dfs-server) for binding: Error returned from rpc service (dfs / dau)
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>          <nil
>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
3.33u 1.52s 0:09 52% 58+148k 0+0io 0pf+0w
 [(rootl)fs] klist
DCE Identity Information:
        Warning: Identity information is not certified
        Global Principal: /.../dewitt.com/rajesh
        Cell:      001EBA90-B04C-1A26-B491-02608C2F2C1D /.../dewitt.com
        Principal: 00000108-B6EB-2A26-8800-02608C2F2C1D rajesh
        Group:     00000000-B057-2A26-B401-02608C2F2C1D system
Identity Info Expires: 92/06/08:16:40:08
Account Expires:       never
Passwd Expires:        never
Kerberos Ticket Information:
Ticket cache: /tmp/dcecred_41000007
Default principal: rajesh@dewitt.com
Server: krbtgt/dewitt.com@dewitt.com
        valid 92/06/04:16:40:08 to 92/06/08:16:40:08
Server: dce-rgy@dewitt.com
        valid 92/06/04:16:40:09 to 92/06/08:16:40:08
Server: dce-ptgt@dewitt.com
        valid 92/06/04:16:41:00 to 92/06/08:16:40:08
Client: dce-ptgt@dewitt.com     Server: krbtgt/dewitt.com@dewitt.com
        valid 92/06/04:16:41:01 to 92/06/08:16:40:08
Client: dce-ptgt@dewitt.com     Server: dce-rgy@dewitt.com
        valid 92/06/04:16:41:01 to 92/06/08:16:40:08
Client: dce-ptgt@dewitt.com     Server: hosts/dewitt/cds-server@dewitt.com
        valid 92/06/04:16:41:02 to 92/06/08:16:40:08
Client: dce-ptgt@dewitt.com     Server: hosts/dewitt/dfs-server@dewitt.com
        valid 92/06/04:16:41:07 to 92/06/08:16:40:08
 [(rootl)fs] exit
 [(rootl)fs] 7.17u 4.85s 56:48 0% 42+106k 0+0io 1pf+0w
 [(rootl):]  $TESTCELL_SCRIPTS/klog cell_admin -dce-
/afs/tr/usr/fred/dfs/test/scripts/testcell/klog: password reset required.
User space cache: FILE:/tmp/dcecred_41000009
 [(rootl)fs]  fts lsfldb
root.dfs
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner          objid
connToServer(dewitt.transarc.com): error setting auth info (0, 0, /.../dewitt.com/hosts/d
ewitt.transarc.com/dfs-server) for binding: Error returned from rpc service (dfs / dau)
dewitt.transarc.com RW       /usr    0:00:00 hosts/dewitt.transarc.com<nil>          <nil
>
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed)
2.33u 1.25s 0:11 31% 55+133k 0+0io 0pf+0w
 [(rootl)fs] pwd
/.../dewitt.com/fs
 [(rootl)fs] cd /opt/dcelocal
 [(rootl)dcelocal] cd var/dfs
 [(rootl)dfs] ls
BosConfig       admin.fl        admin.up        dfstab
adm/            admin.ft        backup/         fldb.DB0
admin.bos       admin.rep.tkn   dfsatab         fldb.DBSYS1
 [(rootl)dfs] mroe dfstab
# blkdev aggname aggtype aggid (fsid-for-UFS)
/dev/hd2        /usr    ufs     1       0,,1
 [(rootl)dfs]
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/5/92 public]
This is a great bug report.  Unfortunately, it omits the root cause of the
problem, which is that the ``hostname'' command on dewitt.transarc.com returns
the text ``dewitt'', not ``dewitt.transarc.com''.  Thus, dce_config will have
set up CDS with host entries like
	/.../dewitt.com/hosts/dewitt/dfs-server
and	/.../dewitt.com/hosts/dewitt/self
rather than
	/.../dewitt.com/hosts/dewitt.transarc.com/dfs-server
and	/.../dewitt.com/hosts/dewitt.transarc.com/self
Note that these names are interchangeable (to some extent) in the domain
name system, but not at all in CDS.
Thus, the cause of the problem was having used ``hosts/dewitt.transarc.com''
rather than ``hosts/dewitt'' in the ``fts crserverentry'' command.
A command like
	fts crserverentry `hostnamev` hosts/`hostname`
would generally do the right thing here.
Now, as to how to avoid getting in to this situation: I'm largely clueless,
unless fts is going to query CDS about whether a proposed name in fact
exists.  As far as I'm concerned, it's too bad that the dfsauth code
masks the underlying error from the RPC or security service.
This bug report should either be cancelled or changed to an enhancement
request (that server principal names be validated).  I'm lowering its
priority in any case.
Filled in Subcomponent Name with `auth' 
Changed Severity from `C' to `D' 
Changed Priority from `2' to `4' 
Changed Status from `open' to `cancel' 
Changed Responsible Engr. from `pakhtar@tranarc.com' to `cfe@transarc.com'

[6/5/92 public]
Changing this to an enhancement for the server principal names to be 
validated.
Changed Defect or Enhancement? from `def' to `enh' 
Changed Status from `cancel' to `open'

[7/16/92 public]
Fixed the one-line summary to reflect the suggestion content better.
Changed Short Description from `Error message from connToServer - error setting 
 auth info ...' to `fts crserverentry does not check principal for existence in 
 CDS'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[7/20/92 public]
I'm going to try removing the ``Fix by'' field content entirely, since
this is an enhancement suggestion.
Changed Fix By Baseline from `1.0.2' to `'

[03/24/94 public]
We are looking at fixing this in 1.1

[3/25/94 public]
Filled in Interest List CC with `jeff@transarc.com'

[3/25/94 public]
Changed Interest List CC from `jeff@transarc.com' to `jeff@transarc.com, 
 cfe@transarc.com'

[09/26/94 public]
Closed.



CR Number                     : 3841
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Limit of 10 active trans in block alloc
Reported Date                 : 5/27/92
Found in Baseline             : 1.0.1
Found Date                    : 5/27/92
Severity                      : C
Priority                      : 2
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot3841-increase-fb-max-trans
Transarc Herder               : jdp@transarc.com

[5/27/92 public]
The freed block code needs to track all blocks freed in all
transactions.  To do this it has a fixed size table which limits the
number of active transactions to ten.  This number should be increased
or, preferably, made setable by a tuning or initialization parameter.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jdp@transarc.com' 
Added field Transarc Status with value `open'

[5/28/92 public]
Changed fixed by now that 1.0.2 is allowed.
Changed Fix By Baseline from `1.1' to `1.0.2'

[7/8/92 public]
Changed Defect or Enhancement? from `def' to `enh'

[6/2/93 public]
Defect Closure Form
-------------------
Increased the limit from 10 to 100.
--Other explanation below--
This is a simple change that should have no observable external
consequences.  I've verified that under heavy least no processes were
observed to be blocked waiting in epib_StartTran to start a transaction,
whereas previously 56 out of 100 were waiting there.
Associated information:
Tested on TA build:  
dfs-carl-1.9
Tested with backing build:  
dce1.0.2a-0427
Filled in Transarc Deltas with `ota-ot3841-increase-fb-max-trans' 
Changed Transarc Status from `open' to `export'

[12/17/93 public]
Closed.



CR Number                     : 3816
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Limit of 20 simultaneous mounts
Reported Date                 : 5/26/92
Found in Baseline             : 1.0.1
Found Date                    : 5/26/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/vnops/efsmount.h
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : rajesh-increase-efsmount-table-size
Transarc Herder               : mason@transarc.com

[5/26/92 public]
This is tr defect # 2255.
Episode currently limits the number of simultaneous mounts to 20.  This should \
either be unlimited (mount structures dynamically allocated) or specified at in\
it time (cfgefs or epiinit).
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[5/27/92 public]
Changed Short Description from `Episode limits # of simultaneous mounts to 20.' 
 to `Limit of 20 simultaneous mounts'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[9/29/92 public]
This is an enhancement; local mounts are not required for DFS functionality.
Changed Defect or Enhancement? from `def' to `enh'

[4/26/93 public]
As per, IBM Austin's (Carl Burnett) request against which no
objections were raised internally, I have exported delta
rajesh-increase-efsmount-table-size which statically increases the
mount table size to 100. But as Bruce pointed out, we still need to do
the work to allocate entries dynamically or specify the size at init
time.
Filled in Interest List CC with `rajesh@transarc.com, jaffe@transarc.com'

[5/4/93 public]
imported in dfs-carl 1.8
Changed Fix By Baseline from `1.0.2' to `1.0.2a' 
Filled in Affected File with `file/episode/vnops/efsmount.h' 
Filled in Transarc Deltas with `rajesh-increase-efsmount-table-size' 
Changed Transarc Status from `open' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 3798
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Salvage, when prompting. won't take EOF for an answer
Reported Date                 : 5/26/92
Found in Baseline             : 1.0.1
Found Date                    : 5/26/92
Severity                      : D
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/salvage/salvage_main.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : ota-ot3798-accept-EOF-as-default-answer
Transarc Herder               : mason@transarc.com

[5/26/92 public]
This is the transarc defect #2346.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[5/26/92 public]
Changed Subcomponent Name from `episode' to `lfs'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[3/4/93 public]
Defect Closure Form
-------------------
--Verification procedure below--
With a suitably trashed aggregate run salvage with <stdin> redirected to
/dev/null:
    zcat ~ota/de/salvage-data/epi-blake.Z |zcp>/tmp/badaggr
    bash /tmp/badaggr 65568. 4 -1	# set nBlocks to -1
    salvage /tmp/badaggr < /dev/null
This used to generate the prompt in an infinite loop.  Now it just acts
as if you'd answered "no".
Associated information:
Tested on TA build:  
dfs-102-2.6
Tested with backing build:  
dce1.0.2b15
Changed Short Description from `Salvage, when prompting. won't take EOF for an 
 answer.' to `Salvage, when prompting. won't take EOF for an answer' 
Filled in Interest List CC with `SSHI@AUSVM1.VNET.IBM.COM' 
Filled in Reported by Company's Ref. Number with `2346' 
Filled in Transarc Deltas with `ota-ot3798-accept-EOF-as-default-answer' 
Changed Transarc Status from `open' to `export'

[5/3/93 public]
imported into dfs-carl 1.8
Filled in Affected File with `file/episode/salvage/salvage_main.c' 
Changed Transarc Status from `export' to `import'

[5/11/93 public]
Changed Status from `open' to `fix' 
Filled in Fixed In Baseline with `1.0.2a'

[12/17/93 public]
Closed.



CR Number                     : 3435
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Protect log from out-of-order sector writes
Reported Date                 : 5/13/92
Found in Baseline             : 1.0.1b15
Found Date                    : 5/13/92
Severity                      : C
Priority                      : 3
Status                        : closed
Duplicate Of                  : 
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.0.3a
Affected File(s)              : file/episode/logbuf/{log,recover}.c
Sensitivity                   : public
Transarc Status               : submit
Transarc Deltas               : mason-ot3435-allow-crc-on-log-pages
Transarc Herder               : mason@transarc.com

[5/13/92 public]
Add a CRC-32 checksum algorithm to the log pages to detect
out-of-order sector writes.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[5/27/92 public]
Changed Short Description from `Need to protect against out-of-order sector 
 writes' to `Protect log from out-of-order sector writes' 
Filled in Transarc Deltas with `mason-ot3435-allow-crc-on-log-pages'

[5/28/92 public]
Changed Fix By Baseline from `1.1' to `1.0.2'

[7/16/92 public]
Changed Defect or Enhancement? from `def' to `enh'

[mason@transarc.com 11/16/92 public] 
 This should be deferred out of 1.0.2, as it is unlikely to happen
before then.  However, this does not require a disk format change to
implement - the change went into 1.0.1.

[2/2/93 public]
Defect Closure Form
-------------------
--Other explanation below--
This is a new feature.  It also includes the fix for sybase defect 3114
The feature was tested using the debugger.

Associated information:

Tested on TA build:  dfs-102-2.3
Tested with backing build:  dce1.0.2b10

Changed Transarc Status from `open' to `export'

[2/2/93 public]

[12/17/93 public]
Closed.



