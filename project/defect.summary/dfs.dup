CR Number                     : 12542
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : /opt/dcelocal/bin/bos: failed to add group 'subsys/dce/dfs-admin
Reported Date                 : 10/6/94
Found in Baseline             : 1.1
Found Date                    : 10/6/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 12536
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[10/6/94 public]

Platform - HPUX
Host - budapest
test - multimachine smoketest

Multimachine smoketest is hung - waiting to check for a Ubik sync site in  hosts/budapest

Earlier dcecp core smoketest had failed on sif.  I don't know if this is related
to OT 12536

extract from /u1/RAT_tools/do_test.dfs.log

S:****** Configuring DTS Local Server...
S:****** Stopping sec_client service...
dcecp -c secval deactivate {ncacn_ip_tcp:130.105.5.88[135]} FAILS: Err
or: permission not valid for this acl
S:****** Starting sec_client service...
dcecp -c secval activate {ncacn_ip_tcp:130.105.5.88[135]} FAILS: Error
: permission not valid for this acl
S:****** Starting dtsd...
1994-10-06-09:58:24.657-04:00I----- dtsd ERROR dts dtserror dtss_servi
ce_main.c 786 0x7aff9298
Error retrieving security configuration information: No currently esta
blished network identity for which context exists (dce / sec)
S:****** Starting dtstimed...
S:****** This node is now a DTS local server.
S:****** Configuring DFS Fileset Location Database Server...
S:****** Modifying the registry database for DFS operation...
S:****** Loading kernel extensions...
initializing Episode:  syscall return value = 0
S:****** Modifying the registry database for DFS server operation...

>>> group member added


>>> group member added

Current site is: registry server at /.../sif_cell/subsys/dce/sec/maste
r
Domain changed to: group
Current site is: registry server at /.../sif_cell/subsys/dce/sec/maste
r
Domain changed to: group
S:****** Starting bosserver...
/opt/dcelocal/bin/bos: failed to add group 'subsys/dce/dfs-admin' (une
xpected internal error; check bosserver log (dfs / bbs))
/opt/dcelocal/bin/bos: failed to add group 'subsys/dce/dfs-admin' (you
 are not authorized for this operation (dfs / bbs))
/opt/dcelocal/bin/bos: failed to create new server instance flserver o
f type 'simple' (you are not authorized for this operation (dfs / bbs)
)
Checking for a Ubik sync site in  hosts/budapest
udebug: failed to obtain ubik server information; Communications failu
re (dce / rpc)
Checking for a Ubik sync site in  hosts/budapest
udebug: failed to obtain ubik server information; Communications failu
re (dce / rpc)
Checking for a Ubik sync site in  hosts/budapest
udebug: failed to obtain ubik server information; Communications failu
re (dce / rpc)

[10/06/94 public]
It is a dup -- can't stop/start secval to update machine identity so
things fail.  Marked as such.



CR Number                     : 12390
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts aggrinfo output jumbled
Reported Date                 : 9/28/94
Found in Baseline             : 1.1
Found Date                    : 9/28/94
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 12328
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/28/94 public]

platform - RIOS 
machine - mudslide
test - multi-machine smoke test

extract from /u1/RAT_tools/mult_mach_configs/run_test.dfs.out:

 ... Exiting cell_registration    
        Entering check_uids ...    
        ... Exiting check_uids    
        Entering check_aggr_data ...    
        check_aggr_data[29]: :: 0403-057 Syntax error  
        ... Exiting check_aggr_data    
        Entering users_filesets ....    
        fts create -ftname dfs_1.fs -server budapest -aggregate

It seems like the test passed except for the syntax error.  In that case,
one might lower the severity/priority.

Also, there seems to be an error in the test script and Annie Poh says
she is fixing it.

[9/28/94 public]
There are extra newlines in the fts aggrinfo output.  The
DFS read write all test trys to parse the output of aggrinfo
using awk.  The extra newlines cause a failure.  Assigned to Ruby.



CR Number                     : 12301
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : sevicability
Short Description             : messages with a severity level
of "notice" are not output
Reported Date                 : 9/22/94
Found in Baseline             : 1.1
Found Date                    : 9/22/94
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 12280
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/22/94 public]

 Sams file messages with severities of "warning" are dropped. The routing
for the warning severity level must be initialized to fix this problem.



CR Number                     : 12056
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : "ls /:" timeout and set context failure
Reported Date                 : 9/6/94
Found in Baseline             : 1.1b14
Found Date                    : 9/6/94
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 11070
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/6/94 public]
 
Multiple bugs:
 
Bug 1. "ls /:" took more than five minutes and timeout. 
Bug 2. TKN_InitTokenState failed to reach the call back routine,
STKN_InitTokenState. 
 
It has happened predictably many times --- at some point after my
"dce-build" test(7 hours 40 minutes long).
 
I got this bug again this moring after my nightly "DCE-build" test.
 
The "ls /:" command got a RPC timeout warning after more than
five minutes of waiting. The dfstrace dump showed the following 
key events:
 
1. VL_GetVolByName took five minutes with return code 0.
 
   In the meantime "flserver" spent 75% and "fxd" 25% of the CPU
   time. But, the command failed with RPC timeout warning.
 
2. AFS_SetContext got called twice or three times with flag=3 and 
   failed with 655257602( TKN_ERR_NEW_NEEDS_RESET). 
 
3. AFS_SetContext called again with flag 7 and failed with 572616706(
   FSHS_ERR_FATALCONN).
 
4. Steps 2-3 repeated again. 
 
++++++++++++++++++++++++++++++++++++++
DFS Trace Dump -
 
   Date: Tue Sep  6 11:41:40 1994
 
Found 2 logs.
 
Contents of log disk:
 
Contents of log cmfx:
Log wrapped; data missing.
time 279.190009, pid 4022376: running Ping Servers 
time 279.190012, pid 4022376: Ping Servers queuing at time 2e6c7917 
time 279.190014, pid 4022376: Ping Servers done at time 2e6c7917 
time 284.200004, pid 4018900: in PeriodicKA 
time 284.200005, pid 4018900: in RunTokens 
time 284.200006, pid 4018900: runtkn done, 0 active 
time 284.200008, pid 4018900: checkflservers checking 1 servers 
time 284.200018, pid 4022376: running write through dslots 
time 284.200025, pid 4022376: running major renewlazyreps 
time 284.200026, pid 4022376: Renewlazyreps starts 0 subjobs 
time 284.200033, pid 4024272: servertokenmgt running 1 subops 
time 284.200034, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 284.200050, pid 4025536: in cm_FlushQueuedServerTokens for server 23e890c 
time 308.140001, pid 4011632: cm_lookup 242800c dce10_cell.osf.org 
time 308.140003, pid 4011632: Trybind on name = dce10_cell.osf.org 
time 308.140004, pid 4011632: cm_lookup 2428144 fs 
time 308.140006, pid 4011632: Trybind on name = fs 
time 309.640001, pid 4011632: Trybind: read helper returns avpp  7ffe71e8, replyCode 0 
time 309.640002, pid 4011632: cm_NewCell dce10_cell.osf.org 
time 309.640003, pid 4011632: getvolbyname looking for volume root.dfs 
time 309.640004, pid 4011632: cm_ConnByMHosts server type 10000 
time 309.640005, pid 4011632: cm_ConnByHost server type 0x10000, serverp 23e890c 
time 309.640006, pid 4011632: cm_ConnByHost: creating conn type 0x10000, conn 204778c 
time 309.640007, pid 4011632: cm_ConnByHost using conn 204778c, service 0x10000 
time 309.640008, pid 4011632: begin VL_GetVolByName 
time 314.210009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 332.220009, pid 4022376: running Check Down Servers 
time 336.210000, pid 4019848: GetTime 
time 336.210001, pid 4019848: fshs_GetPrincipal START 
time 336.210002, pid 4019848: fshs_GetHost, cookie 230298c 
time 336.210003, pid 4019848: fshs_FindHost, cookie 230298c 
time 336.210004, pid 4019848: find a prime host 2400094 
time 336.210006, pid 4019848: find a host in fast path 2400094 
time 336.210007, pid 4019848: fshs_FindPrincipal .. 
time 336.210008, pid 4019848: found a princ 245900c ref 1 
time 336.210010, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 336.210011, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 336.210013, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 336.210014, pid 4019848: GetTime returns 0 
time 339.230009, pid 4022376: running Ping Servers 
time 339.230012, pid 4022376: Ping Servers queuing at time 2e6c7953 
time 339.230014, pid 4022376: Ping Servers done at time 2e6c7953 
time 344.240010, pid 4022376: running write through dslots 
time 344.240017, pid 4022376: running major renewlazyreps 
time 344.240018, pid 4022376: Renewlazyreps starts 0 subjobs 
time 344.240025, pid 4024272: servertokenmgt running 1 subops 
time 344.240026, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 344.240042, pid 4025536: in cm_FlushQueuedServerTokens for server 23e890c 
time 374.250009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 399.260009, pid 4022376: running Ping Servers 
time 399.260012, pid 4022376: Ping Servers queuing at time 2e6c798f 
time 399.260014, pid 4022376: Ping Servers done at time 2e6c798f 
time 404.270004, pid 4018900: in PeriodicKA 
time 404.270005, pid 4018900: in RunTokens 
time 404.270006, pid 4018900: runtkn done, 0 active 
time 404.270008, pid 4018900: checkflservers checking 1 servers 
time 404.270018, pid 4022376: running write through dslots 
time 404.270025, pid 4022376: running major renewlazyreps 
time 404.270026, pid 4022376: Renewlazyreps starts 0 subjobs 
time 404.270033, pid 4024272: servertokenmgt running 1 subops 
time 404.270034, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 404.270050, pid 4025536: in cm_FlushQueuedServerTokens for server 23e890c 
time 426.270000, pid 4019848: GetTime 
time 426.270001, pid 4019848: fshs_GetPrincipal START 
time 426.270002, pid 4019848: fshs_GetHost, cookie 219968c 
time 426.270003, pid 4019848: fshs_FindHost, cookie 219968c 
time 426.270004, pid 4019848: find a prime host 2400094 
time 426.270006, pid 4019848: find a host in fast path 2400094 
time 426.270007, pid 4019848: fshs_FindPrincipal .. 
time 426.270008, pid 4019848: found a princ 245900c ref 1 
time 426.270010, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 426.270011, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 426.270013, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 426.270014, pid 4019848: GetTime returns 0 
time 434.280009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 459.290009, pid 4022376: running Ping Servers 
time 459.290012, pid 4022376: Ping Servers queuing at time 2e6c79cb 
time 459.290014, pid 4022376: Ping Servers done at time 2e6c79cb 
time 464.300010, pid 4022376: running write through dslots 
time 464.300017, pid 4022376: running major renewlazyreps 
time 464.300018, pid 4022376: Renewlazyreps starts 0 subjobs 
time 464.300025, pid 4024272: servertokenmgt running 1 subops 
time 464.300026, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 464.300042, pid 4025536: in cm_FlushQueuedServerTokens for server 23e890c 
time 494.310009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 512.320009, pid 4022376: running Check Down Servers 
time 518.540001, pid 4019848: GetTime 
time 518.540002, pid 4019848: fshs_GetPrincipal START 
time 518.540003, pid 4019848: fshs_GetHost, cookie 21a3b8c 
time 518.540004, pid 4019848: fshs_FindHost, cookie 21a3b8c 
time 518.540005, pid 4019848: find a prime host 2400094 
time 518.540007, pid 4019848: find a host in fast path 2400094 
time 518.540008, pid 4019848: fshs_FindPrincipal .. 
time 518.540009, pid 4019848: found a princ 245900c ref 1 
time 518.540011, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 518.540012, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 518.540014, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 518.540015, pid 4019848: GetTime returns 0 
time 519.330011, pid 4022376: running Ping Servers 
time 519.330014, pid 4022376: Ping Servers queuing at time 2e6c7a07 
time 519.330016, pid 4022376: Ping Servers done at time 2e6c7a07 
time 524.340004, pid 4018900: in PeriodicKA 
time 524.340005, pid 4018900: in RunTokens 
time 524.340006, pid 4018900: runtkn done, 0 active 
time 524.340008, pid 4018900: checkflservers checking 1 servers 
time 524.340018, pid 4022376: running write through dslots 
time 524.340025, pid 4022376: running major renewlazyreps 
time 524.340026, pid 4022376: Renewlazyreps starts 0 subjobs 
time 524.340033, pid 4024272: servertokenmgt running 1 subops 
time 524.340034, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 524.340050, pid 4025536: in cm_FlushQueuedServerTokens for server 23e890c 
time 554.350009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 577.360004, pid 4018900: fshs_HostCheckDaemon .. 
time 577.360005, pid 4018900: fshs_Enumerate START 
time 577.360007, pid 4018900: fshs_CheckHost START 240000c 
time 577.360008, pid 4018900: fshs_GCPrinc from host 240000c 
time 577.360009, pid 4018900: fshs_CheckHost END 
time 577.360011, pid 4018900: fshs_CheckHost START 2400094 
time 577.360012, pid 4018900: fshs_GCPrinc from host 2400094 
time 577.360013, pid 4018900: fshs_CheckHost END 
time 577.360014, pid 4018900: fshs_UpdateHostList called 
time 577.360015, pid 4018900: fshs_Enumerate END 
time 577.360027, pid 4022376: Refresh Tokens at time 2e6c7a41, opcount 0 
time 577.360029, pid 4022376: Refresh Tokens done at time 2e6c7a41 
time 579.370009, pid 4022376: running Ping Servers 
time 579.370012, pid 4022376: Ping Servers queuing at time 2e6c7a43 
time 579.370014, pid 4022376: Ping Servers done at time 2e6c7a43 
time 584.380010, pid 0: current time: Tue Sep  6 10:18:48 1994 
time 584.380010, pid 4022376: running write through dslots 
time 584.380017, pid 4022376: running major renewlazyreps 
time 584.380018, pid 4022376: Renewlazyreps starts 0 subjobs 
time 584.380025, pid 4024272: servertokenmgt running 1 subops 
time 584.380026, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 584.380042, pid 4025536: in cm_FlushQueuedServerTokens for server 23e890c 
time 608.610000, pid 4019848: GetTime 
time 608.610001, pid 4019848: fshs_GetPrincipal START 
time 608.610002, pid 4019848: fshs_GetHost, cookie 23f8ecc 
time 608.610003, pid 4019848: fshs_FindHost, cookie 23f8ecc 
time 608.610004, pid 4019848: find a prime host 2400094 
time 608.610006, pid 4019848: find a host in fast path 2400094 
time 608.610007, pid 4019848: fshs_FindPrincipal .. 
time 608.610008, pid 4019848: found a princ 245900c ref 1 
time 608.610010, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 608.610011, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 608.610013, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 608.610014, pid 4019848: GetTime returns 0 
time 610.530000, pid 4011632: end VL_GetVolByName, code 0 
time 610.530001, pid 4011632: cm_Analyze: conn 204778c, code 0, user 0 
time 610.530004, pid 4011632: Install vol entry for volume ID 1 
time 610.530008, pid 4011632: cm_ConnByMHosts server type 20000 
time 610.530010, pid 4011632: cm_ConnByHost server type 0x20000, serverp 217f50c 
time 610.530011, pid 4011632: cm_ConnByHost: creating conn type 0x20000, conn 204774c 
time 610.530012, pid 4011632: cm_ConnByHost using conn 204774c, service 0x20000 
time 610.750001, pid 4011632: get TGT time 0 for pag 0 
time 610.750002, pid 4011632: cm_ConnByHost: setting UUID 51b026/7a621e6c/8a430800/9215366 in connp 204774c 
time 610.750003, pid 4011632: cm_ConnByHost: passing other UUID 2bcebb/bd881e6b/8a430800/9215366 for connp 204774c 
time 610.750004, pid 4011632: start AFS_SetContext conn 204774c srv level 0x20000 
time 610.910001, pid 4019848: in AFS_SetContext, Flag 3 
time 610.910002, pid 4019848: fshs_GetHost, cookie 21168cc 
time 610.910003, pid 4019848: fshs_FindHost, cookie 21168cc 
time 610.910004, pid 4019848: cannot find a host in fast path 
time 610.910005, pid 4019848: fshs_FindHost, cookie 21168cc 
time 610.910007, pid 4019848: fshs_CreateHost: creating host 240011c, primary=1, flags=0x3; host states now 0x3 
time 610.910008, pid 4019848:     and host 240011c has fshost flags of 0x320 
time 610.920000, pid 4019848: fshs_CreateHost: returning 655257602 
time 610.920001, pid 4011632: end AFS_SetContext code 655257602 
time 610.920002, pid 4011632: cm_ConnAndReset: conn 0x204774c, server 0x217f50c, service 0x20000 
time 610.920003, pid 4011632: start recheck AFS_SetContext: conn 204774c srv level 0x20000 
time 612.530000, pid 4019848: in AFS_SetContext, Flag 3 
time 612.530001, pid 4019848: fshs_GetHost, cookie 21168cc 
time 612.530002, pid 4019848: fshs_FindHost, cookie 21168cc 
time 612.530003, pid 4019848: find a prime host 240011c 
time 612.530005, pid 4019848: fshs_GetHost: host 240011c, states 0x3, flags 0x320: forcing null return 
time 612.530006, pid 4019848: fshs_FindHost, cookie 21168cc 
time 612.530007, pid 4019848: find a prime host 240011c 
time 612.530008, pid 4019848: fshs_CreateHost: returning 655257602 
time 612.530009, pid 4011632: end recheck AFS_SetContext: code 655257602 
time 612.530010, pid 4011632: cm_ConnAndReset: conn 0x204774c, server 0x217f50c: reset 0 tokens 
time 612.530011, pid 4011632: cm_ConnAndReset: conn 0x204774c, service 0x20000: calling AFS_SetContext 
time 612.530013, pid 4019848: in AFS_SetContext, Flag 7 
time 612.530014, pid 4019848: fshs_GetHost, cookie 21168cc 
time 612.530015, pid 4019848: fshs_FindHost, cookie 21168cc 
time 612.530016, pid 4019848: find a prime host 240011c 
time 612.530018, pid 4019848: fshs_GetHost: host 240011c, states 0x3, flags 0x320: forcing null return 
time 612.530019, pid 4019848: fshs_FindHost, cookie 21168cc 
time 612.530020, pid 4019848: find a prime host 240011c 
time 612.530021, pid 4019848: fshs_CreateHost: clearing host 0x240011c 
time 612.530022, pid 4019848: fshs_CreateHost: calling TKN_InitTokenState on host 0x240011c 
time 612.530023, pid 4019848: tokenint_InitTokenState host 240011c 
time 612.530024, pid 4019848: fshs_GetTSRCode: host 240011c, returning 0xc 
time 614.390009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 639.400009, pid 4022376: running Ping Servers 
time 639.400012, pid 4022376: Ping Servers queuing at time 2e6c7a7f 
time 639.400014, pid 4022376: Ping Servers done at time 2e6c7a7f 
time 644.410004, pid 4018900: in PeriodicKA 
time 644.410005, pid 4018900: in RunTokens 
time 644.410006, pid 4018900: runtkn done, 0 active 
time 644.410008, pid 4018900: checkflservers checking 1 servers 
time 644.410018, pid 4022376: running write through dslots 
time 644.410025, pid 4022376: running major renewlazyreps 
time 644.410026, pid 4022376: Renewlazyreps starts 0 subjobs 
time 644.410034, pid 4024272: servertokenmgt running 2 subops 
time 644.410035, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 644.410052, pid 4025536: in cm_FlushQueuedServerTokens for server 217f50c 
time 644.410057, pid 4025220: in cm_FlushQueuedServerTokens for server 23e890c 
time 674.420009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 692.430009, pid 4022376: running Check Down Servers 
time 698.670000, pid 4019532: GetTime 
time 698.670001, pid 4019532: fshs_GetPrincipal START 
time 698.670002, pid 4019532: fshs_GetHost, cookie 2116a8c 
time 698.670003, pid 4019532: fshs_FindHost, cookie 2116a8c 
time 698.670004, pid 4019532: find a prime host 2400094 
time 698.670006, pid 4019532: find a host in fast path 2400094 
time 698.670007, pid 4019532: fshs_FindPrincipal .. 
time 698.670008, pid 4019532: found a princ 245900c ref 1 
time 698.670010, pid 4019532: find a princ (fast path) 245900c, ref 1 
time 698.670011, pid 4019532: fshs_GetPrincipal END 245900c, ref 1 
time 698.670013, pid 4019532: fshs_PutPrincipal 245900c ref 1 
time 698.670014, pid 4019532: GetTime returns 0 
time 704.440010, pid 4022376: running write through dslots 
time 704.440017, pid 4022376: running major renewlazyreps 
time 704.440018, pid 4022376: Renewlazyreps starts 0 subjobs 
time 704.440026, pid 4024272: servertokenmgt running 2 subops 
time 704.440027, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 704.440044, pid 4025536: in cm_FlushQueuedServerTokens for server 217f50c 
time 704.440049, pid 4025220: in cm_FlushQueuedServerTokens for server 23e890c 
time 734.450009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 759.460009, pid 4022376: running Ping Servers 
time 759.460012, pid 4022376: Ping Servers queuing at time 2e6c7af7 
time 759.460014, pid 4022376: Ping Servers done at time 2e6c7af7 
time 764.470004, pid 4018900: in PeriodicKA 
time 764.470005, pid 4018900: in RunTokens 
time 764.470006, pid 4018900: runtkn done, 0 active 
time 764.470008, pid 4018900: checkflservers checking 1 servers 
time 764.470018, pid 4022376: running write through dslots 
time 764.470025, pid 4022376: running major renewlazyreps 
time 764.470026, pid 4022376: Renewlazyreps starts 0 subjobs 
time 764.470034, pid 4024272: servertokenmgt running 2 subops 
time 764.470035, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 764.470052, pid 4025536: in cm_FlushQueuedServerTokens for server 217f50c 
time 764.470057, pid 4025220: in cm_FlushQueuedServerTokens for server 23e890c 
time 765.530000, pid 4019848: fshs_CreateHost: host 240011c TKN_InitTokenState returned 382312556 
time 765.530001, pid 4019848: fshs_CreateHost: returning 572616706 
time 765.530002, pid 4011632: cm_ConnAndReset: conn 0x204774c: AFS_SetContext result is 572616706 
time 765.530003, pid 4011632: server 217f50c just marked down 
time 765.530005, pid 4011632: mark conn bad for pag -1 
time 765.530006, pid 4011632: stomping binding 204774c 
time 765.530008, pid 4011632: cm_ConnByMHosts: found  0-th server down 
time 765.530009, pid 4011632: cm_ConnByMHosts: all filesets bad 
time 765.530010, pid 4011632: cm_Analyze: conn 0, code -1, user 0 
time 765.530011, pid 4011632: checkerror returning code 238 
time 788.750000, pid 4019848: GetTime 
time 788.750001, pid 4019848: fshs_GetPrincipal START 
time 788.750002, pid 4019848: fshs_GetHost, cookie 2199a8c 
time 788.750003, pid 4019848: fshs_FindHost, cookie 2199a8c 
time 788.750004, pid 4019848: find a prime host 2400094 
time 788.750006, pid 4019848: find a host in fast path 2400094 
time 788.750007, pid 4019848: fshs_FindPrincipal .. 
time 788.750008, pid 4019848: found a princ 245900c ref 1 
time 788.750010, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 788.750011, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 788.750013, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 788.750014, pid 4019848: GetTime returns 0 
time 794.480009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 824.490013, pid 4022376: running write through dslots 
time 824.490020, pid 4022376: running major renewlazyreps 
time 824.490021, pid 4022376: Renewlazyreps starts 0 subjobs 
time 824.490029, pid 4024272: servertokenmgt running 2 subops 
time 824.490030, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 824.490047, pid 4025536: in cm_FlushQueuedServerTokens for server 217f50c 
time 824.490052, pid 4025220: in cm_FlushQueuedServerTokens for server 23e890c 
time 854.500009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 857.950001, pid 4019848: in AFS_SetContext, Flag 3 
time 857.950002, pid 4019848: fshs_GetHost, cookie 2116a8c 
time 857.950003, pid 4019848: fshs_FindHost, cookie 2116a8c 
time 857.950004, pid 4019848: find a prime host 2400094 
time 857.950006, pid 4019848: find a host in fast path 2400094 
time 857.950007, pid 4019848: fshs_GetPrincipal START 
time 857.950008, pid 4019848: fshs_GetHost, cookie 2116a8c 
time 857.950009, pid 4019848: fshs_FindHost, cookie 2116a8c 
time 857.950010, pid 4019848: find a prime host 2400094 
time 857.950012, pid 4019848: find a host in fast path 2400094 
time 857.950013, pid 4019848: fshs_FindPrincipal .. 
time 857.950014, pid 4019848: found a princ 245900c ref 1 
time 857.950016, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 857.950017, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 857.950018, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 857.960000, pid 4019848: GetToken fid 5366acd9.1.2.0 type 0x2000, id 0.0, flags 0x89 
time 857.960001, pid 4019848: fshs_GetPrincipal START 
time 857.960002, pid 4019848: fshs_GetHost, cookie 2116b8c 
time 857.960003, pid 4019848: fshs_FindHost, cookie 2116b8c 
time 857.960004, pid 4019848: find a prime host 2400094 
time 857.960006, pid 4019848: find a host in fast path 2400094 
time 857.960007, pid 4019848: fshs_FindPrincipal .. 
time 857.960008, pid 4019848: found a princ 245900c ref 1 
time 857.960010, pid 4019848: find a princ (fast path) 245900c, ref 1 
time 857.960011, pid 4019848: fshs_GetPrincipal END 245900c, ref 1 
time 857.960020, pid 4019848: fshs_PutPrincipal 245900c ref 1 
time 857.960021, pid 4019848: GetToken returns id 778812738.53944, rights 0x0.2000, code 0 
time 872.510009, pid 4022376: running Check Down Servers 
time 877.520004, pid 4018900: fshs_HostCheckDaemon .. 
time 877.520005, pid 4018900: fshs_Enumerate START 
time 877.520007, pid 4018900: fshs_CheckHost START 240000c 
time 877.520008, pid 4018900: fshs_GCPrinc from host 240000c 
time 877.520009, pid 4018900: fshs_CheckHost END 
time 877.520011, pid 4018900: fshs_CheckHost START 2400094 
time 877.520012, pid 4018900: fshs_GCPrinc from host 2400094 
time 877.520013, pid 4018900: fshs_CheckHost END 
time 877.520015, pid 4018900: fshs_CheckHost START 240011c 
time 877.520016, pid 4018900: fshs_CheckHost END 
time 877.520017, pid 4018900: fshs_UpdateHostList called 
time 877.520018, pid 4018900: fshs_GCHost 240011c ref 0 
time 877.520019, pid 4018900: GC a prime host 
time 877.520020, pid 4018900: fshs_FreeHost 240011c 
time 877.520021, pid 4018900: fshs_Enumerate END 
time 877.520034, pid 4022376: Refresh Tokens at time 2e6c7b6d, opcount 0 
time 877.520036, pid 4022376: Refresh Tokens done at time 2e6c7b6d 
time 877.520044, pid 4024272: GC conns 0x0 
time 877.520046, pid 4024272: GC conns 0x0 
time 877.520047, pid 4024272: stomping binding 204778c 
time 879.530009, pid 4022376: running Ping Servers 
time 879.530012, pid 4022376: Ping Servers queuing at time 2e6c7b6f 
time 879.530014, pid 4022376: Ping Servers done at time 2e6c7b6f 
time 884.540004, pid 4018900: in PeriodicKA 
time 884.540005, pid 4018900: in RunTokens 
time 884.540006, pid 4018900: runtkn done, 0 active 
time 884.540008, pid 4018900: checkflservers checking 1 servers 
time 884.540018, pid 4022376: running write through dslots 
time 884.540025, pid 4022376: running major renewlazyreps 
time 884.540026, pid 4022376: Renewlazyreps starts 0 subjobs 
time 884.540034, pid 4024272: servertokenmgt running 2 subops 
time 884.540035, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 884.540052, pid 4025536: in cm_FlushQueuedServerTokens for server 217f50c 
time 884.540057, pid 4025220: in cm_FlushQueuedServerTokens for server 23e890c 
time 914.550009, pid 4022376: flushactivescaches starting 0 concurrent storebacks 
time 944.560010, pid 4022376: running write through dslots 
time 944.560017, pid 4022376: running major renewlazyreps 
time 944.560018, pid 4022376: Renewlazyreps starts 0 subjobs 
time 944.560026, pid 4024272: servertokenmgt running 2 subops 
time 944.560027, pid 4023956: flushactivescaches starting 0 concurrent storebacks 
time 944.560044, pid 4025536: in cm_FlushQueuedServerTokens for server 217f50c 
time 944.560049, pid 4025220: in cm_FlushQueuedServerTokens for server 23e890c 
time 945.570009, pid 4022376: running Check Down Servers 
time 945.570020, pid 4025536: checkdownservers server 217f50c 
time 945.570022, pid 4025536: cm_ConnByHost server type 0x20000, serverp 217f50c 
time 945.570023, pid 4025536: cm_ConnByHost: creating conn type 0x20000, conn 204778c 
time 945.570024, pid 4025536: cm_ConnByHost using conn 204778c, service 0x20000 
time 945.570028, pid 4025536: get TGT time 0 for pag 0 
time 945.570029, pid 4025536: cm_ConnByHost: setting UUID 51b026/7a621e6c/8a430800/9215366 in connp 204778c 
time 945.570030, pid 4025536: cm_ConnByHost: passing other UUID 2bcebb/bd881e6b/8a430800/9215366 for connp 204778c 
time 945.570031, pid 4025536: start AFS_SetContext conn 204778c srv level 0x20000 
time 945.740001, pid 4019848: in AFS_SetContext, Flag 3 
time 945.740002, pid 4019848: fshs_GetHost, cookie 23f8d8c 
time 945.740003, pid 4019848: fshs_FindHost, cookie 23f8d8c 
time 945.740004, pid 4019848: cannot find a host in fast path 
time 945.740005, pid 4019848: fshs_FindHost, cookie 23f8d8c 
time 945.740007, pid 4019848: fshs_CreateHost: creating host 240011c, primary=1, flags=0x3; host states now 0x3 
time 945.740008, pid 4019848:     and host 240011c has fshost flags of 0x320 
time 945.740009, pid 4019848: fshs_CreateHost: returning 655257602 
time 945.740010, pid 4025536: end AFS_SetContext code 655257602 
time 945.740011, pid 4025536: cm_ConnAndReset: conn 0x204778c, server 0x217f50c, service 0x20000 
time 945.740012, pid 4025536: start recheck AFS_SetContext: conn 204778c srv level 0x20000 
time 945.740014, pid 4019848: in AFS_SetContext, Flag 3 
time 945.740015, pid 4019848: fshs_GetHost, cookie 23f8d8c 
time 945.740016, pid 4019848: fshs_FindHost, cookie 23f8d8c 
time 945.740017, pid 4019848: find a prime host 240011c 
time 945.740019, pid 4019848: fshs_GetHost: host 240011c, states 0x3, flags 0x320: forcing null return 
time 945.740020, pid 4019848: fshs_FindHost, cookie 23f8d8c 
time 945.740021, pid 4019848: find a prime host 240011c 
time 945.740022, pid 4019848: fshs_CreateHost: returning 655257602 
time 945.740023, pid 4025536: end recheck AFS_SetContext: code 655257602 
time 945.740024, pid 4025536: cm_ConnAndReset: conn 0x204778c, server 0x217f50c: reset 0 tokens 
time 945.740025, pid 4025536: cm_ConnAndReset: conn 0x204778c, service 0x20000: calling AFS_SetContext 
time 945.750000, pid 4019848: in AFS_SetContext, Flag 7 
time 945.750001, pid 4019848: fshs_GetHost, cookie 23f8d8c 
time 945.750002, pid 4019848: fshs_FindHost, cookie 23f8d8c 
time 945.750003, pid 4019848: find a prime host 240011c 
time 945.750005, pid 4019848: fshs_GetHost: host 240011c, states 0x3, flags 0x320: forcing null return 
time 945.750006, pid 4019848: fshs_FindHost, cookie 23f8d8c 
time 945.750007, pid 4019848: find a prime host 240011c 
time 945.750008, pid 4019848: fshs_CreateHost: clearing host 0x240011c 
time 945.750009, pid 4019848: fshs_CreateHost: calling TKN_InitTokenState on host 0x240011c 
time 945.750010, pid 4019848: tokenint_InitTokenState host 240011c 
time 945.750011, pid 4019848: fshs_GetTSRCode: host 240011c, returning 0xc 
time 948.040000, pid 4019532: GetTime 
time 948.040001, pid 4019532: fshs_GetPrincipal START 
time 948.040002, pid 4019532: fshs_GetHost, cookie 21a3f0c 
time 948.040003, pid 4019532: fshs_FindHost, cookie 21a3f0c 
time 948.040004, pid 4019532: find a prime host 2400094 
time 948.040006, pid 4019532: find a host in fast path 2400094 
time 948.040007, pid 4019532: fshs_FindPrincipal .. 
time 948.040008, pid 4019532: found a princ 245900c ref 1 
time 948.040010, pid 4019532: find a princ (fast path) 245900c, ref 1 
time 948.040011, pid 4019532: fshs_GetPrincipal END 245900c, ref 1 
time 948.040013, pid 4019532: fshs_PutPrincipal 245900c ref 1 
time 948.040014, pid 4019532: GetTime returns 0 
time 974.580009, pid 4024272: flushactivescaches starting 0 concurrent storebacks 
time 999.590009, pid 4024272: running Ping Servers 
time 999.590012, pid 4024272: Ping Servers queuing at time 2e6c7be7 
time 999.590014, pid 4024272: Ping Servers done at time 2e6c7be7 
time 1004.600004, pid 4018900: in PeriodicKA 
time 1004.600005, pid 4018900: in RunTokens 
time 1004.600006, pid 4018900: runtkn done, 0 active 
time 1004.600008, pid 4018900: checkflservers checking 1 servers 
time 1004.600017, pid 4024272: running write through dslots 
time 1004.600022, pid 4024272: flushactivescaches starting 0 concurrent storebacks 
time 1004.600028, pid 4024272: running major renewlazyreps 
time 1004.600029, pid 4024272: Renewlazyreps starts 0 subjobs 
time 1004.600037, pid 4023956: servertokenmgt running 2 subops 
time 1004.600049, pid 4025220: in cm_FlushQueuedServerTokens for server 217f50c 
time 1004.600054, pid 4024904: in cm_FlushQueuedServerTokens for server 23e890c 
time 10.610009, pid 4024272: flushactivescaches starting 0 concurrent storebacks 
time 14.090000, pid 4019532: GetTime 
time 14.090001, pid 4019532: fshs_GetPrincipal START 
time 14.090002, pid 4019532: fshs_GetHost, cookie 21a3f0c 
time 14.090003, pid 4019532: fshs_FindHost, cookie 21a3f0c 
time 14.090004, pid 4019532: find a prime host 2400094 
time 14.090006, pid 4019532: find a host in fast path 2400094 
time 14.090007, pid 4019532: fshs_FindPrincipal .. 
time 14.090008, pid 4019532: found a princ 245900c ref 1 
time 14.090010, pid 4019532: find a princ (fast path) 245900c, ref 1 
time 14.090011, pid 4019532: fshs_GetPrincipal END 245900c, ref 1 
time 14.090013, pid 4019532: fshs_PutPrincipal 245900c ref 1 
time 14.090014, pid 4019532: GetTime returns 0 
time 40.620009, pid 4024272: running write through dslots 
time 40.620014, pid 4024272: flushactivescaches starting 0 concurrent storebacks 
time 40.620020, pid 4024272: running major renewlazyreps 
time 40.620021, pid 4024272: Renewlazyreps starts 0 subjobs 
time 40.620029, pid 4023956: servertokenmgt running 2 subops 
time 40.620041, pid 4025220: in cm_FlushQueuedServerTokens for server 217f50c 
time 40.620046, pid 4024904: in cm_FlushQueuedServerTokens for server 23e890c 
time 70.630009, pid 4024272: flushactivescaches starting 0 concurrent storebacks 
time 74.530000, pid 4019848: fshs_CreateHost: host 240011c TKN_InitTokenState returned 382312556 
time 74.530001, pid 4019848: fshs_CreateHost: returning 572616706 
time 74.530002, pid 4025536: cm_ConnAndReset: conn 0x204778c: AFS_SetContext result is 572616706 
time 74.550000, pid 4025536: server 217f50c just marked down 
time 74.550002, pid 4025536: mark conn bad for pag -1 
time 74.550003, pid 4025536: stomping binding 204778c 
time 95.660011, pid 4022376: running Ping Servers 
time 95.660014, pid 4022376: Ping Servers queuing at time 2e6c7c5f 
time 95.660016, pid 4022376: Ping Servers done at time 2e6c7c5f

[hsiao 9/6/94 ] 
 
I forgot to mention some important infos. The cell is 
a two HP nodes cell, dce10 as cds/sec/dfs server, toaster
as client. The problem happens most of the time
in the server node --- only once in the client.
 
The build is "-O" build based on bl-14. 
 
It seems to me that problem is resource related - 
likely memory.
 
Once, I rebooted dce10 and its dfs and did NOT run the
"DCE-build" test or any other test. At the end of an 18 hours 
operation the "ls /:" still worked.
[09/06 public]
 
Another import info: IP address and port are always correct
before the TKN_InitTokenState call.
[09/08 public]
 
After my nightly DCE-build the bug happens as usual ---
"ls /:" got RPC timeout and AFS_SetContext failed. 
 
ALL DFS user space servers are normal:
 
***bos status -server /.:/hosts/dce10***
Instance flserver, currently running normally.
Instance bakserver, currently running normally.
Instance ftserver, currently running normally.
Instance repserver, currently running normally.
 
***fts lsfldb***
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
dce10.osf.org       RW       /u4     0:00:00 hosts/dce10    <nil>               
----------------------
Total FLDB entries that were successfully enumerated: 1 (0 failed; 0 wrong aggr type)

[09/12/94 public]
 
Lower the severity/priority to B2.
 
With 16.3 default build I have done four "DCE build", since
last Thursday, but "ls /:" still works with no RPC timeout.
So, I'll lower the severity/priority from A1 to B2 for now.

[9/12/94 public]
This is probably a dup of 11070.

[09/12/94 public]

I agree with Craig based on my Krpc traces and the OT notes to 11070.



CR Number                     : 12008
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfsd
Short Description             : PID#4681 ERROR dsd general afsd.c
Reported Date                 : 9/1/94
Found in Baseline             : 1.1
Found Date                    : 9/1/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 12006
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/1/94 public]


Fileset 0,,1 created on aggregate lfs_aggr1 of budapest
S:****** Starting dfsbind...
S:****** Starting fxd...
S:****** Configuring DFS Client...
S:****** Loading kernel extensions...
Too few servers (2), need 3 servers
S:****** Starting dfsd...
1994-09-01-12:50:44.889-04:00I----- PID#4681 ERROR dsd general afsd.c 1255 0x7affbbb0
dfsd: start sweeping disk cache files .... 
1994-09-01-12:51:33.352-04:00I----- PID#4681 ERROR dsd general afsd.c 1324 0x7affbbb0
dfsd: All DFS daemons started.
S:****** Exiting from dce_config.
WARNING: The password for the "cell_admin none none" user is a well-known 
         default value.  Since this is a security hole, it is recommended that 
         the password be changed immediately after exiting this script by using
         "dce_login", then the "rgy_edit change" command.
+ cd /u1/RAT_tools



CR Number                     : 11749
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/episode/utils
Short Description             : Unsatisfied symbols
Reported Date                 : 8/18/94
Found in Baseline             : 1.1
Found Date                    : 8/18/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11747
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/18/94 public]
[ /file/episode/utils at 23:08 (PM) Wednesday ]
makepath utils/. && cd utils &&  exec make MAKEFILE_PASS=BASIC     build_all
utils: created directory
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/utils -I/project/dce/build/dce1.1-snap/src/file/episode/utils  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/utils/epimount.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/utils -I/project/dce/build/dce1.1-snap/src/file/episode/utils  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/utils/epiunmount.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/utils -I/project/dce/build/dce1.1-snap/src/file/episode/utils  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/utils/epiinit.c
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib    -L. -L/u3/devobj/sb/nb_ux/src/file/episode/utils -L/project/dce/build/dce1.1-snap/src/file/episode/utils -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o epimount.X epimount.o  -lftutil -lafssys -licl -losi -lcom_err  -ldce -lBSD  
/bin/ld: Unsatisfied symbols:
   ftu_GetRawDeviceName (code)
   _ftu_Unmount (code)
   _ftu_Mount (code)
*** Error code 1



CR Number                     : 11748
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000,hppa
S/W Ref Platform              : aix,hpux
Component Name                : dfs
Subcomponent Name             : /file/icl
Short Description             : Can't find library for -lncompat
Reported Date                 : 8/18/94
Found in Baseline             : 1.1
Found Date                    : 8/18/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11747
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/18/94 public]

[ /file/icl at 22:50 (PM) Wednesday ]
makepath icl/. && cd icl &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -DICL_DEFAULT_ENABLED -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/icl -I/project/dce/build/dce1.1-snap/src/file/icl  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/icl/icl_dumpCommand.c
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o dfstrace.X icl_dumpCommand.o  -licl -ldacl -ldauth -ldacllfs  -ldacl -ldauth -ldacllfs -lncompat  -lafssys -lcom_err -lcmd -licl -losi -ldce -lBSD  
/bin/ld: Can't find library for -lncompat
*** Error code 1



CR Number                     : 11689
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa,i486
S/W Ref Platform              : osf1,hpux
Component Name                : dfs
Subcomponent Name             : opt/dcelocal/ext/kload
Short Description             : kload failed
Reported Date                 : 8/15/94
Found in Baseline             : 1.1
Found Date                    : 8/15/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11684
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[8/15/94 public]

TEST:		single cell single machine test
SYSTEM:		budapest
TEST PHASE:	dce_config


output from /tmp/dce_config.log


S:****** Loading kernel extensions...
D:         Executing: /opt/dcelocal/ext/kload -N dce_krpc
DEBUG:     Executing: err_exit("/opt/dcelocal/ext/kload -N dce_krpc" failed.
           Continuing will assume you have done the kload by hand.)
ERROR:   "/opt/dcelocal/ext/kload -N dce_krpc" failed.  Continuing will assume
         you have done the kload by hand.
DEBUG:     Executing: menu99()
S:****** Exiting from dce_config.
WARNING: The password for the "cell_admin none none" user is a well-known
         default value.  Since this is a security hole, it is recommended that
         the password be changed immediately after exiting this script by using
         "dce_login", then the "rgy_edit change" command.
D:         Executing: rm -f /usr/bin/dce_login_noexec

output from budapest /u1/RAT_tools/run_commands.dfs.log

+ /u1/opt/dcelocal/bin/fts lsfldb 
Could not access the FLDB for attributes
Error: no servers appear to be up (dfs / ubk)
+ [ 1 != 0 ] 
+ echo fts lsfldb FAILED! 
fts lsfldb FAILED!
+ sleep 240 
+ echo cd /: 
cd /:
+ cd /: 
/u1/RAT_tools/run_commands.dfs: /:: bad directory



output from /u1/RAT_tools/do_test.dfs.log


:****** Configuring DFS Fileset Location Database Server...
1994-08-15-08:11:31.280-04:00I----- dtsd WARNING dts config logevent_v_ultrix.c 265 0x7aff8410
Too few servers (2), need 3 servers
S:****** Modifying the registry database for DFS operation...
S:****** Loading kernel extensions...
/bin/ld: Unsatisfied symbols:
   malloc (code)
Error 0
Unable to create (/bin/ld) /opt/dcelocal/ext/dce_krpc.tmp
ERROR:   "/opt/dcelocal/ext/kload -N dce_krpc" failed.  Continuing will assume 
         you have done the kload by hand.
S:****** Configuring DFS Client...
S:****** Loading kernel extensions...
/bin/ld: Unsatisfied symbols:
   malloc (code)
Error 0
Unable to create (/bin/ld) /opt/dcelocal/ext/dce_krpc.tmp
ERROR:   "/opt/dcelocal/ext/kload -N dce_krpc" failed.  Continuing will assume 
         you have done the kload by hand.
S:****** Exiting from dce_config.

[8/15/94 public]
This is a dup of 11684.



CR Number                     : 11523
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/episode/anode
Short Description             : Can't find library for -ldfstab
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/Makefile, file/Makefile
Sensitivity                   : public

[8/2/94 public]



[ /file/episode/anode at 22:38 (PM) Monday ]
makepath anode/. && cd anode &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/calcLogSize.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/findlog.c
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/newaggr.c
c89 -c    -D_SHARED_LIBRARIES -g  -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/runtest.c
c89 -c    -D_SHARED_LIBRARIES -g  -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/test_anode.c
c89 -c    -D_SHARED_LIBRARIES -g  -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/episode/anode -I/project/dce/build/dce1.1-snap/src/file/episode/anode -IHPUX -I/u3/devobj/sb/nb_ux/src/file/episode/anode/HPUX -I/project/dce/build/dce1.1-snap/src/file/episode/anode/HPUX -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/episode/anode/test_vm.c
gencat dfsepi.cat epi_errs.msf
gencat dfszea.cat epi_trace.msf
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o calcLogSize.X calcLogSize.o  -llogbuf -lasync  -lafsutil -lcom_err -ltools -losi -ldfstab -lcmd  -ldce -lBSD  
/bin/ld: Can't find library for -ldfstab
*** Error code 1

[8/2/94 public]
This problem occurs because the DFS build broke due to Makefile problems
that caused the serviceability items not to build. When CR 11515 is submitted,
this problem will not exist, as the libraries will get built.



CR Number                     : 11522
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/scout
Short Description             : mssing library fsprobe
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/Makefile, file/Makefile
Sensitivity                   : public

[8/2/94 public]



[ /file/scout at 22:38 (PM) Monday ]
makepath scout/. && cd scout &&  exec make MAKEFILE_PASS=BASIC     build_all
scout: created directory
c89 -c    -D_SHARED_LIBRARIES   -Dunix -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/scout -I/project/dce/build/dce1.1-snap/src/file/scout  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/scout/scout.c
c89   -g   -z -Wl,-Bimmediate,-Bnonfatal,-a,default,+b,/lib:/usr/lib     -L/u3/devobj/sb/nb_ux/export/hp800/usr/shlib -L/project/dce/build/dce1.1-snap/export/hp800/usr/shlib -L/usr/shlib -L/u3/devobj/sb/nb_ux/export/hp800/usr/lib     -L/project/dce/build/dce1.1-snap/export/hp800/usr/lib   -o scout.X scout.o  -lfsprobe -lgtx -lvolc -lrep   -lftserver -lfldb  -lnubik -licl -lcmd -lafs4int -lncompat -ltpq  -ldacl -ldauth -ldacllfs -ldacl -ldauth  -ldacllfs -ldfsncs -lbomb  -lcommondata  -lafsutil -lcom_err  -lafssys -losi -ldce -lcurses -lBSD  
/bin/ld: Can't find library for -lfsprobe
*** Error code 1
`build_all' not remade because of errors.
*** Error code 1

[8/2/94 public]
This is fixed in CR 11515. It occurred because the library wasn't built in
a previous step due to the serviceability code omissions which will be fixed
by the previously mentioned 11515.

When the problem in CR 11516, which is fixed in CR 11515, is eliminated
this problem shouldn't exist.



CR Number                     : 11520
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/update
Short Description             : missing include
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/Makefile, file/Makefile
Sensitivity                   : public

[8/2/94 public]


[ /file/update at 22:35 (PM) Monday ]
makepath update/. && cd update &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -Dunix -D_BSD -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/update -I/project/dce/build/dce1.1-snap/src/file/update  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/update/util.c
util.c: 89: Unable to find include file 'dfsudtmac.h'.

[8/2/98 public]
The problem stems from needing to include the serviceability information for
DFS. The fix is to two Makefiles. The first is the src/Makefile which needs
to have the DFS file directory added to the SAMIDL_SUBDIRS directory list.
The second is to the file/Makefile which needs to have the update subdirectory
added to its SAMIDL_SUBDIRS list.

The fix will be submitted in CR 11515.



CR Number                     : 11519
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/dfsbind
Short Description             : missing include
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : 
Sensitivity                   : public

[8/2/94 public]

[ /file/dfsbind at 22:32 (PM) Monday ]
makepath dfsbind/. && cd dfsbind &&  exec make MAKEFILE_PASS=BASIC     build_all
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -DDNS_V3API -DDNS_CDS  -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/dfsbind -I/project/dce/build/dce1.1-snap/src/file/dfsbind  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/dfsbind/dfsbind_svc.c
dfsbind_svc.c: 39: Unable to find include file 'dfsdsbmac.h'.
*** Error code 1

[8/2/98 public]
The problem stems from needing to include the serviceability information for
DFS. The fix is to two Makefiles. The first is the src/Makefile which needs
to have the DFS file directory added to thee SAMIDL_SUBDIRS directory list.
The second is to the file/Makefile which needs to have the dfsbind subdirectory
added to its SAMIDL_SUBDIRS list.

The fix will be submitted in CR 11515.



CR Number                     : 11518
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/pxd
Short Description             : missing includes
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/Makefile, file/Makefile
Sensitivity                   : public

[8/2/94 public]



[ /file/pxd at 22:32 (PM) Monday ]
makepath pxd/. && cd pxd &&  exec make MAKEFILE_PASS=BASIC     build_all
pxd: created directory
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/pxd -I/project/dce/build/dce1.1-snap/src/file/pxd  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/pxd/pxd.c
pxd.c: 224: Unable to find include file 'dfsfxdmac.h'.
pxd.c: 225: Unable to find include file 'dfsfxdsvc.h'.
pxd.c: 226: Unable to find include file 'dfsfxdmsg.h'.
*** Error code 1

[8/2/98 public]
The problem stems from needing to include the serviceability information for
DFS. The fix is to two Makefiles. The first is the src/Makefile which needs
to have the DFS file directory added to thee SAMIDL_SUBDSIRS directory list.
The second is to the file/Makefile which needs to have the pxd subdirectory
added to its SAMIDL_SUBDIRS list.

The fix will be submitted in CR 11515.



CR Number                     : 11517
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/afsd
Short Description             : missing includes
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : src/Makefile, file/Makefile
Sensitivity                   : public

[8/2/94 public]


[ /file/afsd at 22:32 (PM) Monday ]
makepath afsd/. && cd afsd &&  exec make MAKEFILE_PASS=BASIC     build_all
afsd: created directory
c89 -c    -D_SHARED_LIBRARIES   -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D_BSD -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/afsd -I/project/dce/build/dce1.1-snap/src/file/afsd  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/afsd/afsd.c
afsd.c: 359: Unable to find include file 'dfsdsdmac.h'.
afsd.c: 360: Unable to find include file 'dfsdsdsvc.h'.
afsd.c: 361: Unable to find include file 'dfsdsdmsg.h'.
*** Error code 1

[8/2/98 public]
The problem stems from needing to include the serviceability information for
DFS. The fix is to two Makefiles. The first is the src/Makefile which needs
to have the DFS file directory added to thee SAMIDL_SUBDSIRS directory list.
The second is to the file/Makefile which needs to have the dfsd subdirectory
added to its SAMIDL_SUBDIRS list.

The fix will be submitted in CR 11515.
+



CR Number                     : 11516
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : /file/fsprobe
Short Description             : missing includes
Reported Date                 : 8/2/94
Found in Baseline             : 1.1
Found Date                    : 8/2/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 11515
Fix By Baseline               : 1.1
Fixed In Baseline             : 1.1b14
Affected File(s)              : file/MakefilE
Sensitivity                   : public

[8/2/94 public]
[ /file/fsprobe at 21:41 (PM) Monday ]
makepath fsprobe/. && cd fsprobe &&  exec make MAKEFILE_PASS=THIRD     build_all
c89 -c    -D_SHARED_LIBRARIES   -Dunix -DDEBUG -DAFS_DEBUG -DDCESHARED_PATH=\"/opt/dce\" -DDCELOCAL_PATH=\"/opt/dcelocal\" -D__hppa -Dhp9000s800 -Dhp9000s700 -D__hp9000s800 -D__hp9000s700 -DHPUX -D__hpux -Dunix +DA1.0 -D_HPUX_SOURCE    -I- -I. -I/u3/devobj/sb/nb_ux/src/file/fsprobe -I/project/dce/build/dce1.1-snap/src/file/fsprobe  -I/u3/devobj/sb/nb_ux/export/hp800/usr/include -I/project/dce/build/dce1.1-snap/export/hp800/usr/include    /project/dce/build/dce1.1-snap/src/file/fsprobe/fsprobe.c
fsprobe.c: 135: Unable to find include file 'dfsfspmac.h'.
fsprobe.c: 136: Unable to find include file 'dfsfspsvc.h'.
fsprobe.c: 137: Unable to find include file 'dfsfspmsg.h'.
*** Error code 1

[8/2/94 public]
This is a dup of 11515. The fix is to the DFS Makefile in the src/file
directory, which will be submitted under that CR. The top level DFS Makefile
needs to have the name of the directory for fsprobe (fsprobe) added to the
SAMIDL_SUBDIRS list. Additionally, the Makefile in the src directory needs
to have the directory 'file' added to its list of SAMIDL_SUBDIRS to pick
up the DFS svc additions.



CR Number                     : 10925
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : s12y
Short Description             : implement partial s12y in dfs
Reported Date                 : 6/9/94
Found in Baseline             : 1.1
Found Date                    : 6/9/94
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 12618
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : many
Sensitivity                   : public

[6/9/94 public]
This is a placeholder for the S12Y submissions to the DFS commands
and daemons.

[6/9/94 public]
Re-assigning to Ron S. since he's doing the work.



CR Number                     : 10297
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test
Short Description             : file/rep/rtest3 grep problem
Reported Date                 : 4/5/94
Found in Baseline             : 1.0.3a
Found Date                    : 4/5/94
Severity                      : E
Priority                      : 4
Status                        : dup
Duplicate Of                  : 10281
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : test/file/rep/rtest3
Sensitivity                   : public

[4/5/94 public]
 
test/file/rep/rtest3 contains two for loops with
the following line
        xxx=`fts lsheader $SERVER_1 $EPI_AGGRNAME_1 | grep $base_name.2 |
grep On-line`
 
the fact that the grep is on the next line cause an grep useage
message and the loop to run until tries meets the loopcount
maximum
 
although it does not cause any test failure it does cause the
runtime of the test to be longer than necessary
 
either a 'line continuation' should be placed where the grep On-line
is or better yet place it at the end of the previous line as do the
other tests (rtest1 rtest2)



CR Number                     : 9759
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : libafs
Short Description             : libafs depends on libktkc.a which was not built
Reported Date                 : 1/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/17/94
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 9754
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[1/17/94 public]

file/tkc got a signal 10 on the HPUX platform - (CR 9754)

Since libafs depends on libktkc.a, it was not built.

[1/17/94 public]
THis is a dup of 9754.



CR Number                     : 9758
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : noship directory
Short Description             : noship directory did not build
Reported Date                 : 1/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/17/94
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 9754
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[1/17/94 public]

Never got built at all.  make just die.  Not sure why.

[1/19/94 public]
Assigned to annie to verify that using the right set of ode
tools makes this problem disappear.



CR Number                     : 9757
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : file/epidummy
Short Description             : build of file/epidummy caused compilation errors
Reported Date                 : 1/17/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/17/94
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 9754
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[1/17/94 public]

Not sure about this one as the library got created but make
seems to have die.

[1/19/94 public]
Assigned to annie to verify that using the right set of ode
tools makes this problem disappear.



CR Number                     : 9674
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts test21 can use up /tmp
Reported Date                 : 1/3/94
Found in Baseline             : 1.0.3a
Found Date                    : 1/3/94
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 9633
Fix By Baseline               : 1.0.3a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[1/3/94 public]
fts/test21 creates a 1MB file on /tmp and never deletes it.  Multiple runs
will use up /tmp.  Also, it doesn't explicitly check if /tmp is full. I can
only guess what sort of failure mode this would lead to.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/6/94 public]
I inadvertently fixed this as part of the fix for OT CR 9633, so am dup'ing
it as appropriate.
If the bug is not fixed, eventually /tmp fills up and on the next run of fts
test21, it creates a filler file of length zero.  Then, when it tries to
fill up the aggregate by repeatedly cp'ing the filler file into it, its rate
of progress is so slow as to appear to be zero.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `9633'



CR Number                     : 9573
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : dfs fails to copy file when cache size cannot be increased
Reported Date                 : 12/3/93
Found in Baseline             : 1.0.3
Found Date                    : 12/3/93
Severity                      : A
Priority                      : 2
Status                        : dup
Duplicate Of                  : 4040
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[12/3/93 public]
I was trying to copy a file from DFS to a native file system and kept
getting the following eror:

779 ->cp fts.runtests.log /tmp/fts.ufs.log
cp: fts.runtests.log: There is an input or output error.

This error message gave me no clue as to the real problem. I had created
the dfs cache on disk in the /opt file system I had created. However, I
don't believe at the time of the copy that there was enough disk space to
allow the cache size to grow. I deleted some files that were in the /opt
file system and then I was able to copy the file.

questions:
1) Why can't dfs report an error on trying to increase the cache size?
2) Why can't dfs just purge old information from the cache? When the
	request to expand the cache fails, purge the cache until enough
	cache is available. If this fails then report an error that
	is meaningful, such as "dfs: cache to small for requested operation".

[12/10/93 public]
Defer to 1.1, drop priority, change to enhancement.  I believe this 
is a documented restriction on cache size.  You must have enough disk space
pre-allocated for your cache plus some small ancillary files.
You must not let other files infringe on that space else you
may get a panic.  I'm surprised you did not get one here.  It 
is recommended in the docs that you put your cache in it's own 
filesystem.



CR Number                     : 9430
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : TSR did not recover the lock hold by a client.
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 9429
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : import
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Three machines configured by 3.35 (M1, M2, and M3). M1 and M3 were running
 ITL tsr script to try lock (M1 held the lock and M3 was trying). After a reboot
 of M2 where the fileset the test file located exported, M1 still held the
 lock but M3 can get the lock. 
 
 jess-Wed, 13 Oct 1993 13:59:15
 
 ----------------------------------------------------------------------
 
 So far, this is correct.  If M2 crashes, M1 should discard its lock
 tokens and mark the file bad with ESTALE; M3 should be able to get the
 lock.
 
 Basically, lock tokens are treated specially.  As a long-lived token,
 for M1's CM to claim that the lock continues to be held throughout the
 crash, it would have to know things that M2's implementation can't yet
 tell it--designed and architected, but not implemented--so the upshot
 is that CMs lose lock tokens when a server crashes.  The test should
 be that M1 marks the file with ESTALE.  This is one of the things
 tested in Tu's hand tests.
 
 I'm marking this as RETURNED, for clarification as to whether Jess et al.
 still think there's a bug.
 
 cfe-Thu, 14 Oct 1993 09:03:45
 cfe-Thu, 14 Oct 1993 09:05:03
 
 -------------------------------------------------------------------------------
 
 Yes, Craig is right. M1 did get a ESTALE after M2 is rebooted. Then we
 have to change our test scenario accordingly. 
 
 Now let talk about a related problem that is the case of network partition.
 When disconnecting M2 for 105 seconds and then putting it back, M1 held
 the lock and everything was ok with M1 (No ESTALE returned) but M3 got the
 lock. It is ok to disconnect M2 for 30 seconds. Craig could you confirm if
 this is a problem or not. If it is I am going to reopen the defect.
 jess-Thu, 14 Oct 1993 11:59:39
 
 ---------------------------------------------------------------------------
 
 [cfe 14 Oct 1993]
 It should never be the case that M1 and M3 have conflicting locks and neither
 one returns ESTALE.  I'm not sure what the problem is.  To diagnose this,
 it would be most helpful to have dfstrace results (of the cmfx log) for all
 three machines immediately after the discrepancy is noted.  Thanks.
 
 cfe-Thu, 14 Oct 1993 13:01:25
 
 ---------------------------------------------------------------------------
 
 [cfe 15 Oct 1993]
 
 Some more diagnosis.  The server that has the token goes down and the client
 holding the lock (M1) doesn't do partition-TSR until after the lock-grabber
 (M3) does TSR and grabs the lock.  Thus, the server has legitimately marked
 M1 as down, granting the token.  What's bogus is that if the first call
 that M1 does is to release the lock, the file isn't marked with ESTALE;
 TSR happens but the file has no locks (by the time releaselockf tries to
 store the lock token back), so nothing is upset when the lock token cannot
 be renewed.  Unfortunately, the application cannot tell that the lock had
 been lost some time earlier; nothing marked the file with ESTALE.  Had TSR
 taken place some other time than while releasing the lock, this condition
 probably would not have happened.
 
 The CM had indeed marked the server as down.
 cfe-Fri, 15 Oct 1993 15:24:12
 
 ---------------------------------------------------------------------
 
 [cfe 10/18/93]
 The way to fix the problem is probably to check, when doing any lock work
 and particularly doing an unlock, that the CM can guarantee tokens--that
 the server isn't in TSR mode and there's been a successful RPC to the
 server within the server's HostLifetime.  If there hasn't been, then the
 CM should force an RPC (and concomitant possible TSR work) before proceeding
 with the lock.
 
 However, the window in which this problem seems to occur is pretty small,
 and is not as pressing a problem as it first appears.  That is, it seems to
 be a special case of TSR not working, not a general case.
 cfe-Mon, 18 Oct 1993 09:28:38
**Solution Text**
 Delta: cfe-db4486-ignore-tokens-on-old-hosts
 cfe-Thu, 21 Oct 1993 09:24:07
 
 Entered as OT 9429
 
 andi-Wed, 10 Nov 1993 10:53:22
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[11/10/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `9429'



CR Number                     : 9427
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : CM
Short Description             : file append gets ESTALE
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 9426
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : import
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 This is a problem introduced with the TSR protocol revs that attempt
 to handle the situation where an AFS_SetContext is called for the
 first time to a server from the secondary interface. In order for
 these changes to work AFS_SetContext calls had to be serialized. This
 has the side effect that it can cause an AFS_SetContext call in the
 primary interface to block an AFS_SetContext call in the secondary
 interface. Since calls on the primary interface are not protected from
 thread starvation this is bad news.
 Dimitris Varotsis
 Tue Oct 19 11:01:00 1993
 dimitris-Tue, 19 Oct 1993 11:01:06
**Solution Text**
 Delta: cfe-db4482-move-servermutex-to-connandreset 
 Backed by dfs-103 3.35
 cfe-Tue, 19 Oct 1993 15:40:48
 
 Entered as OT 9426
 
 andi-Wed, 10 Nov 1993 10:51:09
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[11/10/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `9426'



CR Number                     : 9421
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : EPISODE
Short Description             : There is no synchronization between volops and rm due to the inactive daemon
Reported Date                 : 11/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 10/13/93
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 9420
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : import
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[11/10/93 public]
**Description Text**
 Before the inactive daemon the entire rm operation was completed
 between a vol_StartVnodeOp vol_EndVnodeOp clause so it was
 synchronized with the volops. Due to the introduction of the inactive
 daemon the VN_RELE call is executed asynchronously thus it can race
 with a volop. This bug causes the kernel to panic when it tries to do
 the epif_ChangeLink call and finds the anode refcount to be 2 instead
 of 1. (volops get the anode without going through the vnode layer)
 Dimitris Varotsis
 Thu Oct 14 07:18:04 1993
 dimitris-Thu, 14 Oct 1993 07:18:10
**Solution Text**
 blake-db4491-volops-must-wait-for-inactive-daemon.
 
 Add synchronization so that vnm_StopUse can wait for inactive
 processing to complete before returning.
 blake-Mon, 18 Oct 1993 18:08:38
 
 Entered as OT 9420
 
 andi-Wed, 10 Nov 1993 10:46:35
**Validation Text**

[11/10/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[11/10/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `9420'



CR Number                     : 9321
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : test21 of fts doesn't set quota correctly.
Reported Date                 : 11/2/93
Found in Baseline             : 1.0.2
Found Date                    : 11/2/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 8700
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : import
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[11/2/93 public]
The mechanism used in test21 of fts test suite does not set the quota size of
a fileset (to be moved) and the size of remaining space  of the target
aggregate correctly. As a result, the size of the fileset in question could
be still smaller than the space left in the aggregate and the move would
succeed which defeats the test purpose. This could happen if the source 
aggregate is much smaller than the target aggregate. 
Furthermore, when the particular scenario fails, the test did not stop. The
subsequent two test scenarios that depend on the result of the first one also
fail. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/8/93 public]
Cancel it. It is a dup of OT8700. 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `8700'



CR Number                     : 9255
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs.block_frag
Short Description             : Test clobbers dfstab if set to edit.
Reported Date                 : 10/25/93
Found in Baseline             : 1.0.3
Found Date                    : 10/25/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 9194
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open

[10/25/93 public]
In systest/file/dfs.block_frag, the line:
 echo "# blkdev aggname aggtype aggid " \
  > $DCELOCAL/var/dfs/dfstab >> $DCELOCAL/var/dfs/dfstab
  echo "$DEVNAME      $AGGNAME        lfs     $AGGID" \
  >> $DCELOCAL/var/dfs/dfstab
Destroys the existing dfstab on RIOS.  It should save the current dfstab
first, then edit it.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[11/3/93 public]
Dup of 9194 (fixed)



CR Number                     : 9072
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : systest
Short Description             : dfs.glue should include kdestroys
Reported Date                 : 10/7/93
Found in Baseline             : 1.0.3
Found Date                    : 10/7/93
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 9043
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[10/7/93 public]

The many dce_logins done by dfs.glue create many creds files - especially
during long runs (48 CHO) on the hp. Let me know if you have questions
or problems fixing this Steve - thanks!

[10/08/93 public]
I've already got an OT open on this, for which I'm waiting for submission
approval.



CR Number                     : 9024
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : TEST SUITES
Short Description             : rep tests do multiple fts releases in quick succesion without checking that the replicas are back on-line
Reported Date                 : 10/5/93
Found in Baseline             : 1.0.2a
Found Date                    : 09/13/93
Severity                      : C
Priority                      : 3
Status                        : dup
Duplicate Of                  : 8057
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : some
Sensitivity                   : public
Transarc Status               : import
Transarc Deltas               : 
Transarc Herder               : andi@transarc.com

[10/5/93 public]
**Description Text**
 -------------------------------------------------------------------------------
 
 The replication tests do multiple fts releases in a row. The tests do
 not wait to make sure that the previous fts release on this fileset has completed and the R/O
 replicas are back On-Line before doing this test. 
 
 khale-Mon, 13 Sep 1993 16:15:12

[10/5/93 public]
  Moved from Transarc database and in dfs-osf-1.5
Added field Transarc Herder with value `andi@transarc.com' 
Added field Transarc Status with value `import'

[10/5/93 public]

[10/5/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `8057'



CR Number                     : 8879
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfstrace
Short Description             : Output of dfstrace lsset should be rationalized
Reported Date                 : 9/30/93
Found in Baseline             : 1.0.2
Found Date                    : 9/30/93
Severity                      : C
Priority                      : 3
Status                        : dup
Duplicate Of                  : 8666
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : open
Transarc Herder               : 

[9/30/93 public]
The dfstrace lsset command labels dormant event sets with the
words "inactive (dormant)". To prevent confusion with the inactive
state and to have it corresponding the the dfstrace setset option -dormant,
this label should be changed to "dormant"
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[10/6/93 public]
Work done under related delta.
Changed Defect or Enhancement? from `enh' to `def' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `8666' 
Added field Transarc Herder with value `'



CR Number                     : 8655
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : test/systest/dfs.lock
Short Description             : filewnr -D option no longer valid
Reported Date                 : 9/22/93
Found in Baseline             : 1.0.3
Found Date                    : 9/22/93
Severity                      : E
Priority                      : 4
Status                        : dup
Duplicate Of                  : 8449
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : test/systest/dfs.lock
Sensitivity                   : public

[9/22/93 public]
The dfs.lock_cl -d option enables the filewnr -D option, which has been
removed recently.  dfs.lock should drop the -d option, and possibly the
DEBUG_CLI environment variable.

[9/23/93 public]
Actually if the -v option is specified to the filewnr program, then the
filewnr clients will print extra info on arguements, etc.  So I think the 
DEBUG_CLI varible should stay, and the command line switch -d for dfs.lock
should enable filewnr -v.

[9/23/93 public]
This is actually a dup of 8449.  Dup'ed thusly.



CR Number                     : 8618
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 7526
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : desirable aggregate-import tool
Reported Date                 : 9/17/93
Found in Baseline             : 1.0.3
Found Date                    : 9/17/93
Severity                      : D
Priority                      : 3
Status                        : dup
Duplicate Of                  : 7594
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : open
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com

[9/17/93 public]
As discussed in lengthy detail in OT CR 7526, it might be pleasant to have
some sort of tool by which filesets and aggregates could be imported from
one cell to another.  I'm reproducing the discussion from OT CR 7526 here
since it's likely to be lost if the doc defect gets fixed.
-----------beginning of discussion from 7526
  [gmd 3/17/93 public]
  BUILD:	nb available 3/11/93 (aka dfs 2.8)
  CONFIG:	3 pmax cell, 2 flservers
  TEST:	admin_checklist
  Before syncfldb, ufs partition dfs exported with fileset id 0,,4.
  After syncfldb, fldbentry for above ufs partition gone.
  Expected behavior (which I believe I have seen - but perhaps only
  when conflict was between 2 lfs filesets?) is to have new fldbentry
  renumbered to next available fileset id.
  BEFORE:
  root@dce12> fts lsfldb
  dce12.u3
          readWrite   ID 0,,4  valid
          readOnly    ID 0,,5  invalid
          backup      ID 0,,6  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  dce12.osf.org       RW       /u3     0:00:00 hosts/dce12    <nil>
  epi.1
          readWrite   ID 0,,22  valid
          readOnly    ID 0,,23  invalid
          backup      ID 0,,24  invalid
  number of sites: 1
    Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
     server           flags     aggr   siteAge principal      owner
  valentine.osf.org   RW       val_u4_aggr 0:00:00 hosts/valentine<nil>
  root.dfs
          readWrite   ID 0,,1  valid
          readOnly    ID 0,,2  invalid
          backup      ID 0,,3  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  valentine.osf.org   RW       val_u4_aggr 0:00:00 hosts/valentine<nil>
  valentine.u2
          readWrite   ID 0,,7  valid
          readOnly    ID 0,,8  invalid
          backup      ID 0,,9  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  valentine.osf.org   RW       /u2     0:00:00 hosts/valentine<nil>
  ----------------------
  Total FLDB entries that were successfully enumerated: 4 (0 failed)
  LSHEADER AND SYNFLDB:
  root@dce5> fts lsheader -server dce5 -aggregate lfs_aggr2
  Total filesets on server dce5 aggregate lfs_aggr2 (id 3): 4
  Barasso.663              0,,10 RW      8 K alloc     22 K quota On-line
  Barasso.663.readonly     0,,11 RO     22 K alloc     22 K quota On-line
  dce5.u3                  0,,25 RW      9 K alloc      9 K quota On-line
  dce5_fset                0,,4 RW      9 K alloc      9 K quota On-line
  Total filesets on-line 4; total off-line 0; total busy 0
  root@dce5> fts syncfldb -server dce5
  -- done processing entry 1 of total 3 --
  Could not fetch FLDB entry (fs=0,,10, type=0)
  Error: FLDB: no such entry (dfs / vls)
          readWrite   ID 0,,10  valid
          readOnly    ID 0,,11  valid
          backup      ID 0,,12  invalid
  number of sites: 0    number of addresses: 1
     server           flags     aggr   siteAge principal      owner
  dce5.osf.org        RW,RO    lfs_aggr2 0:00:00                <nil>
  -- done processing entry 2 of total 3 --
  -- done processing entry 3 of total 3 --
  FLDB synchronized with server dce5
  AFTER:
  root@dce12> fts lsfldb
  Barasso.663
          readWrite   ID 0,,10  valid
          readOnly    ID 0,,11  valid
          backup      ID 0,,12  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  dce5.osf.org        RW,RO    lfs_aggr2 0:00:00 hosts/dce5     <nil>
  dce5.u3
          readWrite   ID 0,,25  valid
          readOnly    ID 0,,26  invalid
          backup      ID 0,,27  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  dce5.osf.org        RW       lfs_aggr2 0:00:00 hosts/dce5     <nil>
  dce5_fset
          readWrite   ID 0,,4  valid
          readOnly    ID 0,,5  invalid
          backup      ID 0,,6  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  dce5.osf.org        RW       lfs_aggr2 0:00:00 hosts/dce5     <nil>
  epi.1
          readWrite   ID 0,,22  valid
          readOnly    ID 0,,23  invalid
          backup      ID 0,,24  invalid
  number of sites: 1
    Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
     server           flags     aggr   siteAge principal      owner
  Error: communications failure (dce / rpc)
  valentine.osf.org   RW       1       0:00:00 hosts/valentine<nil>
  root.dfs
          readWrite   ID 0,,1  valid
          readOnly    ID 0,,2  invalid
          backup      ID 0,,3  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  Error: communications failure (dce / rpc)
  valentine.osf.org   RW       1       0:00:00 hosts/valentine<nil>
  valentine.u2
          readWrite   ID 0,,7  valid
          readOnly    ID 0,,8  invalid
          backup      ID 0,,9  invalid
  number of sites: 1
     server           flags     aggr   siteAge principal      owner
  Error: communications failure (dce / rpc)
  valentine.osf.org   RW       4       0:00:00 hosts/valentine<nil>
  ----------------------
  Total FLDB entries that were successfully enumerated: 6 (0 failed)
  NOTE: the communications failures are due to the fact that fts moves
  of the filesets appeared to have worked and I disabled the ftserver
  process on valentine, and successfully cd'd into /: (root.dfs now
  on dce5) - but the fldb never reflected this. Entering this as
  as separate CR, data here for reference only.
  root@dce12> cd /:
  root@dce12> ls
  dce12_u3  epi_1
  root@dce12> cm whereis /:/epi_1
  dfs: fileset (0,,22) is not available!
  cm: '/:/epi_1': No such device
  root@dce12> cm whereis /:
  The file '/:' resides in the cell 'dce12_cell', in fileset 'root.dfs',
  [cfe@transarc.com 3/18/93 public]
  I believe that the expectations are a little far-fetched.  In particular,
  there was already a fileset with ID 0,,4 in the FLDB, and ``syncfldb'' by
  definition says to change what's in the FLDB to agree with what's on the
  disk.  In this case, what was already on the disk (albeit in the dfstab
  file and not in the fileset itself) was a fileset on dce5 with ID 0,,4.
  Thus, I don't believe that this is a failure, but that it's pilot error:
  if there's a fileset with id X on a disk, then invoking syncfldb on that
  disk should (and must) update or overwrite the fldb entry for id X.
  I don't know of any case in which a free fileset ID will be selected by
  syncfldb.  Could we please cancel this one?
  Filled in Subcomponent Name with `fts'
  Filled in Interest List CC with `cfe'
  Filled in Responsible Engr. with `gmd'
  Filled in Resp. Engr's Company with `osf'
  Added field Transarc Deltas with value `'
  Added field Transarc Herder with value `'
  Added field Transarc Status with value `'
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	scomp: NULL -> fts
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	cc: NULL -> cfe
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	code: NULL -> gmd
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	codecomp: NULL -> osf
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	added/changed [gmd 3/17/93 public]
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	added/changed [cfe@transarc.com 3/18/93 public]
  +HISTORY Thu Mar 18 11:41:25 1993 cfe	added/changed [gmd 3/17/93 public]
  [gmd 3/18/93 public]
  I believe the (albeit uninspired) machine names may have caused some
  confusion here. DCE5 has an lfs aggregate with a fileset id from a
  previous fldb. My current fldb has the same fileset id for a ufs aggregate
  on DCE12. If I ask that my current fldb be sync'd to include the fileset
  info on DCE5's lfs aggregate, shouldn't I get a warning at least that I'm
  tromping out/tromped out an existing fldb entry ( in this case, one for
  a fileset on a different machine, DCE12)? I could've sworn I'd
  seen this type of conflict handled more gracefully by syncfldb before -
  ie. the fileset being added whose fileset id is currently in use gets
  the next available fileset id. In any event - if syncfldb is documented
  to be a "USE AT YOUR OWN RISK - COULD CAUSE DATABASE CORRUPTION" function then,
  yes, what I'm asking for is an enhancement - but if not, I think we
  should document this OR change the destructive behavior ...
  NOTE: this happened in a cell whose flservers (dce5 and dce12)
  BOTH thought they were the sync site.
  +HISTORY Thu Mar 18 12:20:23 1993 gmd	added/changed [gmd 3/18/93 public]
  [cfe@transarc.com 3/18/93 public]
  One of the intended uses of this command is to allow you to remove a disk
  from a hard-crashed machine and attach it to a working machine, and yet
  bring all the data from the disk back on-line as being served by the new
  machine.  In doing this, you want to overwrite all the FLDB entries that
  pointed to the disk's old location and now point them to this new location
  (new server, possibly a new aggregate ID).  That's the kind of repair
  that ``fts syncfldb'' is intended to handle, and that's exactly why the
  server/aggr information in the FLDB entries is overwritten so freely.
  Certainly the doc doesn't say that syncfldb issues warnings and requires
  confirmation.  From the 7.11 section of the DFS admin guide:
  ``The fts syncfldb command....checks the FLDB entry associated with
  the fileset to verify that the FLDB correctly records the fileset's status
  at the site.  If the FLDB is incorrect, fts syncfldb alters it; if no
  entry exists for an online fileset, it creates one.''
  Apparently, folks have gotten used to re-populating their FLDBs after
  a re-configuration by using fts syncfldb, and this should work pretty much
  OK (except that any replication information is lost).  It works out
  because all the filesets on-line in a cell should have been mutually
  consistent.  But, unfortunately, ``fts syncfldb'' isn't a general tool
  for gently incorporating aggregates of any source into a DFS installation;
  it will in fact overwrite existing data.  It's *meant* to overwrite
  existing data.  Perhaps there's a need for a tool to do this kind of
  gentle aggregate incorporation, but ``fts syncfldb'' isn't that tool, and
  trying to make it into that tool will only keep it from being the kind of
  tool that you need to make more gross configuration repairs.
  I believe that a gentler tool could be built, and could even have been
  partially simulated by hand, in that ``fts lshead'' on the newly-attached
  aggregate on dce5 would have shown you the filesets that existed there,
  and ``fts zap'' would have let you destroy the filesets that you didn't
  want before incorporating them into the cell.  What's not built is a tool
  to re-number filesets on a disk in preparation for syncfldb, or any kind
  of automatic collision-detector that would check that all the filesets
  on an aggregate have IDs different from any that are currently in the FLDB.
  Now, fts syncfldb does have some marginally-friendly behavior, in that if
  you're incorporating a fileset that has a fileset ID larger than any
  so far allocated in the FLDB, then syncfldb will ensure that the FLDB's
  high-water-mark for fileset IDs is increased to accommodate the filesets
  to be incorporated.  However, in no case has syncfldb ever re-numbered an
  existing fileset (LFS or otherwise) to give it an unused fileset ID.
  Again, I'm going to ask Jeff K. to take a crack at whether the fts syncfldb
  (and fts syncserv) documentation is sufficiently scary that this sort of
  behavior should have been acceptable.
  Changed Interest List CC from `cfe' to `cfe, jeff@transarc.com'
  +HISTORY Thu Mar 18 14:43:53 1993 cfe	cc: cfe -> cfe, jeff@transarc.com
  +HISTORY Thu Mar 18 14:43:53 1993 cfe	added/changed [cfe@transarc.com 3/18/93 public]
  +HISTORY Thu Mar 18 14:43:53 1993 cfe	added/changed [gmd 3/18/93 public]
  +HISTORY Thu Mar 18 14:43:53 1993 cfe	added/changed [cfe@transarc.com 3/18/93 public]
  +HISTORY Thu Mar 18 14:43:53 1993 cfe	added/changed [cfe@transarc.com 3/18/93 public]
  +HISTORY Thu Mar 18 14:43:53 1993 cfe	added/changed [gmd 3/18/93 public]
  [gmd 3/18/93 public]
  You're right Craig - the friendly behavior I thought I'd seen was NOT
  renumbering but accepting new/higher fileset id numbers.
  I would like to see syncfldb be the single tool for both the "moved" aggregate
  case you mention and the "reconfig" or "added" aggregate case I hit most often -
  a sysadmin could easily have to move an aggregate from a machine in one
  cell to a machine in another cell and not want to delete or renumber by
  hand filesets with duplicate fileset ids OR remember to use another tool ...
  SO ... we can mark this as a doc bug to warn unsuspecting sysadmin's of the
  damage they can do - if it's already well doc'd, can we make this a post-1.0.2
  enhancement request to make syncfldb friendlier and smarter?
  +HISTORY Thu Mar 18 16:36:05 1993 gmd	added/changed [gmd 3/18/93 public]
  [rsarbo 3/18/93 public]
  I vote with Gail.  At the very least, syncfldb should be better
  documented (i.e. with clear warnings about its limited intelligence)
  for 1.0.2.  Craig may remember OT4467 which describes problems
  I had with syncfldb in the 1.0.1 timeframe that sound an awful lot
  like the problems Gail was seeing except in my case syncfldb blew
  away my root.dfs FLDB entry.  2 folks at OSF have used syncfldb now
  and both ran into the same "pilot error".  Sounds conclusive enough
  to justify a doc change to me and probably an open OT defect/enhancement
  for 1.0.3.
  +HISTORY Thu Mar 18 18:03:19 1993 rsarbo	added/changed [rsarbo 3/18/93 public]
  [cfe@transarc.com 3/19/93 public]
  Well, I understand that both Ron and Gail have stumbled into similar
  disasters with syncfldb overwriting existing fldb entries when importing
  essentially-foreign aggregates, and I agree that this is a situation that
  should bear some specific warning.  I spoke with Jeff Kaminski, who claims
  that the documentation for 1.0.2 froze a while ago and thus it wouldn't
  be possible for a warning there, but we can probably all come up with a
  release note that addresses the issue, and some kind of suggestion for
  work to be addressed in a later release (e.g. 1.0.3).
  All that granted, though, I don't know how syncfldb could change to handle
  both points of view, and I'm afraid I keep wondering how syncfldb came
  to be viewed as the tool to use to import foreign aggregates.  As I say,
  one could come up with a tool to import foreign aggregates--possibly an
  fts sub-function--but its purpose honestly seems to be to be at odds with
  the kinds of repairs that syncfldb is supposed to be authorized to do.
  Can the OSF offer any enlightenment here, as to how these two functions
  are viewed as being such close relatives?  For concrete examples, consider
  two cases:
  	(a) an LFS fileset with an ID already in the FLDB for a different
  	    server/aggregate.
  	    (a1) syncfldb overwrites the existing FLDB entry, and has to;
  	    (a2) aggregate-import ideally wants to create a new FLDB entry
  		 and renumber the LFS fileset with the new ID number, but
  		 this might not be possible if the fileset is locally mounted.
  	(b) a UFS fileset exported with an ID that's already in the FLDB
  	    for a different server/aggregate.
  	    (a1) syncfldb, again, overwrites the existing FLDB entry, and
  		 has to do that to do its job;
  	    (a2) aggregate-import ideally will create a new FLDB entry and
  		 request that the administrator edit the server's
  		 /opt/dcelocal/var/dfs/dfstab file to match, after first
  		 detaching (un-exporting) the UFS aggregate and then
  		 re-attaching (re-exporting) it.
  The whole rationale behind ``syncfldb'' is to say that, for the filesets
  on the server/aggregate(s) named in the fts syncfldb command, the
  information in the FLDB is wrong and the information on the aggregate(s)
  is right.  This is fundamentally different from the foreign-aggregate-import
  problem, where an aggregate was created using IDs from some other numbering
  system and you now want to import it into a DCE DFS cell.  Is this helping
  to clarify my point of view here?  Can the OSF help by telling me how these
  two functions are now viewed as so similar?  (Thanks!)
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [cfe@transarc.com 3/18/93 public]
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [gmd 3/18/93 public]
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [rsarbo 3/18/93 public]
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [cfe@transarc.com 3/19/93 public]
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [cfe@transarc.com 3/18/93 public]
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [gmd 3/18/93 public]
  +HISTORY Fri Mar 19 10:15:11 1993 cfe	added/changed [rsarbo 3/18/93 public]
  [gmd 3/19/93 public]
  In general, I feel anything that has a destructive side effect should either
  be well documented and/or have a warning and/or require acknowledgement
  of the destruction to occur.
  I view the two functions as similar because, in both cases, there is
  header information that needs to be recorded/reflected in the FLDB.
  I think there'll be plenty of cases where the FLDB is half of what
  the sysadmin wants and the aggregate header is the other half ...
  I believe most system administators will think in terms of all the filesets
  they manage, whether in one cell or many, by name and NOT in terms of
  cell-specific fileset id's.
  I believe the 2 functions could be handled if the software DID note
  fileset names AND fileset ids and allowed the system admin the option
  to alter or overlay name and fileset id conflicts.
  For instance:
  aggregate has:	fileset id 4 for fileset pmax.bin
  		fileset id 7 for fileset usr.gmd
  		fileset id 10 for fileset user.gail
  fldb has:	fileset id 7 for fileset user.gail
  		fileset id 4 for fileset pmax.bin on dead server
  fts syncfldb says:
  	id conflict: fileset id 0,,7
  	overlay user.gail entry with usr.gmd? (y/n) n
  	add new fileset usr.gmd? (y/n) y
  fileset 0,,13 created ...
  	id conflict: fileset id 0,,4
  	overlay pmax.bin entry with pmax.bin? (y/n) y
  	name conflict: user.gail
  	overlay user.gail id 0,,7 with user.gail id 0,,10? (y/n) n
  	add fileset 0,,10 with new name? (y/n) y
  	name for fileset id 0,,10 : user.marie
  You could have an "-y" switch to turn off interactive portion and just
  answer "yes" to everything. If it's easier for this to be a separate
  tool from synfldb, that's fine with me. Let me know what you think
  Craig - you and Tony and Elliot probably have more DFS sys admin experience
  than I do.
  +HISTORY Fri Mar 19 16:38:06 1993 gmd	added/changed [gmd 3/19/93 public]
  [jeff@transarc.com 3/22/93 public]
  Craig and I discussed the fts syncfldb command as it relates to the problem
  encountered at OSF.  The documentation can potentially be updated to better
  document the somewhat confusing effects of using the command to "move" a
  fileset from one cell to another.  For now, the best I can do is a release
  note that provides the following admonitory information:
     The fts syncfldb command is used to synchronize the filesets that reside on
     an aggregate with the FLDB of the cell in which they reside.  The command
     creates FLDB entries for filesets that do not have them; it also updates
     FLDB entries to record the current locations of filesets whose entries
     do not accurately record their present sites.  However, the fts syncfldb
     command is inappropriate for creating FLDB entries for existing filesets
     from one cell in the FLDB of another cell, a situation that can occur if
     a disk is physically moved from a server machine in one cell to a server
     machine in another cell.
     For example, suppose fileset Foo has fileset ID number 0,,8 in the FLDB of
     cell A.  If the disk that houses fileset Foo is removed from its server
     machine in cell A and installed on a server machine in cell B, the fts
     syncfldb command should not be used to create an entry for the fileset in
     the FLDB of cell B:  If fileset Bar with ID number 0,,8 already exists in
     cell B, the command updates its FLDB entry to reflect the location and
     information associated with fileset Foo, making it impossible to access
     fileset Bar.  Issuing the command on the aggregate that houses fileset Bar
     corrects the entry for that fileset, but fileset Foo is then no longer
     accessible.  Because of potential fileset ID number conflicts of this type,
     there is no convenient way to move a disk partition from a machine in one
     cell to a machine in a second cell and update the FLDB of the second cell.
  This is a first draft of a possible release note.  Please feel free to provide
  any corrections or clarifications to this information, as necessary.  I would
  like to include information about this problem in the release notes sometime
  in the next day or two.  Sorry for any confusion; thanks for any help.
  +HISTORY Mon Mar 22 11:27:04 1993 jeff	added/changed [cfe@transarc.com 3/19/93 public]
  +HISTORY Mon Mar 22 11:27:04 1993 jeff	added/changed [gmd 3/19/93 public]
  +HISTORY Mon Mar 22 11:27:04 1993 jeff	added/changed [jeff@transarc.com 3/22/93 public]
  +HISTORY Mon Mar 22 11:27:04 1993 jeff	added/changed [cfe@transarc.com 3/19/93 public]
  +HISTORY Mon Mar 22 11:27:04 1993 jeff	added/changed [gmd 3/19/93 public]
  [gmd 3/22/93 public]
  Jeff - The Release Note looks good - it's a confusing topic. Could you add
  a short paragraph about what you SHOULD do when there is a fileset id
  number conflict and syncfldb is not appropriate? Ie. how does one change
  the cell A fileset id of Foo to add it to cell B? Thanks.
  Craig - Are we changing this CR to a doc one and adding a separate CR for
  the code enhancement request? Or do you feel the manual steps Jeff will
  describe above are sufficient to cover this?
  +HISTORY Mon Mar 22 11:50:44 1993 gmd	added/changed [gmd 3/22/93 public]
  [cfe@transarc.com 3/22/93 public]
  I believe that I understand the suggestion that Gail is making; I even think
  that I understand its motivation.  I still feel strongly that syncfldb
  isn't a general aggregate-import tool, and that it's weakened by adding
  that requirement to it.  I have nothing against the creation of a tool
  that would do this aggregate-import function, and am happy to deal with
  an enhancement request to that effect.
  We haven't even discussed all the problems with importing filesets into
  a new cell.  For instance, if there are any ACLs at all, even ones that
  are created implicitly, in the fileset, you can't naively import such
  filesets into a cell, because the UUID of the cell will be different from
  the cell UUIDs stored in the ACLs.  Now, it's possible to re-create a cell
  with the same UUID as that of the previous cell (which it must therefore
  be replacing), in which case the operation makes sense.  But the naive
  importing of live filesets will simply fail, for well-understood reasons.
  It continues to make sense to detach a disk from one server and attach
  it to another server in the same cell, then running syncfldb; it
  continues to make sense to use syncfldb to repair some classes of damage
  that might be encountered if fileset operations are interrupted at
  inopportune moments; and it continues to make sense to use syncfldb to
  recreate an FLDB after installing a new cell (with the same UUID as before).
  But, by definition, all concurrently-active cells must have different UUIDs,
  so it doesn't make sense to import filesets across cell boundaries, since
  the UUIDs will no longer match up.  Interestingly, fileset dump and restore
  will preserve complete ACLs, with cell UUIDs, so dump and restore can't be
  used naively to move fileset contents.  One thing that will work today is
  to tar together the contents of the filesets on a disk, move the disk,
  clean it completely with ``newaggr'', re-create the filesets with
  ``fts create'', and restore the non-ACL contents of the filesets via tar.
  This isn't ideal, but to do the job right would require the translation
  of arbitrary ACLs, which is quite a tough problem.
  So, in direct answer to Gail's questions, yes, in my opinion, this has
  turned into a doc defect report, and there might be a code enhancement
  request yet to be created, also.  So far we've resisted the temptation
  to make any of the fts commands interactive (going so far as to remove
  a question that ``fts restore'' used to ask), in hopes of making this
  command suite more useful for script-writing--if you were looking for
  any kind of existing command suite style guide.  I'm hoping that I've
  clarified some of the difficulties to be faced by an aggregate-import
  function, or even a fileset-import function, as motivation for keeping
  the syncfldb/import functions distinct (as I thought they were in this
  first round of tools).
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [gmd 3/19/93 public]
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [jeff@transarc.com 3/22/93 public]
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [gmd 3/22/93 public]
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [cfe@transarc.com 3/22/93 public]
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [gmd 3/19/93 public]
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [jeff@transarc.com 3/22/93 public]
  +HISTORY Mon Mar 22 13:16:37 1993 cfe	added/changed [gmd 3/22/93 public]
  [gmd 3/25/93 public]
  Okay, this CR is now a doc one assigned to Jeff at Transarc. Up'd the
  priority to 1 to reflect the fact that the Release Note has to be done
  in order to ship.
  The enhancement request is CR 7594.
  +HISTORY Thu Mar 25 14:23:34 1993 gmd	area: code -> doc
  +HISTORY Thu Mar 25 14:23:34 1993 gmd	prior: 2 -> 1
  +HISTORY Thu Mar 25 14:23:34 1993 gmd	code: gmd -> jeff
  +HISTORY Thu Mar 25 14:23:34 1993 gmd	codecomp: osf -> tarc
  +HISTORY Thu Mar 25 14:23:34 1993 gmd	added/changed [gmd 3/25/93 public]
  [jeff@transarc.com 4/10/93 public]
  I included the following release note, with some slight modifications, in the
  DCE 1.0.2 Release Notes:
  *** Begin Release Note ********************************************************
  The documentation needs to state that moving a fileset from one cell to another
  is not supported with DFS.  Neither of the following fileset operations is
  supported:
   o Dumping a fileset in one cell and restoring it in another cell.
   o Moving the physical disk on which a fileset resides from a machine in one
     cell to a machine in another cell.
  Moving a fileset from one cell to another causes the following types of
  problems:
   1. The ACLs on file and directory objects in the fileset prohibit access to
      the objects because they are essentially "foreign" ACLs in the new cell.
      Because all ACLs contain the UUID of the cell in which they are are
      created, objects that have ACLs cannot be moved between cells.
   2. The fileset's ID number may conflict with the ID number of a fileset in
      the new cell.  A fileset's ID number is unique within the cell in which
      it is created; it may not be unique in another cell because a different
      fileset may already have that ID number in the other cell.
  Note that if a physical disk is moved from a machine in one cell to a machine
  in a different cell, the fts syncfldb command is inappropriate for creating
  FLDB entries for existing filesets on the disk in the FLDB of the new cell.
  The fts syncfldb command synchronizes the filesets that reside on an aggregate
  with the FLDB of the cell in which they reside.  It creates FLDB entries for
  filesets that do not have them; it also updates FLDB entries to record the
  current locations of filesets whose entries do not accurately record their
  present sites.
  For example, suppose fileset foo has fileset ID number 0,,8 in the FLDB of cell
  A.  Suppose also that fileset bar has fileset ID number 0,,8 in the FLDB of
  cell B.  If the disk that houses fileset foo is removed from its server machine
  in cell A and installed on a server machine in cell B, the fts syncfldb command
  should not be used to create an entry for the fileset in the FLDB of cell B:
  The command updates the entry for fileset bar to reflect the location and
  information associated with fileset foo, making it impossible to access
  fileset bar.  Issuing the command on the aggregate that houses fileset bar
  corrects the entry for that fileset, but fileset foo is then no longer
  accessible.  Such fileset ID conflicts prohibit the moving of a disk partition
  from a machine in one cell to a machine in a second cell.
  *** End Release Note **********************************************************
  Because the crux of this defect has evolved into the need for better
  documentation of intercell fileset moves, I changed the one-line description
  accordingly.  I also lowered both the priority and the severity of the defect
  because it has been documented in the DCE 1.0.2 Release Notes.
-----------end of discussion from 7526
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[3/29/94 public]
Dup'ing to 7594.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `7594'



CR Number                     : 8551
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs
Short Description             : Fileset has two quota statistics
Reported Date                 : 9/2/93
Found in Baseline             : 1.0.2
Found Date                    : 9/2/93
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 6358
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed

[9/2/93 public]
Each read/write fileset has both a "visible" or effective quota and
an "allocated" or actual quota.  Currently, only the former quota
is documented; therefore, the documentation must be updated to
include both quotas.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[9/2/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `6358' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 8490
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : hppa
S/W Ref Platform              : hpux
Component Name                : dfs
Subcomponent Name             : dfs_config
Short Description             : rc.dfs loses execute rights
Reported Date                 : 8/20/93
Found in Baseline             : 1.0.3
Found Date                    : 8/20/93
Severity                      : D
Priority                      : 3
Status                        : dup
Duplicate Of                  : 8450
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : dfs_config
Sensitivity                   : public

[8/20/93 public]

 rc.dfs loses its execution priveledges during the initial configuration
 of DFS.  Therefore, a START after shutting down the machine will not
 executre rc.dfs.  Within the routine enable_in_dfs_rc in dfs_config,
 the line:

           { mv /tmp/rc.dfs$$ $RCDFS;

 should be changed to

           { cp /tmp/rc.dfs$$ $RCDFS;

 to correct the problem.

[08/20/93 public]
I think this is a duplicate of one you filed Ron....



CR Number                     : 8469
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : command_ref
Short Description             : Remove -bakgroup option from bakserver command
Reported Date                 : 8/18/93
Found in Baseline             : 1.0.2
Found Date                    : 8/18/93
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 8420
Fix By Baseline               : 1.2
Fixed In Baseline             : 
Affected File(s)              : See description
Sensitivity                   : public
Transarc Deltas               : 
Transarc Status               : closed
Transarc Herder               : 

[8/18/93 public]
The -bakgroup option has been removed from the bakserver command.
This removal must be reflected in the documentation.
Added field Transarc Deltas with value `' 
Added field Transarc Status with value `open'

[8/18/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `8420' 
Changed Transarc Status from `open' to `closed' 
Added field Transarc Herder with value `'

[9/14/95 public]

Reassigning this to Bill Lawrence, who I think is the right
person.



CR Number                     : 8213
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : admin ref
Short Description             : fts crserverentry principal is described incorrectly
Reported Date                 : 6/29/93
Found in Baseline             : 1.0.2
Found Date                    : 6/29/93
Severity                      : C
Priority                      : 3
Status                        : dup
Duplicate Of                  : 8084
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : fts_lsserverentry.8dfs
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : closed

[6/29/93 public]
    <Note by kwalker (Ken Walker), 93/03/16 17:23:20, action: open>
On the man page for 'fts crserverentry', the following description
is wrong:
-principal name
        Specifies the abbreviation for the DFS server principal
        to be registered in the FLDB for the machine.  The machine's
        principal name in the Registry Database must match this
        name.
While coding the mkdfs command, we tried two different Registry
principals, neither of which worked:  'hosts/hostname/self' and
'hosts/hostname/dfs-server'.  What did work was work (and this
is what dce_config uses) was 'hosts/hostname', which is not a
Registry principal.
I don't know what is really meant here--I'll leave that to
Transarc or the DFS team to puzzle out.

[6/29/93 public]
This is an exact duplicate of defect 8084.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `8084' 
Filled in Responsible Engr. with `jeff@transarc.com' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `closed'



CR Number                     : 8136
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : du -s on UFS fileset panics pMax
Reported Date                 : 6/10/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/10/93
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 7780
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[6/10/93 public]
Scenario:  2 pMax cell with 1-flserver, 2 px DFS filesystem running since
May 23 ( no kidding) for admin systest work.  Each server has several LFS
and UFS partitions, until the time of the incident only the LFS partitions
have been exercised, AT ALL, in fact I never went into the UFS partitions
through the glue.
 
So:  I cp -R  dfs1.0.2/src/test/directory/xds into fileset prelude_ufs2,
mounted over /u2 on prelude, do a fts dump out of it, do a restore into
another UFS partition on the same machine.  Get rpc timeout error on
the restore into the /u1 partition, mounted as prelude_ufs1.  In the
process of documenting this error for what will eventually be yet another
OT report, I thought I'd find out how big the fileset I'd originally
dumped was:
 
>cd /:
>ls
prelude_ufs2 other_fileset    other_fileset
 
plus others . . .
 
>ls prelude_ufs2
xds
>ls prelude_ufs2/xds
file   file   directory  directory
 
du -s prelude_ufs2
panic: ufs_write type
Kdb kernel trap [du]: Interrupt trap, code=0
thread 0xc1567744, pcb 0xc3ad9c08
panic from  ufs_write +1c4
ufs_write from cm_UFSCacheRDWR+74
	  from cm_CFileRDWR+30
	  from cm_CFileRW+48
	  from cm_CacheFetchProc+a8e
	  from AFS_Readdir+70c
	  from cm_FetchDCache+5c4
	  from cm_GetDOnline+470
	  from cm_GetDLock+6fc
	  from cm_readdir+510
	  from nosf_readdir+74
	  from xglue_readdir+d0
	  from getdirentries+11c
	  from syscall+27c
Hard bang.  By the way, when I rebooted, both the /u2 and  /u1 partitions 
had to be newfs'd, since fsck couldn't fix them  up (and I ran it twice) 
after the aborted restore.  
 
  I will be trying to reproduce this, naturally, since I will be trying to
reproduce the error that led to this one.  Any suggestions are welcome,
especially what info would be interesting in Pittsburgh.  I will be turning
any and all tracing on . . .

[6/10/93 public]
I believe that someone at the OSF will need to debug this as it occurs on
the OSF/1 platform, and dies in OSF/1 specific code.  None the less, I'm
assigning it to Craig so that he can describe what type of things to look 
for and what type of additional information will help track down the problem.
Changed Responsible Engr. from `Elliot_Jaffe@transarc.com' to 
 `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[6/10/93 public]
It appears the panic occurred when the CM was trying to write a 
just fetched chunk into the on-disk cache.  Dave will have to confirm 
this, but my guess is Dave's /opt directory lived on /u1, and when /u2 
was restored onto /u1 all the DCE stuff got blown away.  This would 
obviously include the DFS on disk cache and would explain the panic.
 
If this is truly the case, perhaps the way to prevent this in the
future is to prevent users from restoring onto a mounted UFS fileset.
On second thought this might not 't work since a UFS fileset must be 
mounted locally before it can be dfsexported and attached to the DFS 
space, though I'm not sure why that is a requirement.
 
In any case, there should be some kind of safety checks to prevent
users from restoring onto a live UFS fileset (perhaps a check
analogous to the one done at unmount time that returns EBUSY if
there's any activity on the fileset?).

[6/10/93 public]
My conjecture was also that Dave had restored on top of the UFS partition
housing his DFS cache.  If that's really not what it was, the next thing
is to figure out where in ufs_write() it's crashing, and why--what code
it is around ufs_write+1c4, what source code that corresponds to, and like
that.
 
As to Ron's suggestion about interlocking against UFS restorations, there
is code like that in the filesystem-specific VOL_OPEN processing that should
be doing this.  I don't know why it doesn't work.
 
I guess I'd treat this either as pilot error or an OSF/1 UFS-volops bug.
Filled in Interest List CC with `cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `treff@transarc.com' 
Changed Resp. Engr's Company from `tarc' to `osf'

[6/10/93 public]
oops, got the assignment wrong
Changed Responsible Engr. from `treff@transarc.com' to `treff'

[6/10/93 public]

Unfortunately, Ron, you're right -- there goes my opt partition . . . so
what this is is a dup of the general run of 7780/7331/7312, related to
6787/6049 as an Ease-of-Use/Pain-in-the-Keister thing.  Right: it sez so
in the documentation, you're strongly urged not to restore into a
non-empty, mounted fileset, LFS or UFS.  I know: I've done this several times, 
myself.  Pilot error: sure.  But should fts be fixed so that one CAN'T restore 
into  a mounted fileset, and a full restore only if empty, and detect 
restores into filesets with incrementals missing?  Absolutely, and in
1.0.3.  Dup'ed to 7780.



CR Number                     : 8117
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : spin in rep test2
Reported Date                 : 6/9/93
Found in Baseline             : 1.0.2a
Found Date                    : 6/9/93
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 8042
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[6/9/93 public]
I modified the replication rtest3 to serialize the fts release's
in the appropriate place as discussed with Craig.  I ran the
replication tests overnight in a loop.  On the first iteration
of the loop, rtest 1 and 2 passed; rtest3 failed with a cut and
paste error on my part.  On the second iteration, rtest1 passed;
rtest2 is spinning in an fts update on Stevens.1847.  Both
the ftserver and the fts update are accumulating CPU time
as of this morning.  I do not believe this problem is related
to my changes in rtest3, but perhaps the cause of the problem
is the same as the cause I was trying to address with the
changes to rtest3.
 
My feeble interpretation of what's going on is that the repserver
on barge made a FTSERVER_Forward() rpc call to the ftserver on
shotz to suck over an incremental dump of the (distinguished?) RO.
The ftserver attempted to fetch the dump, and failed with 
"repserver forward fileset" (i.e. the ftserver thinks the R/O is 
busy).  lsheader on shotz shows no evidence that this R/O fileset 
even exists.  lsreplicas appears to show it with flags 0x20081.  
I'm having trouble interpreting these flags.  They don't appear to 
correspond to the "states" defined in volume.h.  In fact, tips on 
interpreting any of the information coming from lsreplicas would be useful.
 
I presume the spinning action is simply the repserver on barge
retrying the transaction over and over again (perhaps at the
prompting of the fts update which would explain why it's
accumulating CPU time), but the ftserver keeps telling it
the (primary, distinguished?) R/O is busy.  I don't see
the activity in the RepLog on barge that would corroborate
this theory, though.
 
The raw data including the Ftlog, the verbose test output, and
the output from fts lsfldb, lsreplicas, lsheader, statftserver,
statrepserver is in ~notuser/rsarbo/rep0609.  I notice the
verbose output is apparently not keeping up with the test
itself.

[6/18/93 public]
It's about time that I looked into this!
I agree with Ron that his rtest3 changes didn't cause the deadlock.  The
most that it caused was the Rubble.<PID>.[12345] filesets, with
maybe some repserver activity in the background.
 
The runtests.log file isn't tremendously interesting, since as Ron
noted the verbose output isn't kept up-to-date there.  Instead, it's
kept up-to-date in some file like /tmp/rep.1.<PIDnumber> by the
runtests program, and it's appended to runtests.out when a given
test exits.  It's a little too bad, as it was in the second rtest2
run that this failed.  But only a little, since it wouldn't have helped
that much anyway.
 
The situation as of the fts.diag.out status and the FtLog is basically
this--not that different from Ron's analysis.  The repserver on barge
is in the middle of an RPC call to forward a dump of the Stevens.1847
R/W fileset to its local instance of Stevens.1847.readonly.  That repserver
on barge is making this call to the ftserver on shotz.  My surmise is that
something went wrong with the restoration process.  But as discussed at
the end of OT 8042, the ftserver services FTSERVER_Forward() calls by
pthread_create()ing a helper thread.  The parent thread calls the
remote FTSERVER_Restore() procedure (from the ftserver on shotz to the
ftserver on barge), something went wrong there, and the ftserver on
shotz is trying to clean up.  It's doing a pthread_join() trying to
clean up its sub-thread.  The sub-thread, meanwhile, is trying to write
to a local in-core ``pipe'', which is used for buffering between the
dumper and the restorer.  It wasn't a pipe exception that killed the
restoration process, but was rather some other, not-yet-logged error code.
(An exception would have caused a log message to be generated.)
Thus, the call executor never finishes the RPC service call, and the
repserver on barge never gets an answer other than ``still working on
your call; don't quit yet''.  Thus, the repserver on barge doesn't ever
fail the call (and it would be tough to have a timeout here, since the
_Forward call could legitimately take a huge amount of time).  Meanwhile,
the ftserver on shotz continues to hold the reference count high on its
in-core transaction, so that the transaction-GC'er can't delete it.  And
even though the shotz/Stevens.1847 R/W fileset has long since been
declared no-longer-busy by the kernel on shotz, the ftserver on shotz
still has its active transaction, so that when the repserver on shotz
goes to ask for a dump of data from the R/W, the shotz ftserver gives it the
error code that the barge repserver gave it when preparing for its
FTSERVER_Forward() call.  Of course, that code translates to
``repserver forward fileset''.
 
All the processes in this case (shotz repserver, shotz ftserver, barge
repserver) are in some level of loop and will accumulate CPU time.  The
fileset is set for scheduled replication, so it's the duty of the shotz
repserver to attempt to update it, particularly since it hasn't updated it
for well over the target siteMaxAge.  The barge repserver is stuck in
the RPC runtime, probing the shotz ftserver to ask if the call is still
in progress.  The shotz ftserver is continuing to run its background
transaction-GC'er as well as answering the probes from the barge repserver.
 
As to the output of fts lsrep/fts statrepserver, indeed the ``flags'' word
is a collection of the bits defined in src/file/rep/repser.h (yes, it's
probably installed in usr/include/dcedfs): the symbols starting with
repflag_XXX.  For example, the 0x20001 bits are repflag_HaveWVTRead and
repflag_KnowVLDB.  There's also a ``volstates'' word that, if present,
contains the fileset states bits of the published replica, should that
be interesting.  Here's a quick summary of one of the status messages:
   On barge.osf.org:
[message from the repserver on barge.osf.org]
   Stevens.1847, cell 134240781,,1166149805: src 0,,39 (lfs_aggr2) (on shotz.osf.org) => barge.osf.org 0,,40 (lfs_aggr)
[Here's a work item.  The fileset is named Stevens.1847, the DFS ID of the
cell is as listed.  The source of data is fileset 0,,39 on aggregate lfs_aggr2
of the machine shotz.osf.org, and the destination being managed is fileset
0,,40 on barge.osf.org's aggregate named lfs_aggr.]
      flags 0x20001, volstates 0.  NumKAs 0; lastKA sweep=Wed Dec 31 19:00:00 1969
[The repserver's internal repflag_XXX bits are 0x20001.  There are no available
fileset states.  There are no vnodes being kept alive for this fileset by
this repserver.]
      srcVV: 0,,219; curVV: 0,,0; WVT ID = 739577335,,1111
[The fileset version of the data source is 0,,219, and the fileset version
of the fileset being managed locally is unknown.  Since repflag_HaveWVTRead
is set, the token ID of the whole-fileset token is as given.]
      Lost token 7821871 ago; token expires -31471 hence; new version published 739633996 ago
[The last time that the token was lost was many seconds ago (probably never).
The token has already expired (31471 seconds ago).  There was never a new
version of this fileset published.]
      vvCurr 731812125.766556 (7821871 ago); vvPingCurr 739588125.845792 (45871 ago)
[These are copies of variables that the repserver maintains for the fileset
in the kernel, indicating when the repserver last made contact, or tried
to make contact, with the R/W for this fileset.  They're timestamps.]
      Last update attempt 739588127.287495 (45869 ago); next scheduled attempt 0.000000 (-739633996 hence)
[This repserver last tried to obtain an update from the source ftserver at
the given timestamp.  Since the next-scheduled-attempt is zero, the
repserver is supposed to attempt to get one of these updates ASAP.]
      Status msg: Calling FTSERVER_Forward() on primary
[This is just the last message dumped into the work-item structure by the
repserver, more or less as a trace to say what was most recently attempted
or achieved on behalf of this replica.]
 
The bottom line is that I believe that this problem, such as it is, will be
fixed as part of the fix for 8042.  This is probably a better capturing of
the problem itself, but I have work enqueued under that OT.
Filled in Interest List CC with `cfe@transarc.com' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `8042' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'



CR Number                     : 8098
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 3551
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : host
Short Description             : the scout does not correctly update the hosts
Reported Date                 : 6/7/93
Found in Baseline             : 1.0.2
Found Date                    : 6/7/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5722
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[6/7/93 public]
The fshost package does not update the number workstation hosts correctly.
This is shown in scout. 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[7/14/93 public]
This is essentially a dup of ot5722.
Elliot, please cancel it. 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5722'



CR Number                     : 8024
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : repserver
Short Description             : fts release doesn't return
Reported Date                 : 5/20/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/20/93
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 7797
Fix By Baseline               : 1.0.3
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[5/20/93 public]
 
BUILD:	dfs.carl
CONFIG:	4 mach cell, 3 pmax, 1 rios - 3 flservers, 4 ftservers
TEST:	dfs.repfs_checklist
 
relevant steps:
	- detach aggregate housing read-only replicas
	- attempt fts release of fileset whose staging replica is detached
	- note that command never returns but you can ctl-c out of it
 
root@valentine> dfsexport /dev/rz1e -detach
dfsexport: Revoking tokens for filesets on aggregate 2...
root@valentine> fts release epi.a
The fileset 0,,54 could not be released: No such file or directory
(ctl-c)
root@valentine> fts release epi.a
fts: Using unauthenticated connection for cell /.../p102a_cell.qadce.osf.org: auth ticket expired (dce / rpc).
Could not lock FLDB entry (vol=0,,54, type=0, op=32)
Error: FLDB: status report: last release was aborted (dfs / vls)
(ctl-c)
 
root@dce5> dce_login cell_admin -dce-
Password must be changed!
root@dce5> fts release epi.a
Could not lock FLDB entry (vol=0,,54, type=0, op=32)
Error: FLDB: status report: last release was aborted (dfs / vls)
The fileset 0,,54 could not be released: No such file or directory

[5/20/93 public]
I had erroneously thought the repservers were healthy but
after reattaching the aggregate, I'm still having trouble. Specifically,
fts addsite to the rios hangs and fts statrep of the repserver on the
rios hangs. (You can ctl-c out of both). After receiving "rpc call to
(valentine addr in cell blah) timed out messages, a second attempt at
fts addsite doesn't hang, but errors out correctly. Still hang during
fts statrep -server cobbler -long. Note the "release clone" entry in
the fts lsfldb output for epi.a.
 
root@valentine> fts addsite epi.2 -server cobbler -aggr lfs_aggr4
(ctl-c)
root@valentine> cd /:/epi_a
dfs: Warning: the rpc call to server 130.105.201.7 in cell p102a_cell.qadce.osf.org timed out.
root@valentine> dfstrace dump > /tmp/rpc_call_to.dump
root@valentine> dfstrace clear
root@valentine> cd /:/epi_a
root@valentine> ls
dce.clean          dce_config.1105    dce_config.test    timestamp
dce.clean.new      dce_config.getip   rc.dce
dce.rm             dce_config.rsarbo  rc.dce.orig
dce_config         dce_config.sav     rc.dce_debug
root@valentine>  fts addsite epi.2 -server cobbler -aggr lfs_aggr4
There is already a R/O replica on cobbler.osf.org (lfs_aggr4).
Error in addsite: status ffffffff (unknown facility) (unknown)
root@cobbler> cm whereis /:/epi_a
dfs: Warning: the rpc call to server 130.105.201.7 in cell p102a_cell.qadce.osf.org timed out.
The file '/:/epi_a' resides in the cell 'p102a_cell.qadce.osf.org', 
in fileset 'epi.a.readonly', on hosts valentine.osf.org, cobbler.osf.org.
root@cobbler> cd /:/epi_a
root@cobbler> ls
dce.clean          dce_config.1105    dce_config.test    timestamp
dce.clean.new      dce_config.getip   rc.dce
dce.rm             dce_config.rsarbo  rc.dce.orig
dce_config         dce_config.sav     rc.dce_debug
 
config info:
valentine (pmax) 
fts aggrinfo -server valentine
LFS aggregate lfs_aggr (/dev/rz1a): 96896 K free out of total 102392
LFS aggregate lfs_aggr2 (/dev/rz1e): 336170 K free out of total 342376
Non-LFS aggregate /u2 (/dev/rz1b): 49823 K free out of total 89145 (9905 reserved)
fts lsheader -server valentine -aggr lfs_aggr2
Total filesets on server valentine aggregate lfs_aggr2 (id 2): 3
epi.2                    0,,25 RW   1605 K alloc   1605 K quota On-line
epi.a.readonly           0,,55 RO   1063 K alloc   1063 K quota On-line
epi.b.readonly           0,,58 RO     17 K alloc     17 K quota On-line
Total filesets on-line 3; total off-line 0; total busy 0
cobbler (rios)	
fts aggrinfo -server cobbler
LFS aggregate lfs_aggr4 (/dev/hd1): 262364 K free out of total 266232
fts lsheader -aggr lfs_aggr4 -server cobbler
Total filesets on server cobbler aggregate lfs_aggr4 (id 7): 5
epi.4                    0,,42 RW     10 K alloc     10 K quota On-line
epi.a.readonly           0,,55 RO   1063 K alloc   1063 K quota On-line
epi.b                    0,,57 RW     16 K alloc     16 K quota On-line
epi.gmd                  0,,50 RW      9 K alloc      9 K quota On-line
root.dfs.readonly        0,,19 RO     17 K alloc     17 K quota On-line
Total filesets on-line 5; total off-line 0; total busy 0
fts lsfldb
epi.1  
        readWrite   ID 0,,33  valid
        readOnly    ID 0,,34  invalid
        backup      ID 0,,35  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner    
valentine.osf.org   RW       lfs_aggr 0:00:00 hosts/valentine<nil>
 
epi.2  
        readWrite   ID 0,,25  valid
        readOnly    ID 0,,26  valid
        backup      ID 0,,27  invalid
number of sites: 3
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner          
valentine.osf.org   RW       lfs_aggr2 0:00:00 hosts/valentine<nil>       
dce5.osf.org        RO       lfs_aggr3 0:30:00 hosts/dce5     <nil>       
cobbler.osf.org     RO       lfs_aggr4 0:30:00 hosts/cobbler  <nil>      
 
epi.3  
        readWrite   ID 0,,29  valid
        readOnly    ID 0,,30  invalid
        backup      ID 0,,31  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner        
dce5.osf.org        RW       lfs_aggr3 0:00:00 hosts/dce5     <nil>        
 
epi.4  
        readWrite   ID 0,,42  valid
        readOnly    ID 0,,43  valid
        backup      ID 0,,44  invalid
number of sites: 2
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner         
cobbler.osf.org     RW,RO    lfs_aggr4 0:00:00 hosts/cobbler  <nil>       
valentine.osf.org   RO       lfs_aggr 0:00:00 hosts/valentine<nil>       
 
epi.a  
        readWrite   ID 0,,54  valid
        readOnly    ID 0,,55  valid
        backup      ID 0,,56  invalid
        releaseClone        0,,72
number of sites: 3
  Locked for ='&release on Wed Dec 31 19:00:00 1969
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner         
valentine.osf.org   RW       lfs_aggr 0:00:00 hosts/valentine<nil>        
valentine.osf.org   RO       lfs_aggr2 0:00:00 hosts/valentine<nil>       
cobbler.osf.org     RO       lfs_aggr4 0:00:00 hosts/cobbler  <nil>        
 
epi.b  
        readWrite   ID 0,,57  valid
        readOnly    ID 0,,58  valid
        backup      ID 0,,59  invalid
number of sites: 2
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner          
cobbler.osf.org     RW       lfs_aggr4 0:00:00 hosts/cobbler  <nil>   
valentine.osf.org   RO       lfs_aggr2 0:30:00 hosts/valentine<nil>
 
epi.gmd  
        readWrite   ID 0,,50  valid
        readOnly    ID 0,,51  invalid
        backup      ID 0,,52  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner     
cobbler.osf.org     RW       lfs_aggr4 0:00:00 hosts/cobbler  <nil>  
 
root.dfs  
        readWrite   ID 0,,18  valid
        readOnly    ID 0,,19  valid
        backup      ID 0,,20  valid
number of sites: 3
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner        
valentine.osf.org   RW,BK    lfs_aggr 0:00:00 hosts/valentine<nil>     
cobbler.osf.org     RO       lfs_aggr4 0:30:00 hosts/cobbler  <nil>    
dce5.osf.org        RO       lfs_aggr3 0:30:00 hosts/dce5     <nil>      
----------------------
Total FLDB entries that were successfully enumerated: 8 (0 failed)

[5/20/93 public]
Bumped to an A - required restarting repserver. RepLog contents were:
93-May-19 12:38:08 repserver: log initialized to /opt/dcelocal/var/dfs/adm/RepLog
93-May-19 12:38:13 Replication server started.  Mainprocs=1; tokenprocs=4; verbose=0.
93-May-19 12:38:38 FLDB finds: 'root.dfs', 0,,19, aggr 7
93-May-19 12:38:38 FLDB finds: 'epi.a', 0,,55, aggr 7
93-May-19 12:38:38 Disk scan: attaching 0,,19 (aggr 7, backing 0,,0) to rep 'root.dfs'
93-May-19 12:38:38 repserver: VOL_GETSTATUS (aggr ix 0, lfs_aggr4, ID 7, type 2, id 0,,57) returns failure 16
93-May-19 12:38:38 Disk scan: attaching 0,,55 (aggr 7, backing 0,,0) to rep 'epi.a'
93-May-19 14:45:21 0,,55: RenewTokens-2/getFSconn: setContext fails to server 130.105.5.27: (382312470) communications failure (dce / rpc)

[6/22/93 public]
This is fixed by a bunch of changes that are being queued under OT 7797.
Changed Subcomponent Name from `repserver?' to `repserver' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `7797' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'



CR Number                     : 7963
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : rep
Short Description             : cm not switching between ro filesets
Reported Date                 : 5/7/93
Found in Baseline             : 1.0.2a
Found Date                    : 5/7/93
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 7735
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/7/93 public]
I had a rw fileset epi.1 with 2 ro release replicas.  I deleted
the rw; I was still able to access the ro.  I deleted one of the
two ro, the cm was not able to access the other ro with the following
error:
root@shotz  # ls /:/epi_1_ro
dfs: fileset (0,,8) error (code 691089410) on server 130.105.5.22 in cell singsing_cell.
ls: The file /:/epi_1_ro does not exist.
The dfstrace looks like:
DFS Trace Dump -
   Date: Fri May  7 14:44:38 1993
Found 1 logs.
Contents of log cmfx:
Log wrapped; data missing.
time 523.384371, pid 251: cm_lookup c3b2c1f8 epi_1_ro
time 523.384371, pid 251: gettokens vp c3b2c1f8, rights.low 0x404
time 523.384371, pid 251: nh_dolookup dvp c3b2c1f8, name epi_1_ro
time 523.384371, pid 251: gettokens vp c3b2c1f8, rights.low 0x404
time 523.384371, pid 251: cm_GetScache vp c3b2c4ec, volume.low 0x1, vnode 0x5
time 523.384371, pid 251: found fid b4a13a36.1.5.4 (hex)
time 523.384371, pid 251: lookup crossing mount point
time 523.384371, pid 251: gettokens vp c3b2c4ec, rights.low 0x404
time 523.388277, pid 251: in EvalMountPoint, vp c3b2c4ec
time 523.388277, pid 251: gettokens vp c3b2c4ec, rights.low 0x404
time 523.388277, pid 251: cm_ConnByMHosts server type 20000
time 523.388277, pid 251: cm_ConnByHosts server type 0x200, connp 0
time 523.388277, pid 251: cm_ConnByHost using conn c290c800, service 0x20000
time 523.388277, pid 251: begin AFS_LookupRoot
time 523.403901, pid 0: in AFS_LookupRoot Volume 0.8
time 523.403901, pid 0: fshs_GetPrincipal START
time 523.403901, pid 0: fshs_GetHost, cookie c2963f00
time 523.403901, pid 0: fshs_FindHost, cookie c2963f00
time 523.407807, pid 0: find a prime host c291a110
time 523.407807, pid 0: find a host in fast path c291a110
time 523.407807, pid 0: fshs_FindPrincipal ..
time 523.407807, pid 0: found a princ c294e06c ref 1
time 523.407807, pid 0: find a princ (fast path) c294e06c, ref 1
time 523.407807, pid 0: fshs_GetPrincipal END c294e06c, ref 1
time 523.407807, pid 0: fshs_PutPrincipal c294e06c ref 1
time 523.407807, pid 0: AFS_LookupRoot returning Vnode 3982e8ff, Unique c14ef744
, code 691089410
time 523.419525, pid 251: end AFS_LookupRoot, code 691089410
time 523.419525, pid 251: cm_Analyze: conn c290c800, code 691089410, user 109051
9047
time 523.419525, pid 251: cm_Analyze: volerr subcode 1
time 523.419525, pid 251: cm_ConnByMHosts server type 10000
time 523.419525, pid 251: cm_ConnByHosts server type 0x100, connp 0
time 523.419525, pid 251: cm_ConnByHost using conn c290c780, service 0x10000
time 523.419525, pid 251: beign VL_GetEntryByID
time 523.540611, pid 251: end VL_GetEntryByID, code 0
time 523.540611, pid 251: cm_Analyze: conn c290c780, code 0, user 0
time 523.540611, pid 251: Install vol entry for volume ID 8
time 524.548423, pid 0: flushactivescaches starting 0 concurrent storebacks
time 524.548423, pid 0: running write through dslots
time 524.556235, pid 0: servertokenmgt running 6 subops
time 524.556235, pid 0: running major renewlazyreps
time 524.556235, pid 0: Renewlazyreps starts 0 subjobs
time 524.556235, pid 0: in cm_FlushQueuedServerTokens for server c2791800
time 524.556235, pid 0: in cm_FlushQueuedServerTokens for server c2943100
time 524.556235, pid 0: in cm_FlushQueuedServerTokens for server c2943200
time 524.556235, pid 0: in cm_FlushQueuedServerTokens for server c2943300
time 524.556235, pid 0: in cm_FlushQueuedServerTokens for server c290be00
time 524.560141, pid 0: in cm_FlushQueuedServerTokens for server c2949e00
time 524.560141, pid 251: cm_Analyze: moved volume 8
time 527.567466, pid 251: cm_ConnByMHosts server type 20000
time 527.571372, pid 251: cm_ConnByMHosts: all filesets bad
time 527.571372, pid 251: cm_Analyze: conn 0, code -1, user 1090519047
time 527.571372, pid 251: mount point lookup failed
time 527.571372, pid 251: checkerror returning code 19
time 533.683883, pid 342: cm_lookup c3b2c1f8 dfstrace
time 533.683883, pid 342: gettokens vp c3b2c1f8, rights.low 0x404
time 533.683883, pid 342: nh_dolookup dvp c3b2c1f8, name dfstrace
time 533.683883, pid 342: gettokens vp c3b2c1f8, rights.low 0x404
time 533.687788, pid 342: cm_ConnByMHosts server type 20000
time 533.687788, pid 342: cm_ConnByHosts server type 0x200, connp 0
time 533.687788, pid 342: cm_ConnByHost using conn c290c740, service 0x20000
time 533.687788, pid 342: nh_dolookup calling AFS_Lookup
time 533.719028, pid 342: nh_dolookup back from AFS_Lookup, code 2
time 533.719028, pid 342: cm_Analyze: conn c290c740, code 2, user 1090519047
time 533.719028, pid 342: cm_Analyze: volerr subcode -1011480852
time 533.719028, pid 342: in nh_delete, scp c3b2c1f8, name dfstrace
time 533.719028, pid 342: checkerror returning code 2
time 533.719028, pid 342: checkerror returning code 2
DFS Trace Dump - Completed
root@shotz  # fts lsfldb
epi.1  
        readWrite   ID 0,,7  invalid
        readOnly    ID 0,,8  valid
        backup      ID 0,,9  invalid
number of sites: 1
  Release repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00
   server           flags     aggr   siteAge principal      owner               
shotz.osf.org       RO       lfs_aggr2 0:00:00 hosts/shotz    <nil>               
epi.2  
        readWrite   ID 0,,10  valid
        readOnly    ID 0,,11  valid
        backup      ID 0,,12  invalid
number of sites: 2
  Sched repl: maxAge=2:00:00; failAge=1d0:00:00; reclaimWait=18:00:00; minRepDelay=0:05:00; defaultSiteAge=0:30:00
   server           flags     aggr   siteAge principal      owner               
shotz.osf.org       RW       lfs_aggr2 0:00:00 hosts/shotz    <nil>               
singsing.osf.org    RO       lfs_aggr 0:30:00 hosts/singsing <nil>               
root.dfs  
        readWrite   ID 0,,1  valid
        readOnly    ID 0,,2  invalid
        backup      ID 0,,3  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
singsing.osf.org    RW       lfs_aggr 0:00:00 hosts/singsing <nil>               
shotz_set  
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
shotz.osf.org       RW       lfs_aggr2 0:00:00 hosts/shotz    <nil>               
ufs_set  
        readWrite   ID 0,,17  valid
        readOnly    ID 0,,18  invalid
        backup      ID 0,,19  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
explorer.osf.org    RW       /u0     0:00:00 hosts/explorer <nil>               
----------------------
Total FLDB entries that were successfully enumerated: 5 (0 failed)

[5/11/93 public]
Filled in Interest List CC with `cfe@transarc.com, kazar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `tu@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/11/93 public]
Assuming that you deleted the R/W copy with ``fts delete'', then this is a
known and documented limitation on replication.  It's documented in the
release notes that deleting the R/W for a fileset will cause all repservers
to delete the secondaries that they're managing.  Presumably you deleted
the R/W for a fileset with ``fts delete'' and then deleted the designated
replica (on the server where the R/W was).  This is a known limitation of
the current repserver.
 
There's no need to delete the R/W fileset if you're trying to check
whether the CM will switch between replicas.  It never switches to the
R/W copy of a fileset; particularly in a release-replication scenario,
there could be big semantic differences between the R/W copy and the
R/O copies.  It's just as good a test to add a second rep site, wait for it
to be created, and then delete the first rep site.

[5/17/93 public]
Given that nobody else has made noises about this one, I'm dup'ing it to 7735.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `7735'



CR Number                     : 7935
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : bak
Short Description             : bak restoreft with invalid options returns 0
Reported Date                 : 5/5/93
Found in Baseline             : 1.0.2
Found Date                    : 5/5/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 7934
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : vijay-ot7934-bak-return-appropriate-exit-code
Transarc Herder               : 
Transarc Status               : 

[5/5/93 public]
bak restoreft with either an invalid extension or an invalid tcid returns 0.
This should be returning a non-zero return code.
Added field #Transarc Deltas with value `' 
Added field #Transarc Herder with value `jaffe@transarc.com' 
Added field #Transarc Status with value `open'

[5/5/93 public]
Changed Responsible Engr. from `jaffe@transarc.com' to `vijay@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[5/12/93 public]
This is essentially the same problem as OT 7934. I have checked in a fix for
that OT. I'm dup'ing this to 7934.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `7934' 
Filled in Transarc Deltas with `vijay-ot7934-bak-return-appropriate-exit-code'



CR Number                     : 7853
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : episode should allow more than 20 mounted filesets
Reported Date                 : 5/3/93
Found in Baseline             : 1.0.2
Found Date                    : 5/3/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 3816
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : file/episode/vnops/efsmount.h
Sensitivity                   : public
Transarc Deltas               : rajesh-increase-efsmount-table-size
Transarc Herder               : jaffe@transarc.com
Transarc Status               : import

[5/3/93 public]
Dfs currently limits the number of locally mounted episode filesets
to 20.  This delta increases the limit to 100.
Added field Transarc Deltas with value `rajesh-increase-efsmount-table-size' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `import'

[5/4/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `3816'



CR Number                     : 7793
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : unable to reach ufs aggregate/fileset via mount point
Reported Date                 : 4/23/93
Found in Baseline             : 1.0.2a
Found Date                    : 4/23/93
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 7762
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/23/93 public]
 
BUILD:	102a available 4/20/93
CONFIG:	3 pmax cell, 3 flservers, 1 pmax w/ ufs aggregate
 
Idle cell - all machines rebooted and dce/dfs restarted ~19 hours ago.
Both with and without dce credentials, can't reach ufs aggregate/fileset
through mount point.
 
root@valentine> cd /:/root_dfs/dce12_u3
dfs: fileset (0,,4) error (code 691089410) on server 130.105.202.31 in cell p102a_cell.qadce.osf.org.
/:/root_dfs/dce12_u3: No such device
 
but aggregate is exported and fldb aggrees:
 
root@dce12> dfsexport
dfsexport: /dev/rz3c, ufs, 3, 0,,0
root@dce12> fts lsfldb dce12.u3
        readWrite   ID 0,,4  valid
        readOnly    ID 0,,5  invalid
        backup      ID 0,,6  invalid
number of sites: 1
   server           flags     aggr   siteAge principal      owner               
dce12.osf.org       RW       /u3     0:00:00 hosts/dce12    <nil>               
 
Dumps from the 3 pmax machines are in ~notuser/gmd/(this CR number)/*.dump.
Unable to gather more data due to CR 7767.

[4/23/93 public]
Unless dce12_u3 is a mount point for fileset dce12.u3 (note the difference
between dot and underscore) this looks like pilot error.  To verify, something
like ``fts lsmount /:/root_dfs/dce12_u3'' should do it.
 
Ah, another possibility, given what ``dfsexport'' says.  Looks like this
UFS fileset was exported incorrectly, since the fileset ID given by
dfsexport is 0,,0.  At best, you'll have to un-export /u3, correct the
dfstab entry to give the correct fileset ID, then re-export /u3.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'

[4/23/93 public]
Hmm - yes, the dfstab was corrupt - it was written by dce_config so I'd
prefer to think to myself as an accomplice ... detach'ing and re-exporting
WOULD probably have cleared everything up except now:
root@valentine> fts lsmount -dir /:/root_dfs/dce12_u3
dfs: the fx server 130.105.5.27 in cell p102a_cell.qadce.osf.org is in TSR mode!
fts lsmount: error for file '/:/root_dfs/dce12_u3': Connection timed out
root@valentine> fts lsmount -dir /:/root_dfs/dce12_u3
dfs: fileset (0,,2) error (code 691089410) on server 130.105.5.27 in cell p102a_cell.qadce.osf.org.
fts lsmount: error for file '/:/root_dfs/dce12_u3': No such device
ie. now I'm getting complaints about fileset 0,,2, the root.dfs replica ...
sigh ... I'll look around a bit more and see what's up - this is the same
cell that's been experiencing 7767 assertions and that has the 7762
corruption.

[4/26/93 public]
Looks like the next step is for Gail to collect additional information.
Reassigned to her.
Changed Responsible Engr. from `pakhtar' to `gmd' 
Changed Resp. Engr's Company from `tarc' to `osf'

[4/26/93 public]
Dup'ing this to 7762 since the fileset moved or deleted complaints have
disappeared under similar testing AFTER newaggr'ing away the 7762 state
of replication.



CR Number                     : 7756
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : fts addsite hang
Reported Date                 : 4/19/93
Found in Baseline             : 1.0.2b23
Found Date                    : 4/19/93
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 7656
Fix By Baseline               : 1.0.2a
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/19/93 public]
 
Craig - assigned directly to you - reassign if appropriate.
 
CONFIG:	3 pmax cell, 3 flservers
BUILD:	b23
TEST:	admin_checklist/repfs_checklist type stuff
 
root@valentine> fts create epi.4 -server valentine -aggr lfs_aggr3
        readWrite   ID 0,,25  valid
        readOnly    ID 0,,26  invalid
        backup      ID 0,,27  invalid
number of sites: 0    number of addresses: 1
   server           flags     aggr   siteAge principal      owner               
valentine.osf.org   RW       lfs_aggr3 0:00:00                <nil>               
Fileset 0,,25 created on aggregate lfs_aggr3 of valentine
root@valentine> fts setrepinfo epi.4 -release
fts setrepinfo: Using default value for maxage of 2:00:00
fts setrepinfo: Using derived value for failage of 1d0:00:00
fts setrepinfo: Using default value for reclaimwait of 18:00:00
root@valentine> fts addsite epi.4 -server valentine -aggr lfs_aggr2
<hang>
 
The only potentially interesting thing I can think of that's gone on
previously in this cell is that root.dfs is replicated.
 
root@dce5> fts crmount -dir /:/root_dfs -fileset root.dfs
root@dce5> fts setrepinfo -fileset root.dfs -release
fts setrepinfo: Using default value for maxage of 2:00:00
fts setrepinfo: Using derived value for failage of 1d0:00:00
fts setrepinfo: Using default value for reclaimwait of 18:00:00
root@dce5> fts addsite root.dfs -server dce5 -aggr lfs_aggr
Added replication site dce5 lfs_aggr for fileset root.dfs
root@dce5> fts addsite root.dfs -server valentine -aggr lfs_aggr2
Added replication site valentine lfs_aggr2 for fileset root.dfs
 
Debug info:
root@dce12> fts statrep -server valentine 
Status of rep server valentine (valentine.osf.org) at Mon Apr 19 18:15:58 1993
Replicas managed: 1; host/connection blocks: 2
IDs: 0 allocated, 0 in use, 0 re-used
Next forced keep-alive in 83330 secs, at Tue Apr 20 17:24:48 1993
Primary comm blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
Primary keep-alive blocks: 0 done, 0 oversize; 0 tot overage, (0:0) squared.
 
ps on valentine shows the fts addsite command is sleeping.
fts lsheader of lfs_aggr2 shows epi.4.readonly has not yet been created

[4/20/93 public]
More debug info:
 
The ftserver and repserver on another machine within the cell were
hung:
 
root@dce5> fts statft dce5
Total transactions: 1
--------------------------------------
trans: 7  created: Mon Apr 19 14:31:44 1993
descriptor: 0; ref count 1;  last call: Mon Apr 19 14:31:44 1993
fileset: 0,,10  aggregate Id: 1   aggrtype: 2/1
action: repserver forward fileset (dfs / xvl); ops: Dump, GetStatus
Fileset status 0x18014005: R/W, busy, know dally, type-RW
--------------------------------------
 
Shutting down the repserver on dce5 unhung the addsite command - even 
though the addsite command did NOT involve dce5 (contention over FLDB
lock(s)?).
 
The ORIGINAL problem here may have something to do with local scheduled
replicas - the activity in this cell has NOT been stressful - but I
have been testing replication by hand.

[4/20/93 public]
I expect that, as far as this cascade of hung processes goes, the ultimate
hangup is that a worker thread in the repserver on dce5 got an exception
from FTSERVER_Forward and was cancelled, a bug that should have been fixed
by the fix for OT 7656 submitted to 1.0.2a.  This thread, though, had
claimed a lock in repserver that was also needed by the server stub (again
in the repserver on dce5) to answer an RPC from the peer repserver on
valentine.  The single worker thread in the repserver on valentine got
stuck waiting for the answer from this RPC, while holding a different
lock in repserver on valentine (the callRootLock).  Then, all calls
to SREP_AllCheckReplicationConfig() (e.g. this one, from fts) will hang
waiting to acquire the callRootLock in the repserver on valentine.
 
There's already a fix for the ultimate hang-up here.  Some defensive
coding could be added to repserver and/or fts to implement a call timeout,
so that either repserver or the user running fts doesn't really wait forever,
but ultimately, this is a dup of 7656, and the defensive coding should go
in under a different OT.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `7656' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'



CR Number                     : 7697
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : doc
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : port_gd
Short Description             : running icldump
Reported Date                 : 4/13/93
Found in Baseline             : 1.0.2
Found Date                    : 4/13/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 6222
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[4/13/93 public]
A note from the FAQ that should get incorporated in the Porting
Guide (Doug if you think this belongs someplace else, feel free
to change the CR).
From: treff@osf.org
To: melman
Cc: davel
Subject: DFS: Running icldump
Date: Thu, 19 Nov 92 12:37:28 -0500
> Date: Wed, 18 Nov 1992 12:03:13 -0500 (EST)
> From: Mike_Kazar@transarc.com
> To: treff@osf.org ("Dave Treff")
> Subject: running icldump
> Cc: Pervaze_Akhtar@transarc.com
> 
> Running icldump is really easy.  Copy the binary to your machine; we've
> been putting icldump into /opt/dcelocal/bin.  Make sure that you have
> all of the new .cat files (they are all of the form dfsz*.cat) installed
> in your NLSPATH, since icldump will print out very raw-looking info if
> it can't find the debugging strings in its path.  Then, as soon after an
> error occurs as possible, just do
> 
> /opt/dcelocal/bin/icldump >/tmp/dump
> 
> It'll write out about 600 or 700 K of data, and will take about 5
> minutes to do so.
> 
> Then copy /tmp/dump to whereever you want.

[04/13/93 public]
Unfortunately, it doesn't work this way anymore.
The command is dfstrace.    You can get more
help by "dfstrace help".  Basically to get
a dump you "dfstrace dump " this will put the
output to stdout.  Also, the buffer sizes are
different for each platform.  OSF uses about
2k and AIX uses something much much larger.
The default behavior is to log cache manager
activity.  I am not sure if there are any other
options (like episode events, etc).
There are also some related hooks to make krpc
on the pmax to log its debugging events to the
dfstrace package.  Also, some of the dfs daemons
(dfsbind, flserver) will do user space logging
do a kill -30 <pid> to get them to dump their logs.
Maybe Eliott can elaborate on the not so obvious
things.

[6/22/93 public]
Changed "Fix By Baseline" from 1.0.3 to 1.1.

[6/23/93 public]
This defect is a duplicate of defect 6222, which is currently open against me.
I dup'ed it to that defect because I see no reason to maintain both defects.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `6222' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'



CR Number                     : 7683
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : panic in cm_ReleToken
Reported Date                 : 4/12/93
Found in Baseline             : 1.0.2b22
Found Date                    : 4/12/93
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 5677
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : shl@transarc.com
Transarc Status               : open

[4/12/93 public]
Configuration: standard 4 machine cho 
Tests runnning: standard cho
During the course of the cho testing the pmax client only (M4) panic'd in 
cm_ReleToken.  This client was running a connectathon to a pmax server
and a low.moderate (executing prog5) to a rios server.
cm_UpdateTokenLifeTime  (line 904)
cm_ReleToken
panic
Here's the code clip from cm_UdateTokenLifeTime so you can see which
cm_ReleToken caused the problem:
   900                              return;   /* The server is down */
   901                          }
   902                      }
   903                      nlp = (struct cm_tokenList *) tlp->q.next;
   904                      cm_ReleToken(tlp);
   905                  } /* for */
   906                  lock_ReleaseWrite(&scp->llock);
   907                  lock_ObtainWrite(&cm_scachelock);
   908                  CM_RELE(scp);
I will see if we can get any other useful information out of this crash.

[4/12/93 public]
Changed Responsible Engr. from `jsk@transarc.com' to `kazar@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `shl@transarc.com' 
Added field Transarc Status with value `open'

[04/12/93 public]
Possibly interesting stack traces of other processes:
cm_DoPartialWrite
cm_GetToken
cm_GetTokensRange
AFS_GetToken
rpc_call_transcieve
rpc__dg_call_transcieve
rpc__dg_call_receive_init
rpc__dg_call_wait
pthread_cond_wait
syscall
execve
common_exec
xglue_getattr
nosf_getattr
cm_GetTokens
cm_GetTokensRange
AFS_GetToken
rpc_call_transcieve
rpc__dg_call_transcieve
rpc__dg_call_receive_init
rpc__dg_call_wait
pthread_cond_wait
Don tells me he's also seen this recently as well.

[4/12/93 public]
Changed Responsible Engr. from `kazar@transarc.com' to `tu@transarc.com'

[4/12/93 public]
I take it that you are running against dfs 2.9. True ? 
thanks, 
tu

[04/21/93 public]
We've hit it again.  This code which is post b23 (this is the
first 1.0.2a build - theoretically this build syncs us up with Transarc).

[4/21/93 public]
Just spoke with Diane on the phone. I now understand what she was doing:
she was running the CHO test while moving the fileset and eventually the 
panic occurred. 
                                                                           
I have seen this during my own testing while moving the fileset. However, 
this fix is in part of the current CM work responding to an abnormal fileset 
move. I will talk to Prevaze or Elliot to see how we could deliver this fix
to OSF.

[4/21/93 public]
Filled in Interest List CC with `pakhtar'

[4/23/93 public]
Tu has fixed this defect as part of the fixes for ot 5677.  That ot happens
to fix a large number of problems related to TSR and fts move.  I will
upgrade its priority to A1 to reflects its current severity, and tu will
add more text describing the scope of problems that it fixes.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5677'



CR Number                     : 7505
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts dump/restore use of vnode type isn't portable
Reported Date                 : 3/15/93
Found in Baseline             : 1.0.2b17
Found Date                    : 3/15/93
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 7362
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Status               : import
Transarc Deltas               : dimitris-dump-vnode-device
Transarc Herder               : jaffe@transarc.com

[3/15/93 public]
Another portability problem in the dump/restore interface is that dump
puts the local system's idea of the vnode "type" (enum vtype, vattr
member va_type) into the dump image, and that restore puts the dumped
vnode type into a newly-created vnode with no interpretation.  If a
dump is restored onto a system with a different set of vnode types than
the originating machine, this can lead to bogus vnodes being
generated.
In the case of the two reference platforms, the differences between the
local enum vtype definitions are minor, so I assume resolving this
problem can wait till after 1.0.2.  If anyone feels otherwise, feel
free to raise the priority of the bug.
For reference, the two definitions of enum vtype follow:
	AIX:
		enum vtype { VNON, VREG, VDIR, VBLK, VCHR,
			VLNK, VSOCK, VBAD, VFIFO, VMPC };
	OSF/1:
		enum vtype { VNON, VREG, VDIR, VBLK, VCHR,
			VLNK, VSOCK, VFIFO, VBAD };
As you can see, of the common vnode types only VFIFO has a different
value in the two systems.  (And OSF/1 doesn't support type VMPC.)  So
this bug in and of itself would prevent only vnodes of type VFIFO (or
VMPC, of course) from being transmitted between architectures or
from causing a clean failure, if the vnode type were not supported
on the target system.

[3/16/93 public]
Don's comments are on the money.  Will handle, but probably not for 1.0.2;
I plan to fold this work into the dump/restore of special file types.
Changed Interest List CC from `cfe@transarc.com' to `pakhtar@transarc.com, 
 jdp@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `cfe@transarc.com' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[4/14/93 public]
Defect Closure Form
-------------------
--Regression test program below--
--Verification procedure below--
In an episode fileset create a named pipe. Copy the fileset using 
'fts dump | fts restore' from a RIOS to a PMAX/OSF1 
--Other explanation below--
Associated information:
Tested on TA build:  
dfs-102-2.8
Tested with backing build:  
dce1.0.2b19
Changed Interest List CC from `pakhtar@transarc.com, jdp@transarc.com' to 
 `pakhtar@transarc.com, jdp@transarc.com, cfe@transarc.com' 
Changed Responsible Engr. from `cfe@transarc.com' to `dimitris@transarc.com' 
Filled in Transarc Deltas with `dimitris-dump-vnode-device' 
Changed Transarc Status from `open' to `export'

[4/14/93 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `7362'



CR Number                     : 6790
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : file exporter
Short Description             : Needs to install error code mappings
Reported Date                 : 1/11/93
Found in Baseline             : 1.0.1
Found Date                    : 1/11/93
Severity                      : D
Priority                      : 3
Status                        : dup
Duplicate Of                  : 5131
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : closed

[1/11/93 public]

Currently the standardized error code mappings used by the file exporter
to encode errno type errors are not installed.  These mappings are needed
by any person wishing to develop a cache manger that communicates with the
file exporter.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[2/9/93 public]
This is a duplicate of 5131, insofar as the correct fix for 5131 will
fix this also.
Changed Defect or Enhancement? from `def' to `enh' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5131' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 6787
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 6822
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : Must detect restores with missing incrementals
Reported Date                 : 1/11/93
Found in Baseline             : 1.0.2
Found Date                    : 1/11/93
Severity                      : D
Priority                      : 3
Status                        : dup
Duplicate Of                  : 6049
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[1/11/93 public]
There is currently no code that prevents fts from restoring full and
incremental dumps incorrectly.  In particular, fts/test21 was doing a
restore of a dump consisting of a full and two incrementals but skipping
the first increemental.  This results in an inconsistent file system but
the INCONSISTENT bit is off.  Attempting to modify such a file system
will certainly result in a panic.
There has been some discussion that restoring part of a dump (or not all
of a dump) would be a valuable thing to do.  I suggest that if we want
to support this that we allow an inconsistent file system to be
mountable (exportable) as a read-only file system.  We will need to test
it to make sure that Episode (perhaps UFS is problematic) doesn't panic
when reading an inconsistent file system even if it does produce odd
error codes.  This should be straightforward "merely" a testing problem.
We must make sure, however, that when the ftutil module is asked to
restore a dump out of order or with missing pieces it detects this and
does not clear the inconsistent bit when the fileset is close.  Further
it must remember this and not clear the inconsistent bit following some
subsequent fileset operation that completes successfully (say the
restore of another incremental dump).  I suggest the creation of another
on-disk fileset status bit that means "MISSING_SOME_VVs".  This bit
would inhibit clearing the INCONSISTENT bit and would stay set until the
fileset was restored from a full dump (or deleted or recloned into or
that sort of thing).
Contrary to what Vijay, Jeff and I determined last week, Craig claims
that the VV-dumpted-from is stored in the dump header, so we can easily
verify that there is an overlap between the incoming restore and the
on-disk fileset.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/13/93 public]
I'd like to address this work in two phases, and I'd like to make this defect
be associated with the later phase.  Thus, I've created OT 6822 to cover the
first phase and have down-graded the priority of this one.
In particular, OT 6822 covers detecting the bad application of incremental
dumps, and returning failure.  This OT will be used to make it possible to
use these incremental dumps more gracefully, possibly with the
MISSING_SOME_VVS option or its analogue.
Filled in Inter-dependent CRs with `6822' 
Changed Severity from `B' to `D' 
Changed Priority from `1' to `3'

[2/24/93 public]
There had been an earlier version of this defect entered as OT 6049.
I'm dup'ing this one to it, and including the discussion from this to
its OT report.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `6049' 
Changed Fix By Baseline from `1.0.2' to `1.1' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 6683
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 6548
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : dfs acls
Short Description             : Not returning distinct enough errors
Reported Date                 : 12/29/92
Found in Baseline             : 1.0.2
Found Date                    : 12/29/92
Severity                      : D
Priority                      : 3
Status                        : dup
Duplicate Of                  : 5955
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : security/client/acl/dfs_dce_acl.c,file/...?
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[12/29/92 public]
For OT6548, I'm replacing a hardcoded return of sec_acl_no_acl_found with
a call to a routine that converts errcodes to DCE acl errors.  Unfortunately,
as I found out via mail from Bruce Leverett, the syscalls are overloading
the errno/errcodes so much that you can't really tell what happened.  This
should be fixed by Transarc on the DFS side of things (not security), to
pass back more errcodes if possible, or add a status parameter for more
versatility if there aren't enough errcodes.  After I submit the dfs_dce_acl.c
with the conversion routine, feel free to update it and submit it when you
have error codes that are more meaningful.

[12/31/92 public]
Given that fixing this would require an API-level change (we'd have to
pass back an extended error code, as the heart of the above complaint
is the 8 bits which we have to work with isn't enough), either this
happens now, or it cannot possibly happen until 1.1

I've also noted that this is an enhancement, not a defect.
Changed Defect or Enhancement? from `def' to `enh' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[1/4/93 public]
This is a dup of 5955... made it so...



CR Number                     : 6467
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : episode computes caller access incorrectly for machine/self id
Reported Date                 : 12/15/92
Found in Baseline             : 1.0.2
Found Date                    : 12/15/92
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 5631
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[12/15/92 public]
This bug seems trivially duplicatable.

I created a brand-new fileset, whose mode bits appear to be 777, owned by root.

I then cd'd to the directory and tried to list it.  Since I have "r"
access, I should be able to, but I was getting a protection failure in
the SAFS_Readdir RPC call.  According to kdbx, the return result from
VOPX_GETATTR (called with a 1 value for the "use extended fields"
attribute) was: 

(dataVersion = (high = 0, low = 12), fileID = (high = 1, low = 1), volVersion =
(high = 0, low = 4010), author = 4294967295, callerAccess = 0, anonAccess = 55,
parentVnode = 4294967295, parentUnique = 4294967295, serverModTime = (sec = 7244
31796, usec = 803955), fstype = 2, objid = (time_low = 120, time_mid = 0, time_h
i_and_version = 0, clock_seq_hi_and_reserved = '\0', clock_seq_low = '\0', node
= ""), timeUncertainty = 0, representationFlags = 319, backingIndex = 2, backing
VolIndex = 5, aclIx = 0, initDirAclIx = 0, initFileAclIx = 0, plistIx = 0, uPlis
tIx = 0, spare1 = 804780392)

Note that callerAccess (0) is less than anonAccess (55), which is by
definition wrong.

This this access is being used by the cache manager as the cached
access, this is causing confusion by the cache manager with respect to
protection checks.

Running as cell-admin, I looked at the directory in question:

# SEC_ACL for .:
# Default cell = /.../alfalfa.com
user_obj:rwxcid
group_obj:rwx-id
other_obj:rwx-id
sec_acl_edit> ab
# ls -lid .
65570 drwxrwxrwx   8 root     transarc     512 Dec 15 10:03 .
# suspend

Basically, it looks like Episode is computing the callerRights
incorrectly.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/15/92 public]
This bug is the same as 5631, too.  I'm duping it in the only order I can.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5631' 
Changed Responsible Engr. from `kazar@transarc.com' to `bwl'

[12/16/92 public]
Changed Subcomponent Name from `episode' to `lfs'



CR Number                     : 6313
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : newaggr should not proceed when the aggr is attached.
Reported Date                 : 12/3/92
Found in Baseline             : 1.0.2
Found Date                    : 12/3/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 4378
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[12/3/92 public]

Currently, the DFS allows 'newaggr' to init an aggregate while the aggregate,
is ,in fact, alreay attached. The 'newaggr' command does not give any warning
or even stops doing it. We should think of a way to enhance it in the future
release. 

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[12/3/92 public]
Changed Subcomponent Name from `epi' to `lfs' 
Changed Interest List CC from `pakhtar@transarc.com' to `pakhtar, ota, jdp' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `4378' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 6271
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : reserving frags in COW file gets too few frags
Reported Date                 : 12/1/92
Found in Baseline             : 1.0.2
Found Date                    : 12/1/92
Severity                      : B
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5926
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[12/1/92 public]
The code in epia_Reserve that handles extending a fragmented file
doesn't check to see if the file's block is InBacking or not.  It only
reserves those needed to extent the length.  This will results in too
few fragments being reserved which will then result in various problems
if the volume is really so full it can not allocate the space.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[2/12/93 public]
Is this getting fixed for 1.0.2.  It seems important since it
can lead to problems as a volume is filling up.
Changed Interest List CC from `ota' to `ota,demail1!carl' 
Changed Severity from `C' to `B' 
Changed Priority from `2' to `1'

[2/12/93 public]
After discussion with Transarc, I agree this is not pri 1
material.
Changed Priority from `1' to `2'

[5/26/93 public]
This was fixed by Ted as part of the changes to fix OT 5926.  So I have
dup'ed it to 5926.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5926'



CR Number                     : 6184
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : 
Short Description             : DFS include file install directory changed
Reported Date                 : 11/23/92
Found in Baseline             : 1.0.2
Found Date                    : 11/23/92
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 5131
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[11/23/92 public]

This "defect" is just a placeholder to record the fact that the
installation directory for DFS include files is being changed,
from .../afs to .../dcedfs.

[11/23/92 public]
Oops... 5131 describes this change (along with others).  So
close the present defect.



CR Number                     : 6031
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 5708
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : panic in RecoverSCacheToken
Reported Date                 : 11/11/92
Found in Baseline             : 1.0.2
Found Date                    : 11/11/92
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 6017
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mcinerny@transarc.com
Transarc Status               : open

[11/11/92 public]

While running the defect closure verification procedures for OT CR
number 5708, I was able to panic my RIOS with the following stack trace:

    brkpoint() at 0x5a54
    panic(0xb) at 0x60178
    cm_RecoverSCacheToken(scp = 0x657c480, Flags = 257, renew = 1), line 1597 in "cm_tokens.c"
    cm_RestoreMoveTokens(volp = 0x6597108), line 1523 in "cm_tokens.c"
    RevokeHereToken(scp = 0x657c5a0, volp = 0x6597108), line 794 in "cm_tknimp.c"
    RevokeOneToken(scp = 0x657c5a0, baseIDp = 0x65bb034, typep = 0x2ff7fad4, rlockerp = 0x2ff7faec, colAp = (nil), colBp = (nil), flagsp = 0x2ff7fae8), line 826 in "cm_tknimp.c"
    STKN_TokenRevoke(h = 0x65b8b80, descp = 0x65bb018), line 535 in "cm_tknimp.c"
    op2_ssr(0x65b8b80, 0x6528a00, 0x2ff7fdfc, 0x2ff7fdd8, 0x60a2dd8, 0x5d45504, 0x2ff7fd80) at 0x604cb54
    rpc__dg_execute_call(0x6528a00, 0x0) at 0x5fee6ac
    cthread_call_executor(0x5d45584) at 0x5fe104c
    thread_boot(0x0, 0x2ff7fff0, 0x8) at 0x5fa9a04
    procentry(0xb, 0x2ff7f6d8, 0x2ff98000, 0xb) at 0x2aa94

The test was run:

RIOS client running
	<cwd is in bazset>
	% tsrtest -verbose -files 10 -flock -dir .
	<wait for pause>
	% fts move bazset echo echo.epi0 puzzle puzzle.epi0 -verbose
	Cloning fileset 0,,24 to 0,,28 (name bazset.move-temp, Reclone=0)
	Creating token keep-alive thread.
	Revocation socket is: inet/192.55.207.156/1070
	Creating RPC listener thread.
	About to get token 404 for fileset 0,,24 from echo.transarc.com.
	Listening for net calls (calling rpc_server_listen)
	Connecting to file server on echo.transarc.com...STKN_InitTokenState called with flags of 0x0
	STKN_InitTokenState() called: clearing token state.
	done (result 0)
	Token 721495363,,6031, type 0x404, obtained on fileset 0,,24.
	Releasing token (1) for fileset 0,,24 from echo.transarc.com.
	Releasing token on file server...done.
	Creating dest fileset 0,,24...done
	Creating tag keep-alive thread.
	Dumping from clone 0,,28 (src) to fileset 0,,24 (dest)...done
	Deleting the clone 0,,28...done
	About to get token 404 for fileset 0,,24 from echo.transarc.com.
	Connecting to file server on echo.transarc.com...STKN_InitTokenState called with flags of 0x0
	STKN_InitTokenState() called: clearing token state.
	done (result 0)
	Token 721495363,,6034, type 0x404, obtained on fileset 0,,24.
	Dumping from original 0,,24 (src) (incr from VV 0,,949) to fileset 0,,24 (dest).
	..About to get token 4808 for fileset 0,,24 from echo.transarc.com.

The cell was configured:

PMAX-core-scm-fl-ft(ufs,lfs)-dfs
RIOS-ft(ufs,lfs)-dfs

The PMAX is still up.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mcinerny@transarc.com' 
Added field Transarc Status with value `open'

[11/11/92 public]
Changed H/W Ref Platform from `rs6000' to `all' 
Changed S/W Ref Platform from `aix' to `all' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `6017'



CR Number                     : 6025
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : fts
Short Description             : fts lsq caused error 691089516 message from dfs
Reported Date                 : 11/10/92
Found in Baseline             : 1.0.2
Found Date                    : 11/10/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5114
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[11/10/92 public]
Senario:	Running moderate mode testing in DFS/LFS on AIX (dfs-102-1.55)
		Tests currently in test7.

	Log into machine, and type 

		fts lsq /:/.

	Some DFS component prints out the following error on its controlling
	terminal:

	dfs: fileset (id = 1) is busy with error 691089516
	dfs: waiting (691089516) for busy fileset 0,,0

The error translates into:

	translate_et: 691089516: get status on fileset (dfs / xvl)

The bug is that this does not seem to be an error.  The first question is
why did fts lsq cause this message?  The second is why should a fileset
busy message claim that there is an error, when it may just be that the
fileset was legally busy?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[11/11/92 public]
This is fundamentally a dup of OT 5114, which is an enhancement request.
I'll append the text of this OT to that one, suggesting that the CM also
be changed so that ``busy''-style codes don't get reported as ``error''s.
Changed Defect or Enhancement? from `def' to `enh' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5114'



CR Number                     : 5877
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : The system is looping inside episode
Reported Date                 : 10/30/92
Found in Baseline             : 1.0.2
Found Date                    : 10/30/92
Severity                      : B
Priority                      : 0
Status                        : dup
Duplicate Of                  : 5508
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[10/30/92 public]

I was running two copies of low test again 1.53 build and the system
was hung (looping) after 20 mins of running. Through the kdbx
I noticed that the system was looping between 

   GC_Tran and ComputeActiveLog.

(Sorry, I lost the stack trace, but the Ted asked to report it anyway.)

It is reproducible (at leaset two for two runs). 

Here is how I ran the test;

  - run the first low test with moderate mode through DFS, ie., the
    test dir is /:/testdir

  -run another copy of low test through the glue layer, ie., epimount the same
   fileset (root.dfs) and aggregate on /mnt

   The test dir is in /mnt/testdir1.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[11/2/92 public]
I'm quite sure this is fixed by (super-)delta
ota-ot3804-new-block-security.  When this delta appears in a
configuration (dfs-102-1.55?) this should be checked.
Changed Subcomponent Name from `epi' to `lfs'

[11/4/92 public]
 I certainly saw this failure in 1.51; it was fixed in the delta
subsumed by the NBS super-delta.  I'm marking this as a duplicate of
5508.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5508'



CR Number                     : 5846
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : zlc
Short Description             : zlc_init calls osi_NewProc - bad
Reported Date                 : 10/28/92
Found in Baseline             : 1.0.2
Found Date                    : 10/28/92
Severity                      : A
Priority                      : 1
Status                        : dup
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : zlc_mgr.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[10/28/92 public]

zlc_init() should call osi_ThreadCreate, not osi_NewProc.
Also must #include <dce/ker/pthread.h> for KERNEL builds.

[10/28/92 public]
This is a dup of 5839.  A fix for 5839 has already been submitted.
Changed Status from `open' to `dup' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'



CR Number                     : 5679
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : 
Short Description             : Remote client sees old file contents when file overwritten
Reported Date                 : 10/16/92
Found in Baseline             : 1.0.2b4
Found Date                    : 10/16/92
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 5664
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mcinerny@transarc.com
Transarc Status               : open

[10/16/92 public]
Another data consistency problem, probably related to OT 5664.  Two
machine cell, both PMAX (same one as in 5664) -- one machine running
core + DFS servers, exporting root.dfs; other machine running clients
only.

By accident, on the local DFS client I overwrote a log file used in
a previous test.  Though the new file contents were visible in ufs
and on the local client, the remote client still saw (and still sees
now, maybe half an hour later) the old contents of this file.

The attributes of the file on the remote client have been updated
correctly, but its contents are unchanged from its previous 
incarnation.  There's no corruption visible, just a mismatch between
the attributes and the file contents.

[10/19/92 public]
Assigned to Mike 'cuz 5664 was assigned to him.  Reassign if inappropriate, 
of course.

Filled in Interest List CC with `pakhtar@transarc.com' 
Filled in Responsible Engr. with `kazar' 
Filled in Resp. Engr's Company with `tarc' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mcinerny@transarc.com' 
Added field Transarc Status with value `open'

[10/27/92 public]

I tried this in a two machine cell, one a PMAX and the other an RS/6000,
and it worked.  I was running against the Transarc 1.53 code.  I have marked
it as a dup of 5664 since they are the same problem, inconsistent caches.

Also changed the responsible engineer to kazar@transarc.com.

Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5664' 
Changed Responsible Engr. from `kazar' to `kazar@transarc.com'



CR Number                     : 5643
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : CM loses authentication spuriously
Reported Date                 : 10/14/92
Found in Baseline             : 1.0.2
Found Date                    : 10/14/92
Severity                      : B
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5631
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[10/14/92 public]
Scenario: three machine cell, machine "A" is the primary DFS and DCE server
machine, machine "B" is a DCE client and secondary DFS file server (no
flserver), machine "C" is a DCE and DFS client machine.

As an authenticated user I'd been looking at a source file in the sandbox
(fileset located on machine "B") from machine "C".  I typed the following:

(...)
% cd /:
% cd usr
% ls
aprasad  blumer   bww      fred     jess     ota      travis
azs      bob      cfe      jaffe    jsk      rajesh   tu
bab      bruce    comer    jdp      kazar    sanzi    vijay
blake    bwl      foo      jeff     mason    saxena
% cd mason
% ls
bar  bin  dfs  ff   foo
% cd dfs
% ls
export    link      obj       rc_files  src       tools
% cd src
% ls
Makeconf  file
% cd file/episode/async
% ls
Makefile         asevent.c        async.p.h        us_io.c
RCS              asevent_debug.h  us.h
asev_errs.et     astest.c         us_conf.c
% more asevent.c
(file contents omitted)
% cd ../..
dfs: set auth binding failed (code 12)
dfs: Warning: created unauthenticated binding
../..: The file access permissions do not allow the specified action.
% ls
Makefile         asevent.c        async.p.h        us_io.c
RCS              asevent_debug.h  us.h
asev_errs.et     astest.c         us_conf.c
% cd ../..
../..: The file access permissions do not allow the specified action.
% cd ..
..: The file access permissions do not allow the specified action.
2 Tea-Sea-Shell>dce_login mason mason
% cd /:
% cd usr
% cd mason
% cd dfs
% cd src/file
% cd episode

I suspect the mere fact I can examine the directory after being informed I am
no longer authenticated is because of the bug reported in OT 5625.

I've grabbed the icldump log from machine "C".  Once I've got the OT #, I'll
name it and then edit the OT report to include the location of the icldump
log.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/14/92 public]
I've placed the icldump log in /afs/tr/usr/mason/debug-info/icldump as
ot5643-rios145-14oct92-spurious-building-failure.Z

[12/15/92 public]
This looks like yet another security returning ENOMEM problem.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5631'



CR Number                     : 5559
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : /file/episode/libefs/OSF1
Short Description             : Numerous undefined symbols trying to link vmunix.sys.
Reported Date                 : 10/5/92
Found in Baseline             : 1.0.2
Found Date                    : 10/5/92
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[10/5/92 public]
 [ /file/episode/libefs/OSF1 ]
makepath OSF1/. && cd OSF1 &&  exec make MAKEFILE_PASS=BASIC     build_all
/project/dce/build/nb_pmax/tools/pmax/gcc/ld -G 18 -N -T 80030000  -e start -o s
efsvmunix -L/project/dce/build/nb_pmax/export/pmax/usr/lib  ../../../../../../ob
j/pmax/kernel/DCE/vmunix.sys  -lkdfskutil -lktkm -lktpq -lvolreg -lkfp  -lefs -l
kefsops -lkdir -lkeacl -lkanode  -lklogbuf -lkasync -lktools -lkolddacl -lxvnode
 -lktkc -lvolume -lkdacl -lkdacllfs -lkdfskutil  -laggr -lufsops -lkdacl -lknck 
-lkxcred -lkicl -lkosi
/project/dce/build/nb_pmax/tools/pmax/gcc/ld:
Undefined:
rpc_ss_swap_client_alloc_free
rpc_ss_allocate
rpc_ss_free
des_key_sched
sec_des_is_weak_key
des_cbc_cksum
des_ecb_encrypt
des_cbc_encrypt
rpc_ss_register_node
rpc_ss_marsh_change_buff
rpc_ss_return_pointer_to_node
rpc_ss_mem_alloc
rpc_ss_new_recv_buff
rpc_ss_lookup_pointer_to_node
rpc_ss_server_is_set_up
rpc_ss_init_server_once
rpc_ss_create_support_ptrs
rpc_ss_init_node_table
rpc_x_ss_pipe_comm_error
rpc_ss_send_server_exception
rpc_ss_destroy_support_ptrs
rpc_ss_mem_free
rpc_x_ss_remote_comm_failure
rpc_ss_strsiz
rpc_x_invalid_bound
rpc_x_no_memory
rpc_ss_client_is_set_up
rpc_ss_init_client_once
rpc_ss_client_establish_alloc
rpc_ss_call_end
rpc_ss_report_error
ndr_g_ebcdic_to_ascii
ndr_g_ascii_to_ebcdic
*** Error code 1 (continuing)

[10/05/92 public]
Dup of OT 5554.



CR Number                     : 5527
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : test
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : elbl_Init will start with a zero pass number
ass #
Reported Date                 : 10/2/92
Found in Baseline             : 1.0.2
Found Date                    : 10/2/92
Severity                      : A
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5508
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : file/episode/logbuf/log.c
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[10/2/92 public]
Pass #'s of zero are invalid.  Fix is to copy the logic in NewLogPage.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/19/92 public]
Ted's mucking in this code anyway and may have already fixed it.
Changed Responsible Engr. from `mason@transarc.com' to `ota@transarc.com'

[10/22/92 public]
This bug was fixed as part of the delta for OT 5508.

The specific test for this bug are the new scripts passno.test and
passno.rcvr.test.  These are now part of all_tests.
Changed Interest List CC from `ota,mcinerny@transarc.com' to 
 `ota,mcinerny@transarc.com,mason' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5508'

[10/22/92 public]
Changed Short Description from `elbl_Init will start with a zero pass #' to 
 `elbl_Init will start with a zero pass number'



CR Number                     : 5487
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : panic in tkm_OutRPC_ReAdd
Reported Date                 : 9/29/92
Found in Baseline             : 1.0.2b2
Found Date                    : 9/29/92
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 4833
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[9/29/92 public]
A panic with the following stack trace occurred on a PMAX running 
Transarc 1.50. 

tkm_OUtRPC_ReAdd
tkm_HostRevokeList_DoRevoke
tpq_Pardo_DoOp
tpq_HelperThread
osi_ThreadBase (called from osi_NewProc_ReleaseCallSpecCell)

The panic was caused by a reference to address zero.  I have seen
this exactly once.  If I see it again, I will bump the priority.

[9/29/92 public]

Well, it appears this is a dup of 4833, which I just re-opened from
"fix".  Since it has been seen twice, I bumped it to B1.  Enjoy.



CR Number                     : 5316
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 3820, 3773, 5070
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Use unified event set handling in FsyncForce
Reported Date                 : 9/2/92
Found in Baseline             : 1.0.2
Found Date                    : 9/2/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5508
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[9/2/92 public]
The new event set handling code in HandleLogFull (OT 5070) should be
adopted by the FsyncForce code.  This will go a long ways to addressing
the concerns of OT 3820.  See also OT 3773 for some discussion of this
issue.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/2/92 public]
Filled in Interest List CC with `mason@transarc.com'

[10/22/92 public]
This work was implemented as part of the delta for OT 5508.

Particularly, I added fsync calls in several places in mp.int.test.
This caused mp.test and mp_repeat.test to fail.  These tests are part of
all_tests.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5508'



CR Number                     : 5307
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : power1 invalid log 920812
Reported Date                 : 9/2/92
Found in Baseline             : 1.0.2
Found Date                    : 9/2/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 1020
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[9/2/92 public]
Copied aggregate to:
    ~ota/de/salvage-data/power1.920812.invalid-log.no_rcvr.Z
Patched AVL_FLAGS to 0.  Running recovery gets:
    CheckAggregate: Unknown code DFS:lgb 6 (609947654) initializing log and running recovery
    Message for 609947654 is 'the specified log was not valid (dfs / lgb)'

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/2/92 public]
Changed aggregate name above and added error code translation.

[11/4/92 public]
 The failure here was fixed under 5508 (the pass # field was set to 0,
even though 0 was not valid).

Marked as duplicate.

It really is a dup of 5508; OT will not allow me to mark it as a dup
of 5508 because 5508 > 5307, even though the fix is already done and
about to be submitted to OSF.

Changed Status from `open' to `dup' 
Filled in Duplicate Of with `1020'



CR Number                     : 5249
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : completed trans handling is badly confused
Reported Date                 : 8/28/92
Found in Baseline             : 1.0.2
Found Date                    : 8/28/92
Severity                      : C
Priority                      : 1
Status                        : dup
Duplicate Of                  : 5508
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : open

[8/28/92 public]
The handling of completed transactions is very confused.  It was thought
that a transaction had to pin the log until the end record was
successfully written to the disk.  This is only true for external
observers (i.e. a human that is about to pull the power plug from the
wall).  For internal consistency all that is required is that the end
record be successfully formatted into the current log page.  This is
because of the innate serialization of log records.

The result of this confusion is a lot of thrashing around trying to
track completed but uncommitted transactions.  It doesn't presently seem
as if this is causing any bugs but it greatly complicates and confuses
the code.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[10/22/92 public]
This work was completed as part of the delta for 5508.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `5508'



CR Number                     : 5025
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : Cache manager failed to pass cthon/test3 on the client(rios).
Reported Date                 : 8/12/92
Found in Baseline             : 1.0.2
Found Date                    : 8/12/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 5292
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[8/12/92 public]
It is four machine cell configured by dfs-102-1.46 backed by 0730 osf daily.
Running cthon/test3 on a rios client against a rios server results a failure
of "getwd()" in the test.

./test3: Calls to getwd() and stat()
./test3: getwd failed
0.2u 0.17s 0:02 6% 13+2k 0+0io 2pf+0w
Exit status was 1

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[9/2/92 public]
This is a dup of the ENOTTY bug.
Filled in Duplicate Of with `5292' 
Changed Transarc Status from `open' to `export'

[9/2/92 public]
changing Status to ``dup'' so it doesn't look open.
Changed Status from `open' to `dup' 
Changed Transarc Status from `export' to `closed'



CR Number                     : 4821
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : repserver
Short Description             : repserver multi-threading support broken
Reported Date                 : 7/24/92
Found in Baseline             : 1.0.1
Found Date                    : 7/24/92
Severity                      : D
Priority                      : 3
Status                        : dup
Duplicate Of                  : 1132
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jaffe@transarc.com
Transarc Status               : closed

[7/24/92 public]
I thought that this was already in the OT database, but I could be wrong.
The replication server (repserver) cannot currently be run with more than
one main process thread due to some deficiencies in the locking model that
it uses.  There's a small matter of programming to make it do the
assembly of lots of tasks together in a multi-thread-safe manner.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jaffe@transarc.com' 
Added field Transarc Status with value `open'

[8/13/92 public]
This is a duplicate of (deferred) bug 1132.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `1132' 
Changed Transarc Status from `open' to `closed'



CR Number                     : 4194
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : salvage refuses to fix allocation mismatch
Reported Date                 : 6/10/92
Found in Baseline             : 1.0.1
Found Date                    : 6/10/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 4192
Fix By Baseline               : 1.0.2
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : jdp@transarc.com
Transarc Status               : open

[6/10/92 public]
We encountered an aggregate which, after fixing many indirect block
errors, cannot be salvaged into an error free condition.  It looks like
it is trying to repair the allocated field of two anodes twice, once
from 0 to 1(2) and then later from 1(2) back to zero.  The output looks
like this:
    W% $epiobj/salvage/salvage /tmp/salvage-data/lastIndex
    Salvaging /tmp/salvage-data/lastIndex
    Will run recovery on /tmp/salvage-data/lastIndex
    epia_VerifyAnode: anode allocated (0)/calculcated (1) mismatch
    Repairing allocated mismatch
    epia_VerifyAnode: anode allocated (0)/calculcated (2) mismatch
    Repairing allocated mismatch
    epia_VerifyAnode: anode allocated (1)/calculcated (0) mismatch
    Repairing allocated mismatch
    epia_VerifyAnode: anode allocated (2)/calculcated (0) mismatch
    Repairing allocated mismatch
    Processed 2 vols 106 anodes 7 dirs 113 files 0 acls
    Done.  /tmp/salvage-data/lastIndex checks out as Episode aggregate.
    W%

The offending aggregate is taking up space in:
    ~ota/de/salvage-data/gemini.920610.alloc_mismatch.Z.
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `jdp@transarc.com' 
Added field Transarc Status with value `open'

[6/12/92 public]
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `4192'



CR Number                     : 4143
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : DFS cache manager generates authentication warnings.
Reported Date                 : 6/9/92
Found in Baseline             : 1.0.1b18
Found Date                    : 6/9/92
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 4026
Fix By Baseline               : 1.0.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[6/9/92 public]

I was running test in a two  machine cell, one a server the other a
client.  The tests were being run from the client in a UFS fileset.  I was
also running unauthenticated and had set the environment variable
RPC_SUPPORTED_PROTSEQS to ncadg_ip_udp.  I got the following error message.

dfs: set authen binding failed (code 12)
dfs: Warning: create an unauthenticated binding

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[6/9/92 public]
I believe that this may duplicate an existing defect. I'm unable to find
the duplicate at this time.
Filled in Interest List CC with `kazar@transarc.com' 
Changed Responsible Engr. from `pakhtar@transarc.com' to `tu@transarc.com'

[6/9/92 public]
This is actually a duplication of OT#4026. It looks like the problem 
could result from a "bad" environment. Error code (=12) indicated that 
the system did not have enough core memory.

[6/9/92 public]
This defect is the same as in OT 4026. Hence copying all the entries in this
defect to OT 4026 and marking this a duplicate as suggested by Mr. Akhtar.

Changed Interest List CC from `kazar@transarc.com' to `kazar@transarc.com, 
 rajesh@transarc.com, bk@transarc.com' 
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `4026'



CR Number                     : 4094
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : pmax
S/W Ref Platform              : osf1
Component Name                : dfs
Subcomponent Name             : episode
Short Description             : dfsexport on episode returns ENXIO
Reported Date                 : 6/5/92
Found in Baseline             : 1.0.1b18
Found Date                    : 6/5/92
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 
Fix By Baseline               : 1.0.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[6/5/92 public]
More precisely, the error message is:

dfsexport: Failed to attach /dev/rz1d:/episode (err=6).  Ignoring it.

This is using the kernel out of the 6/4 nightly build with core components 
out of last weekend's nightly build.  I started tracking the problem using 
kdb and the error appears to be coming from epig_InitAggregate() within 
ag_efsAttach().

I am able to export UFS partitions (at least root.dfs) just fine.

The partition being exported was not mounted locally at the time.
"salvage -verify" reports that the partition "checks out" as an
Episode aggregate.

[6/5/92 public]
This is a dup of 3839.



CR Number                     : 3980
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : cm
Short Description             : DFS cache manager loses context with fx server.
Reported Date                 : 6/2/92
Found in Baseline             : 1.0.1b17
Found Date                    : 6/2/92
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 
Fix By Baseline               : 1.0.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : tu-ot3048-cm-stateserver-wrong 1.4
Transarc Herder               : mason@transarc.com
Transarc Status               : submit

[6/2/92 public]

I get the following message after I have had a DFS system up for awhile.

dfs: Warning: losts the context from fx server192.55.207.63 in cell dfstest.transarc.com !

I then tried to do an ls inside of a DFS directory and got the following 
message

ls: 0653-341 The file . does not exist.

A pwd returns,

pwd: A remote host did not respond within the timeout period.

This occurs on a single machine.

Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[06/02/92 public]

This is a duplicate of 3968

[6/2/92 public]
This one is fixed. The fix has been already been in Transarc's 
May 29 submit to OSF. 
Changed H/W Ref Platform from `rs6000' to `all' 
Filled in Interest List CC with `cfe@transarc.com' 
Filled in Responsible Engr. with `tu@transarc.com' 
Filled in Transarc Deltas with `tu-ot3048-cm-stateserver-wrong 1.4' 
Changed Transarc Status from `open' to `'

[6/3/92 public]
This was submitted 6/1.
Filled in Transarc Status with `submit'



CR Number                     : 3815
Defect or Enhancement?        : enh
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : all
S/W Ref Platform              : all
Component Name                : dfs
Subcomponent Name             : lfs
Short Description             : Re-evaluate episode open file limit.
Reported Date                 : 5/26/92
Found in Baseline             : 1.0.1
Found Date                    : 5/26/92
Severity                      : B
Priority                      : 1
Status                        : dup
Duplicate Of                  : 2340
Fix By Baseline               : 1.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : mason@transarc.com
Transarc Status               : open

[5/26/92 public]
This is tr defect #2042.
The current Episode open file limit is 128.  What should it be?
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `mason@transarc.com' 
Added field Transarc Status with value `open'

[7/20/92 public]
Changed "Fix by" to 1.0.2

[11/2/92 public]
This defect deserves a priority much higher than a D 3!  128 simultaineous
opens is no where near enough.  For example the AIX JFS supports
a base of 4000 opens + a number based on the amount of physical
memery on the machine (approx. 32 more opens for each meg).
I would say Episode at least needs to support at least 2000 simultaneous
opens on a moderatly configured machine (ie. 16+ meg).
Filled in Interest List CC with `demail1!carl' 
Changed Severity from `D' to `A' 
Changed Priority from `3' to `1'

[11/3/92 public]
 I'm reassigning to myself; while we can change the limits
artificially by increasing the defaults, I believe the real solution
here is to modify dce_config to ask on setup; I've just finished
changing rc.dfs and dce_config to use dcelfs on AIX and epiinit on
OSF/1; the next task is to add some additional steps when setting up
LFS to prompt for the requisite information and use that as the
initialization parameters.
In the meantime, the work-around is to modify your rc.dfs to use the
correct switches.  For AIX:
  /opt/dcelocal/ext/cfglfs -a <filename> [-single] [-remote] [-bcache
  <1K blocks>] [-vnodes <num vnodes>] [-anodes <num anodes>] [-dircache
  <buffers>] [-locks <num locks>]
For OSF/1:
  /opt/dcelocal/bin/epiinit [-single] [-remote] [-bcache <1K blocks>]
  [-vnodes <num vnodes>] [-anodes <num anodes>] [-dircache <buffers>]
  [-locks <num locks>]
(note the similarity between the two systems...)
I'd suggest the following for a typical 32MB AIX system:
cfglfs -a dcelfs.ext -vnodes 2000 -anodes 2000 -dircache 32 -locks 32
Increase the # of locks if you expect to use flock substantially.
For OSF/1 I'd suggest:
epiinit -bcache 1024 -vnodes 1000 -anodes 1000 -dircache 32 -locks 32
because LFS buffers data as well as meta-data.
Finally, FYI, the default is:
<initcmd> -bcache 384 -vnodes 128 -anodes 1024 -dircache 15 -locks 15
src/file/episode/utils/epiinit.c
I've lowered severity because the tools are there and are even
documented.  The problem is really in the configuration stuff.
Changed Interest List CC from `demail1!carl' to `demail1!carl,bwl,ota,rsarbo' 
Changed Severity from `A' to `B' 
Changed Responsible Engr. from `bwl@transarc.com' to `mason@transarc.com'

[11/16/92 public]
 This really is a dup of 2340.  The facilities for using a higher
limit are there.  Each vendor is going to have to decide on the
"right" set of limits.  Setting it artificially high means that a
vendor might miss setting it correctly, while setting it low assures
someone will stumble across it and try to make the right decision.
Changed Status from `open' to `dup' 
Filled in Duplicate Of with `2340'



CR Number                     : 3720
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : tkm
Short Description             : tkm_test.l and tkm_test.y were defuncted but not removed out of machine dependenct Makefile
Reported Date                 : 5/21/92
Found in Baseline             : 1.0.1
Found Date                    : 5/21/92
Severity                      : A
Priority                      : 0
Status                        : dup
Duplicate Of                  : 
Fix By Baseline               : 1.0.1
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public
Transarc Deltas               : 
Transarc Herder               : 
Transarc Status               : 

[5/21/92 public]
There were two instances of make: don't know how to make, one
each for tkm_test.l and tkm_test.y in file/tkm.  According to
the submit log the two files were defuncted, apparently the
RIOS/Makefile in file/tkm was not updated and expected to
build them.

[5/21/92 public]
This is a duplicate of 3719 a fix for which has already been submitted.
Changed Status from `open' to `dup' 
Added field Transarc Deltas with value `' 
Added field Transarc Herder with value `' 
Added field Transarc Status with value `'



CR Number                     : 1133
Defect or Enhancement?        : def
CR in Code, Doc, or Test?     : code
Inter-dependent CRs           : 
Project Name                  : dce
H/W Ref Platform              : rs6000
S/W Ref Platform              : aix
Component Name                : dfs
Subcomponent Name             : 
Short Description             : add support for read-only aggregates
Reported Date                 : 12/19/91
Found in Baseline             : .60d
Found Date                    : 
Severity                      : C
Priority                      : 2
Status                        : dup
Duplicate Of                  : 1115
Fix By Baseline               : 
Fixed In Baseline             : 
Affected File(s)              : 
Sensitivity                   : public

[01/29/92 public]

Orbit reference number 4309
reference     
product
phaseFound    development

History:
lastUpdate    91/12/19 13:41:24
endDate                                
assignDate    91/12/19 13:41:24   





    addDate              action          userLogin (userName)
    -------------------- --------------- ----------------------------------
    91/12/19 13:41:24    create          hirsch (Phil Hirsch)

[12/19/91 public]

If you export a UFS partition that is locally mounted as a read-only
filesystems, the exported aggregate will be writable. There should be
a way to specify that an aggregate is to be exported in a read-only
manner.

[03/19/92 public]
Duplicate of 1115, which has been deferred into the 1.1 timeframe.



