<!--
# @OSF_COPYRIGHT@
# 
# 
# HISTORY
# $Log: issues.sgm,v $
# Revision 1.1.2.8  1996/12/15  23:02:05  wardr
# 	{edit,R1.2.2}
# 	[D[DFiformatting problems before penultimate build
# 	[1996/12/15  23:00:36  wardr]
#
# Revision 1.1.2.7  1996/12/14  23:14:26  wardr
# 	{edit,R1.2.2}
# 	Fixed figure problems
# 	[1996/12/14  23:12:46  wardr]
# 
# Revision 1.1.2.6  1996/12/12  23:02:01  jeff
# 	Incorporate OSF edits from SGML review
# 	[1996/12/12  23:00:35  jeff]
# 
# Revision 1.1.2.5  1996/11/07  16:09:40  weir
# 	Moved changebar code from empty paragraph
# 	[1996/11/07  16:09:14  weir]
# 
# Revision 1.1.2.4  1996/11/06  18:47:13  weir
# 	Cleaned up history
# 	[1996/11/06  18:46:07  weir]
# 
# Revision 1.1.2.3  1996/10/28  17:33:55  carrig
# 	{enh,R1.2.2}
# 	Ready for editor
# 	[1996/10/28  17:19:38  carrig]
# 
# Revision 1.1.2.2  1996/10/24  20:10:31  carrig
# 	{enh,R1.2.2}
# 	Fixed tables
# 	[1996/10/24  20:08:38  carrig]
# 
# Revision 1.1.2.1  1996/10/22  20:42:59  wardr
# 	{edit,R1.2.2}
# 	Initial checkin after sgml conversion
# 	[1996/10/22  20:41:17  wardr]
# 
# Revision 1.1.1.2  1996/10/22  20:41:17  wardr
# 	{edit,R1.2.2}
# 	Initial checkin after sgml conversion
# 
# $EndLog$
-->
<!---->
<!-- Fragment document type declaration subset:
ArborText, Inc., 1988-1993, v.4001
<!DOCTYPE Book PUBLIC "-//Davenport//DTD DocBook V2.4//EN" [
<!ENTITY  DFSAGR.ISSU.ent.1  SYSTEM "eps/pref1.ps" NDATA eps>
<!ENTITY  DFSAGR.ISSU.ent.2  SYSTEM "eps/pref2.ps" NDATA eps>
<!ENTITY  DFSAGR.ISSU.ent.3  SYSTEM "eps/pref3.ps" NDATA eps>
<!ENTITY  DFSAGR.ISSU.ent.4  SYSTEM "eps/override1.ps" NDATA eps>
]>
-->
<!---->
<!-- COPYRIGHT NOTICE-->
<!-- Copyright (c) 1990, 1991, 1992, 1993 Open Software Foundation, Inc.-->
<!-- ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE in the-->
<!-- src directory for the full copyright text.-->
<!---->
<!---->
<!-- HISTORY-->
<!-- Revision 1.1.11.4  1996/09/16  20:11:07  wfl-->
<!-- 	Added editorial changes-->
<!-- 	[1996/09/16  20:10:19  wfl]-->
<!---->
<!-- Revision 1.1.11.3  1996/08/20  13:15:28  wfl-->
<!-- 	{enh,13605,R1.2.2}-->
<!-- 	Security enhancements-->
<!-- 	[1996/08/20  13:14:55  wfl]-->
<!-- -->
<!-- Revision 1.1.11.2  1996/07/16  19:50:48  wfl-->
<!-- 	{enh, 13566, R1.2.2}-->
<!-- 	Added multihomed servers-->
<!-- 	[1996/07/16  19:50:17  wfl]-->
<!-- -->
<!-- Revision 1.1.11.1  1996/05/14  19:52:32  wardr-->
<!-- 	{enh,R1.2.2}-->
<!-- 	Removed changebars-->
<!-- 	[1996/05/14  19:51:18  wardr]-->
<!-- -->
<!-- Revision 1.1.8.3  1996/01/23  16:57:53  weir-->
<!-- 	Test checkin/out-->
<!-- 	[1996/01/23  16:57:26  weir]-->
<!-- -->
<!-- Revision 1.1.8.2  1995/10/04  16:02:48  wfl-->
<!-- 	{def, 13140, R1.2.1}-->
<!-- 	Fixed quotes in change markers-->
<!-- 	[1995/10/04  16:02:19  wfl]-->
<!-- -->
<!-- Revision 1.1.8.1  1995/09/16  16:08:39  wfl-->
<!-- 	{enh,13093,R1.2.1}-->
<!-- 	Add server preference-->
<!-- 	[1995/09/16  16:08:17  wfl]-->
<!-- -->
<!-- Revision 1.1.6.26  1995/07/07  19:06:00  buckler-->
<!-- 	More 1.1 edits.-->
<!-- 	[1995/07/07  19:04:43  buckler]-->
<!-- -->
<!-- 	1.1 edits and Prentice Hall reformat-->
<!-- 	[1995/07/07  16:43:11  buckler]-->
<!-- -->
<!-- Revision 1.1.6.25  1994/10/14  19:34:40  jeff-->
<!-- 	{defect, 12535, R1.1}-->
<!-- 	Incorporate dcecp commands in DFS documentation.-->
<!-- 	[1994/10/14  19:32:51  jeff]-->
<!-- -->
<!-- Revision 1.1.6.24  1994/08/10  22:28:28  jeff-->
<!-- 	Editorial work.-->
<!-- 	[1994/08/10  22:27:24  jeff]-->
<!-- -->
<!-- Revision 1.1.6.23  1994/07/18  23:41:31  jeff-->
<!-- 	{defect, 11350, R1.1}-->
<!-- 	Document root/self privileges.-->
<!-- 	[1994/07/18  23:40:33  jeff]-->
<!-- -->
<!-- Revision 1.1.6.22  1994/07/18  22:40:36  jeff-->
<!-- 	Editorial work.-->
<!-- 	[1994/07/18  22:40:22  jeff]-->
<!-- -->
<!-- Revision 1.1.6.21  1994/06/01  11:48:20  jeff-->
<!-- 	Minuscule editorial change.-->
<!-- 	[1994/06/01  11:48:05  jeff]-->
<!-- -->
<!-- Revision 1.1.6.20  1994/05/31  22:40:57  jeff-->
<!-- 	Correct formatting.-->
<!-- 	[1994/05/31  22:40:43  jeff]-->
<!-- -->
<!-- Revision 1.1.6.19  1994/05/25  15:04:54  jeff-->
<!-- 	{defect, 10752, R1.1}-->
<!-- 	Minor technical clarification for database access with Ubik.-->
<!-- 	[1994/05/25  15:04:35  jeff]-->
<!-- -->
<!-- Revision 1.1.6.18  1994/05/24  22:00:52  jeff-->
<!-- 	{defect, 10752, R1.1}-->
<!-- 	Minor technical clarification for fileset names.-->
<!-- 	[1994/05/24  22:00:26  jeff]-->
<!-- -->
<!-- Revision 1.1.6.17  1994/05/19  20:49:49  jeff-->
<!-- 	{defect, 8118, R1.1}-->
<!-- 	Correct use of double quotes.-->
<!-- 	[1994/05/19  20:48:56  jeff]-->
<!-- -->
<!-- Revision 1.1.6.16  1994/05/18  14:47:36  jeff-->
<!-- 	Fix more formatting problems.-->
<!-- 	[1994/05/18  14:46:45  jeff]-->
<!-- -->
<!-- Revision 1.1.6.15  1994/05/17  14:53:53  jeff-->
<!-- 	Correcting small formatting errors.-->
<!-- 	[1994/05/17  14:49:18  jeff]-->
<!-- -->
<!-- Revision 1.1.6.14  1994/05/13  18:19:11  jeff-->
<!-- 	{defect, 9472, R1.1}-->
<!-- 	Document that database servers must be in each other's admin lists.-->
<!-- 	[1994/05/13  18:18:51  jeff]-->
<!-- -->
<!-- Revision 1.1.6.13  1994/05/06  13:52:18  jeff-->
<!-- 	{defect, 10552, R1.1}-->
<!-- 	Fix index entry inconsistencies.-->
<!-- 	[1994/05/06  13:49:54  jeff]-->
<!-- -->
<!-- Revision 1.1.6.12  1994/04/28  22:21:17  jeff-->
<!-- 	{defect, 10439, R1.1}-->
<!-- 	Correct cross-references for reorganization of DFS documentation.-->
<!-- 	[1994/04/28  22:14:26  jeff]-->
<!-- -->
<!-- Revision 1.1.6.11  1994/04/20  21:05:55  jeff-->
<!-- 	{enh, 10306, R1.1}-->
<!-- 	Confirm/correct removal of diskless references.-->
<!-- 	[1994/04/20  21:01:34  jeff]-->
<!-- -->
<!-- Revision 1.1.6.10  1994/04/07  19:56:26  rom-->
<!-- 	{enh, 10306, R1.1}-->
<!-- 	Remove diskless documentation from the DCE doc set.-->
<!-- 	[1994/04/07  15:31:41  rom]-->
<!-- -->
<!-- Revision 1.1.6.9  1993/11/11  16:36:29  zahn-->
<!-- 	Checked out for possible Postscript problem.-->
<!-- 	None found, no changes necessary.-->
<!-- 	[1993/11/11  16:36:02  zahn]-->
<!-- -->
<!-- Revision 1.1.6.8  1993/10/15  12:18:47  kdu-->
<!-- 	{def,8393,R1.0.3}-->
<!-- 	Document the udebug command.-->
<!-- 	[1993/10/15  12:17:57  kdu]-->
<!-- -->
<!-- Revision 1.1.6.7  1993/10/13  20:23:58  tmw-->
<!-- 	Added index entries for second version of master index.-->
<!-- 	[1993/10/13  15:25:30  tmw]-->
<!-- -->
<!-- Revision 1.1.6.6  1993/09/23  13:18:11  kdu-->
<!-- 	{def,7715,R1.0.3}-->
<!-- 	Examples of sysname.-->
<!-- 	[1993/09/23  13:17:31  kdu]-->
<!-- -->
<!-- Revision 1.1.6.5  1993/09/17  13:33:54  kdu-->
<!-- 	{def,8616,R1.0.3}-->
<!-- 	Foreign groups cannot own server entries.-->
<!-- 	[1993/09/17  13:33:01  kdu]-->
<!-- -->
<!-- Revision 1.1.6.4  1993/09/15  21:28:01  kdu-->
<!-- 	{def,8515,R1.0.3}-->
<!-- 	Availability of read-only filesets.-->
<!-- -->
<!-- 	{def,8590,R1.0.3}-->
<!-- 	Configuring a fileset database machine.-->
<!-- 	[1993/09/15  21:26:57  kdu]-->
<!-- -->
<!-- Revision 1.1.6.3  1993/08/10  23:50:26  jeff-->
<!-- 	Changed versions for defect fixes from 1.0.2A to 1.0.3.-->
<!-- 	[1993/08/10  23:49:46  jeff]-->
<!-- -->
<!-- Revision 1.1.6.2  1993/07/30  17:31:38  kdu-->
<!-- 	{def,8386,R1.0.3}-->
<!-- 	Incorporate OSF editorial comments into DFS Admin Guide and related-->
<!-- 	documentation.-->
<!-- 	[1993/07/30  17:18:47  kdu]-->
<!-- -->
<!-- Revision 1.1.4.12  93/02/22  19:17:29  jeff-->
<!-- 	Fix for defect 5795, update backup documentation.-->
<!-- 	[1993/02/22  19:16:47  jeff]-->
<!-- -->
<!-- Revision 1.1.4.11  1993/02/22  01:11:24  jeff-->
<!-- 	Fix for defect 7219, review comments.-->
<!-- 	[1993/02/22  01:10:40  jeff]-->
<!-- -->
<!-- Revision 1.1.4.10  1993/02/19  18:38:14  jeff-->
<!-- 	Fix for defect 6055, clarify root privileges (also-->
<!-- 	some work for defect 7219).-->
<!-- 	[1993/02/19  18:37:07  jeff]-->
<!-- -->
<!-- Revision 1.1.4.9  1993/02/05  20:16:08  jeff-->
<!-- 	Fix for defect 7135, change key file to keytab file.-->
<!-- 	[1993/02/05  20:14:54  jeff]-->
<!-- -->
<!-- Revision 1.1.4.8  1993/02/04  01:56:15  jeff-->
<!-- 	Fix for defects 6888 and 4016.-->
<!-- 	[1993/02/04  01:54:36  jeff]-->
<!-- -->
<!-- Revision 1.1.4.7  1993/01/28  19:05:07  dbelch-->
<!-- 	Embedding copyright notice-->
<!-- 	[1993/01/28  18:29:00  dbelch]-->
<!-- -->
<!-- Revision 1.1.4.6  1993/01/27  20:07:01  buckler-->
<!-- 	Fixed cross-refs and figure calls for new book org-->
<!-- 	[1993/01/27  20:04:52  buckler]-->
<!-- -->
<!-- Revision 1.1.4.5  1993/01/27  19:10:23  jeff-->
<!-- 	... TEMPORARY CHECK-IN TO ALLOW FOR OSF WORK....-->
<!-- 	[1993/01/27  19:00:44  jeff]-->
<!-- -->
<!-- Revision 1.1.4.4  1993/01/13  19:13:19  jeff-->
<!-- 	Fix for defect 6811, -ioprocs of dfsd is AIX specific.-->
<!-- 	[1993/01/13  19:12:31  jeff]-->
<!-- -->
<!-- Revision 1.1.4.3  1993/01/09  18:46:36  jeff-->
<!-- 	Fix for defects 6326 and 6594, update Ubik and related-->
<!-- 	information and document dfsbind changes, respectively.-->
<!-- 	[1993/01/09  18:46:04  jeff]-->
<!-- -->
<!-- Revision 1.1.4.2  1992/08/26  12:09:22  weir-->
<!-- 	Removed change bars-->
<!-- 	[1992/08/26  11:51:46  weir]-->
<!-- -->
<!-- Revision 1.1.2.8  1992/07/05  19:25:40  jeff-->
<!-- 	Corrected Private File Server machine processes.-->
<!-- 	[1992/07/05  19:24:24  jeff]-->
<!-- -->
<!-- Revision 1.1.2.7  1992/07/04  15:48:35  jeff-->
<!-- 	Documented that inclusion in the fxd admingroup and being-->
<!-- 	root are not equivalent.-->
<!-- 	[1992/07/04  15:48:07  jeff]-->
<!-- -->
<!-- Revision 1.1.2.6  1992/06/06  16:48:43  jeff-->
<!-- 	Updated descriptions of the privileges provided by inclusion-->
<!-- 	in the group specified with the fxd -admingroup option.-->
<!-- 	[1992/06/06  00:41:19  jeff]-->
<!-- -->
<!-- Revision 1.1.2.5  1992/06/04  21:52:40  jeff-->
<!-- 	Edited to emphasize that the fs junction is not well known.-->
<!-- 	[1992/06/04  21:48:40  jeff]-->
<!-- -->
<!-- Revision 1.1.2.4  1992/06/04  18:32:41  jeff-->
<!-- 	Updated the information described as prerequisite for the-->
<!-- 	Ubik synchronization mechanism.  A couple large chunks of-->
<!-- 	text were added, and some existing facts were modified.-->
<!-- 	[1992/06/04  18:30:02  jeff]-->
<!-- -->
<!-- Revision 1.1.2.3  1992/05/12  15:42:17  jeff-->
<!-- 	Verified and/or modified italics and other editorial-->
<!-- 	aspects of the file.-->
<!-- 	[1992/05/11  19:21:54  jeff]-->
<!-- -->
<!-- Revision 1.1.2.2  1992/03/05  23:42:18  jeff-->
<!-- 	Testing ODE.-->
<!-- 	[1992/03/05  22:37:40  jeff]-->
<!-- -->
<!-- Revision 1.1  1992/01/29  16:14:39  damon-->
<!-- 	Initial revision-->
<!-- -->
<!---->
<!-- (c) Copyright 1991, Open Software Foundation, Inc.  ALL RIGHTS RESERVED-->
<!--no-op:  Copyright (C) 1989, 1991, Transarc Corporation-->
<!--no-op:  The Gulf Tower-->
<!--no-op:  707 Grant Street-->
<!--no-op:  Pittsburgh, PA  15219-->
<!-- CHANGED-->
<!-- 12-11-91:  Section 2.3.5:  Removed first two sentences from last par.-->
<!-- 12-11-91:  Added references to Private File Server machine to Sections-->
<!--            2.1.1, 2.1.1.3, and 2.1.1.7.  References consist of merely-->
<!--            using the term.-->
<!-- 12-11-91:  Section 2.1.1.7:  Added four index tags for Private File-->
<!--            Server machine.-->
<!-- END CHANGED-->
<!--DOCUMENTSTYLE [12pt]{book}-->
<Chapter Id="DFSAGR.ISSU.div.1">
<Title>DFS Configuration Issues</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.1">
<Primary>Distributed File Service (DFS)</Primary>
<Secondary>configuration</Secondary>
</IndexTerm>
<Para>This chapter provides summary information about the following DFS configuration
issues: choosing DFS machine roles, DFS server and client configuration issues,
setting up DCE LFS filesets, understanding DFS data access management, and
understanding the DFS distributed database technology.  Subsequent chapters
provide specific details about managing DFS server machines, processes,
filesets, and files.
</Para>
<Para>This chapter is intended as an overview of DFS configuration issues. It also
serves as a reference for the issues and considerations that go into the
configuration of a cell and its administrative domains.  You should read and 
become familiar with the information in this chapter before attempting to 
use any of the commands described later in this guide.
</Para>
<Sect1 Id="DFSAGR.ISSU.div.2">
<Title>Choosing DFS Machine Roles</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.2">
<Primary>machines</Primary>
<Secondary>roles in DFS</Secondary>
</IndexTerm>
<Para>DFS server and client machines can run the following processes:
the BOS Server to monitor other processes; the Fileset Server, Fileset
Location Server, and Replication Server to manipulate DFS filesets
and their replicas; the Backup Server to contact the Backup Database;
the <Command>butc</Command> process to back up file system data to tape; and the
<Command>dfsd</Command> process to initialize the Cache Manager on a client machine.
</Para>
<Para>Each DFS server or client machine must also run the RPC, CDS, and Security
processes necessary for configuration as a DCE client machine.  An RPC binding
must be created in CDS for the DCE pathname of each server machine, and a DFS
server principal and associated account must also be created in the Registry
Database for each server machine.  The following sections assume that these
requirements have been satisfied prior to configuring a machine as a DFS
server or client machine. (See your vendor's installation and configuration
documentation for more information about fulfilling these requirements.)
</Para>
<Para>The system administrator determines, at installation, which processes are to
be run on which machines.  A machine's role is determined by the types of
processes it runs.  The information in the following subsections details the 
different roles a machine can assume.
</Para>
<Para>Each DFS server process has an associated administrative list.  Users, groups,
and server machines included on a process's administrative list can issue
commands or calls that affect the process.  Members can be added to
administrative lists at any time.  Chapter 4 provides detailed information
about the procedures used to create and maintain administrative lists. (See
Part 2 of this guide and reference for complete information about the
administrative privileges and permissions required to issue each DFS command.)
</Para>
<Para>The Basic OverSeer (BOS) Server, or <Command>bosserver</Command> process, is not
associated with any one machine role; it runs on every DFS server machine.
Its primary function is to minimize system outages.  It monitors other server
processes on the local machine and restarts failed processes automatically.
</Para>
<Para><?sml-need 10>By default, the BOS Server on each server machine stops and immediately
restarts all DFS processes (including itself) on the machine once a week, at
4:00 a.m. on Sunday.  It also checks for any newly installed binary files in
the <Symbol Role="Variable">dcelocal</Symbol><Filename>/bin</Filename> directory every morning at 5:00 a.m. (Note that
these restart times can be configured.) If it finds any new files, which
it does by checking for timestamps later than the time at which the 
corresponding process last started, it restarts the corresponding process. 
Because restarting processes causes a service outage, the default times are 
in the early morning hours, when an outage disturbs the fewest number of 
users.  This brief suspension of services should have no effect on processes 
that are currently executing; the processes should continue normally once 
service resumes.
</Para>
<Para>Install the BOS Server on all server machines to assist in administrative tasks
on the machines.  The <Filename>admin.bos</Filename> list is used to designate administrative
users who can issue <Command>bos</Command> commands that affect the <Command>bosserver</Command> process
on a server machine.  Members of the <Filename>admin.bos</Filename> list can vary among
different DFS administrative domains.
</Para>
<Sect2 Id="DFSAGR.ISSU.div.3">
<Title>Overview of DFS Machine Roles</Title>
<Para>Following is a brief summary of the DFS roles a machine can assume:
</Para>
<ItemizedList>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.3">
<Primary>System Control machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>System Control machine: A single machine acts as the System Control
machine for a domain, updating the other machines in the domain with identical
versions of common configuration files such as administrative lists.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.4">
<Primary>Binary Distribution machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>Binary Distribution machine: One Binary Distribution machine of each
CPU/operating system (OS) type is installed in a cell.  The Binary Distribution
machine updates other machines of its CPU/OS type with identical versions of
system binary files.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.5">
<Primary>File Server machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>File Server machine: A File Server machine runs the basic set of
processes necessary for storing and exporting DCE LFS and non-LFS data.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.6">
<Primary>Fileset Database machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>Fileset Database machine: This type of database machine runs the process
that maintains the Fileset Location Database (FLDB).
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.7">
<Primary>Backup Database machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>Backup Database machine: This type of database machine runs the process
that maintains the Backup Database.
<?sml-break><?sml-need 10></Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.8">
<Primary>client machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>DFS client machine: Any machine can run the Cache Manager and its associated
processes to act as a DFS client.  This machine serves primarily as a single or
multiuser workstation.  It can also be configured as a Private File Server
machine to export data.
</Para>
</ListItem>
</ItemizedList>
<Para>Depending on the number of machines in your cell, assign the following roles
to your server machines:
</Para>
<ItemizedList>
<ListItem>
<Para>In a cell with only one server machine, the machine runs all processes and
fills all the necessary machine roles.  Note that the System Control machine 
and Binary Distribution machine roles are unnecessary in this configuration.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.9">
<Primary>Fileset Database machines</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.10">
<Primary>Backup Database machines</Primary>
</IndexTerm>
<Para>In a cell with two server machines, both machines act as Fileset Database
machines and Backup Database machines to replicate the databases.  For each
database, one of the machines automatically assumes the role of the
synchronization site and houses the source copy of the database.  If one of the
machines becomes unavailable, the information in the database may not be able
to be changed. (See Section 2.5 for a detailed description of database
synchronization.)
</Para>
</ListItem>
<ListItem>
<Para>In a cell with three or more server machines, three machines run as Fileset
Database machines and three machines run as Backup Database machines.  This
configuration allows the cell to benefit from the database replication
capabilities of DFS.  An odd number of database machines is best.
</Para>
</ListItem>
</ItemizedList>
<Para>The software for all server processes can be installed on every server
machine, even though a machine need not run every process.  To change the role
of a machine, simply start or stop the appropriate processes.  Machine roles
are not mutually exclusive; that is, any server machine can assume multiple
server machine roles, any server machine can be configured as a client machine,
and any client machine can be configured as a server machine.
<?sml-break><?sml-need 15></Para>
<Sect3 Id="DFSAGR.ISSU.div.4">
<Title>System Control Machines</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.11">
<Primary>System Control machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.12">
<Primary>Update Server</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>The System Control machine in a domain stores and distributes system
configuration information, such as administrative lists, shared by all DFS
server machines in the domain.  Configure the first server machine for
any new domain as the System Control machine for that domain.  It can then be 
used to distribute the administrative lists for that domain from its 
<Symbol Role="Variable">dcelocal</Symbol><Filename>/var/dfs</Filename> directory to any subsequent server machines 
added to the domain.
</Para>
<Para>The following processes run on a System Control machine:
</Para>
<ItemizedList>
<ListItem>
<Para>An <Command>upserver</Command> process (the server portion of the Update Server), which
controls the distribution of common configuration files to all other server
machines in the domain.
</Para>
</ListItem>
<ListItem>
<Para>An <Command>upclient</Command> process (the client portion of the Update Server), which
retrieves binary files from the Binary Distribution machine of the proper
CPU/OS type. (See Section 2.1.1.2 for a description of the Binary Distribution
machine.)
</Para>
</ListItem>
<ListItem>
<Para>A BOS Server (<Command>bosserver</Command> process). (See Section 2.1 for more information
about the BOS Server.)
</Para>
</ListItem>
</ItemizedList>
<Para>The Update Server helps ensure that all server machines in a domain run
the same version of common configuration files such as administrative lists.
Configuration files are created and modified on the System Control machine,
which runs the server portion, or <Command>upserver</Command> process, of the Update Server.
Other server machines in the domain run the client portion, or <Command>upclient</Command>
process, of the Update Server.  The <Command>upclient</Command> processes on the other
server machines in the domain frequently contact the <Command>upserver</Command> process on
the System Control machine to verify that the most recent version of each
configuration file is in use.  If the most recent version of a file is not in
use, the <Command>upclient</Command> process on each machine retrieves the most recent
version from the System Control machine and installs it locally.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.13">
<Primary><Filename>admin.up</Filename> file</Primary>
</IndexTerm>The server portion of the Update Server must be run on any machine that acts
as a System Control machine for a domain.  The <Filename>admin.up</Filename> list is used to
identify all server principals that can obtain updates from the System Control
machine.  The list should include the names of all of the server machines in a
domain.
<IndexTerm Id="DFSAGR.ISSU.indx.14" SpanEnd="DFSAGR.ISSU.indx.12"><IndexTerm Id="DFSAGR.ISSU.indx.15" SpanEnd="DFSAGR.ISSU.indx.11"></Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.5">
<Title>Binary Distribution Machines</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.16">
<Primary>Binary Distribution machines</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.17">
<Primary>binary files</Primary>
<Secondary>distributing</Secondary>
</IndexTerm>
<Para>A Binary Distribution machine stores DFS binary files for processes and
command suites for distribution from its <Symbol Role="Variable">dcelocal</Symbol><Filename>/bin</Filename> and related
directories to all other server machines of its CPU/OS type in a cell.  Each
server keeps a copy of server process binaries in a local directory; however,
all the machines must be running the same version of the process for the system
to perform correctly.  Therefore, the binaries are installed on a single Binary
Distribution machine, which acts as a source for the others.  Configure one
Binary Distribution machine for each CPU/OS type for which multiple machines
exist in the cell.
</Para>
<Para>A Binary Distribution machine runs the following processes:
</Para>
<ItemizedList>
<ListItem>
<Para>An <Command>upserver</Command> process (the server portion of the Update Server), which
controls the distribution of binary files to other server machines of the same
CPU/OS type in the cell.
</Para>
</ListItem>
<ListItem>
<Para>An <Command>upclient</Command> process (the client portion of the Update Server), which
retrieves configuration files from the System Control machine.
</Para>
</ListItem>
<ListItem>
<Para>A BOS Server (<Command>bosserver</Command> process). (See Section 2.1 for more information
about the BOS Server.)
</Para>
</ListItem>
</ItemizedList>
<Para>A second Update Server, different from the one used to distribute
configuration files from the System Control machine, helps ensure that
all server machines of the same CPU/OS type in a cell run the same
binary files.  Like System Control machines, Binary Distribution machines
run an <Command>upserver</Command> process.  The <Command>upclient</Command> processes on
the other server machines of the same CPU/OS type in the cell frequently
contact the <Command>upserver</Command> process to verify that the most recent
version of each binary file is in use.  If it is not, the
<Command>upclient</Command> processes on the other server machines retrieve the
most recent version from the Binary Distribution machine and install it
locally.  You do not have to install new software on each individual
server machine because the Update Server does so automatically.
</Para>
<Para>The server portion of the Update Server must be run on any machine that acts
as a Binary Distribution machine for a cell.  The <Filename>admin.up</Filename> list associated
with this Update Server is used to identify all server principals that can
obtain updates from the Binary Distribution machine.  The list should include
the names of all machines of the same CPU/OS type in a cell.
</Para>
<Para><?sml-need 8>Unless a server machine is fulfilling the roles of both System Control machine
and Binary Distribution machine, different Update Servers handle the
distribution of configuration and binary files.  A machine configured to
perform both roles runs only a single Update Server to distribute both common
configuration files and system binary files.
<IndexTerm Id="DFSAGR.ISSU.indx.18" SpanEnd="DFSAGR.ISSU.indx.16"></Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.6">
<Title>File Server Machines</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.19">
<Primary>File Server machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.20">
<Primary>Fileset Server</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>A File Server machine is used to store and export DCE LFS or non-LFS
data for use in the global namespace.  Configure enough File Server machines to
contain the data to be exported from the domain.  A File Server machine must run
the following processes, most of which are necessary for storing filesets,
exporting data, and storing replicas of filesets:
</Para>
<ItemizedList>
<ListItem>
<Para>A Fileset Server (<Command>ftserver</Command> process).
</Para>
</ListItem>
<ListItem>
<Para>The File Exporter, which is initialized by the <Command>fxd</Command> process, in the 
kernel.
</Para>
</ListItem>
<ListItem>
<Para>The <Command>dfsbind</Command> process.
</Para>
</ListItem>
<ListItem>
<Para>The Replication Server (<Command>repserver</Command> process).
</Para>
</ListItem>
<ListItem>
<Para>Two <Command>upclient</Command> processes: one to retrieve configuration files from the
System Control machine, and one to retrieve binary files from the Binary
Distribution machine of the proper CPU/OS type.
</Para>
</ListItem>
<ListItem>
<Para>A BOS Server (<Command>bosserver</Command> process).  (See Section 2.1 for more information
about the BOS Server.)
</Para>
</ListItem>
</ItemizedList>
<Para>The Fileset Server, or <Command>ftserver</Command> process, provides an interface for
commands that affect filesets (commands that create, delete, or move 
filesets, and commands that prepare filesets for archiving to tape or other 
media).  The most common
occurrences of fileset creation and deletion are when you add or remove users
from the system.  Filesets are most often moved to provide load balancing among
File Server machines.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.21">
<Primary><Filename>admin.ft</Filename> file</Primary>
</IndexTerm>The Fileset Server must run on any machine that exports data for use in the
global namespace.  The <Filename>admin.ft</Filename> list is used to designate administrative
users who can issue <Command>fts</Command> commands that affect the <Command>ftserver</Command> process
on a machine and to designate other server machines from which the machine can
accept filesets.  Users, groups, and machines listed in the <Filename>admin.ft</Filename> list
can differ among DFS administrative domains.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.22">
<Primary>File Exporter</Primary>
<Secondary>about</Secondary>
</IndexTerm>The File Exporter (sometimes called the <Replaceable>Protocol Exporter</Replaceable>) runs as part
of the kernel on each File Server machine.  It provides the same services across
the network that the local operating system provides on a local disk:
</Para>
<ItemizedList>
<ListItem>
<Para>Delivering requested files and programs to clients; storing files and programs
when clients finish with them
</Para>
</ListItem>
<ListItem>
<Para>Maintaining the directory hierarchy structure
</Para>
</ListItem>
<ListItem>
<Para>Handling file-related or directory-related requests (creating, deleting, 
copying, and moving filesets)
</Para>
</ListItem>
<ListItem>
<Para>Tracking status information (including size and modification status) about 
each file and directory
</Para>
</ListItem>
<ListItem>
<Para>Creating symbolic links between files
</Para>
</ListItem>
</ItemizedList>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.23">
<Primary>File Exporter</Primary>
<Secondary>administrative mechanisms</Secondary>
</IndexTerm>Unlike the DFS server processes, the File Exporter is not associated with an
administrative list.  Instead, the command line for the <Command>fxd</Command> process,
which is used to initialize the File Exporter and start related kernel
daemons, includes an <Option>admingroup</Option> option that specifies the administrative
group for the File Exporter on each File Server machine.  The group specified
with this option must be defined in the Registry Database, as must all groups
used with DFS.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.24">
<Primary>permissions</Primary>
<Secondary>changing on exported filesets</Secondary>
</IndexTerm>Members of this administrative group can change the ACL and UNIX permissions
of <Replaceable>all</Replaceable> data exported from the machine.  They have the equivalent of the
ACL <Literal>c</Literal> permission on all of the files and directories in each exported DCE
LFS fileset, and they can effectively change the UNIX permissions on all of the
files and directories in each exported non-LFS fileset.  Members of the group
can also change the owner and owning group of all files and directories
exported from the machine.  Include only highly trusted system administrators
in this group.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.25">
<Primary>access control lists (ACLs)</Primary>
<Secondary><Literal>root</Literal> permissions</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.26">
<Primary>access control lists (ACLs)</Primary>
<Secondary><Literal>self</Literal> permissions</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.27">
<Primary><Literal>self</Literal> principal</Primary>
<Secondary>ACL permissions</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.28">
<Primary><Literal>root</Literal></Primary>
<Secondary>ACL permissions</Secondary>
</IndexTerm>Though similar in many respects, inclusion in the administrative
group associated with the File Exporter and being logged in as
<Literal>root</Literal> are <Symbol Role="Variable">not</Symbol> equivalent. 
A user who is logged into the local machine as <Literal>root</Literal> can
perform different operations on a file or directory, depending on how he or
she accesses the file or directory:
<?sml-break><?sml-need 12></Para>
<ItemizedList>
<ListItem>
<Para><Replaceable>When accessing a file or directory via its DCE pathname</Replaceable>, if the user
is logged into the local machine as <Literal>root</Literal> but is not authenticated to
DCE, DFS treats the user as the
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/hosts/</Literal><Symbol Role="Variable">hostname</Symbol><Literal>/self</Literal> principal of the local
machine; in this case, the <Literal>root</Literal> user receives the permissions associated
with the machine's <Literal>self</Literal> principal, which is treated as an authenticated
user from the local cell.  If the user is also authenticated to DCE as
<Literal>root</Literal>, DFS treats the user according to the DCE identity <Literal>root</Literal>. (Note
that you do not have to be logged into the local machine as <Literal>root</Literal> to be
logged into DCE as <Literal>root</Literal>.)
</Para>
<Note>
<Para>The DCE identity <Literal>root</Literal> effectively has <Literal>root</Literal> privileges for data in
all exported non-LFS filesets in the cell.  The identity is very powerful and
represents a serious security risk.  Either use the DCE <Literal>root</Literal> identity
<Replaceable>very</Replaceable> cautiously or disable it altogether.
</Para>
</Note>
</ListItem>
<ListItem>
<Para><Replaceable>When accessing a file or directory via its local pathname</Replaceable>, the <Literal>root</Literal>
user has all of the privileges commonly associated with <Literal>root</Literal>.  For local
access, <Literal>root</Literal> can perform any file system operation on a file or
directory; for example, <Literal>root</Literal> can change the UNIX mode bits of a file or
directory, change the ACL permissions of a DCE LFS file or directory, change
the owner or owning group of a file or directory, or create or remove a file
or directory. (A file or directory in a non-LFS fileset can always be accessed
via a local pathname because a non-LFS fileset must always be mounted locally, 
as a file system on its File Server machine; a file or directory in a DCE LFS
fileset can be accessed via a local pathname only if its fileset is mounted
locally.)
</Para>
</ListItem>
</ItemizedList>
<Para>Being a member of the <Command>fxd</Command> administrative group allows you to perform any
operation on a file or directory in an exported fileset, but you may have to
change the file's or directory's protections first.  Being logged into the local
machine as <Literal>root</Literal> lets you perform any operation on a file or directory in
a locally mounted fileset immediately, without changing the protections first.
Being authenticated as DCE <Literal>root</Literal> lets you perform any operation on a file
or directory in an exported non-LFS fileset immediately.
</Para>
<Para>The File Exporter also manages the distribution of tokens to clients.  It
maintains an inventory of outstanding tokens, including the clients to which
it has granted tokens, the data for which it has granted those tokens, and the
type of each token it has granted. (A token's type dictates the operations that
the client holding the token can perform on the data to which the token
applies.) (See Section 2.4 for more information about the File Exporter's
token-management mechanism.)
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.29">
<Primary><Command>fxd</Command> process</Primary>
</IndexTerm><?sml-need 8>The <Command>fxd</Command> process must be run on any machine used to export data to
the global namespace. (See Part 2 of this guide and reference for complete
information about the <Command>fxd</Command> process.)
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.30">
<Primary><Command>dfsbind</Command> process</Primary>
<Secondary>about</Secondary>
</IndexTerm>The <Command>dfsbind</Command> process on a File Server machine maintains user
authentication information required by the File Exporter on the machine.  The
File Exporter uses this information to ensure that only authenticated users
access data from the machine.  The <Command>dfsbind</Command> process must be run on any
machine used to export data to the global namespace.
</Para>
<Para>The <Command>dfsbind</Command> process must also be run on all client machines.  Its role on
client machines is described along with client machines and their processes in
Section 2.2.2. (See Part 2 of this guide and reference for complete information
about the <Command>dfsbind</Command> process.)
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.31">
<Primary>Replication Server</Primary>
<Secondary>about</Secondary>
</IndexTerm>The Replication Server, or <Command>repserver</Command> process, manages replicas of
filesets on all File Server machines.  Depending on the replication method in
use, you either release a new version of a fileset for distribution by the
Replication Server, or the Replication Server creates replicas automatically
at specified intervals.  Install the Replication Server on all File Server
machines, which are the machines that can store read-only replicas of 
filesets.  No administrative list is associated with 
the <Command>repserver</Command> process.
</Para>
<Para RevisionFlag="Changed"><?og-ChangeStart enh,13566,R1.2.2,Add multihomed server">In addition, each File Server machine must have a server entry
registered in the FLDB before it can house filesets. Each File Server
machine can have up to four server entries, with each entry specifying
a different host name or IP address. The server entry
must exist before the <Command>fts create</Command> or <Command>fts crfldbentry</Command>
command can be used to create an entry in the FLDB for a DCE LFS or
non-LFS fileset from the machine. The following section discusses
server entries in more detail. (See Chapter 6 for more information
about creating server entries.)
<?og-ChangeEnd enh,13566,R1.2.2,Add multihomed server"></Para>
<Para>A client machine can also be configured as a Private File Server machine to
export data to the global namespace. (See Section 2.1.1.7 for more information
about configuring a client machine to export data.)
<IndexTerm Id="DFSAGR.ISSU.indx.32" SpanEnd="DFSAGR.ISSU.indx.22"><IndexTerm Id="DFSAGR.ISSU.indx.33" SpanEnd="DFSAGR.ISSU.indx.19"></Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.7">
<Title>Fileset Database Machines</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.34">
<Primary>Fileset Database machines</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.35">
<Primary>Fileset Location Database</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>A Fileset Database machine stores the Fileset Location Database.
Optimally, you should configure three or a larger, odd number of Fileset
Database machines sufficient to support the File Server machines in the cell.
</Para>
<Para>Each Fileset Database machine runs the following processes:
</Para>
<ItemizedList>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.36">
<Primary>Fileset Location Server</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>A Fileset Location Server (<Command>flserver</Command> process).
</Para>
</ListItem>
<ListItem>
<Para>Two <Command>upclient</Command> processes: one to retrieve configuration files from the
System Control machine, and one to retrieve binary files from the Binary
Distribution machine of the proper CPU/OS type.
</Para>
</ListItem>
<ListItem>
<Para>A BOS Server (<Command>bosserver</Command> process). (See Section 2.1 for more information
about the BOS Server.)
</Para>
</ListItem>
</ItemizedList>
<Para>The Fileset Location Server (FL Server), or <Command>flserver</Command> process,
is used to track the locations of all filesets in a cell, making file access
transparent.  It tracks the locations of filesets and records changes to them
in the FLDB.  There is one master copy of the FLDB per cell.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.37">
<Primary>files</Primary>
<Secondary>locating</Secondary>
</IndexTerm>The first time it needs to retrieve a requested file, the Cache Manager
contacts the FL Server to learn which File Server machine houses the fileset
containing the file.  Because of this dependency, the Cache Manager cannot
retrieve a requested file if the information in the FLDB is inaccessible, even
if the File Exporter on the machine that houses the fileset containing the file
is working properly.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.38">
<Primary><Filename>admin.fl</Filename> file</Primary>
</IndexTerm>The <Filename>admin.fl</Filename> list is used to designate administrative users who can
issue commands that affect the <Command>flserver</Command> process (operations that affect
the FLDB) on a Fileset Database machine.  The same <Filename>admin.fl</Filename> list should
be used for all FL Servers in a cell.
</Para>
<Para>A user can issue commands that affect FLDB entries for filesets on a
server machine without being listed in the <Filename>admin.fl</Filename> list,
provided he or she owns the machine's server entry in the FLDB.  A user gains
ownership of a server entry in the FLDB by being included in the group
specified as the owner of that machine's entry with the <Command>fts
crserverentry</Command> command. (See Chapter 6 for more information about
creating server entries in the FLDB.)
<IndexTerm Id="DFSAGR.ISSU.indx.39" SpanEnd="DFSAGR.ISSU.indx.34"><IndexTerm Id="DFSAGR.ISSU.indx.40" SpanEnd="DFSAGR.ISSU.indx.35"></Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.8">
<Title>Backup Database Machines</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.41">
<Primary>Backup Database machines</Primary>
</IndexTerm>
<Para>A Backup Database machine houses the Backup Database. As with Fileset
Database machines, it is best to configure three or a larger, odd number of
Backup Database machines sufficient to back up the cell's data.
</Para>
<Para>Each Backup Database machine runs the following processes:
</Para>
<ItemizedList>
<ListItem>
<Para>A Backup Server (<Command>bakserver</Command> process).
</Para>
</ListItem>
<ListItem>
<Para>Two <Command>upclient</Command> processes: one to retrieve configuration files from the
System Control machine, and one to retrieve binary files from the Binary
Distribution machine of the proper CPU/OS type.
</Para>
</ListItem>
<ListItem>
<Para>A BOS Server (<Command>bosserver</Command> process). (See Section 2.1 for more information
about the BOS Server.)
</Para>
</ListItem>
</ItemizedList>
<Para>A Backup Database machine stores the Backup Database. The Backup Database
houses administrative information used in the DFS Backup System, such as the
dump schedule for backups and the groups of filesets to be dumped to tape in
each backup.  The information in the database can be used to restore data from
tape to the file system in the event of a system failure.  There is one master
copy of the Backup Database per cell.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.42">
<Primary>Backup Server</Primary>
<Secondary>about</Secondary>
</IndexTerm>The Backup Server, or <Command>bakserver</Command> process, maintains the Backup
Database.  The <Command>bakserver</Command> process must run on all machines that store a
copy of the Backup Database.  The <Filename>admin.bak</Filename> list is used to designate
administrative users who can issue commands in the <Command>bak</Command> suite, most of
which communicate with the Backup Server.  The same <Filename>admin.bak</Filename> list should
be used for all Backup Servers in a cell.
</Para>
<Para>Commands in the <Command>bak</Command> suite are used to communicate with the DFS Backup
System.  They can be entered from any machine in the cell.  Data is physically
backed up and restored on a Tape Coordinator machine, which is a client or
server machine that has a tape drive and runs the <Command>butc</Command> process to manage
the drive.  Information stored in the Backup Database determines the data to
be backed up by a Tape Coordinator machine. (See Chapter 9 for more information
on configuring and using Tape Coordinator machines.)
<IndexTerm Id="DFSAGR.ISSU.indx.43" SpanEnd="DFSAGR.ISSU.indx.41"></Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.9">
<Title>DFS Client Machines</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.44">
<Primary>client machines</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>A DFS client machine serves primarily as a single or multiuser
workstation.  It communicates with File Server machines to access files for
application programs, provides local data storage, and provides computer
cycles.  A domain should include enough client machines to allow its users to
access exported data from the local or foreign cells.
</Para>
<Para>Each client machine must run
</Para>
<ItemizedList>
<ListItem>
<Para>The Cache Manager, which is initialized by the <Command>dfsd</Command> process, in 
the kernel
</Para>
</ListItem>
<ListItem>
<Para>The <Command>dfsbind</Command> process
</Para>
</ListItem>
</ItemizedList>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.45">
<Primary>Cache Manager</Primary>
<Secondary>about</Secondary>
</IndexTerm><?sml-need 12>The Cache Manager runs as part of the client machine's kernel.  It communicates
with server processes running on File Server machines to fetch data on behalf
of application programs.  When an application program on a client machine
requests data, the Cache Manager contacts the FL Server to learn the
location of the fileset that houses the data.  It then translates the
application program's data request into a Remote Procedure Call (RPC) to the
File Exporter running on the appropriate File Server machine.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.46">
<Primary>cache</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.47">
<Primary>tokens</Primary>
<Secondary>storing</Secondary>
</IndexTerm>When the Cache Manager receives the requested data, it stores the data in its
local cache, which is an area reserved for data storage on disk or in memory on
the client machine.  It then passes the data to the application program.  The
Cache Manager also stores tokens it receives from the File Exporter on the File
Server machine. 
</Para>
<Para>Within limits, the Cache Manager attempts to make the most current data 
available to users.  The Cache Manager judges the currency of the data in its 
cache based on the type of fileset from which the data was retrieved:
</Para>
<ItemizedList>
<ListItem>
<Para>If the data comes from a read/write fileset, the Cache Manager uses the
tokens to track the currency of the data.  The cached data remains current for
as long as the Cache Manager's tokens remain valid.  If the read/write source
of the data changes, the File Exporter revokes the tokens.  The next time the
data is requested, the Cache Manager retrieves the newer version to its
cache before providing it to the application program.
</Para>
</ListItem>
<ListItem>
<Para>If the data comes from a read-only fileset, the Cache Manager compares the
amount of time since the data was last verified as being current with a
configurable time period associated with the fileset.  If the read-only copy of
the data changes, the Cache Manager continues to distribute the cached data
until the time since verification equals or exceeds the configurable time
period.  The next time data is requested, the Cache Manager retrieves the newer
version to its cache before providing it to the application program.
</Para>
</ListItem>
</ItemizedList>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.48">
<Primary><Command>dfsd</Command> process</Primary>
<Secondary>about</Secondary>
</IndexTerm>The <Command>dfsd</Command> process initializes the Cache Manager on a client machine.  It
can be used to alter aspects of the Cache Manager's cache, such as its location
and size.  It also starts several background daemons, which help the Cache
Manager manage the data stored in its cache.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.49">
<Primary><Command>dfsbind</Command> process</Primary>
<Secondary>about</Secondary>
</IndexTerm><?sml-need 10>The <Command>dfsbind</Command> process, in addition to its role on File Server machines,
is used by the Cache Managers on client machines to help with the resolution
of DCE pathnames.  It also obtains authentication information about users that
Cache Managers require for RPC bindings to File Server machines. (See Section
2.2.2 for more information about the Cache Manager and the <Command>dfsd</Command> and
<Command>dfsbind</Command> processes.)
</Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.10">
<Title>Exporting Data from a Client Machine (Private File Server Machine)</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.50">
<Primary>client machines</Primary>
<Secondary>as File Servers</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.51">
<Primary>Private File Server machine</Primary>
</IndexTerm>
<Para>The primary function of a client machine is to communicate with File Server
machines to access files for application programs.  However, a client machine
can also be configured as a Private File Server machine to export data 
from its local disk for use in the global namespace.  To export data as 
a Private File Server machine, a client machine must meet the following 
additional requirements:
</Para>
<ItemizedList>
<ListItem>
<Para>Have an RPC binding in CDS
</Para>
</ListItem>
<ListItem>
<Para>Have a DFS server principal and associated account in the Registry Database
</Para>
</ListItem>
<ListItem>
<Para>Have a server entry in the FLDB
</Para>
</ListItem>
<ListItem>
<Para>Run the Fileset Server (<Command>ftserver</Command> process)
</Para>
</ListItem>
<ListItem>
<Para>Run the File Exporter, which is initialized with the <Command>fxd</Command> process
</Para>
</ListItem>
<ListItem>
<Para>Run the <Command>upclient</Command> process to retrieve binary files from the proper Binary
Distribution machine
</Para>
</ListItem>
<ListItem>
<Para>Run the BOS Server (<Command>bosserver</Command> process)
</Para>
</ListItem>
<ListItem>
<Para>Optionally, run the Replication Server (<Command>repserver</Command> process)
</Para>
</ListItem>
</ItemizedList>
<Para>Although meeting these requirements qualifies a client machine as a File
Server machine, that is not the machine's primary role.  The machine's local
disk is not to be used for data storage for an entire cell or domain.  A client
machine meets the previous requirements solely to allow users who administer
the machine to make data on its local disk available in the global namespace.
(See Section 2.1.1.3 for more information about these additional processes.)
</Para>
<Para>To prohibit other users from creating filesets on the client machine, the users
who administer the machine should be the only ones listed in the <Filename>admin.ft</Filename>
list and the <Command>fxd</Command> administrative group for the machine.  They should 
also be listed in the group that is given ownership of the server entry for 
the machine in the FLDB.  These local privileges do not grant the owners of 
the workstation administrative privilege beyond the local machine.  However, 
the owners have all of the privileges required to administer the filesets on 
their machine and the entries for those filesets in the FLDB.  These 
privileges, and the ability to set the ACLs for any data that is exported 
from the workstation, allow the owners to prevent other users from storing 
data on the machine.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.52">
<Primary><Filename>admin.up</Filename> file</Primary>
</IndexTerm>Because a client machine that exports data must run DFS server processes (such
as <Command>bosserver</Command>, <Command>ftserver</Command>, and <Command>fxd</Command>), it must also run the
<Command>upclient</Command> process to retrieve current versions of binary files for the
processes from the Binary Distribution machine for its CPU/OS type in the
cell.  It must therefore be included in the <Filename>admin.up</Filename> list of the Binary
Distribution machine of its CPU/OS type.  Beyond that, neither the machine nor
its owners need to be included in the administrative lists used by the other
machines in their cell or administrative domain.
<IndexTerm Id="DFSAGR.ISSU.indx.53" SpanEnd="DFSAGR.ISSU.indx.44"><IndexTerm Id="DFSAGR.ISSU.indx.54" SpanEnd="DFSAGR.ISSU.indx.50"></Para>
</Sect3>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.11">
<Title>Summary of DFS Machine Roles</Title>
<Para>Table 2-1 summarizes the DFS machine roles described in the previous
sections. For each machine role, the table provides a brief description of its
purpose and lists the DFS processes that a machine filling the role must run. 
The table also provides suggestions for how to configure machines of a specific
type and other roles a machine of each type can assume.  A machine that is 
assuming any of the roles listed in the table must be configured as a DCE 
client machine.  A machine assuming a role as a DFS server must have both an 
RPC binding in CDS for its pathname and a DFS server principal in the Registry
Database.
</Para>
<Para>Recall that any server machine can be configured to perform any of the
other server machine roles.  Also, a server machine can be configured as a
client machine, and vice versa.  To fill an additional role, a machine must
run the processes listed for that role in the third column of the table. (See
Section 2.1.1 for expanded descriptions of the machine roles.)
</Para>
<Note>
<Para>Table 2-1 uses the numbers <Literal>1</Literal> and <Literal>2</Literal> to differentiate the
<Command>upserver</Command> and <Command>upclient</Command> processes running on the machines.  The
notations <Command>upserver</Command><Superscript><?sml-size -4>1<?sml-size 0></Superscript> and <Command>upclient</Command><Superscript><?sml-size -4>1<?sml-size 0></Superscript>
denote the Update Server that distributes common configuration files from
a System Control machine.  The notations <Command>upserver</Command><Superscript><?sml-size -4>2<?sml-size 0></Superscript> and
<Command>upclient</Command><Superscript><?sml-size -4>2<?sml-size 0></Superscript> denote the Update Server that distributes
binary files from a Binary Distribution machine.
</Para>
<?sml-need 6.5i>
<Table Frame="all" Remap="center" Orient="Port">
<Title>Summary of DFS Machine Roles</Title>
<TGroup Rowsep="0" Colsep="0" Cols="4">
<ColSpec Colsep="1" Align="Left" Colwidth="0.8*" Colname="col1" Colnum="1">
<ColSpec Colsep="1" Align="Left" Colwidth="0.8*" Colname="col2" Colnum="2">
<ColSpec Colsep="1" Align="Left" Colwidth="0.6*" Colname="col3" Colnum="3">
<ColSpec Align="Left" Colwidth="1.5*" Colname="col4" Colnum="4">
<THead>
<Row>
<Entry Rowsep="1"><Literal>Machine Role</Literal></Entry>
<Entry Rowsep="1"><Literal>Purpose</Literal></Entry>
<Entry Rowsep="1"><Literal>Processes</Literal></Entry>
<Entry Rowsep="1"><Literal>Suggestions</Literal></Entry>
</Row>
</THead>
<TBody>
<Row>
<Entry Rowsep="1">System Control machine
</Entry>
<Entry Rowsep="1">To distribute common configuration files for a domain
</Entry>
<Entry Rowsep="1"><Command>bosserver</Command>
<?Pub _newline><Command>upserver</Command><Superscript>1</Superscript>
<?Pub _newline><Command>upclient</Command><Superscript>2</Superscript>
</Entry>
<Entry Rowsep="1">Use a Binary Distribution machine as the System Control
machine for a domain.
</Entry>
</Row>
<Row>
<Entry Rowsep="1">Binary Distribution machine
</Entry>
<Entry Rowsep="1">To distribute system binary files for its CPU/OS type
</Entry>
<Entry Rowsep="1"><Command>bosserver</Command>
<?Pub _newline><Command>upserver</Command><Superscript>2</Superscript>
<?Pub _newline><Command>upclient</Command><Superscript>1</Superscript>
</Entry>
<Entry Rowsep="1">Use the System Control machine for a domain as a
Binary Distribution machine.
</Entry>
</Row>
<Row>
<Entry Rowsep="1">File Server machine
</Entry>
<Entry Rowsep="1">To export and store DCE LFS and non-LFS data
</Entry>
<Entry Rowsep="1"><Command>bosserver</Command>
<?Pub _newline><Command>ftserver</Command>
<?Pub _newline><Command>fxd</Command>
<?Pub _newline><Command>dfsbind</Command>
<?Pub _newline><Command>repserver</Command>
<?Pub _newline><Command>upclient</Command><Superscript>1</Superscript>
<?Pub _newline><Command>upclient</Command><Superscript>2</Superscript>
</Entry>
<Entry Rowsep="1">A File Server machine must also have a server entry in
the FLDB.  In a large cell, dedicate one File Server machine to housing
read-only replicas.
</Entry>
</Row>
<Row>
<Entry Rowsep="1">Fileset Database machine
</Entry>
<Entry Rowsep="1">To store the Fileset Location Database (FLDB)
</Entry>
<Entry Rowsep="1"><Command>bosserver</Command>
<?Pub _newline><Command>flserver</Command>
<?Pub _newline><Command>upclient</Command><Superscript>1</Superscript>
<?Pub _newline><Command>upclient</Command><Superscript>2</Superscript>
</Entry>
<Entry Rowsep="1">Configure three Fileset Database machines.  Configure
Fileset Database machines as Backup Database machines.
</Entry>
</Row>
<Row>
<Entry Rowsep="1">Backup Database machine
</Entry>
<Entry Rowsep="1">To store the Backup Database
</Entry>
<Entry Rowsep="1"><Command>bosserver</Command>
<?Pub _newline><Command>bakserver</Command>
<?Pub _newline><Command>upclient</Command><Superscript>1</Superscript>
<?Pub _newline><Command>upclient</Command><Superscript>2</Superscript>
</Entry>
<Entry Rowsep="1">Configure three Backup Database machines.  Configure Backup Database
machines as Fileset Database machines.
</Entry>
</Row>
<Row>
<Entry>DFS client machine
</Entry>
<Entry>To serve as a single-user or multiuser workstation;
to access files for application programs
</Entry>
<Entry><Command>dfsd</Command>
<?Pub _newline><Command>dfsbind</Command>
</Entry>
<Entry>Export DCE LFS and non-LFS data from the machine by
running <Command>bosserver</Command>, <Command>ftserver</Command>,
<Command>fxd</Command>,
<Command>upclient</Command><Superscript>2</Superscript>, and optionally
<Command>repserver</Command>, creating an RPC binding in CDS, registering
a DFS server principal in the Registry Database, and creating a server
entry in the FLDB.
</Entry>
</Row>
</TBody>
</TGroup>
</Table>
<?sml-point-size 11>
<!--no-op:  12-->
<!--no-op:  l-->
<?sml-indent -.3i>
<?sml-point-size 12>
<!--no-op:  14-->
<!--no-op:  b-->
<?sml-indent>
<IndexTerm Id="DFSAGR.ISSU.indx.55" SpanEnd="DFSAGR.ISSU.indx.2">
</Note>
</Sect2>
</Sect1>
<Sect1 Id="DFSAGR.ISSU.div.12">
<Title>DFS Server and Client Configuration Issues</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.56">
<Primary>directories</Primary>
<Secondary>server machines (DFS)</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.57">
<Primary>server machines</Primary>
<Secondary>configuring</Secondary>
</IndexTerm>
<Para>The following subsections describe some general issues to consider
before configuring DFS server and client machines.  They also provide
additional information about the files that must reside on server and client
machines and a few of the processes only briefly described in earlier sections
of this chapter.  They also serve as an introduction to some issues to be 
considered before configuring a domain.
</Para>
<Sect2 Id="DFSAGR.ISSU.div.13">
<Title>Server Machine Processes and Files</Title>
<Para>As mentioned previously, you should combine machine roles for the
machines in your cell and domains.  For example, you may want to set up
a database server machine to house both the FLDB and the Backup Database.  A
machine that houses these databases needs to be stored in a secure location so
that unauthorized users cannot access and possibly damage fileset data or the
databases.
</Para>
<Para><?sml-need 12>In any cell, there is only one version of the FLDB and one version of the
Backup Database, even though these databases can be replicated at other sites.
The initial copies of these databases are created when the Fileset Location
and Backup Servers are first started in the cell.  They are replicated to
other machines automatically as additional instances of their respective
server processes are started on those machines.  When configuring a new domain
in an existing cell, do not attempt to create a new FLDB or Backup Database
for the domain; configure additional instances of the existing database as
necessary.
</Para>
<Para>Several directories contain files related to DFS server processes.  The
directories in the following list store files on a server machine's local
disk.  Files stored on the local disk are generally required for DFS to start
without accessing the global namespace. (See your vendor's documentation for
information about the files that reside on the local disk of a server machine.)
</Para>
<ItemizedList>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.58">
<Primary>binary files</Primary>
<Secondary>directory</Secondary>
</IndexTerm>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/bin</Filename> directory contains DFS binaries that are appropriate 
for the machine's CPU/OS type.  The binary files are for server processes, 
command suites, and other processes and programs.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.59">
<Primary>administrative lists</Primary>
<Secondary>directory</Secondary>
</IndexTerm>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/var/dfs</Filename> directory houses administrative lists for server
processes; for example, <Filename>admin.bos</Filename> and <Filename>admin.ft</Filename>.  It also contains
configuration files that are used by the BOS Server and the <Command>dfsexport</Command> 
command.  If the machine is running the Fileset Location Server, this 
directory also contains the FLDB.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.60">
<Primary>log files</Primary>
<Secondary>directory (DFS)</Secondary>
</IndexTerm>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/var/dfs/adm</Filename> directory stores log files generated by
server processes.  These files detail events that occur during the operation of
server processes.  Server processes do not use these log files to reconstruct
failed operations because only completed events are recorded in them.  However,
because the information in the files is in human-readable format, examination
of these files is the first step in the troubleshooting procedure.  They can
help you evaluate process failures and related problems.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.61">
<Primary>image files</Primary>
<Secondary>directory</Secondary>
</IndexTerm>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/var/dfs/adm</Filename> directory also contains the core image file
that is generated if a process being monitored by the BOS Server crashes.  The 
BOS Server adds an extension to the standard <Literal>core</Literal> name to indicate which
process generated the file; for example, <Literal>core.flserver</Literal>.  However, if two
processes abort at exactly the same time, the BOS Server may not be able to
assign the correct extension to the core file.
</Para>
</ListItem>
</ItemizedList>
<Para>In addition, the <Symbol Role="Variable">dceshared</Symbol><Filename>/bin</Filename> directory also stores all of the binary
files that are housed in the <Symbol Role="Variable">dcelocal</Symbol><Filename>/bin</Filename> directory.  Current 
versions of the files are always available from <Symbol Role="Variable">dceshared</Symbol><Filename>/bin</Filename> for 
installation on the local disk of a server machine.  The directory also 
contains the binary files for a number of programs that are not integral to 
starting DFS, such as the <Command>scout</Command> program and a number of programs 
related to the DFS Backup System.
<IndexTerm Id="DFSAGR.ISSU.indx.62" SpanEnd="DFSAGR.ISSU.indx.57"><IndexTerm Id="DFSAGR.ISSU.indx.63" SpanEnd="DFSAGR.ISSU.indx.56"></Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.14">
<Title>Client Machine Processes and Files</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.64">
<Primary>client machines</Primary>
<Secondary>configuring</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.65">
<Primary>disk space</Primary>
<Secondary>saving on client machines</Secondary>
</IndexTerm>
<Para>Client machines run the <Command>dfsd</Command> process, which initializes the Cache
Manager, and the <Command>dfsbind</Command> process.  You can save disk space on a client
machine by storing commonly used files in the DFS filespace.  You can then
create symbolic links on the local disk that refer to the files in the
filespace.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.66">
<Primary>chunks</Primary>
<Secondary>about</Secondary>
</IndexTerm>When the Cache Manager retrieves a requested file, it caches the data before
passing it on to an application program.  It does not cache the entire file; it
instead caches "chunks," or pieces, of data.  By default, each chunk of
cached data contains 64 kilobytes of data in a disk cache or 8 kilobytes of
data in a memory cache.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.67">
<Primary><Command>dfsd</Command> process</Primary>
<Secondary>about</Secondary>
</IndexTerm>The <Command>dfsd</Command> process initializes the Cache Manager on a client machine by
transferring configuration information into kernel memory.  It also mounts the
root of the global namespace (<Filename>/...</Filename>).  You can use the options available
with the command line for the <Command>dfsd</Command> process to alter the definitions for
the type of cache to be used (disk or memory), total cache size, cache chunk
size, the local disk directory to be used for caching, and other configuration
information.
</Para>
<Para>In addition, the <Command>dfsd</Command> process starts several background daemons.  These
daemons include one or more maintenance daemons that perform routine
maintenance tasks such as garbage collection, background daemons that improve
performance by performing delayed writing of updated data, token daemons that
respond to token revocation requests from File Exporters, and (on the AIX
operating system) I/O daemons that move data between disk and memory.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.68">
<Primary><Command>dfsbind</Command> process</Primary>
<Secondary>about</Secondary>
</IndexTerm><?sml-need 10>The <Command>dfsbind</Command> process resolves CDS pathnames and returns information about
Fileset Database machines to the Cache Manager.  The information allows the
Cache Manager to contact the FL Server on an appropriate Fileset Database
machine in the cell to determine the locations of filesets that house data
requested by users.
</Para>
<Para>The <Command>dfsbind</Command> process also returns user authentication information from the
Security Server to the kernel RPC Runtime of the client machine.  Authentication
information must be included in RPC bindings that request data from a File
Server machine for a user.  The Cache Manager uses the RPC bindings to access
data for the user from the File Server machine.
</Para>
<Para>(See Part 2 of this guide and reference for complete information about the
<Command>dfsd</Command> and <Command>dfsbind</Command> commands that start the respective processes and
the options available with the commands.)
</Para>
<Para>Two types of files must reside on the local disk of a client machine: boot
sequence files needed during reboot, and files that are useful during File
Server machine outages.
</Para>
<Para>During a reboot, DFS is inaccessible until the <Command>dfsd</Command>
process reinitializes the Cache Manager; the <Command>dfsbind</Command> process
must be running before the <Command>dfsd</Command> process can be run.  Any
files needed during reboot and before the <Command>dfsd</Command> process
starts must reside on the local disk.
Following is a list of recommended DFS files to store on a local disk. (See
your vendor's documentation for information about the files that reside on
the local disk of a client machine.)
</Para>
<ItemizedList>
<ListItem>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/bin/dfsbind</Filename> command is the start-up command for the 
<Command>dfsbind</Command> process.
</Para>
</ListItem>
<ListItem>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/bin/dfsd</Filename> command is the start-up command for the Cache 
Manager.
</Para>
</ListItem>
<ListItem>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/etc/CacheInfo</Filename> file is a file that specifies aspects of 
Cache Manager configuration.
</Para>
</ListItem>
<ListItem>
<Para>The <Symbol Role="Variable">dcelocal</Symbol><Filename>/var/adm/dfs/cache</Filename> directory is a directory that 
contains cache-related files, such as <Literal>V</Literal><Symbol Role="Variable">n</Symbol> files and the 
<Filename>CacheItems</Filename> file, generated and used by the Cache Manager.
</Para>
</ListItem>
</ItemizedList>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.69">
<Primary><Command>bos</Command> command suite</Primary>
<Secondary>access on client machines</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.70">
<Primary><Command>cm</Command> command suite</Primary>
<Secondary>access on client machines</Secondary>
</IndexTerm><?sml-need 12>You may also want to store diagnostic and recovery files on a local disk.
Certain commands in the <Command>bos</Command> and <Command>cm</Command> command suites can help users
diagnose problems caused by a File Server outage.  It is useful to have local
disk copies of the binary files for the <Command>bos</Command> and <Command>cm</Command> suites because
the File Server outage that requires their use can also make them inaccessible.
In addition, you may want to keep the binaries for a text editor, such as
<Command>ed</Command> or <Literal>vi</Literal>, on the local disk for use during outages.
</Para>
<Para RevisionFlag="Changed"><?og-ChangeStart enh,13566,R1.2.2,Add multihomed server">Additionally, if you wish to modify the default Cache Manager
preferences for accessing File Servers and FLDB machines, you can add
<Command>cm setpreference</Command> commands to the machine's initialization
file.  Doing so ensures that such preferences are loaded each time the machine
is initialized.  For more information about Cache Manager preferences for File
Servers and FLDB machines, see the following section.
<?og-ChangeEnd enh,13566,R1.2.2,Add multihomed server"><IndexTerm Id="DFSAGR.ISSU.indx.71" SpanEnd="DFSAGR.ISSU.indx.64"><?og-ChangeStart enh,13566,R1.2.2,Add multihomed server"></Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.15">
<Title RevisionFlag="Changed">Multihomed Server Configuration Issues</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.72">
<Primary RevisionFlag="Changed">multihomed servers</Primary>
<Secondary RevisionFlag="Changed">configuring</Secondary>
</IndexTerm>
<Para RevisionFlag="Changed">Multihomed server capabilities allow
administrators to specify up to four interfaces (either hostnames or IP
addresses) in the FLDB for each File Server and FLDB machine. Servers can
have more than four network connections; however, the FLDB can accept only
four entries per server. This capability, coupled with server preference
lists maintained by the individual Cache Managers, allows you to configure
DFS to work optimally within your network.
</Para>
<Para RevisionFlag="Changed">For example, a single File Server can have up to
four IP addresses (specified for use by DFS), and the various clients that use
that server can have their Cache Manager preference lists configured so that
the preferred access to that server is through the most efficient possible
network connection. Should a single connection to a File Server become
unavailable, the various clients that previously used that connection would
consult their Cache Manager's preference lists and reroute their requests to
another address for a File Server containing the required fileset. This
 behavior lets you configure DFS for the most efficient use of the network
while providing additional fail-over capabilities for the file system.
</Para>
<Sect3 Id="DFSAGR.ISSU.div.16">
<Title RevisionFlag="Changed">How Multihomed Servers and Preferences Work Together</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.73">
<Primary RevisionFlag="Changed">multihomed servers</Primary>
<Secondary RevisionFlag="Changed">description</Secondary>
</IndexTerm>
<Para RevisionFlag="Changed">Each Cache Manager maintains a list of File Server and Fileset
Location (FL) Server preferences.  Each entry in that list contains
both the address of a server and a ranking. The ranking value
determines the order in which these servers are accessed, or their
"preference." The FLDB can contain up to four addresses for each server
machine; therefore, the preference list can also contain up to four
entries for each server (each with its own address and preference
rank).
</Para>
<Para RevisionFlag="Changed">In operation, when a Cache Manager requires a particular fileset, it
first consults its list of FL Servers and attempts to contact an FL
Server at the address with the lowest ranking in the preference list.
The FL Server provides the addresses of the various File Servers that
contain that fileset. (The fileset location information is then cached
by the Cache Manager and is updated periodically.) If the fileset is
replicated, multiple File Servers may contain that fileset.  The Cache
Manager again consults its preference list and contacts a suitable
File Server at the address with the lowest ranking value. Should the
Cache Manager not be able to contact a server during this process, it
simply checks its preference list and attempts to contact a suitable
server at the next most preferred IP address.
</Para>
<Para RevisionFlag="Changed">The preference list is created automatically
each time a Cache Manager is initialized.  It consists of the IP addresses of
FL Servers and File Servers and an automatically assigned preference value for
each.  New entries are added to the preference list as necessary when filesets
are first required. By default, the Cache Manager assigns preferences that make
sensible choices based on the location of servers.  The default values make the
Cache Manager try to connect to servers in the following order:
</Para>
<OrderedList>
<ListItem>
<Para RevisionFlag="Changed">The same machine as the client (default rank of
5000).
</Para>
</ListItem>
<ListItem>
<Para RevisionFlag="Changed">The same subnetwork as the client (default rank of
20,000).
</Para>
</ListItem>
<ListItem>
<Para RevisionFlag="Changed">The same network as the client (default rank of
30,000).
</Para>
</ListItem>
<ListItem>
<Para RevisionFlag="Changed">Different networks (default rank of 40,000).
</Para>
</ListItem>
</OrderedList>
<Para RevisionFlag="Changed">Cache Manager preferences are explained in detail
in Chapter 9.
</Para>
<Para RevisionFlag="Changed">For example, a server on the same machine as the
Cache Manager receives a rank of 5000, while a server on the same subnetwork
receives a rank of 20,000.  The entry with the lowest ranking value has the
highest preference.  Thus, a server with a preference value of 5000 will be
chosen before a server with a rank of 20,000.
</Para>
<Para RevisionFlag="Changed">You can change Cache Manager preferences by using
the <Command>cm setpreferences</Command> command.  Additionally, you can create
a file specifying server preferences that is read each time a Cache Manager is
initialized, thus providing a method for overriding the default server
preference values. You can also load preference entries from standard input
or any combination of all three sources. This procedure is also explained in
Chapter 9.
</Para>
<Para RevisionFlag="Changed">Should two servers be assigned the same preference
value, such as two File Servers on the same subnetwork both receiving a default
value of 20,000, the server with the lowest round-trip value is chosen. Each
server is assigned a random round-trip value when the Cache Manager is
initialized. The assigned round-trip value is always higher than the upper
bound for stored actual round-trip values.  This ensures that an actual
round-trip value is always chosen over assigned values.
</Para>
<Para RevisionFlag="Changed">By judiciously providing multiple addresses for FL Servers and File
Servers and properly configuring the Cache Manager preference lists,
you can configure DFS to make the most efficient use of servers within
the network.  For example, you may want to provide a connection from
commonly used File Servers and FL Servers to the same subnetworks that are
shared by the majority of the DFS clients.  This connection reduces
cross-router and gateway traffic through the network.  As a backup, you can
provide higher-ranking preference entries for server connections to other
areas of the network. This configuration provides continued access to the
servers should a particular network connection become unavailable.
</Para>
<Para RevisionFlag="Changed">The following simplified scenario illustrates how multihomed servers
can be configured to make the most efficient use of the local network.
In this example, a read-only fileset is replicated on two File
Servers.  The File Servers have connections to both subnetworks within
the network, and these connections are the preferred connections used
by DFS clients on each respective subnetwork.  When a DFS client must
fetch data from the read-only fileset, it first consults the list of
suitable Files Servers.  The Cache Manager then consults its list of
preferences and chooses the connection to a suitable File Server that
has the lowest rank.  Because both File Server connections on the local
subnetwork have the same rank, the connection with the lowest
round-trip value is chosen, as shown in Figure 2-1.
</Para>
<?sml-need 4i>
<Figure>
<Title RevisionFlag="Changed">Cache Manager Contacting File Server Address With Lowest Rank</Title>
<Graphic Entityref="DFSAGR.ISSU.ent.1" role="page-wide"></Graphic>
</Figure>
<Para RevisionFlag="Changed">Should the Cache Manager lose contact with the
preferred File Server connection (either through a network or server problem),
the Cache Manager again consults its preference list and attempts to contact
a suitable File Server at the address with the next lowest rank, as shown in
Figure 2-2. In this figure, when the Cache Manager can no longer contact a
File Server at a given connection, it attempts to connect to the File Server
address with the next-lowest preference value.
</Para>
<?sml-need 4i>
<Figure>
<Title RevisionFlag="Changed">Cache Manager Connecting to File Server Address With Next Lowest Rank</Title>
<Graphic Entityref="DFSAGR.ISSU.ent.2" role="page-wide"></Graphic>
</Figure>
<Para RevisionFlag="Changed">If the Cache Manager again loses contact with
a File Server through its current connection, it once more consults the
preference list for the address of a suitable File Server with the next
lowest value.  In this case, the Cache Manager must now establish a connection
to another subnetwork. There are two possible connections to suitable File
Servers in that subnetwork, both having the same rank. The Cache Manager,
therefore, chooses the connection with the lowest round-trip time value, as
shown in Figure 2-3. In this figure, should the Cache Manager again lose its
connection, it checks the preference list for a connection to a suitable File
Server with the next lowest ranking.
</Para>
<?sml-need 4i>
<Figure>
<Title RevisionFlag="Changed">Cache Manager Again Losing Connection and Contacting File Server Address in Another Subnet</Title>
<Graphic Entityref="DFSAGR.ISSU.ent.3" role="page-wide"></Graphic>
</Figure>
<Para RevisionFlag="Changed">The entire process of changing connections as required is carried out 
automatically, without the DFS client users being aware that it has 
occurred.   
</Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.17">
<Title RevisionFlag="Changed">Tasks to Administer Multihomed Servers</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.74">
<Primary RevisionFlag="Changed">multihomed servers</Primary>
<Secondary RevisionFlag="Changed">administering</Secondary>
</IndexTerm>
<Para RevisionFlag="Changed">The following are tasks to configure a multihomed File Server environment:
</Para>
<ItemizedList>
<ListItem>
<Para RevisionFlag="Changed">You must add each host name or IP address for each
File Server to the Fileset Location Database (FLDB).  The initial entry, along
with one address for that entry, is created using the <Command>fts
crserverentry</Command> command. Additional addresses can be added or deleted
from the entry by using the <Command>fts edserverentry</Command> command.  See
Chapter 6 for more information.
</Para>
</ListItem>
<ListItem>
<Para RevisionFlag="Changed">Optionally, you can modify the preferences for
each client's Cache Manager to take advantage of the most efficient connections
to the File Servers.  However, you should modify the preferences only when
there are compelling reasons do so.  The Cache Manager's default preferences
are generally the most efficient for any given network configuration.
</Para>
<Para RevisionFlag="Changed">Client preference lists are transient in that
they are reestablished at their default values each time the Cache Manager
is initialized.  However, a list of preferences can be loaded into the Cache
Manager at initialization through a preferences file. Chapter 9 explains how
to both create such a file and ensure that it is loaded each time the Cache
Manager is initialized.
</Para>
</ListItem>
</ItemizedList>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.18">
<Title RevisionFlag="Changed">IP Layer Override of Preferences</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.75">
<Primary RevisionFlag="Changed">multihomed servers</Primary>
<Secondary RevisionFlag="Changed">IP layer override</Secondary>
</IndexTerm>
<Para RevisionFlag="Changed">While the FLDB can only contain up to four addresses for a given File Server
or FL Server, such servers can have more than four connections to the
network. In such instances, the DCE RPC mechanism can allow the IP
layer to choose a source address for a server response that is
different, and presumably more efficient, than the specified
destination of the corresponding request.  In this case, the chosen
server address is likely to be a function of the client address to
which the response is being set; however, the exact algorithm for
choosing the address will differ for each operating system
vendor. Such a routing decision is observed by the Cache Manager as
a change in the server-binding's address.
</Para>
<Para RevisionFlag="Changed">Should the IP layer select a different server
address, this connection becomes the connection used by the Cache Manager,
regardless of its preference rank or whether or not it is one of the addresses
listed in the FLDB for a given server.  This scenario is shown in Figure 2-4.
</Para>
<?sml-need 4.5i>
<Figure>
<Title RevisionFlag="Changed">An Example of the IP Layer Overriding the Cache Manager's Preference</Title>
<Graphic Entityref="DFSAGR.ISSU.ent.4" role="page-wide" Scalefit="1" Reprowid="375" Reprodep="375" ></Graphic>
</Figure>
<Para RevisionFlag="Changed">Should the IP layer select a different connection
and override the preference choice, the <Command>cm getpreferences</Command>
command returns the address of the currently used connections (the connection
selected by the IP layer) as the entry in the preference list, even though it
may not be listed in the FLDB.
</Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.19">
<Title RevisionFlag="Changed">Creating Additional Default Entries in the Routing Table</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.76">
<Primary RevisionFlag="Changed">multihomed server</Primary>
<Secondary RevisionFlag="Changed">server routing table entries</Secondary>
</IndexTerm>
<Para RevisionFlag="Changed">The preference list provides each Cache Manager
with a list of known connections to various File Servers and FL Servers.  This
list allows the Cache Manager to select alternative connections to communicate
with the appropriate servers should a network or server fault make a particular
connection unavailable.  Similarly, each File Server or FL Server that has
multiple connections to the network should have multiple default entries in
its routing table that define the various routers available to that server.
Thus, if a network fault makes a particular router unavailable, that server
has additional router choices that would allow it to reply to Cache Manager
requests.  Refer to your operating system documentation for information
concerning adding default entries to a server's routing table.
<?og-ChangeEnd enh,13566,R1.2.2,Add multihomed server"></Para>
</Sect3>
</Sect2>
</Sect1>
<Sect1 Id="DFSAGR.ISSU.div.20">
<Title>Setting Up Filesets</Title>
<Para>DCE LFS filesets are created with the <Command>fts create</Command> command. Non-LFS
filesets are created in the local operating system and registered in DFS with
the <Command>fts crfldbentry</Command> command.  Mount points to the global namespace for
both DCE LFS and non-LFS filesets are created with the <Command>fts crmount</Command>
command.
</Para>
<Para>The following subsections discuss setting up a cell's root fileset, binary and
configuration filesets, and user filesets.  Information about fileset
replication and the <Literal>@sys</Literal> and <Literal>@host</Literal> variables, which simplify cell
administration, is also provided. (See Chapter 6 for complete information
about creating and mounting filesets; see Part 2 of this guide and reference
for complete information about <Command>fts</Command> and other DFS commands.)
</Para>
<Sect2 Id="DFSAGR.ISSU.div.21">
<Title>Setting Up the Root Fileset</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.77">
<Primary>filesets</Primary>
<Secondary>root</Secondary>
</IndexTerm>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.78">
<Primary><Literal>root.dfs</Literal> file</Primary>
<Secondary>creating</Secondary>
</IndexTerm>The main read/write fileset, <Literal>root.dfs</Literal>, is
required in every cell's file system.  It is the first fileset created in a
cell during DFS configuration.  It is the implied fileset for the root of a
cell's DFS filespace
(<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs</Filename>,
by default).  It can be a DCE LFS fileset or it can be a non-LFS fileset. 
However, it must be a DCE LFS fileset if functionality such as replication is
to be available in the cell.
</Para>
<Para><?sml-need 12>To create <Literal>root.dfs</Literal> as a DCE LFS fileset,
issue the <Command>fts create</Command> command to create the fileset on a
specified server machine and exported DCE LFS aggregate.  For example:
</Para>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>fts create root.dfs -server </Literal><Symbol Role="Variable">machine</Symbol> <Option>aggregate</Option> <Symbol Role="Variable">name</Symbol>
</UserInput></ProgramListing></Para>
</InformalExample>
<Para>Once the root fileset is created, start the <Command>dfsd</Command>
process if it is not already running.  The <Command>dfsd</Command> process
mounts the root of the global namespace (<Filename>/...</Filename>)
automatically.  Once the global namespace is mounted, the
<Literal>root.dfs</Literal> fileset resides at the top level of the cell's
DFS filespace.
</Para>
<Para>You must enter the <Command>fts crmount</Command> command with the <Option>rw</Option> option to
create an explicit read/write mount point for the fileset below the top level
of the cell's DFS filespace.  For example:
</Para>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>fts crmount /:/.rw root.dfs -rw</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<Para>Once these steps are complete, you can replicate <Literal>root.dfs</Literal>. Replication
is then available for DCE LFS filesets created in the cell.  It is important
that you follow these instructions if you plan to replicate filesets in your
cell.  Due to the nature of mount points, if you replicate <Literal>root.dfs</Literal>
before creating its read/write mount point, you effectively make it impossible
to access the read/write version of <Literal>root.dfs</Literal>.
</Para>
<Para>(See Chapter 6 for more information about creating and mounting filesets,
using mount points, and creating and exporting aggregates.)
</Para>
<Note>
<IndexTerm Id="DFSAGR.ISSU.indx.79">
<Primary>junctions</Primary>
</IndexTerm>
<Para>By default, the junction to the DFS filespace is defined at
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs</Filename>.  However, the name of the junction is not
considered to be well known and can be changed during installation and
configuration of DCE. (See your vendor's installation and configuration
documentation for more information.) The examples in this part of the
guide use the default, <Command>fs</Command>, as the junction of the DFS filespace.
</Para>
</Note>
<?sml-break>
<?sml-need 12>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.22">
<Title>Choosing Fileset Names</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.80">
<Primary>filesets</Primary>
<Secondary>naming conventions</Secondary>
</IndexTerm>
<Para>Each directory in <Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs</Filename> usually corresponds to a
separate, mounted fileset (mounted filesets can also occur anywhere in
the file tree).  Subdirectories of
<?Pub _newline><Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/fs/</Literal><Symbol Role="Variable">directory_name</Symbol>
can be either standard directories or mount points to separate filesets.  For
simplified administration, group the directories and their contents into small,
easily managed filesets.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.81">
<Primary>mount points</Primary>
<Secondary>fileset names</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.82">
<Primary>filesets</Primary>
<Secondary>name and mount points</Secondary>
</IndexTerm>Each fileset has a name unique to the cell in which it resides.  Fileset names
are stored in the FLDB.  A fileset's name is not the same as the name of its
mount point, although you can assign the same name to a fileset and its mount 
point.
</Para>
<Para>There is a 111-character limit on the length of fileset names.  However,
because a 9-character <Literal>.readonly</Literal> extension is added when you
replicate a fileset, you need to specify fileset names that contain no more
than 102 characters.  When creating filesets, do not add the
<Literal>.readonly</Literal> and <Literal>.backup</Literal> extensions
yourself; DFS automatically adds the appropriate extension when it creates a
read-only or backup fileset. (DFS reserves the <Literal>.readonly</Literal>
and <Literal>.backup</Literal> extensions for use with read-only and backup
filesets.  You cannot create a fileset whose name ends with either of these
extensions.)
</Para>
<Para>You can give filesets any names that you feel are appropriate. For simplified
administration, however, a fileset's name needs to
</Para>
<ItemizedList>
<ListItem>
<Para>Reflect the fileset's contents
</Para>
</ListItem>
<ListItem>
<Para>Reflect the name of the fileset's mount point
</Para>
</ListItem>
<ListItem>
<Para>Be consistent with other filesets that contain similar types of data so that 
you can easily manipulate groups of filesets when using the DFS Backup System
</Para>
</ListItem>
</ItemizedList>
<Para>You may find it helpful to use a common prefix for related filesets. The
following list summarizes this type of naming scheme:
</Para>
<ItemizedList>
<ListItem>
<Para>Use the <Literal>common.</Literal><Symbol Role="Variable">type</Symbol>
prefix for common filesets.  For example, use <Literal>common.etc</Literal>
for common configuration files (mounted at
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/fs/common/etc</Literal>),
and <Literal>common.forms</Literal> for common
forms (mounted at <Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs/common/forms</Filename>).
</Para>
</ListItem>
<ListItem>
<Para>Use the <Literal>src.</Literal><Symbol Role="Variable">type</Symbol> prefix for source filesets.  For example, use
<Literal>src.dfs</Literal> for DFS source files (mounted at 
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs/src/dfs</Filename>).
</Para>
</ListItem>
<ListItem>
<Para>Use the <Literal>user.</Literal><Symbol Role="Variable">username</Symbol> prefix for all user filesets.  For example, use
<Literal>user.terry</Literal> for user <Literal>terry</Literal>'s fileset (mounted at 
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/fs/usr/terry</Literal>).
</Para>
</ListItem>
<ListItem>
<Para>Use the
<Literal>public.</Literal><Symbol Role="Variable">username</Symbol>
prefix for each user's public fileset.  For example, use
<Literal>public.terry</Literal> for <Literal>terry</Literal>'s public fileset,
which contains information the user wants to make available to everyone.  The
<Literal>public.terry</Literal> fileset is mounted at 
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/fs/public/terry</Literal>.
</Para>
</ListItem>
<ListItem>
<Para>Use the <Symbol Role="Variable">sys_type</Symbol><Literal>.</Literal><Symbol Role="Variable">distribution_dir</Symbol> 
prefix for operating-system-specific 
filesets.  For example, use <Literal>pmax_osf1.bin</Literal> for OSF/1
binary files (mounted at <Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs/pmax_osf1/bin</Filename>), and
<Literal>pmax_osf1.lib</Literal> for OSF/1 library files (mounted at
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs/pmax_osf1/lib</Filename>).  In DFS, symbolic links are often
created from the <Filename>/bin</Filename> and <Filename>/lib</Filename> directories (or their equivalents)
on the local disk of a workstation to these DFS mount points.
</Para>
</ListItem>
</ItemizedList>
<Para>(See Chapter 6 for more information on additional rules for naming filesets.)
<IndexTerm Id="DFSAGR.ISSU.indx.83" SpanEnd="DFSAGR.ISSU.indx.80"></Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.23">
<Title>Setting Up Binary and Configuration Filesets</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.84">
<Primary>binary files</Primary>
<Secondary>storing</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.85">
<Primary>filesets</Primary>
<Secondary>binary and configuration</Secondary>
</IndexTerm>
<Para>You may find it convenient to store DCE binaries, system binaries, and
configuration files (for example, those commonly found in directories such as
<Filename>/bin</Filename> and <Filename>/etc</Filename> or their equivalents) in the DFS filespace, instead
of on the local disk of each machine.  Because binary files are 
operating-system specific, you may want to create a different fileset for each
system type (for example, <Literal>pmax_osf1</Literal> or <Literal>rs_aix32</Literal>) and distribution
directory (for example, <Filename>/etc</Filename> and <Filename>/bin</Filename>) and store the filesets on
a DFS File Server machine.  You can then create symbolic links from the local
disk to the fileset.
</Para>
<Para>Note that DFS simplifies the creation of such links by providing the <Literal>@sys</Literal>
variable, which is set on a per-Cache Manager basis.  When the Cache Manager
encounters the <Literal>@sys</Literal> variable in a pathname, it substitutes its system
name for the variable. (See Section 2.3.7 for a more detailed description of
the <Literal>@sys</Literal> variable.)
</Para>
<Para><?sml-need 12>For example, while it is a good practice to store the binary files for a single
text editor on the local machine, the binaries for other text editors do not
need to be stored on each machine.  A system administrator can create filesets
that store text editor binaries for each system type.  The administrator can
then construct a symbolic link from the local disk of each machine to the
appropriate fileset in DFS.  For instance, system administrators in the
<Literal>abc.com</Literal> cell, which runs the OSF/1 and AIX 3.2 operating
systems, can configure part of their file tree as shown in Table 2-2.
<IndexTerm Id="DFSAGR.ISSU.indx.86">
<Primary>binary files</Primary>
<Secondary>fileset names and mount points (table)</Secondary>
</IndexTerm></Para>
<Table Frame="all" Remap="center" Orient="Port">
<Title>Examples of Fileset Names and Mount Points for Binary Files</Title>
<TGroup Rowsep="0" Colsep="0" Cols="2">
<ColSpec Colsep="1" Align="Left" Colwidth="1.125*" Colname="col1" Colnum="1">
<ColSpec Align="Left" Colwidth="2*" Colname="col2" Colnum="2">
<THead>
<Row>
<Entry Rowsep="1"><Literal>Fileset Name</Literal></Entry>
<Entry Rowsep="1"><Literal>Mount Point</Literal></Entry>
</Row>
</THead>
<TBody>
<Row>
<Entry><Literal>pmax_osf1</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/pmax_osf1</Literal></Entry>
</Row>
<Row>
<Entry><Literal>pmax_osf1.bin</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/pmax_osf1/bin</Literal></Entry>
</Row>
<Row>
<Entry><Literal>pmax_osf1.etc</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/pmax_osf1/etc</Literal></Entry>
</Row>
<Row>
<Entry><Literal>rs_aix32</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/rs_aix32</Literal></Entry>
</Row>
<Row>
<Entry><Literal>rs_aix32.bin</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/rs_aix32/bin</Literal></Entry>
</Row>
<Row>
<Entry><Literal>rs_aix32.etc</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/rs_aix32/etc</Literal></Entry>
</Row>
</TBody>
</TGroup>
</Table>
<Para>Storing common files in a central location eliminates the need to store
copies on every client's local disk, which in turn saves local disk space.
Replication further enhances the availability of common files. (Some binaries,
however, must remain on the local disk of every machine.)
<IndexTerm Id="DFSAGR.ISSU.indx.87" SpanEnd="DFSAGR.ISSU.indx.85"><IndexTerm Id="DFSAGR.ISSU.indx.88" SpanEnd="DFSAGR.ISSU.indx.84"></Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.24">
<Title>Setting Up User Filesets</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.89">
<Primary>filesets</Primary>
<Secondary>user</Secondary>
</IndexTerm>
<Para>Each user has a unique DCE account.  You may also want to create a
single, separate fileset for each user and mount the fileset at
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs/usr/</Filename><Symbol Role="Variable">username</Symbol>, where <Symbol Role="Variable">username</Symbol> is the
name of the user who owns the fileset.  For example, assign the name
<Literal>user.terry</Literal> to the fileset for the user named
<Literal>terry</Literal>.  When you mount the fileset at
<Filename>/.../abc.com/fs/usr/terry</Filename>, the root directory
of the fileset (the user's home directory) is named
<Filename>/.../abc.com/fs/usr/terry</Filename>.
The user's home directory contains all of the files, subdirectories, and
mount points in the fileset named <Literal>user.terry</Literal>.
</Para>
<Para>As with any other fileset, you may want to create additional filesets
based on logical file groupings and mount them below
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs/usr/</Filename><Symbol Role="Variable">username</Symbol> if the user's fileset becomes too
large.  For example, if <Literal>terry</Literal> has 5000 kilobytes of data in the
<Literal>project1</Literal> subdirectory and 3000 kilobytes of data in the <Literal>project2</Literal>
subdirectory, you may want to create two smaller filesets organized below
<Filename>/.../abc.com/fs/usr/terry</Filename>.  Table 2-3 lists the
organization and names of the filesets in this example.
<IndexTerm Id="DFSAGR.ISSU.indx.90">
<Primary>filesets</Primary>
<Secondary>names and mount points (table)</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.91">
<Primary>mount points</Primary>
<Secondary>fileset names</Secondary>
</IndexTerm></Para>
<Table Frame="all" Remap="center" Orient="Port">
<Title>Examples of Fileset Names and Mount Points for User Data</Title>
<TGroup Rowsep="0" Colsep="0" Cols="2">
<ColSpec Colsep="1" Align="Left" Colwidth="1.125*" Colname="col1" Colnum="1">
<ColSpec Align="Left" Colwidth="2*" Colname="col2" Colnum="2">
<THead>
<Row>
<Entry Rowsep="1"><Literal>Fileset Name</Literal></Entry>
<Entry Rowsep="1"><Literal>Mount Point</Literal></Entry>
</Row>
</THead>
<TBody>
<Row>
<Entry><Literal>user.terry</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/usr/terry</Literal></Entry>
</Row>
<Row>
<Entry><Literal>user.terry.project1</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/usr/terry/project1</Literal></Entry>
</Row>
<Row>
<Entry><Literal>user.terry.project2</Literal></Entry>
<Entry><Literal>/.../abc.com/fs/usr/terry/project2</Literal></Entry>
</Row>
</TBody>
</TGroup>
</Table>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.25">
<Title>Moving Data from Non-LFS Directories to DCE LFS Directories</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.92">
<Primary>filesets</Primary>
<Secondary>mounting non-LFS</Secondary>
</IndexTerm>
<Para>The guidelines in Section 2.3.4 assume that the user does not have an
existing home directory in the file system.  A user who has data in an existing
home directory in a non-LFS fileset mounted in the global namespace can
continue to use that fileset.  However, if you choose to create and mount a DCE
LFS fileset for the user, you must be careful: DFS does not allow you to mount
a fileset at an existing directory.  You must move the user's data from the
existing home directory in the DCE namespace to a temporary directory.  You
must then remove the existing home directory before creating and mounting the
user's DCE LFS fileset.  You can then move the user's data to the new fileset.
</Para>
<Para>For example, suppose the user named <Literal>terry</Literal> in the
previous example has an existing home directory in a non-LFS fileset mounted at
<Filename>/.../abc.com/fs/usr/terry</Filename>.  In this case, move the user's
data from <Filename>/.../abc.com/fs/usr/terry</Filename> to a temporary
location (such as a subdirectory of <Filename>/tmp</Filename> on the local
disk), and remove the <Literal>/.../abc.com/fs/usr/terry</Literal> directory
and its contents.  Then create and mount the user's DCE LFS fileset as
described in Section 2.3.4.  Finally, move the user's data from the temporary
location into the new DCE LFS fileset mounted at
<Literal>/.../abc.com/fs/usr/terry</Literal>.  When these steps are complete,
the user can access the data as before.
<IndexTerm Id="DFSAGR.ISSU.indx.93" SpanEnd="DFSAGR.ISSU.indx.89"></Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.26">
<Title>Replicating DCE LFS Filesets</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.94">
<Primary>filesets</Primary>
<Secondary>replicating</Secondary>
</IndexTerm>
<Para>You replicate DCE LFS filesets by placing read-only copies of them on one or
more File Server machines in a cell.  If a machine that houses a read-only copy
of the fileset becomes unavailable, the information is usually still available
from a copy of the fileset on another machine.  However, for a fileset that uses
Release Replication, if the read-only fileset that resides at the same site as
the read/write fileset becomes unavailable, all other read-only versions of
that fileset become unavailable after a configurable amount of time.  Similarly,
for a fileset that uses Scheduled Replication, if the read-write fileset
becomes unavailable, all read-only versions of the fileset become unavailable
after a configurable amount of time. (See Chapter 6 for detailed information
on the availability of read-only filesets.)
</Para>
<Para>Replicate only those DCE LFS filesets that meet the following criteria:
</Para>
<ItemizedList>
<ListItem>
<Para>The files in the fileset are read much more often than they are modified.
</Para>
</ListItem>
<ListItem>
<Para>The files in the fileset are used heavily (for example, binary files for
text editors or other heavily used application programs).  Replicating the
fileset lets you distribute the load for the files that it contains across
several machines.
</Para>
</ListItem>
<ListItem>
<Para>The files in the fileset must remain available.  By replicating the fileset on
multiple File Server machines, even if one of the machines that houses a 
replica of the fileset becomes unavailable, replicas are usually still 
available from other machines.
</Para>
</ListItem>
<ListItem>
<Para>The fileset is mounted at a high level in the cell's file tree; for example,
<Literal>root.dfs</Literal> and its subdirectories.
</Para>
</ListItem>
</ItemizedList>
<IndexTerm Id="DFSAGR.ISSU.indx.95">
<Primary>disk space</Primary>
<Secondary>replicas</Secondary>
</IndexTerm>
<Para>If your cell is large, you may want to use a small set of File
Server machines to store just read-only filesets.  These machines can then
distribute frequently used data, reducing the load on other machines.  Keep
in mind that each replica not stored on the same aggregate as its read/write
source fileset uses as much disk space as its source fileset.  A read-only
fileset created on the same aggregate as its source fileset is created as a
clone of its source and so requires potentially much less space than a full
read-only replica created on a different aggregate.
</Para>
<Para>Each Cache Manager maintains preferences in the form of numerical ranks
that bias its selection of File Server machines for read-only fileset access.
When accessing a read-only fileset, the Cache Manager consults its collection
of preferences and attempts to access the read-only fileset from the File
Server machine that has the lowest recorded rank. If the Cache Manager cannot
access the fileset from that machine, it tries to access the fileset from the
machine that has the next-lowest rank. It continues in this manner until it
either succeeds in accessing the fileset or determines that all of the machines
that house the fileset are unavailable.
</Para>
<Para>By default, the Cache Manager assigns preferences to File Server machines
based on IP addresses. You can set or change the Cache Manager's preferences to
suit your needs. (See Chapter 8 for more information about the Cache Manager
and File Server machine preferences.)
</Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.27">
<Title>Using the @sys and @host Variables</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.96">
<Primary>Distributed File Service (DFS)</Primary>
<Secondary>variables</Secondary>
</IndexTerm>
<Para>DFS simplifies the administration of operating system-specific or host-specific
files by providing the <Literal>@sys</Literal> and <Literal>@host</Literal> variables.  When the Cache
Manager encounters <Literal>@sys</Literal> or <Literal>@host</Literal> in a pathname, it replaces the
variable with either the system name (defined with the <Command>cm sysname</Command>
command) or the hostname (defined with the local operating system's
<Literal>hostname</Literal> command or its equivalent).
</Para>
<IndexTerm Id="DFSAGR.ISSU.indx.97">
<Primary><Literal>@sys</Literal> variable</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.98">
<Primary><Literal>@host</Literal> variable</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.99">
<Primary>system variable</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.100">
<Primary>host variable</Primary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.101">
<Primary>symbolic links</Primary>
<Secondary>and variables</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.102">
<Primary>variables</Primary>
<Secondary><Literal>@host</Literal> and <Literal>@sys</Literal></Secondary>
</IndexTerm>
<Para>The <Literal>@sys</Literal> and <Literal>@host</Literal> variables are
especially useful when constructing symbolic links from the local disk to the
DFS filespace.  You create identical links on all machines, yet each machine
accesses the files that are appropriate to its system type or hostname.  Use
the <Literal>@sys</Literal> variable to access files that are organized on a
per-system type basis; use <Literal>@host</Literal> to access files that are
organized on a per-machine basis.  The following subsections provide examples
of  these variables.
</Para>
<Sect3 Id="DFSAGR.ISSU.div.28">
<Title>The @sys Variable</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.103">
<Primary><Command>cm</Command> command suite</Primary>
<Secondary><Literal>sysname</Literal></Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.104">
<Primary>Cache Manager</Primary>
<Secondary>interpretations of variables</Secondary>
</IndexTerm>
<Para>The <Literal>@sys</Literal> variable is expanded to the name of a CPU/OS type. The <Literal>cm
sysname</Literal> command sets and displays the current value of the <Literal>@sys</Literal>
variable.  The following examples show how the Cache Manager interprets the
same pathname differently, depending on the value of <Literal>@sys</Literal>.
</Para>
<Para>On a machine running OSF/1:
</Para>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Command>cm sysname</Command>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>Current sysname is `pmax_osf1'
</ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>cd /.../abc.com/fs/@sys</Literal>
$ <Literal>pwd</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>/.../abc.com/fs/pmax_osf1
</ProgramListing></Para>
</InformalExample>
<Para>On a machine running AIX 3.2:
</Para>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Command>cm sysname</Command>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>Current sysname is `rs_aix32'
</ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>cd /.../abc.com/fs/@sys</Literal>
$ <Literal>pwd</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>/.../abc.com/fs/rs_aix32
</ProgramListing></Para>
</InformalExample>
<Para>The <Literal>@sys</Literal> variable is commonly used in symbolic links from a DFS client
machine to a fileset in the DFS filespace.  A single copy of a binary file for
each system type is stored on a single File Server machine in DFS instead of
on the local disk of each client machine.  Links are then created from client
machines to the central copy of the binary file, eliminating the need to store
the same binary file on each client machine.  Accessing binary files this way
saves disk space on client machines and ensures that users on all client
machines are using the same version of the binary file.  It also eases system
administration by allowing administrators to update central copies of binary
files, rather than requiring them to update the copies stored on each client
machine.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.105">
<Primary>binary files</Primary>
<Secondary>access from client machines</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.106">
<Primary>client machines</Primary>
<Secondary>use of <Literal>@sys</Literal> variable</Secondary>
</IndexTerm>A link that includes the <Literal>@sys</Literal> variable can be created on each client
machine.  The Cache Manager on each machine interprets the <Literal>@sys</Literal> variable,
so each machine accesses the binary file for its system type from the global
namespace.  Symbolic links that include the <Literal>@sys</Literal> variable are commonly
used to access binary files for programs such as <Command>make</Command> and <Literal>emacs</Literal>.
</Para>
<Para>The following examples create a symbolic link used to access the proper binary
files for programs traditionally stored in <Filename>/usr/local</Filename>.  In the examples,
the Cache Managers on two machines interpret the link differently, depending on
their respective values of <Literal>@sys</Literal>.
</Para>
<Para><?sml-need 8>On a machine running OSF/1:
</Para>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>ln -s /.../abc.com/fs/@sys/usr/local /usr/local</Literal>
$ <Literal>ls -l /usr/local</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>lrwxrwxrwx 1 root 34 Nov 22 1991 /usr/local ->
/.../abc.com/fs/@sys/usr/local
</ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>cd /usr/local</Literal>
$ <Literal>pwd</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>/.../abc.com/fs/pmax_osf1/usr/local
</ProgramListing></Para>
</InformalExample>
<Para>On a machine running AIX 3.2:
</Para>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>ln -s /.../abc.com/fs/@sys/usr/local /usr/local</Literal>
$ <Literal>ls &minus;l /usr/local</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>lrwxrwxrwx 1 root 32 Aug 1 06:44 /usr/local ->
/.../abc.com/fs/@sys/usr/local
</ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing><UserInput>$ <Literal>cd /usr/local</Literal>
$ <Literal>pwd</Literal>
</UserInput></ProgramListing></Para>
</InformalExample>
<InformalExample>
<Para><ProgramListing>/.../abc.com/fs/rs_aix32/usr/local
</ProgramListing></Para>
</InformalExample>
<Para>When creating links on server machines, do not use links to access binary files
for DFS server processes.  These files must reside on the local disk of each
server machine to avoid bootstrapping problems.
</Para>
<Para>(See Part 2 of this guide and reference for more information about the <Literal>cm
sysname</Literal> command.)
<IndexTerm Id="DFSAGR.ISSU.indx.107" SpanEnd="DFSAGR.ISSU.indx.104"><IndexTerm Id="DFSAGR.ISSU.indx.108" SpanEnd="DFSAGR.ISSU.indx.101"></Para>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.29">
<Title>The @host Variable</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.109">
<Primary><Literal>@host</Literal> variable</Primary>
</IndexTerm>
<Para>The <Literal>@host</Literal> variable is expanded to the value defined
by the <Literal>hostname</Literal> command (or its equivalent) of the local
operating system.  The <Literal>@host</Literal> variable is especially useful
when configuring machines that must execute a machine-specific set of start-up
routines.
</Para>
<Para>For example, suppose two machines, <Literal>fs1.abc.com</Literal> and <Literal>fs2.abc.com</Literal>,
use two different, machine-specific versions of an initialization file for
an application that they start following a reboot.  The name of the
initialization file is <Command>start</Command>.  The file <Command>start</Command> can be stored in
DFS and accessed on a machine-specific basis via the <Literal>@host</Literal> variable.
To access the proper copy of the file, both machines can have symbolic links
from <Filename>/etc/rc/start</Filename> to <Literal>/.../abc.com/fs/etc/@host/rc/start</Literal>.  On the first machine, the symbolic link resolves to the
file named
</Para>
<InformalExample>
<Para><ProgramListing><UserInput><Filename>/.../abc.com/fs/etc/fs1.abc.com/rc/start</Filename>
</UserInput></ProgramListing></Para>
</InformalExample>
<Para>On the second machine, the symbolic link resolves to the file named
</Para>
<InformalExample>
<Para><ProgramListing><UserInput><Filename>/.../abc.com/fs/etc/fs2.abc.com/rc/start</Filename>
</UserInput></ProgramListing></Para>
</InformalExample>
<!-- .cS-->
<!-- For this reason, the \*L@host\*O-->
<!-- functionality is most frequently used in cells that support diskless machines.-->
<!-- .P-->
<!-- For example, all diskless machines in a cell could have symbolic links from-->
<!-- the proper initialization file (\*L/etc/rc\*O or its equivalent) to-->
<!-- \*L/.../abc.com/fs/diskless_config/@host/rc\*O.  A diskless machine with a-->
<!-- hostname defined as \*Ldiskless1.abc.com\*O would execute the file named-->
<!-- .iS-->
<!-- \*L/.../abc.com/fs/diskless_config/diskless1.abc.com/rc\*O-->
<!-- .iE-->
<!-- .P-->
<!-- A diskless machine with a hostname of \*Ldiskless2.abc.com\*O would execute the-->
<!-- file named-->
<!-- .iS-->
<!-- \*L/.../abc.com/fs/diskless_config/diskless2.abc.com/rc\*O-->
<!-- .iE-->
<!-- .cE-->
</Sect3>
</Sect2>
</Sect1>
<Sect1 Id="DFSAGR.ISSU.div.30">
<Title>Data Access Management in DFS</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.110">
<Primary>File Exporter</Primary>
<Secondary>access control by</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.111">
<Primary>tokens</Primary>
<Secondary>about</Secondary>
</IndexTerm>
<Para>All access to data and metadata on a File Server machine is managed by
the File Exporter.  Clients contact the File Exporter to access data.  The
Cache Manager is the client of the File Exporter most visible to the user,
as well as the one most frequently discussed in this guide, but other clients
do exist.  For example, the <Command>fts</Command> program can become a client
of the File Exporter when a fileset is moved from one aggregate or machine to
another, and the Replication Server is a frequent client of the File Exporter
as it manages replicas of read/write filesets.
</Para>
<Para><?sml-need 8>The File Exporter uses tokens to manage the distribution of
data and metadata to clients.  A client that wants to access or change data
must first request and obtain the proper tokens for the data from the File
Exporter on the machine on which the data resides.  If the File Exporter can
grant the client's request, it passes the tokens to the client; otherwise, it
either queues the request for service later or rejects the request.  A client
that receives the requested tokens can then use them to access the data it
wants from the File Exporter.
</Para>
<Para>The following subsections provide more detailed information about tokens,
their management by the File Exporter, and the token state recovery that occurs
after a communications failure between a File Exporter and its clients.
</Para>
<Sect2 Id="DFSAGR.ISSU.div.31">
<Title>Tokens</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.112">
<Primary>tokens</Primary>
<Secondary>types</Secondary>
</IndexTerm>
<Para>Tokens and their distribution and management by the File Exporter are
completely transparent at the user level.  The File Exporter uses tokens to
</Para>
<ItemizedList>
<ListItem>
<Para>Track the clients to which it has given data and the types of operations they
are permitted to perform on the data.
</Para>
</ListItem>
<ListItem>
<Para>Ensure that multiple clients are not simultaneously accessing the same data in
a conflicting manner.
</Para>
</ListItem>
<ListItem>
<Para>Guarantee that each client always has access to the most recent versions
of read/write data.  If data stored on a File Server machine changes while a 
client has a copy of it, the File Exporter on that machine notifies the 
client; the client then obtains the new version of the data the next time
the data is needed.
</Para>
</ListItem>
</ItemizedList>
<Para>Different operations require different types of tokens. DFS includes four
general classes of tokens:
</Para>
<VariableList>
<VarListEntry role="linebreak">
<Term>Open Tokens</Term>
<ListItem>
<?sml-break>
<Para>Allow a client to open an entire file or fileset to read from it, write
to it, delete it, or prevent it from being deleted.  For example, a client that
wants to open a file for reading requests an open token for the file.
</Para>
</ListItem>
</VarListEntry>
<VarListEntry role="linebreak">
<Term>Status Tokens</Term>
<ListItem>
<?sml-break>
<Para>Allow a client to read or write file status information.  For example, a client
that wants to append data to a file, thus changing its size, needs a status
token for the file.
</Para>
</ListItem>
</VarListEntry>
<VarListEntry>
<Term>Data Tokens</Term>
<ListItem>
<?sml-break>
<Para>Allow a client to read from or write to a range of bytes in a file.  For
example, a client that wants to modify the first 10 bytes of a file requests
a data token for those bytes.
</Para>
</ListItem>
</VarListEntry>
<VarListEntry role="linebreak">
<Term>Lock Tokens</Term>
<ListItem>
<?sml-break>
<Para>Allow a client to read lock or write lock a range of bytes in a file.  For
example, a client that must ensure that only one process is locking the first
10 bytes of a file requests a lock token for those bytes.
</Para>
</ListItem>
</VarListEntry>
</VariableList>
<Para>Each token class includes a number of token types; for example, the data class
includes the read data and write data types.  The different classes and types
of tokens combine to allow for the different kinds of data access required by
file system clients.  Most operations require that a client possess multiple
tokens for the data it wants to manipulate; for instance, appending text to
a file requires open tokens to access the file, data tokens to modify the
contents of the file, and status tokens to change the size of the file.
</Para>
<Para>Some tokens can be granted to different clients simultaneously, while others
cannot.  Two tokens that can be granted simultaneously are said to be
<Replaceable>compatible</Replaceable>; two tokens that cannot be granted at the same time are said
to be <Replaceable>conflicting</Replaceable>.  A token is always compatible with tokens from other
classes, but it may conflict with other token types from within its class.
In general, the token types associated with read operations are mutually
compatible, while those associated with write operations conflict with other
tokens.
</Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.32">
<Title>Token Management</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.113">
<Primary>tokens</Primary>
<Secondary>management by File Exporter</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.114">
<Primary>File Exporter</Primary>
<Secondary>managing tokens</Secondary>
</IndexTerm>
<Para>To determine whether it can grant a client's request for tokens, the File
Exporter checks for outstanding tokens that conflict with those requested.  If
no other client has conflicting tokens, the File Exporter grants the requested
tokens.  If another client has conflicting tokens, the File Exporter takes the
action associated with the first condition met from the following list:
</Para>
<ItemizedList>
<ListItem>
<Para>If the existing tokens can be revoked, the File Exporter revokes them and
grants those requested.  When its tokens are revoked, a client such as the
Cache Manager flushes cached data for which the tokens applied, writing any
modified data back to the File Server machine.
</Para>
</ListItem>
<ListItem>
<Para>If the existing tokens cannot be revoked, the File Exporter either places
the request in a queue, to be serviced as soon as possible, or refuses to grant
the requested tokens outright.  The client dictates the File Exporter's
response to this situation when it requests the tokens.
</Para>
</ListItem>
</ItemizedList>
<Para>In general, if a client's existing tokens conflict with those requested
by another client, the File Exporter attempts to revoke the existing tokens
to grant the request.  Many factors influence the File Exporter's ability to
revoke a client's tokens.  The File Exporter can usually revoke some types of
tokens, but clients can refuse to relinquish other types of tokens in various
situations.  In addition, lifetimes that the File Exporter assigns to the 
tokens it grants and to the clients to which it grants them also affect its 
ability to revoke tokens, as follows:
</Para>
<VariableList>
<VarListEntry role="linebreak">
<Term>Token Lifetime</Term>
<ListItem>
<Para>Specifies the length of time for which a token is valid.  All tokens have
a fixed token lifetime.  Once its lifetime has elapsed, a token expires.  The
File Exporter needs to revoke only valid tokens.  Because expired tokens are
no longer valid, the File Exporter does not need to revoke them; it can simply
grant new tokens as if the expired tokens did not exist.  A client can contact
the File Exporter to request that its tokens' lifetimes be extended before
they expire.
</Para>
</ListItem>
</VarListEntry>
<VarListEntry role="linebreak">
<Term>Host Lifetime</Term>
<ListItem>
<Para>Indicates the length of time for which the File Exporter considers a client
to be alive.  Each client that has tokens from the File Exporter has a host
lifetime within which it must contact the File Exporter to let it know that
it is still alive, thus renewing its host lifetime.  The File Exporter needs
the client's permission to revoke tokens that are held by the client as long 
as the client's host lifetime has not expired.
</Para>
</ListItem>
</VarListEntry>
<VarListEntry role="linebreak">
<Term>Host RPC Lifetime</Term>
<ListItem>
<?sml-break>
<Para>Defines the length of time for which the File Exporter guarantees to attempt
to make an RPC to a client before the File Exporter revokes its tokens.  If 
the client responds to the RPC (thus renewing its host lifetime), the 
File Exporter cannot revoke the client's tokens without the client's 
permission.  If the client fails to respond to the RPC but its host lifetime 
has not expired, the File Exporter cannot revoke the client's tokens; if 
the client fails to respond and its host 
lifetime has expired, the File Exporter can revoke any tokens the client holds 
without attempting to contact it further.  The File Exporter can revoke the 
tokens of any client whose host RPC lifetime has expired without contacting 
the client; the client needs to either reclaim its tokens or request new ones 
as necessary.
</Para>
</ListItem>
</VarListEntry>
</VariableList>
<Para>Each File Exporter defines the lengths of its clients' host lifetimes and host
RPC lifetimes, so a client can have different lifetimes for different File
Exporters.  For any File Exporter, however, a client's host RPC lifetime must
be equal to or greater than its host lifetime. (By default, both lifetimes are
only a few minutes in length.)
</Para>
<Para>The following general rules govern the File Exporter's revocation of valid
tokens held by a client:
</Para>
<ItemizedList>
<ListItem>
<Para>If the client's host lifetime has not expired, the File Exporter tries to
contact the client; the File Exporter must have the client's permission to 
revoke its tokens.
<?sml-break><?sml-need 10></Para>
</ListItem>
<ListItem>
<Para>If the client's host lifetime has expired but its host RPC lifetime has
not, the File Exporter tries to contact the client one time.  If the client
responds, the File Exporter cannot revoke the client's tokens without its
permission; otherwise, the File Exporter can revoke any tokens the client
holds without contacting it further.
</Para>
</ListItem>
<ListItem>
<Para>If the client's host RPC lifetime has expired, the File Exporter can revoke
the client's tokens without contacting it.
</Para>
</ListItem>
</ItemizedList>
<IndexTerm Id="DFSAGR.ISSU.indx.115" SpanEnd="DFSAGR.ISSU.indx.114">
<IndexTerm Id="DFSAGR.ISSU.indx.116" SpanEnd="DFSAGR.ISSU.indx.113">
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.33">
<Title>Token State Recovery</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.117">
<Primary>tokens</Primary>
<Secondary>recovering</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.118">
<Primary>File Exporter</Primary>
<Secondary>recovering tokens</Secondary>
</IndexTerm>
<Para>Token state recovery refers to clients regaining their tokens following a
communications failure between themselves and a File Exporter.  The following
problems can interrupt communications between a File Exporter and its clients:
</Para>
<ItemizedList>
<ListItem>
<Para>If a File Exporter is restarted (for example, after its File Server machine 
crashes), it loses all knowledge of the tokens it granted prior to the 
restart.  For a brief period after it first returns to service, the File 
Exporter refuses all requests for new tokens from all clients, accepting 
requests only to reestablish tokens from those clients that held them before 
the File Exporter became unavailable.  This is the first form of token state 
recovery.
</Para>
</ListItem>
<ListItem>
<Para>If a network failure prevents a client from contacting a File Exporter, the
client may be unable to prevent its host lifetime from expiring.  Once
communications are restored, the client must either reclaim its tokens or, if
necessary, request new ones.  This is the second form of token state recovery.
</Para>
</ListItem>
<ListItem>
<Para>If a client is restarted, it loses all knowledge of the tokens it possessed
prior to the restart; recovery of its tokens is not possible.
</Para>
</ListItem>
</ItemizedList>
<Para>During the first form of token state recovery, the File Exporter attempts to
preserve the state of its tokens across restarts by initially accepting
requests only to reestablish existing tokens.  While the File Exporter is
unavailable, clients that have tokens from it continue to probe it at regular
polling intervals until it returns to service.  When it is again available, the
File Exporter enters token state recovery to give these clients the opportunity
to recover their tokens without threat of conflicts with tokens that were
granted to new clients.
</Para>
<Para>Different File Exporters remain in token state recovery for different lengths
of time after a restart.  However, each File Exporter ensures that its recovery
period lasts long enough to give all of its clients the opportunity to
reestablish their tokens, basing the duration on the host lifetimes or polling
intervals that it assigns, whichever are greater.
</Para>
<Para>During the second form of token state recovery, the File Exporter does not
provide the client with an opportunity to reestablish its tokens without fear 
of conflicting tokens.  The client continues to poll the File Exporter until the
network outage is resolved.  However, if its host lifetime expires before it
can contact the File Exporter, the client may be unable to recover tokens that
it held prior to the network problem.
</Para>
<Para RevisionFlag="Changed">Values that the File Exporter uses to determine the host lifetimes, host RPC
lifetimes, and polling intervals of its clients are specified with options of
the <Command>fxd</Command> command. (See Part 2 of this guide and reference for complete
information about the <Command>fxd</Command> command and its options.)
<IndexTerm Id="DFSAGR.ISSU.indx.119" SpanEnd="DFSAGR.ISSU.indx.110"><IndexTerm Id="DFSAGR.ISSU.indx.120" SpanEnd="DFSAGR.ISSU.indx.111"><?og-ChangeStart enh, 13605, R1.2.2, Security enhancements"></Para>
</Sect2>
</Sect1>
<Sect1 Id="DFSAGR.ISSU.div.34">
<Title RevisionFlag="Changed">Data Access Security in DFS</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.121">
<Primary RevisionFlag="Changed">RPC authentication levels</Primary>
</IndexTerm>
<Para RevisionFlag="Changed">DFS includes administrative commands to establish
and modify RPC authentication levels for communications between Cache Managers
and File Servers.  DFS provides very flexible tools for managing these RPC
authentication levels, allowing you to set RPC authentication levels for each
Cache Manager and RPC authentication bounds for each File Server.  You can
also set advisory RPC authentication bounds for each fileset.
</Para>
<Para RevisionFlag="Changed">The default values for security settings at
the Cache Manager and File Server ensure that communications between a Cache
Manager and File Server are authenticated at the DCE packet integrity
security level.  All data received has been authenticated as originating at
the expected host and has been verified to have not been modified during
transmission.  However, you can choose to set higher or lower RPC
authentication levels for each Cache Manager and File Server.  Note that
higher authentication levels result in some degradation of performance
(due to increased overhead).
</Para>
<Para RevisionFlag="Changed">Each Cache Manager maintains a pair of initial
RPC authentication level settings and RPC authentication lower bound settings. 
One pair governs Cache Manager communications with File Servers in the same
cell, while the second set governs communications with File Servers in foreign
cells.  Similarly, each File Server maintains a pair of RPC authentication
lower and upper bound settings.  Again, one pair governs communications with
Cache Managers in the same cell, while the second pair controls communications
with Cache Managers in foreign cells.
</Para>
<Para RevisionFlag="Changed">When a Cache Manager must contact a File Server
to access a given fileset the Cache Manager and File Server negotiate for a
mutually acceptable RPC authentication level.  In operation, the process works
as follows.
</Para>
<Para RevisionFlag="Changed">The Cache Manager sends an RPC to the File Server that is using the Cache Manager's initial RPC authentication level.  The File Server checks the RPC and compares it to the authentication level range determined by the File Server's upper and lower authentication level bounds.  If the RPC falls within the authentication level range, communications between the Cache Manager and File Server are established.  However, if the RPC authentication level is above or below the File Server's range, the File Server responds with an instruction to increase or decrease the authentication level accordingly.  This negotiation continues until the Cache Manager and File Server arrive at a mutually agreeable RPC authentication level or until the File Server requests an authentication level below the minimum allowed for the Cache Manager (causing the Cache Manager to refuse communications with the File Server).
</Para>
<Para RevisionFlag="Changed">After arriving at a mutually agreeable RPC authentication level, the Cache Manager stores that information so that it does not need to renegotiate an authentication level during further communications with that particular file server.
</Para>
<Para RevisionFlag="Changed">Note that Cache Managers in versions of DFS
earlier than 1.2.2 cannot negotiate RPC authentication levels.  Setting the
minimum authentication level bound at a File Exporter higher than packet
integrity prevents that File Server from communicating with Cache Managers
based on earlier versions of DFS.
</Para>
<Para RevisionFlag="Changed">You can establish a Cache Manager's initial and
lower bound RPC authentication levels by using the <Command>dfsd</Command>
command.  You must assume the <Literal>root</Literal> identity on the Cache
Manager machine to issue this command. You can adjust these settings by using
the <Command>cm setprotectlevels</Command> command.  You can check the Cache
Manager's current RPC authentication level settings with the <Command>cm
getprotectlevels</Command> command.
</Para>
<Para RevisionFlag="Changed">You can establish the upper and lower File Exporter RPC authentication bounds by using the <Command>fxd</Command> command. You cannot display a File Exporter's RPC authentication bound settings. For more information about setting the File Exporter's authentication bounds with the <Command>fxd</Command> command, see Part 2 of this guide and reference.
</Para>
<Sect2 Id="DFSAGR.ISSU.div.35">
<Title RevisionFlag="Changed">Fileset Advisory RPC Authentication Bounds</Title>
<Para RevisionFlag="Changed">You can establish advisory minimum and maximum RPC authentication bounds for each fileset.  As with the File Server RPC authentication bounds, the FLDB holds a pair of bounds for each fileset.  One set of bounds governs communications with Cache Managers that are in the same cell as the File Server within which the fileset resides; the other set of bounds controls communications with Cache Managers in foreign cells. While these advisory bounds are not currently enforced (although they may be in a future release of DFS), they do serve to bias the initial RPC authentication level when a Cache Manager attempts to access that fileset. The advisory bounds work as follows.
</Para>
<Para RevisionFlag="Changed">When the Cache Manager contacts a Fileset Location (FL) Server to ascertain the location (or locations) of a given fileset, the information returned by the FL Server includes that fileset's lower and upper RPC bounds.  The Cache Manager compares its initial RPC authentication level to the range set by the advisory bounds.  If the initial level falls within that range, the Cache Manager begins negotiations with a File Server using the initial level.  However, if the initial level is above or below the range, the Cache Manager adjusts its initial level to match the closest bound level. (If the File Server requests that the Cache Manager lower its authentication level below the minimum level specified for the Cache Manager, the Cache Manager refuses communications with that File Server.) The Cache Manager then uses the modified initial level to begin negotiations with a File Server.
</Para>
<Para RevisionFlag="Changed">You establish the fileset advisory RPC authentication level bounds by using the <Command>fts setprotectlevels</Command> command. You can check if a given fileset has advisory bounds and display the bound level values by using either the <Command>fts lsfldb</Command> command or the <Command>fts lsft</Command> command. 
<?og-ChangeEnd enh, 13605, R1.2.2, Security enhancements">
<IndexTerm Id="DFSAGR.ISSU.indx.122" SpanEnd="DFSAGR.ISSU.indx.121">
</Para>
<!--  -->
<!--  -->
<!-- Moved the ChangeEnd and IndexTerm tags above from the (otherwise) empty -->
<!--  paragraph below... -->
<!--  -->
<!-- <Para></Para> -->
<!--  -->
</Sect2>
</Sect1>
<Sect1 Id="DFSAGR.ISSU.div.36">
<Title>DFS Distributed Database Technology</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.123">
<Primary>Ubik</Primary>
</IndexTerm>
<Para>DFS includes two administrative databases: the Fileset Location Database
(FLDB) and the Backup Database.  You can increase system efficiency, file
availability, and system reliability by replicating (copying) these two
databases on multiple server machines.  If one machine housing a copy of a
database then becomes unavailable, the information can still be accessed from
a copy of the database on another machine.
</Para>
<Para>Unlike replicated filesets, replicated databases may change frequently. To
ensure consistent system behavior, all copies of a database must be identical.
DFS uses a library of utilities, <Replaceable>Ubik</Replaceable>, as a mechanism for synchronizing
multiple copies of a replicated database. (Because Ubik is a subroutine
library, it does not appear in listings of the processes running on a server
machine.)
</Para>
<Para><?sml-need 10>In DFS, one server machine houses a master copy of a replicated database such
as the FLDB.  When a user alters information in the database, Ubik coordinates
the distribution of the change from the master copy to the copies of the
database on other machines; the distribution is automatic and nearly
instantaneous.  Ubik dynamically selects a master copy of a database from among
the servers that house it.  The selection process and the propagation of
changes to all copies of a database are managed entirely by Ubik and are
transparent to administrators and users.
</Para>
<Sect2 Id="DFSAGR.ISSU.div.37">
<Title>Ubik Database Synchronization</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.124">
<Primary>Distributed File Service (DFS)</Primary>
<Secondary>database synchronization</Secondary>
</IndexTerm>
<Para>The Ubik library has a client portion and a server portion. Clients such as
the <Command>fts</Command> and <Command>bak</Command> programs call subroutines in the Ubik library's
client portion to contact the Fileset Location (FL) Server or Backup Server.
These database server processes in turn call subroutines in the server portion
of the Ubik library to access or modify information in the FLDB or Backup
Database.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.125">
<Primary>Ubik</Primary>
<Secondary>synchronization site</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.126">
<Primary>Ubik</Primary>
<Secondary>coordinator</Secondary>
</IndexTerm>The master copy of an FLDB or Backup Database is referred to as the
<Replaceable>synchronization site</Replaceable>.  The other copies of the database are referred to
as <Replaceable>secondary sites</Replaceable>.  A separate occurrence of Ubik, referred to as a <Replaceable>Ubik
coordinator</Replaceable>, maintains the copy of the database at each site.  A database
server process makes a change to a database by issuing a call to the Ubik
coordinator at the synchronization site, which makes the change to that copy
of the database and distributes the change to the Ubik coordinators at the
secondary sites.  The coordinator at each secondary site then updates the copy
of the database at its site.
</Para>
<Para>Each copy of a database has a version number, which should always be the same
for all copies of the database.  Each change to a database increments the
version number of the database by one.  The coordinator at the synchronization
site uses the version number to determine whether each secondary site has a
copy of the most recent version of the database.
</Para>
<Para><?sml-need 10>For example, if a service outage isolates a secondary site from the
synchronization site, the secondary site no longer receives database updates
from the synchronization site.  When communications are restored, the
coordinator at the synchronization site examines the version number of the
database at the secondary site to determine whether the secondary site has the
most recent version.  If necessary, it sends the copy with the highest version
number to the secondary site.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.127">
<Primary>Ubik</Primary>
<Secondary>electing synchronization site</Secondary>
</IndexTerm>The Ubik coordinator at the synchronization site periodically sends an RPC to
each secondary site.  A response to the RPC from the coordinator at a secondary
site serves as a <Replaceable>vote</Replaceable> to maintain the current synchronization site in its
role for a fixed amount of time.  Within that time, the synchronization site
sends a subsequent RPC to the secondary site in an attempt to retain its role.
</Para>
<Para>The coordinator at the synchronization site constantly tallies the votes
it receives from the secondary sites.  It continues in its role as
synchronization site, confident that the other sites have not chosen a new
synchronization site and begun making competing changes to the database, as
long as it receives the votes of a strict majority (more than 50%) of all
database sites, including itself.  The necessary majority of database sites
is referred to as a <Replaceable>quorum</Replaceable>.
</Para>
<Para>Because Ubik relies on the actions of a quorum, having an odd number of
database sites is helpful; in most cases, storing a replicated database at
three sites is sufficient.  Note, however, that the vote of the coordinator on
the database server machine with the lowest network address of all database
server machines of its type (those that house the FLDB or those that house the
Backup Database) carries more weight than the votes of the coordinators at the
other sites.  This weighting allows Ubik to attain a quorum if an even number
of sites exist.
</Para>
<Para>The synchronization site stops sending RPCs to the secondary sites if
hardware, software, or network problems result in any of the following:
</Para>
<ItemizedList>
<ListItem>
<Para>The synchronization site stops receiving votes from a quorum of the
database sites.
</Para>
</ListItem>
<ListItem>
<Para>The synchronization site cannot propagate changes to a quorum of the
database sites.
</Para>
</ListItem>
<ListItem>
<Para>The synchronization site or its machine fails.
</Para>
</ListItem>
</ItemizedList>
<Para><?sml-need 10>If the coordinator at the synchronization site stops
sending RPCs for any reason, Ubik elects a new synchronization site.  In an
election, each coordinator is biased to vote for the site with the lowest
network address from among the sites it can contact.  The vote of the site
with the lowest network address of all database server machines of that type
carries slightly more weight than the votes of the other sites.  One site,
usually the one with the lowest network address, typically gathers the
necessary majority quickly and is elected the new synchronization site.
</Para>
<Para>Immediately following the election, the newly elected synchronization
site polls all sites to find the database with the highest version number.  It
adopts this version as the master copy and distributes it to the sites that
do not yet have it.  The election and database distribution are typically
brief, usually taking no longer than a few minutes.
</Para>
<Para>While Ubik cannot obtain quorum and during the subsequent
election and database distribution, the affected database cannot be modified
in any way.  If the Backup Database is affected, information cannot be read
from the database; the database is completely unavailable.  If the FLDB is
affected, Cache Managers can still read information from the database about
the locations of filesets from which they need to access information; however,
<Command>fts</Command> commands such as <Command>fts lsfldb</Command> cannot
be used to get information from the database.  Because the FLDB is most often
accessed by Cache Managers seeking fileset location information, a Ubik
election and ensuing database distribution do not interfere with the
database's primary purpose.
<IndexTerm Id="DFSAGR.ISSU.indx.128" SpanEnd="DFSAGR.ISSU.indx.124"></Para>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.38">
<Title>Providing Information for Ubik</Title>
<Para>For the most part, Ubik operates without human intervention.  However,
it does depend on other DCE facilities and services for some things.  The
following list describes the interaction between Ubik and the remainder of
DCE.  It also provides an overview of the configuration information necessary
for Ubik to operate properly.  Section 2.5.3 discusses the database server
configuration steps required for Ubik to function properly.
</Para>
<ItemizedList>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.129">
<Primary>Distributed Time Service (DTS)</Primary>
<Secondary>interaction with Ubik</Secondary>
</IndexTerm>
<Para><Replaceable>Ubik relies on DTS</Replaceable> to synchronize the clocks
on server machines that house copies of a replicated database.  Ubik
coordinators must agree on the time; clock differences among Ubik sites can
cause them to believe they are no longer in contact with each other, even if
they are operating correctly.  If a
site falls out of touch, it may try to elect a new synchronization site or
refuse to give out information.  You can prevent such service outages by using
DTS to synchronize the clocks on database server machines.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.130">
<Primary>Security Service</Primary>
<Secondary>interaction with Ubik</Secondary>
</IndexTerm>
<Para><Replaceable>Ubik relies on the DCE Security Service</Replaceable> for
secure communications between all Fileset Database machines (machines that
house the FLDB) and Backup
Database machines (machines that house the Backup Database).  Each type of
database server has its own security group, of which all machines that house a
copy of that type of database must be members.  A machine's membership in this
group enables the Ubik coordinator on that machine to communicate with the
Ubik coordinators on the other database servers of that type, thus allowing
the coordinator to participate in Ubik elections.
</Para>
<Para>Abbreviated forms of the DFS server principals of all Fileset Database machines
must be listed in the <Literal>subsys/dce/dfs-fs-servers</Literal> group in the Registry
Database.  Similarly, abbreviated forms of the DFS server principals of all
Backup Database machines must be listed in the <Literal>subsys/dce/dfs-bak-servers</Literal>
group in the Registry Database.  To view the members of either of these security
groups, use the <Command>dcecp group list</Command> command.
</Para>
<Para>A machine's DFS server principal is of the form
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/hosts/</Literal><Symbol Role="Variable">hostname</Symbol><Filename>/dfs-server</Filename>.  The abbreviated
form of a machine's DFS server principal is of the form
<Literal>hosts/</Literal><Symbol Role="Variable">hostname</Symbol><Filename>/dfs-server</Filename>.  For example, in the cell named
<Replaceable>abc.com</Replaceable>, the abbreviated server principals of all Fileset Database
machines are listed in <Literal>subsys/dce/dfs-fs-servers</Literal> in the form
<Literal>hosts/</Literal><Symbol Role="Variable">hostname</Symbol><Filename>/dfs-server</Filename>.
</Para>
</ListItem>
<ListItem>
<IndexTerm Id="DFSAGR.ISSU.indx.131">
<Primary>Cell Directory Service (CDS)</Primary>
<Secondary>interaction with Ubik</Secondary>
</IndexTerm>
<Para><Replaceable>Ubik relies on CDS</Replaceable> for a complete list of all Fileset Database and Backup
Database machines.  Each type of database server has its own RPC server group in
CDS.  Ubik examines the machines listed in the appropriate RPC group to
determine how many sites constitute a majority and where to send votes in the
event of an election.
</Para>
<Para><IndexTerm Id="DFSAGR.ISSU.indx.132">
<Primary>Remote Procedure Call (RPC)</Primary>
<Secondary>interaction with Ubik</Secondary>
</IndexTerm>The names of the RPC bindings of all Fileset Database machines must be listed
in the RPC group in CDS at <Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs</Filename>, the junction to the
DFS filespace.  Likewise, the names of the RPC bindings of all Backup Database
machines must be listed in the RPC group in CDS at
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/subsys/dce/dfs/bak</Literal>.  To view the members of either
of these RPC server groups, use the <Command>dcecp rpcgroup list</Command> command.
</Para>
<Para>The name of a machine's RPC binding is of the form
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/hosts/</Literal><Symbol Role="Variable">hostname</Symbol><Literal>/self</Literal>. 
For example, in the cell named <Literal>abc.com</Literal>, the names of
the RPC bindings of all Fileset Database machines are listed in
<Literal>/.../abc.com/fs</Literal> in the form
<Literal>/.../abc.com/hosts/</Literal><Symbol Role="Variable">hostname</Symbol><Literal>/self</Literal>.
</Para>
</ListItem>
</ItemizedList>
<Note>
<IndexTerm Id="DFSAGR.ISSU.indx.133">
<Primary>directories</Primary>
<Secondary>well known names (DFS)</Secondary>
</IndexTerm>
<Para>In a server machine's DFS server principal or the name of its RPC binding, the
element that follows the <Symbol Role="Variable">cellname</Symbol> component is not considered to be well
known; for example, <Literal>hosts</Literal> could be <Literal>dfs-hosts</Literal>.  However, the string
used for the element must be applied consistently to all such names in a cell.
</Para>
<Para>In addition, the names <Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs</Filename> and
<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/subsys/dce/dfs/bak</Literal> in CDS are not considered
to be well known; either can be changed during installation and configuration
of a cell.  Conversely, the names of the <Literal>subsys/dce/dfs-fs-servers</Literal> and
<Literal>subsys/dce/dfs-bak-servers</Literal> groups are well known and cannot be changed.
</Para>
</Note>
</Sect2>
<Sect2 Id="DFSAGR.ISSU.div.39">
<Title>Configuring Database Server Machines for Ubik</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.134">
<Primary>Ubik</Primary>
<Secondary>configuring database server machines</Secondary>
</IndexTerm>
<IndexTerm Id="DFSAGR.ISSU.indx.135">
<Primary>DFS servers</Primary>
<Secondary>configuring for Ubik</Secondary>
</IndexTerm>
<Para>A cell's initial database server machines are configured when DFS is installed
and configured in the cell.  If it becomes necessary to add or remove database
server machines after initial cell configuration, perform the steps in Sections
2.5.3.1 and 2.5.3.2 to properly configure information for Ubik. (You may be
able to use the DCE installation and configuration program to modify the
database servers configured in your cell.  See your vendor's installation
and configuration documentation for more information.)
</Para>
<Para><?sml-need 8>When the Cache Manager on a DFS client machine needs information from the
FLDB in a cell, the <Command>dfsbind</Command> process on the machine provides it with
information about the names and network addresses of the Fileset Database
machines for the cell.  The information is valid for a limited amount of time,
24 hours by default, after which the Cache Manager requests refreshed
information from <Command>dfsbind</Command>; the Cache Manager also needs to
refresh the information when it is restarted.
</Para>
<Para>The <Command>fxd</Command> process on a File Server
machine passes the same information about Fileset Database machines for the
local cell to the File Exporter on its machine, but only when it is restarted
(generally when the machine is rebooted).
</Para>
<Para>It is seldom necessary to restart client or server machines if you reconfigure
a cell's Fileset Database machines.  As long as at least one Fileset Database
machine remains the same after reconfiguration, all machines can continue to
access the FLDB via that machine.  Eventually, all machines recognize the
current set of Fileset Database machines as a result of routine machine
administration and maintenance.  It is never necessary to restart client or
server machines if you reconfigure a cell's Backup Database machines.
</Para>
<Para>Sections 2.5.3.1 and 2.5.3.2 describe the steps required to add or remove a
database server machine after initial cell configuration.  Recall that each
Fileset Database machine must run the FL Server (<Command>flserver</Command> process), and
each Backup Database machine must run the Backup Server (<Command>bakserver</Command>
process).  These processes should be controlled by the Basic OverSeer (BOS)
Server (<Command>bosserver</Command> process) on their machines, as recommended; if they
are, you can use the appropriate <Command>bos</Command> commands to manipulate them.
</Para>
<Para>Also recall that each FL Server must use the same
<Filename>admin.fl</Filename> list and that each Backup Server must use the
same <Filename>admin.bak</Filename> list.  In addition, the abbreviated DFS
server principal of each Fileset Database machine must be included in the
<Filename>admin.fl</Filename> list, and the abbreviated DFS server principal
of each Backup Database machine must be included in the
<Filename>admin.bak</Filename> list.  A DFS server principal can be added
directly to a list, or it can be present as a member of a group included in
the list (for example, the group <Literal>subsys/dce/dfs-fs-servers</Literal>
can be included in the <Filename>admin.fl</Filename> list).
</Para>
<Para><?sml-need 10>Inclusion in the appropriate administrative list allows the database server
process at the synchronization site to distribute changes to the database
server processes at the secondary sites.  The Update Server should be used to
propagate these administrative lists from the System Control machine to their
respective database server machines.
</Para>
<Para>You can use the <Literal>udebug</Literal> command to obtain status information on Ubik
database servers.  The command is useful for diagnosing problems associated
with Ubik. (See Part 2 of this guide and reference for complete information
about the <Literal>udebug</Literal> command and its options.  Refer to the sections at the
beginning of this chapter for more information about the processes that must
run on either type of database server machine and the administrative lists
used to specify who can control them; see Chapter 5 for more information about
<Command>bos</Command> commands.)
</Para>
<Sect3 Id="DFSAGR.ISSU.div.40">
<Title>Adding a Database Server Machine</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.136">
<Primary>DFS servers</Primary>
<Secondary>adding</Secondary>
</IndexTerm>
<Para>To add a database server machine, do the following:
</Para>
<OrderedList>
<ListItem>
<Para><Replaceable>If you intend to configure the machine as a Fileset Database machine</Replaceable> and
the machine does not currently have a server entry in the FLDB, use the <Command>fts
crserverentry</Command> command to create a server entry in the FLDB for the
abbreviated DFS server principal of the machine.  The machine already has a
server entry in the FLDB if it is configured as a File Server machine. (See
Chapter 6 for more information about using the <Command>fts crserverentry</Command> command
to create server entries.)
</Para>
</ListItem>
<ListItem>
<Para>Use the <Command>dcecp group add</Command> command to add the abbreviated DFS server
principal of the new database server machine to the appropriate security group
(<Literal>subsys/dce/dfs-fs-servers</Literal> or <Literal>subsys/dce/dfs-bak-servers</Literal>) in the
Registry Database.
</Para>
</ListItem>
<ListItem>
<Para>Use the <Command>dcecp rpcgroup add</Command> command to add the name of the RPC binding
of the new database server machine to the appropriate RPC server group
(<Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Filename>/fs</Filename> or <Filename>/.../</Filename><Symbol Role="Variable">cellname</Symbol><Literal>/subsys/dce/dfs/bak</Literal>)
in CDS.
<?sml-break><?sml-need 12></Para>
</ListItem>
<ListItem>
<Para>Use the <Command>bos addadmin</Command> command to add the abbreviated DFS server principal
of the new database server machine to the appropriate administrative list
(<Filename>admin.fl</Filename> or <Filename>admin.bak</Filename>).  Doing so
allows the synchronization site to propagate changes to the secondary sites. 
These administrative lists are usually updated on the cell's System Control
machine, which then distributes the updated lists via the Update Server.
</Para>
<Para>Alternatively, you can use the <Command>dcecp group add</Command> command to add the
abbreviated DFS server principal to a security group included in the list.
Note that, if a group such as <Literal>subsys/dce/dfs-fs-servers</Literal> is included in
the administrative list, the DFS server principal is already present in the
list as a member of that group.
</Para>
</ListItem>
<ListItem>
<Para>Copy the appropriate administrative list (<Filename>admin.fl</Filename> or <Filename>admin.bak</Filename>)
to the <Symbol Role="Variable">dcelocal</Symbol><Filename>/var/dfs</Filename> directory on the new database server machine.
These administrative lists are typically propagated from the cell's System
Control machine via the Update Server.  Modify the Update Server as necessary
if the list is propagated from the cell's System Control machine.
</Para>
</ListItem>
<ListItem>
<Para>Stop and restart the appropriate database server process
(<Command>flserver</Command> or <Command>bakserver</Command>) on each database
server machine of that type.  Restarting the existing database server processes
causes them to read the updated RPC server group, which ensures that each Ubik
coordinator agrees on the number and identities of the other database server
machines of its type.  This agreement is vital to Ubik's use of a quorum of
database server machines to maintain database consistency.
</Para>
</ListItem>
<ListItem>
<Para>Start the appropriate database server process (<Command>flserver</Command> or
<Command>bakserver</Command>) on the new database server machine.
</Para>
</ListItem>
</OrderedList>
</Sect3>
<Sect3 Id="DFSAGR.ISSU.div.41">
<Title>Removing a Database Server Machine</Title>
<IndexTerm Id="DFSAGR.ISSU.indx.137">
<Primary>DFS servers</Primary>
<Secondary>removing</Secondary>
</IndexTerm>
<Para>To remove a database server machine, do the following:
</Para>
<OrderedList>
<ListItem>
<Para>Stop the appropriate database server process (<Command>flserver</Command> or
<Command>bakserver</Command>) on the database server machine to be removed.
</Para>
</ListItem>
<ListItem>
<Para>Use the <Command>dcecp group remove</Command> command to remove the abbreviated DFS server
principal of the database server machine to be removed from the appropriate
security group.
</Para>
</ListItem>
<ListItem>
<Para>Use the <Command>dcecp rpcgroup remove</Command> command to remove the reference to the RPC
binding of the database server machine to be removed from the appropriate RPC
server group.
</Para>
</ListItem>
<ListItem>
<Para>Use the <Command>dcecp rpcentry show</Command> command on each database server machine of
the appropriate type to update the entry for the appropriate RPC server group
from CDS.  The command forces CDS to update information that it caches from the
entry for the group in the namespace.
</Para>
</ListItem>
<ListItem>
<Para>Stop and restart the appropriate database server process
(<Command>flserver</Command> or <Command>bakserver</Command>) on each database
server machine of that type.  Restarting the existing database server processes
causes them to read the updated RPC server group, which ensures that each Ubik
coordinator agrees on the number and identities of the other database server
machines of its type.  This agreement is vital to Ubik's use of a quorum of
database server machines to maintain database consistency.
</Para>
</ListItem>
<ListItem>
<Para>Use the <Command>bos rmadmin</Command> command to remove the abbreviated DFS server
principal of the database server machine to be removed from the appropriate
administrative list (<Filename>admin.fl</Filename> or <Filename>admin.bak</Filename>).  These administrative
lists are usually updated on the cell's System Control machine, which then
distributes the updated lists via the Update Server.
</Para>
<Para>If you chose instead to add the abbreviated DFS server principal to a security
group included in the list, you can use the <Command>dcecp group remove</Command> command
to remove the server principal from the group.  Note that if the DFS server
principal was present in the administrative list as a member of a group such
as <Literal>subsys/dce/dfs-fs-servers</Literal>, the server principal is already removed
from the list.
</Para>
</ListItem>
<ListItem>
<Para>Remove the appropriate administrative list (<Filename>admin.fl</Filename> or <Filename>admin.bak</Filename>)
from the <Symbol Role="Variable">dcelocal</Symbol><Filename>/var/dfs</Filename> directory on the database server machine to
be removed.  Modify the Update Server as necessary if the list is propagated
from the cell's System Control machine.
</Para>
</ListItem>
</OrderedList>
<IndexTerm Id="DFSAGR.ISSU.indx.138" SpanEnd="DFSAGR.ISSU.indx.134">
<IndexTerm Id="DFSAGR.ISSU.indx.139" SpanEnd="DFSAGR.ISSU.indx.135">
<IndexTerm Id="DFSAGR.ISSU.indx.140" SpanEnd="DFSAGR.ISSU.indx.123">
<IndexTerm Id="DFSAGR.ISSU.indx.141" SpanEnd="DFSAGR.ISSU.indx.1">
<Para>
</Para>
</Sect3>
</Sect2>
</Sect1>
</Chapter>
<!--+ 10/19/96 18:43:16
    | tagMorph:  $Id: issues.sgm,v 1.1.2.8 1996/12/15 23:02:05 wardr Exp $
    | tagMorph library:  $Id: issues.sgm,v 1.1.2.8 1996/12/15 23:02:05 wardr Exp $
    | sml-to-docbook:  1.23
    +-->
