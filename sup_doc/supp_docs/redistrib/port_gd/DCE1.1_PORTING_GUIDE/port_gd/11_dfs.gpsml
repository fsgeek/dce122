...\" @OSF_COPYRIGHT@
...\" COPYRIGHT NOTICE
...\" Copyright (c) 1990, 1991, 1992, 1993, 1994 Open Software Foundation, Inc.
...\" ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
...\" the full copyright text.
...\" 
...\" HISTORY
...\" $Log: 11_dfs.gpsml,v $
...\" Revision 1.1.2.15  1994/10/30  22:47:21  weir
...\" 	Last updates
...\" 	[1994/10/30  22:46:12  weir]
...\"
...\" Revision 1.1.2.14  1994/10/30  16:25:21  weir
...\" 	Updates
...\" 	[1994/10/30  16:23:56  weir]
...\" 
...\" Revision 1.1.2.13  1994/10/29  23:25:41  weir
...\" 	More Updates
...\" 	[1994/10/29  23:24:20  weir]
...\" 
...\" Revision 1.1.2.12  1994/10/28  20:49:54  weir
...\" 	DCE 1.1 updates
...\" 	[1994/10/28  20:48:33  weir]
...\" 
...\" Revision 1.1.2.11  1994/10/26  20:40:35  weir
...\" 	DCE 1.1 Updates
...\" 	[1994/10/26  20:39:06  weir]
...\" 
...\" Revision 1.1.2.10  1994/10/14  18:38:41  weir
...\" 	Minor edit changes
...\" 	[1994/10/14  18:37:29  weir]
...\" 
...\" Revision 1.1.2.9  1994/06/20  20:28:34  weir
...\" 	Beta Update
...\" 	[1994/06/20  20:27:09  weir]
...\" 
...\" Revision 1.1.2.8  1994/06/19  20:53:46  weir
...\" 	Beta Update
...\" 	[1994/06/19  20:52:34  weir]
...\" 
...\" Revision 1.1.2.7  1994/06/17  13:50:11  weir
...\" 	Beta Update
...\" 	[1994/06/17  13:48:37  weir]
...\" 
...\" Revision 1.1.2.6  1994/06/13  19:25:25  devobj
...\" 	cr10872 - fix copyright
...\" 	[1994/06/13  19:24:27  devobj]
...\" 
...\" Revision 1.1.2.5  1994/06/12  17:31:51  weir
...\" 	No change-- for copyright insertion
...\" 	[1994/06/12  17:30:20  weir]
...\" 
...\" Revision 1.1.2.4  1994/06/08  18:47:11  weir
...\" 	Beta Update
...\" 	[1994/06/08  18:46:03  weir]
...\" 
...\" Revision 1.1.2.3  1994/06/02  21:13:06  weir
...\" 	Beta Updates
...\" 	[1994/06/02  21:11:28  weir]
...\" 
...\" Revision 1.1.2.2  1994/05/03  20:13:57  weir
...\" 	Reorganization (part 2)
...\" 	[1994/05/03  20:13:26  weir]
...\" 
...\" Revision 1.1.2.1  1994/05/03  19:00:09  weir
...\" 	Reorganization
...\" 	[1994/05/03  18:58:56  weir]
...\" 
...\" $EndLog$
...\" 
...\" 
...\" 
...\" 
...\" 
...\"	
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 1 "DCE Distributed File Service"
...\" ----------------------------------------------------------------------
...\" 
.P
The DCE Distributed File Service (DFS) provides data sharing services
for use within the DCE environment by extending the local file system
model to remote systems. It provides the ability to store and access data
at remote locations and utilizes the client/server model common to other
distributed file systems.
...\" 
...\" .zA "def,8422,R1.0.3,alphabetized symbol lists"
...\" ----------------------------------------------------------------------
...\" ----------------------------------------------------------------------
...\" ----------------------------------------------------------------------
.H 2 "Overview"
...\" ----------------------------------------------------------------------
DFS consists of the following components:
...\" 
.BL
.LI
DCE Local File System (LFS), which can store the file system data on
the disk.
...\" 
.nS "Note"
This component, alone among the DFS components, is optional.
You can retain your existing file system instead of DCE LFS and use
DFS to export that file system. However, there are advantages to bringing
up LFS in conjunction with DFS. See ``Porting DCE LFS and DFS Separately'' later
in this chapter.
.nE
...\" 
.LI
The File Exporter, which exports data using Remote Procedure Call (RPC).
.LI
The Token Manager, installed on DFS servers, which synchronizes access
to exported file systems on DFS servers.
.LI
The Cache Manager, installed on DFS clients, which retrieves and
stores data from the File Exporter.
.LI
The Token Cache Manager, installed on DFS clients, maintains liaisons
with the Token Manager, and controls server access to exported local
filesystems.
.LI
Fileset services, which handle administrative file system functions.
These include the following servers:
...\" 
.AL
.LI
the Fileset Location Server, which supplies network locations for
filesets.
.LI
the Fileset Server, which provides access to entire filesets for
administrative functions, such as moving and backing them up.
.LI
the Replication Server, which provides fileset replication on
different machines (for greater availability).
.LE
...\" 
.LI
The Basic Overseer (\*Vbos\*O) service, which monitors other server
processes and facilitates system administration tasks.
.LI
\*VScout\*O, which gathers file server statistics.
.LI
\*VBackup\*O, which provides a mechanism for backing up data stored on
the file server.
.LE
...\" 
.P
Command interfaces are provided for these server processes and tools.
...\" 
...\" 
.P
DFS lets users access a remote file by its location-independent DCE
pathname. It then finds the file, just as if it existed locally.
Users do not have to know the physical location of files. The \*VCache
Manager\*O, which runs on client machines, translates file system calls
into references to the client machine's file system cache.
If necessary, it then executes RPCs to the file server machine
containing the data.
.P
The local file system (LFS) on the DFS server stores the master copy
of filesystem data.
The \*VFile Exporter\*O can export any Virtual File System (VFS)
resident on the server machine.
DFS uses a token-based cache synchronization mechanism to maintain
cache consistency and provide single-site semantics.
.P
DCE LFS is a log-based file system that supports filesets, access
control lists, and extended fileset features.
These include copy-on-write clones, quotas, and multiple filesets per
partition.
...\" It can be used as a local file system in which filesets can be
...\" mounted.
.P
The DCE LFS code is designed to run in the server's kernel.
It is based on a standard UNIX disk partition, using the facilities of
the kernel device driver.
DCE LFS operations are accessed through the system call layer, which
calls the VFS switch.
...\"	
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Considerations and Dependencies"
...\" ----------------------------------------------------------------------
...\" 
.P
DFS requires and depends on the following components:
...\" 
.BL
.LI
Threads, for intra-process concurrency
.LI
RPC, for communications
.LI
KRPC, for kernel communications.
...\" 
.BL
.LI
.zA "enh, 10306, R1.1, remove diskless documentation"
Since DFS the only DCE component that uses KRPC, you have no reason to
port KRPC if you do not also port DFS.
.zZ "enh, 10306, R1.1, remove diskless documentation"
.LI
If you port DFS separately from other DCE components, you can delay
porting KRPC until the time you begin porting DFS.
.LE
...\" 
.LI
CDS, for directory services.
.LI
Security, for secure communications, authentication, and authorization
services. DFS also uses the
.DS
    \*Vdce-root-dir\*L/dce/src/security/krb5/comerr\*O
.DE
.P
library, built under the security
source directory, for the \*Lcompile_et\*O command, which formats
error codes for the message catalog files.
...\" 
.nS "Note"
This command is documented under ``Security Test Command'' at the end of
Chapter 9 of this guide.
.nE
...\" 
.LI
DTS, for clock synchronization.
.LE
...\" 
.P
DFS must be incorporated into the kernel of all DFS file server and
client machines. On traditional UNIX implementations, DFS libraries must
be linked into the kernel. Some UNIX systems, such as AIX on the IBM RISC
System/6000 (one of the reference platforms), support extending the kernel
dynamically. Others require that the kernel be rebuilt and booted with DFS
libraries linked in. Note that a DFS kernel incorporating KRPC can be more
than 1 megabyte larger than a non-DFS kernel, so memory availability can
become an issue.
.P
Before porting DFS to your operating system, you should understand the
procedures used to modify the kernel. Also, note that on most systems,
kernel-based DFS daemons cannot be killed or restarted without rebooting
the system.
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Porting DFS and LFS Separately"
...\" ----------------------------------------------------------------------
...\" 
.P
DFS can export any Virtual File System resident on the server machine
by extending the machine's set of VFS operations to support the extended
(VFS+) interface, which DFS runs. Constructing the VFS+ interface is a
significant part of the work involved in porting DFS to a new platform.
.P
Therefore, DFS can be ported independently of DCE LFS: you can use
your existing local file system on the servers, and use DFS to export
it to clients. However, the operations available for a non-DCE LFS filesystem
are typically more restricted than a DFS combined with DCE LFS.
In particular, DCE LFS has the following features which facilitate its
joint operation with DFS and the rest of DCE:
...\" 
.BL
.LI
DCE LFS implements POSIX Access Control Lists (ACLs), which match those for
DFS. Consequently, there is no need to translate permissions or ACLs between
LFS and DFS, with possible degradations of speed and capabilities.
...\" .LI
...\" In addition, DCE LFS identifies both the owner and group attributes of
...\" files with a cell qualification, using UUIDs (Universal Unique Identifiers)
...\" instead of standard UNIX UIDs. This feature permits intercell ownership of
...\" files, and lets a trusted principal (from another cell) own DCE LFS files,
...\" a feature not likely to be shared by native local file systems on the target
...\" platform.
.LI
DCE LFS supports filesets and permits fileset replication and backup.
These features can enhance performance, and are also unlikely to exist
on your target platform.
.LE
...\" 
.P
In addition, DCE LFS can be ported before the other DCE components have
been built, whereas DFS requires essentially a full suite of DCE functions
before you can start building it. Consequently, you can port DCE LFS in
parallel with the earlier parts of your port of DCE, and gain the advantages
of using DCE LFS in conjunction with DFS without paying the full time cost of
serializing the two porting efforts.
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS File Locations"
...\" ----------------------------------------------------------------------
...\" 
.P
The following tables list DFS subcomponents, including sources and executables,
as provided in the \*Ltar\*O tape on which DFS (and the rest of DCE) are packaged.
...\" 
.nS "note"
A subcomponent may consist of multiple source files.
.nE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS Library Files"
...\" ----------------------------------------------------------------------
...\" 
.P
Some libraries must be built twice, once in user space and once in kernel
space. These differences exist because symbol names and data structures can
have different bindings in their user and kernel versions. The kernel versions
of these libraries are named with a \*L.klib\*O suffix. These directories are
used to build with kernel flags set. Typically, each \*L.klib\*O directory only
contains a \*LMakefile\*O: the actual library sources are in the directory where
the user-level library is built.
.P
All paths listed below are relative from:
.DS
    \*Vdce-root-dir\*L/dce/src/file\*O
.DE
.P
The path indicates the directory in which the \*LMakefile\*O builds the
component.
.P
None of the \*Vlib...\*O files in this table are exported.
...\" 
.ne 2i
.ad l
.TB "Locations of DFS Subcomponent Library Files"
.TS H
expand center tab (@) box;
lB | lB | lB
lB | lB | lB
lB | l | lB.
DFS@@Location of
Subcomponent@Function\h'2.5i'@Source Files
_
.TH
_
libafs4int.a@DFS file server and client RPC stubs.@fsint
_
libafsrpcd.a@Definition of DFS server/client interface.@fsint
_
libafssys.a@AFS system call interface.@sys
_
libafsutil.a@General utility functions.@util
_
libaggr.a@Aggregate ops interface.@xaggr.klib
...\"	[Removed xaggr - per garyf, 7/7/92]
_
libaixexport.a@AIX support for symbol exporter.@export/RIOS
_
libanode.a@DCE LFS anode module.@episode/anode
_
libasync.a@DCE LFS asynchronous I/O module.@episode/async
_
libbos.a@The \*Lbosserver\*O client interface.@bosserver
_
libbubasics.a@Backup system interfaces.@bubasics
_
libbudb.a@Backup database operations.@bakserver
_
libbutm.a@Backup system file operations.@butm
_
libbxdb.a@Backup command interfaces.@bak
_
libcm.a@Cache manager module.@cm
_
libcmd.a@General parsing routines for commands.@tools/cmd
_
libcmdAux.a@Extra parsing routines for commands.@tools/cmd
...\"	_
...\"	libcom_err.a@Error formatting routines for \*Lcompile_et\*O.@tools/comerr
...\"	removed - gone
_
libcommondata.a@Interfaces for managing fundamental DFS data types.@config, config.klib
_
libdacl.a@DFS acl support.@security/dacl
_
libdacllfs.a@DCE LFS acl support.@security/dacl_lfs
_
libdauth.a@DFS authentication support.@security/dfsauth
_
libdfskutil.a@Kernel utilities.@kutils
_
libdir.a@DCE LFS dir module.@episode/dir
_
libeacl.a@DCE LFS ACL module.@episode/eacl
_
libedsk.a@DCE LFS device handling module.@episode/anode
_
libefsops.a@DCE LFS vfs/vnode module.@episode/vnops
...\"	_
...\"	libepitest.a@DCE LFS Testing support.@episode/test
_
libfldb.a@The \*Lflserver\*O RPC interface.@flserver, flserver.klib
...\" _
...\" libfp.a@Free pool manager.@fp, fp.klib
_
libfsprobe.a@Fileserver probe library.@fsprobe
_
libftserver.a@Fileset server RPC interface.@ftserver
_
libftutil.a@API for fileset operations.@ftutil
_
libgtx.a@Gator toolkit library.@gtx
_
libgwauth.a@DFS/NFS Gateway kernel authentication interface@gateway/libgwauth
_
libhost.a@Host module, part of VFS+ token management support.@fshost
_
libkafs4clt.a@Kernel version of DFS client RPC stubs.@fsint.klib
_
libkafs4srv.a@Kernel versions of DFS server RPC stubs.@fsint.klib
_
libkanode.a@Kernel DCE LFS anode module functions.@episode/anode.klib
_
libkasync.a@Kernel DCE LFS asynchronous i/o module functions.@episode/async.klib
_
libkcommondata.a@Kernel version interfaces for fundamental DFS data types.@config.klib
_
libkdacl.a@Kernel versions of DCE LFS ACL module.@security/dacl.klib
_
libkdacllfs.a@More kernel support for DCE LFS ACL functions.@security/dacl_lfs.klib
_
libkdir.a@Kernel versions of DCE LFS dir module.@episode/dir.klib
_
libkdfskutil.a@Kernel version of DFS utilities.@kutils.klib
_
libkdfsncs.a@Kernel RPC utilities for DFS use.@kutils.klib
_
libkeacl.a@Kernel version of DCE LFS ACL module.@episode/eacl.klib
_
libkefsops.a@Kernel version of DCE LFS vfs vnode operations.@episode/vnops.klib
_
...\"	libkepitest.a@Kernel DCE LFS development test support.@episode/test.klib
...\"	_
libkfldb.a@Kernel version of \*Lflserver\*O interface.@flserver.klib
...\" _
...\" libkfp.a@Kernel version of free pool manager.@fp.klib
_
libklogbuf.a@Kernel version of DCE LFS buffer log module.@episode/logbuf.klib
_
libkosi.a@Kernel version of OSI module.@osi.klib
_
libkrepcli.a@T{
Kernel version of RPC stubs and auxiliary files
for the \*Lrepserver\*O interface definition.
T}@fsint.klib
_
libktkc.a@Kernel version of UFS glue token vnode cache.@tkc
_
libktkm.a@Kernel token manager module.@tkm.klib
_
libktools.a@Kernel version DCE LFS tools functions.@episode/tools.klib
_
libkxcred.a@Kernel version of extended credential package.@xcred.klib
_
liblogbuf.a@User version of DCE LFS buffer log module.@episode/logbuf
...\"	... and NOT logbuf.klib (per garyf - 7/10)
_
libncompat.a@Kernel RPC compatibility support.@ncscompat
_
libnubik.a@Server and client \*Lubik\*O utilities.@ncsubik
_
libosi.a@User version of OSI module.@osi
_
libpx.a@Protocol exporter module.@px
_
librep.a@RPC stub routines for \*Lrepserver\*O.@fsint
_
...\"	libtknrpcd.a@Token manager RPC interface definition.@fsint
...\"	_
libtkset.a@Token set module (part of File Exporter).@tkset
_
libtools.a@DCE LFS tools.@episode/tools
_
libufsops.a@UFS fileset and aggregate operations.@ufsops
_
libvolc.a@Support for \*Lfts\*O and \*Lbackup\*O server commands.@userInt/fts
_
libvolreg.a@Volume registry utilities and routines.@volreg
_
libvolume.a@Fileset operations interface.@xvolume.klib
...\"	not xvolume (per garyf - 7/10)
_
libxcred.a@The external credential module.@xcred
_
libxvnode.a@VFS+ vnode operations and glue.@xvnode
.TE
...\" 
...\" 
...\" 
...\" 
.TB "Locations of DFS Support Library Files in Other Components"
.TS H
expand center tab (@) box;
lB | lB | lB
lB | lB | lB
lB | l | lB.
DFS@@Location of
Library@Function\h'2.5i'@Source Files
_
.TH
_
libksec.a@Kernel sec routines for DFS delegation@security/kutils
_
libknck.a@KRPC@rpc/kruntime
_
libkidl.a@kernel idl runtime@rpc/kidl
_
libauthelper.a@User space security routines for dfsbind@security/authelper
.TE
...\" 
...\" 
...\" 
...\" 
...\"	
...\" ----------------------------------------------------------------------
.H 3 "AIX Kernel Extension Files"
...\" ----------------------------------------------------------------------
...\" 
.P
The following files support DFS-related extensions to the AIX kernel.
All source file pathnames are relative from the
.DS
    \*Vdce-root-dir\*L/dce/src/file\*O
.DE
.P
directory. These files are exported to the
.DS
    \*Vdce-root-dir\*L/dce/install/\*Vmachine_type\*L/opt/dce1.1/ext\*O
.DE
.P
directory.
...\" 
.ne 2i
.TB "Locations of AIX Kernel Extension Files"
.ad l
.TS H
expand center tab (@) box;
lB | lB | lB
lB | lB | lB
lB | l | lB.
DFS@@Location of@
Subcomponent@Function\h'2.5i'@Source Files
_
.TH
_
cfglfs@Loads \*Ldcelfs.ext\*O@episode/libefs/RIOS
_
dcelfs.ext@T{
AIX DCE LFS kernel extension.
T}@episode/libefs/RIOS
_
dfscmfx.ext@T{
AIX Cache Manager/File Exporter kernel extension.
T}@libafs/RIOS
_
dfscore.ext@T{
AIX DFS core kernel extension, contains functions used by both
\*Ldcelfs.ext\*O and \*Ldcecmfx.ext\*O.
T}@libafs/RIOS
_
sdcelfs.ext@T{
AIX DCE LFS without separate core kernel extension.
T}@episode/libefs/RIOS
_
export.ext@T{
AIX symbol exporter kernel extension.
T}@export/RIOS
_
cfgdfs@T{
Loads the \*Ldfscore.ext\*O and \*Ldfscmfx.ext\*O AIX DFS kernel
extensions.
T}@libafs/RIOS
_
cfgexport@T{
Loads AIX exporter kernel extension (export).
T}@export/RIOS
.TE
...\" 
.ad b
...\" 
...\" 
...\" 
.TB "Locations of HPUX Kernel Extension Files"
.ad l
.TS H
expand center tab (@) box;
lB | lB | lB
lB | lB | lB
lB | l | lB.
DFS@@Location of@
Subcomponent@Function\h'2.5i'@Source Files
_
.TH
_
dfs_core.ext@T{
DFS libraries shared by client and server
T}@file/libafs
_
dfs_client.ext@T{
DFS client libraries
T}@file/libafs
_
dfs_server.ext@T{
DFS server libraries
T}@file/libafs
_
dfs_episode.ext@T{
DFS episode libraries
T}@file/libafs
_
dce_krpc.ext@T{
KRPC libraries
T}@rpc/kruntime
.TE
...\" 
...\" 
.ad b
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
...\" .H 3 "OSF/1 Kernel Files"
...\" ----------------------------------------------------------------------
...\" 
...\" .P
...\" The three following files provide kernels with DFS support on the OSF/1
...\" reference platform. Unlike AIX, this kernel is not paged and is statically
...\" configured. These are the actual \*Lvmunix\*Oes the system runs, independently
...\" of whether DFS is installed or operational. The source file pathnames are
...\" relative from the
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file\*O
...\" .DE
...\" .P
...\" directory.
...\" 
...\" .ne 2i
...\" .TB "Locations of OSF/1 Kernel Files"
...\" .ad l
...\" .TS H
...\" expand center tab (@) box;
...\" lB | lB | lB
...\" lB | lB | lB
...\" lB | l | lB.
...\" DFS@@Location of@
...\" Subcomponent@Function\h'2.5i'@Source Files
...\" _
...\" .TH
...\" _
...\" dfsvmunix@OSF/1 kernel with DFS support@libafs/OSF1
...\" _
...\" efsvmunix@OSF/1 kernel with DFS and DCE LFS support@libafs/OSF1
...\" _
...\" sefsvmunix@OSF/1 kernel with DCE LFS support@episode/libafs/OSF1
...\" .TE
...\" 
...\" .ad b
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS-Related Command Files"
...\" ----------------------------------------------------------------------
...\" 
.P
The following files support DFS commands and the various services (\*Lbos\*O,
\*Lbackup\*O, \*LScout\*O, ...) required to operate DFS. Like all other files
relating to DFS, the sources reside under the
.DS
    \*Vdce-root-dir\*L/dce/src/file\*O
.DE
.P
directory.
.P
All executables except \*Vtdump\*O (which is not exported) are
exported to the
.DS
    \*Vdce-root-dir\*L/dce/install/\*Vmachine_name\*L/opt/dce1.1/bin\*O
.DE
.P
directory.
...\" 
.br
.ne 2i
.TB "Locations of DFS Command and Service Files"
.ad l
.TS H
expand center tab (@) box;
lB | lB | lB
lB | lB | lB
lB | l | lB.
DFS@@Location of
Subcomponent@Function\h'2.5i'@Source Files
_
.TH
_
bak@T{
The \*Lbak\*O command.
T}@bak
_
bakserver@T{
Backup Server.
T}@bakserver
_
bos@T{
The \*Lbos\*O command.
T}@userInt/bos
_
bosserver@T{
Basic Overseer Server.
T}@bosserver
_
butc@T{
Backup Tape Coordinator.
T}@butc
_
cm@T{
The \*Lcm\*O command.
T}@userInt/cm
_
dfsbind@T{
Cache Manager's user space helper.
T}@dfsbind
_
dfsd@T{
Cache Manager startup daemon.
T}@afsd
_
dfsexport@T{
Makes aggregates available for export by DFS file servers
T}@xaggr
_
dfsgw@T{
DFS/NFS gateway admin command
T}@gateway/dfsgw
_
dfsgwd@T{
DFS/NFS gateway daemon
T}@gateway/dfsgwd
_
dfs_login@T{
DFS/NFS gateway login command
T}@gateway/dfs_login
_
dfs_logout@T{
DFS/NFS gateway logout command
T}@gateway/dfs_login
_
flserver@T{
Fileset Location Server.
T}@flserver
_
fms@T{
The \*Lfms\*O command.
T}@bakutil
_
fts@T{
The \*Lfts\*O command.
T}@userInt/fts@bin
_
ftserver@T{
Fileset Server.
T}@ftserver
_
fxd@T{
File Exporter startup daemon.
T}@pxd
_
newaggr@T{
Creates DCE LFS aggregates on partitions.
T}@episode/anode
_
repserver@T{
Replication Server.
T}@rep
_
salvage@DCE LFS Salvager program.@episode/salvage
_
scout@T{
Scout monitoring tool.
T}@scout
_
tdump@T{
Dumps tape blocks.  (\*VNot exported\*O).
T}@butc
_
udebug@T{
Tool for monitoring ubik.
T}@ncsubik
_
upclient@T{
Update Client.
T}@update
_
upserver@T{
Update Server.
T}@update
.TE
.ad b
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "LFS Files"
...\" ----------------------------------------------------------------------
...\" 
.P
The following files provide facilities for building and testing DCE LFS.
The source files are under the
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/utils\*O
.DE
.P
directory. The executable files are exported to the
.DS
    \*Vdce-root-dir\*L/dce/install/\*Vmachine_name\*L/opt/dce1.1/bin\*O
.DE
.P
directory.
...\" 
.br
.ne 2i
.TB "Locations of DCE LFS Files"
.ad l
.TS H
expand center tab (@) box;
lB | lB | lB
lB | lB | lB
lB | l | lB.
DFS@@Location of
Subcomponent@Function\h'2.5i'@Source Files
_
.TH
_
epidaemon@DCE LFS VM daemon@episode/utils
_
epimount@T{
Command for mounting DCE LFS filesets locally.
T}@episode/utils
_
epiinit@T{
DCE LFS kernel init program for non-AIX platforms.
T}@episode/utils
_
epiunmount@DCE LFS unmount program.@episode/utils
.TE
.ad b
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 2 "Porting DFS"
...\" ----------------------------------------------------------------------
...\" 
.P
To port DFS to a new platform, you must go through several general steps:
...\" 
.BL
.LI
Bring up Kernel space Remote Procedure Protocol (KRPC). For information on
doing so, refer to the procedures in Chapter 5 of this guide, on DCE RPC.
.LI
Build an interface from your (kernel) file system and the VFS+ file
system used by DCE.
.LI
Build the reverse interface, from VFS+ to your kernel file system.
This interface includes the VFS+ \*Lvnode\*O operations (in the
\*Lxvnode\*O subdirectory) and the fileset and aggregate operations,
in the \*Lufsops\*O subdirectory.
.LI
Build the glue code for token synchronization between local file
system accesses and exported accesses.
.LI
Build the Operating System Independent (OSI) module, which
encapsulates DFS within the host system's user and kernel
environments.
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Reference Platforms"
...\" ----------------------------------------------------------------------
...\" 
.P
DCE contains DFS code ported to the reference platforms listed in
Chapter 1 of this guide. If you are porting to a different platform, you
need to consider the information in the following sections.
.P
When you undertake to port DFS, you are strongly urged to obtain a
reference platform running DCE (including DFS), so you can test your
code's functionality against it. In addition, you can use the platform as
a demonstration system, to identify correct operation of DFS functions,
where the precise operation, and interaction with other DCE components,
is not necessarily obvious from the code. This suggestion applies across
DCE, of course, but is most important for porting DFS, since DFS operates
in both kernel and user space.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Macro Definitions"
...\" ----------------------------------------------------------------------
...\" 
.iX "Macro"
...\" 
.P
Most platform dependencies within DFS are contained in conditionally
compiled macro sections. The \*L#define\*O compiler flags which determine
whether these macros are compiled, and with what bindings, are contained in
.DS
    \*L/usr/include/dcedfs/param.h\*O
.DE
.P
which is generated in the
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/\*VTARGET_MACHINE\*O
.DE
.P
directory. There is a separate directory for each target machine.
Virtually every subcomponent of DFS includes this header file.
.P
When porting DFS to a new environment, you must develop two kinds of
environment-specific information, namely operating system-specific and
(hardware) platform-specific. The distribution tape contains
\*L.../\*VTARGET_MACHINE\*O directories for the reference platforms and
possibly for other machines which are not supported reference platforms.
Each such directory contains a machine-specific \*Lparam.h\*O.
.P
When you port DFS to a new machine, create a new target-specific
subdirectory for your hardware and place your machine's \*Lparam.h\*O
in it. You can copy and modify the closest existing version, or write the
file yourself.
...\" 
.nS "Note"
The distribution tape contains additional subdirectories under:
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/\*O
.DE
.P
These directories contain compiler flags for machines and operating
systems other than the reference platforms. However, unlike the reference
ports, these definitions are not complete, nor have they necessarily been
tested. They are only intended as examples and commentaries.
.nE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Operating System Specific definitions"
...\" ----------------------------------------------------------------------
...\" 
.iX "#define"
.iX "Compiler" "constants"
...\" 
.P
As the source code is currently organized, the macros in
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/\*VTARGET_MACHINE\*L/param.h\*O
.DE
.P
contain conditional \*L#define\*Os to accommodate different operating
systems on that machine. You may want to create separate definition files,
possibly in separate subdirectories of
.DS
    \*Vdce-root-dir\*L/dce/src/file/config\*O
.DE
.P
for the machine and the operating system to which you are porting,
particularly if you are porting to a heterogeneous hardware
environment, such as a family of related machines.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Symbol Definitions for the Reference Ports"
...\" ----------------------------------------------------------------------
...\" 
.P
The following symbols are defined for the AIX port in the
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/RIOS/param.h\*O
.DE
.P
header file:
...\" 
.oS
    #ifdef AFS_AIX32_ENV
    #define SYS_NAME            "rs_aix32"  /* for @sys */
    #else
    #define SYS_NAME            "rs_aix31"
    #endif /* AFS_AIX32_ENV */
    #define     RIOS            1  /* POWERseries 6000 */
    #define     AFS_AIX_ENV     1  /* All AIX systems */
    #define     AFS_AIX31_ENV   1  /* AIX 3.1 specific */
    #ifdef notdef
    #define     AFS_AIX31_VM    1  /* Enable AIX3.1 VM support */
    #endif
...\" 
...\"     #define SYS_NAME                "rs_aix31"  /* for @sys */
...\"     #define RIOS                    1       /* POWERseries 6000 */
...\"     #define AFS_AIX_ENV             1       /* All AIX systems */
...\"     #define AFS_AIX31_ENV           1       /* AIX 3.1 specific */
...\"     #define AFS_AIX31_VM            1       /* Enable AIX3.1 VM support */
.oE
...\" 
.P
The AIX reference platform runs AIX 3.2. However, the AFS_AIX31_ENV symbol
must still be defined.
...\" 
.nS "Note"
In addition, there is a AFS_AIX32_ENV symbol which also must be
defined.
Currently, AFS_AIX32_ENV is defined as a compile flag in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/RIOS/machdep.mk\*O
.DE
.P
.nE
...\" 
.nS "Note"
The AFS_AIX32_ENV definition will be moved into:
.DS
    \*Vdce-root-dir\*L/dce/src/file/RIOS/param.h\*O
.DE
.P
Check that file if you do not find the definition in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/RIOS/machdep.mk\*O
.DE
.P
Check the \*VOSF DCE Release Notes\*O for possible further information
regarding this definition.
.nE
...\" 
...\" .P
...\" The following two symbols are defined for the OSF/1 port to the
...\" DECpc 450ST reference platform in:
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/config/AT386/param.h\*O
...\" .DE
...\" .P
...\" 
...\" .oS
...\"     #define SYS_NAME "at386_osf1"
...\"     #define AFS_OSF_ENV             1
...\"     #define AFS_OSF11_ENV
...\" .oE
...\" 
...\" .P
...\" If you are porting to a new OSF/1 platform, this is the only
...\" machine-dependent file which needs to be created or modified in DFS.
...\" However, you must also set the compilation options correctly in:
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/AT386/machdep.mk\*O
...\" .DE
...\" .P
...\" Note also the existence of the \*LAFS_OS11_ENV\*O, described in the section
...\" ``Preprocessor Variables'' below.
...\" 
.P
See also ``Platform-Specific Files'' in the section ``DFS vnode and VFS
Operations'', later in this chapter.
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Port-Sensitive Code Sections: AFS_DEFAULT_ENV"
...\" ----------------------------------------------------------------------
...\"
...\" 
.P
The machine-specific macros and code sections are spread over many
files in the DFS code. The \*LAFS_DEFAULT_ENV\*O symbol, defined in
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/\*VTARGET_MACHINE\*L/param.h\*O
.DE
.P
has been inserted in the source code in locations where you are likely
to need to insert or modify target-specific code. It identifies code which
is either absent or questionable for an unknown target platform.
In this case, ``questionable'' means the code may compile and seem to
operate correctly in isolation, but interacts with code that must
be added or rewritten, and is unlikely to work correctly with newly
constructed or ported code.
.P
If \*LAFS_DEFAULT_ENV\*O is defined, each instance of it outputs a specific
report onto the standard error when you compile the file. You can silence such
reporting by not defining this symbol, and can eliminate each instance of
\*LAFS_DEFAULT_ENV\*O as you replace it with your own code for that function.
.P
For example, the following lines appear in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/fshost/fshs_prutils.c
.DE
...\" 
...\" 
.in +0.5i
\*C#ifdef AFS_DEFAULT_ENV
.br
#error "fshs_getcred may need rework for short uid_t and gid_t"
.br
#endif\*O
.in -0.5i
...\" 
...\" 
.P
The placements of \*LAFS_DEFAULT_ENV\*O are not exhaustive, since it is not
known in advance where machine-specific or operating system specific
code for your port must be placed. However, it is designed to simplify the
process of identifying such points.
.P
.iX "Compiler" "ANSI"
\*LAFS_DEFAULT_ENV\*O generates an error message when it is compiled by an
ANSI C compiler. It will fail (and generate a compiler error) if you do not
use an ANSI-compatible C compiler to build DFS. Other portions of DFS, and
DCE in general, also will not work under such circumstances.
.P
As noted above, code marked with \*LAFS_DEFAULT_ENV\*O may seem to work,
meaning that it will compile and even operate correctly when run in
isolation. However, it can still be wrong for a given platform: it may not
provide necessary services for a working environment, or not integrate
correctly with other parts of DFS and DCE.
...\" 
...\" 
...\" .zA "Added conditional symbols"
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Preprocessor Variables"
...\" ----------------------------------------------------------------------
...\" 
...\" 
.P
In addition to the conditionally-defined variables described in the
previous sections, the following variables control important aspects
of the DFS build.
.P
Note that if any of these variables are to be enabled, they should be
defined in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/\fIplatform\fP/param.h\*O
.DE
.P
unless otherwise noted.
...\" 
...\" 
...\" 
.VL .5i
...\" 
...\" 
.LI
\*LAFSDEBMEM\*O
.P
This switch is used only in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/osi/osi_misc.c\*O
.DE
.P
If defined, it enables plumber code for debugging DFS memory leaks.
This variable should be turned off in any production system because
it introduces significant performance penalties.
...\" 
...\" 
.LI
\*LAFS_DEBUG\*O
.P
If defined, enables various types of kernel debugging code.
...\" 
...\" 
.LI
\*LAFS_DYNAMIC\*O
.P
If defined, Cache Manager, Protocol Exporter, and DFS LFS initialization
is not performed at time of first DFS use. This may be appropriate for
platforms (such as AIX 3.2) that use dynamically loaded kernel extensions
and initialize DFS when the extensions are loaded.
...\" 
...\" 
.LI
\*LAFS_HPUX_ENV\*O
.P
Enables HP/UX specific code.
...\" 
...\" 
.LI
\*LAFS_NOBCALL\*O
.P
Should be defined if there is no \*LB_CALL\*O flag in your \*Lbuf\*O
struct. Defined and used only in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/async/asevent.c\*O
.DE
...\" 
...\" 
.LI
\*LAFS_NOBWANTED\*O
.P
Should be defined if there is no no \*LB_WANTED\*O flag in your
\*Lbuf\*O struct. Defined and used only in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/async/asevent.c\*O
.DE
...\" 
...\" 
...\" .LI
...\" \*LAFS_OSF11_ENV\*O
...\" .P
...\" If defined, enables OSF/1 1.1.1 specific code.
...\" 
...\" 
.LI
\*LAFS_PAG_IN_CRED\*O
.P
Should be defined if the PAG (Process Authentication Group) is to be
stored in a separate field (\*Lcr_pag\*O) in the \*Lucred\*O structure.
Otherwise, the PAG is stored in the first two groups of the group list
in the \*Lucred\*O.
...\" 
...\" 
.LI
\*LAFS_PIN\*O
.P
Should be defined if your platform supports a pageable kernel. Note that
the code protected by this \*L#ifdef\*O will probably require some porting.
...\" 
...\" 
.LI
\*LAFS_UIOFMODE\*O
.P
Should be defined if your platform contains an \*Lfmode\*O field in the
\*Luio\*O struct.
...\" 
...\" 
.LI
\*LAFS_VFSCACHE\*O
.P
Should be defined if the Cache Manager is to cache to \*Vany\*O VFS
file system, not just UFS. Is currently defined on the AIX reference
platform.
...\" 
...\" 
.LI
\*LAFS_VFS40\*O
.P
Should be defined if your platform uses the 4.0 VFS interface.
...\" 
...\" 
.LI
\*LAFS_VFS41\*O
.P
Should be defined if your platform uses the 4.1 VFS interface.
...\" 
...\" 
.LI
\*LEFS_PRIVATE_POOL\*O
.P
Should be defined if DFS LFS is to use a private \*Lvnode\*O pool. Defined
and used only in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops/efs_misc.c\*O
.DE
...\" 
...\" 
.LI
\*LFS_EXTENDFS\*O
.P
Should be defined if your platform supports a logical volume manager that
allows aggregates (partitions) to grow.
...\" 
...\" 
.LI
\*LSCACHE_PUBLIC_POOL\*O
.P
Should be defined if the Cache Manager is to use a public \*Lvnode\*O pool.
...\" 
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Global Flags"
...\" ----------------------------------------------------------------------
...\" 
.P
The following flags, which control various aspects of the DFS build, are
found in \*Lmachdep.mk\*O, in the target directory for your platform:
...\" 
.VL .5i
...\" 
.LI
\*LAFS_DEBUG\*O
.P
Used to turn on debugging.
...\" 
.LI
\*LDCEPATHS\*O
.P
Used to define DCELOCAL and DCESHARED.
...\" 
.LI
\*LKERNEL\*O
.P
Used to enable the kernel-specific sections of source code.
...\" 
.LI
\*L_KERNEL\*O
.P
Used to enable the kernel-specific sections of code for AIX-related
(\*L*.h\*O) files.
...\" 
.LE
...\" 
...\" 
...\" 
...\" 
...\" 
...\" .zZ "Added conditional symbols"
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Kernel Configuration Considerations"
...\" ----------------------------------------------------------------------
...\" 
.P
Both DFS and DCE LFS modify the target machine's kernel file system
and system call procedures. When porting to a new platform, the appropriate
strategy for doing so depends on your target environment and particular goals.
.P
For example, you may want to integrate DFS system calls in with your
existing set at all points of contact, or pipe them through a single
interface system call, such as \*Lafs_syscall(\|)\*O, defined in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/kutils/syscall.c\*O
.DE
.P
.iX "daemons"
Note that the DFS daemons cannot be killed or restarted without
rebooting the machine. This feature represents a possible complication when
you test or reconfigure the system, since existing daemons can interfere
with your procedures.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Statically Bound (Standard) Kernels"
...\" ----------------------------------------------------------------------
...\" 
.iX "Kernel" "Binding-static"
...\" 
.P
On traditional BSD-like UNIX implementations, DFS libraries must be linked
into the UNIX kernel. This is accomplished by modifying various files in the
directory where the kernel is linked, thereby making new or modified system
calls available.
...\" .P
...\" For the OSF/1 reference platform,
...\" the
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/libafs/OSF1\*O
...\" .DE
...\" .P
...\" subdirectory is provided with a \*LMakefile\*O that can build a kernel
...\" including DFS support (\*Ldfsvmunix\*O) or including both DFS and DCE LFS
...\" support (\*Lefsvmunix\*O). In addition, an example of a version which uses
...\" dynamic loading is provided in the
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/OSF1.dynamic\*O
...\" .DE
...\" .P
...\" subdirectory.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Dynamically Extensible Kernels: AIX"
...\" ----------------------------------------------------------------------
...\" 
.P
Some operating systems, such as AIX, are different with respect to linking
and loading: they support extending the kernel dynamically. In this environment,
kernel extensions are loaded into the kernel segment, and references to symbols
in the UNIX kernel or other kernel extensions are resolved at load time.
.P
Kernel interfaces and global data are ``exported'' from the UNIX namespace by the
UNIX kernel and by kernel extensions. Only these interfaces may be resolved at
load time. Other kernel interfaces are not available to kernel extensions. A
kernel extension makes interfaces, global data, and system calls available to
other extensions and programs by providing an ``exports list'' when linking the
extension. The supported exports for AIX can be found in \*L/lib/kernex.exp\*O
on an IBM RISC System/6000 machine.
.P
DFS for AIX builds three kernel extensions:
...\" 
.in +0.5i
\*Lexport.ext\*O,
.br
\*Ldfscore.ext\*O, and
.br
\*Ldfscmfx.ext\*O.
.in -0.5i
...\" 
.P
In addition, DCE LFS builds \*Ldcelfs.ext\*O.
.P
Before porting DFS to a new platform, you should understand the procedures for
modifying or extending the operating system kernel on that platform. Also, even
if you are porting to an operating system other than UNIX, you must also
understand UNIX kernel operations, since DFS operations are built on a UNIX
design base.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Virtual Memory Management"
...\" ----------------------------------------------------------------------
...\" 
.iX "Virtual Memory"
.iX "Memory" "Virtual"
...\" 
.P
If your DFS port includes pageable kernel memory or memory mapping, you must
modify your kernel \*Lvnode\*O handling to manage this feature. The AIX
reference port contains sample code for virtual memory management in the
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops\*O
.DE
.P
directory and its \*LRIOS\*O subdirectory, which handle virtual memory
management for the DCE LFS. You will have to resolve issues of mapping
strategy, file granularity in terms of page boundaries, and possible
races or hazards when the same page can conceivably be written into and
mapped out at the same time. To find the code dealing with virtual memory
management, \*Lgrep\*O for \*Lvmm\*O in the source files in that directory.
Also, look at
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops/RIOS/vmm.c\*O
.DE
.P
which is the code file in the RIOS subdirectory.
.P
Many of the differences between mapped and unmapped memory management
consist of including special-purpose header files and handling
slightly different operating system internal routine signatures.
Some of these functions are handled in the Operating System
Independent (OSI) layer, described in the next section, which is a
major focus of any porting project. However, funneling memory management
through specialized \*Losi\*O functions can have performance costs.
When you plan your port of DFS, you will have to decide how to balance
placing additional code in the VMM-specific directory against more
systematic, but possibly slower, extensions to the OSI layer.
...\" 
...\" 
...\" 
...\" 
.zA "enh,2757,R1.1,Improved OSI Material"
...\" ----------------------------------------------------------------------
.H 3 "Porting Guide to Platform Dependent Portions of the OSI Code"
...\" ----------------------------------------------------------------------
...\" 
.P
The Operating System Independent (OSI) layer provides the interfaces
between the DFS kernel operations and the platform's kernel. Roughly
speaking, it encapsulates the host operating system's \*Lvnode\*O and
\*LVFS\*O operations, as well as file objects and system calls, for
uniform treatment by the DFS code. It makes the DFS internal structure
``independent'' of its surrounding environment. However, the structure of
the OSI layer is strongly dependent on the host platform, and its
construction is a significant portion of the DFS porting effort.
...\" 
...\" .zA "def,5335,R1.0.2,Correct ISO Expansion"
.nS "Note"
In this guide, the term OSI is unrelated to the International
Organization of Standardization (ISO) Open Systems Interface, the layered
communications protocol.
.nE
...\" .zZ "def,5335,R1.0.2,Correct ISO Expansion"
...\" 
...\" 
...\" FOLLOWING SECTION IS PRE-1.1...
...\" ----------------------------------------------------------------------
.H 4 "The OSI Layer"
...\" ----------------------------------------------------------------------
...\" 
.P
The OSI layer performs the following functions:
...\" 
.BL
.LI
Maps or redefines data structures that differ in kernel and user space.
.LI
Translates host system calls, such as \*Lgetpid(\|)\*O, to and from
their DFS versions.
.LI
Provides the ``kernel context,'' defining such objects as locks and the
associated parameters, macros for file allocation and handling,
allocation and freeing of memory, scheduling primitives, and
compiler-specific structures.
.LI
Structures interactions between DFS and other DCE components, such
as Security and CDS, in contexts where those interactions depend on
the host environment. For example, \*Lsetpag(\|)\*O governs the setting
and reading of the Process Access Group (PAG) identifier. Among its uses,
this 32-bit number is part of the \*Lxcred\*O extended credential, used to
identify the DFS analog of UNIX user IDs. It is also used to inform DCE
Security that the kernel is configured to run DFS. Since different operating
systems can store such user IDs differently, as a function of various
conditions, including the  machine's byte order and word length, you must
write a customized \*Lsetpag(\|)\*O  for each platform to which you port DFS.
...\" 
.nS "Note"
...\" 
...\" 
...\" The mechanisms for storing the \*Lpag\*O differ significantly between
...\" the two reference platforms. Briefly, on the OSF/1 platform,
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/security/client/login/sec_login_pag\*O
...\" .DE
...\" .P
...\" makes a direct system call to \*Lafscall_cm_setpag(\|)\*O. If the call fails,
...\" Security is effectively informed that DFS is not present, and loads user
...\" credentials with a PAG of \-2. This mechanism eliminates compile or link
...\" time dependencies.
...\" .P
...\" In contrast, 
...\" 
The AIX version of DCE Security only compiles a stub in:
.DS
    \*Vdce-root-dir\*L/security/client/login/sec_login_pag.c\*O
.DE
.P
This stub is enabled if DCE_DFS_PRESENT is defined. The DFS version of DCE is
then linked against the \*Llibafssys.a\*O library, built from
.DS
    \*Vdce-root-dir\*L/file/sys/syscalls.c\*O
.DE
.P
which holds the sources for the RIOS version of \*Lsetpag(\|)\*O.
.P
For more information on \*Lsetpag(\|)\*O, look for the \*LAFS_PAGINCRED\*O
conditional in the source code.
...\"	
.nE
...\" 
.LE
...\" 
...\" ...END OF PRE-1.1 SECTION. FOLLOWING WAS ADDED FROM 1.0.3a RELEASE
...\"  NOTES...
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Porting the OSI Code"
...\" ----------------------------------------------------------------------
...\" 
.P
This section describes the platform dependent functions and macros that need
to be ported when porting the OSI (Operating System Independent) code to another
platform. The ``File Layout'' section below gives a brief description of the
layout of files and the conventions used when including files.
.P
A number of symbols describing platform characteristics are defined in various
header files. These are always defined as either 0 or 1, and are tested with
\*L#if\*O. The details of these symbols are provided later. (This convention
can be contrasted to that in \*Lconfig/param.h\*O, in which symbols are either
defined or not defined and are tested with \*L#ifdef\*O or \*L#ifndef\*O.)
.P
The prototypes shown here can be implemented either as functions/procedures
or as macros.  
.P
The relevant header files included by the rest of the system are:
...\" 
...\" 
.VL 1.5i
.LI "\*Losi_port_mach.h\*O"
Should contain the bulk of the macros, function prototypes etc.,
for the various platform dependent OSI functions and macros.
...\" 
.LI "\*Losi_buf_mach.h\*O"
Should contain definitions of buffer management routines.
...\" 
.LI "\*Losi_user_mach.h\*O"
Should contain platform specific definitions for functions
and/or macros to  obtain user resource limits.
...\" 
.LI "\*Losi_intr_mach.h\*O"
Should contain routines for blocking interrupts and raising interrupt
priority levels.
...\" 
.LI "\*Losi_device_mach.h\*O"
Should contain routines for device management.
...\" 
.LI "\*Losi_uio_mach.h\*O"
Should contain standard macros that are used to isolate OS
system dependencies for uio services.
...\" 
.LI "\*Losi_kvnode_mach.h\*O"
Should contain platform specific definitions that allow portions of
episode to be compiled in user space for testing.
...\" 
.LI "\*Losi_cred_mach.h\*O"
Should contain the functions, macros and constants for definition and
manipulation of credentials.
...\" 
.LI "\*Losi_lock_mach.h\*O"
Should contain the definitions of locks and functions for manipulating
these locks.
...\" 
.LI "\*Losi_ufs_mach.h\*O"
Should contain OSI wrappers for various UFS functions used directly
by the cache manager.
...\" 
.LI "\*Losi.h\*O"
Should contain various kernel file system related function and macro
definitions.
...\" 
This file is actually located in the \*Lfile/osi\*O subdirectory, rather than
in the platform dependent subdirectories. However, the file system functions
and macros defined here are not platform independent and need to be rewritten
for each platform.
...\" 
.LI "\*Losi_dfserrors.h\*O"
Should contain definitions for functions and macros required to translate
DFS errors to platform specific errors and vice versa.
...\" 
This file is actually located in the \*Lfile/osi\*O subdirectory, rather than
in the platform dependent subdirectories. However, the conversion functions
and macros defined here are not platform independent and need to be rewritten
for each platform.
...\" 
.LE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 5 "osi_port_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The following functions, macros and definitions should be found in
\*Losi_port_mach.h\*O:
...\" 
...\" 
.VL 2i
.LI "\*LOSI_SINGLE_THREADED\*O"
If set, use conventional sleep/wakeup model for synchronization. In this case,
the original OSI locking code is used. If false (as in the SunOS kernel),
OS-specific locking code must be supplied.
...\" 
.LI "\*LOSI_BSD_TIMEOUT\*O"
If set, use BSD convention where \*Luntimeout\*O is passed the \*Ltimeout\*O
function pointer and argument. The SVR4 convention is to use an index returned
by \*Ltimeout\*O instead. Affects the implementation of
\*Losi_CallProc/osi_CancelProc\*O.
...\" 
.LE
...\" 
...\" 
.P
The DFS protocol encodes a device with major number J minor number N in two words.
\*LdeviceNumber\*O contains the lower 16 bits of J in its upper 16 bits, and the
lower 16 bits of N in the lower 16 bits. \*LdeviceNumberHighBits\*O contains the
upper 16 bits of J in its upper 16 bits and the upper 16 bits of N in the lower 16
bits. For the RIOS, this means \*LdeviceNumber\*O corresponds to the native \*Ldev_t\*O.
In HP/UX 9.0, major numbers are 8 bits and minor numbers are 24. The \*LafsVolsync\*O
structure passes an encoded device spec over the wire. The following macros do the
encoding and decoding.
...\" 		
...\" 
...\" 
.BL
.LI
\*Lvoid osi_EncodeDeviceNumber(struct afsVolSync *statP, dev_t dev)\*O
.br
Places \*Ldev\*O into \*LstatP\*O status structure.
...\" 
.LI
\*Lint osi_DecodeDeviceNumber(struct afsVolSync *statP, dev_t *devP)\*O
.br
Takes major/minor information from the \*Lstat\*O structure and puts it in
\*L*devP\*O. Returns 1 if the device is representable on the local architecture.
...\" 
.LI
\*Lstruct proc *osi_curproc(\|)\*O
.br
Returns pointer to current \*Lproc\*O structure.
...\" 
.LI
\*Llong osi_ThreadUnique(\|)\*O
.br
Returns a thread identitifer that is unique.
...\" 
.LI
\*Llong osi_ThreadID(\|)\*O
.br
Returns current thread ID. This routine is almost obsolete.
...\" 
.LI
\*Lpid_t osi_GetPid(\|)\*O
.br
Returns current process id.
...\" 
.LI
\*Lint osi_copyin(caddr_t uaddr, caddr_t kaddr, size_t len)\*O
.br
Copy \*Llen\*O bytes from user address \*Luaddr\*O to kernel address \*Lkaddr\*O.
...\" 
.LI
\*Lint osi_copyinstr(caddr_t uaddr, caddr_t kaddr, size_t len, size_t *copied)\*O
.br
Copy a string of maximum length \*Llen\*O bytes from user address \*Luaddr\*O to
kernel address \*Lkaddr\*O.
...\" 
.LI
\*Lint osi_copyout(caddr_t kaddr, caddr_t uaddr, size_t len)\*O
.br
Copy \*Llen\*O bytes from kernel address \*Lkaddr\*O to user address \*Luaddr\*O.
...\" 
.LI
\*Lvoid osi_MakeInitChild(\|)\*O
.br
Makes the calling process a child of \*Linit\*O. Used so that autonomous kernel
generated threads eventually get \*Lwait3\*O calls issued for them, leave the
zombie state and go away after exiting.
...\" 
.LI
\*Lint osi_ThreadCreate( void (*clientRoutine)(), void  *clientArgs,\\
                         int blockPreemptionBeforeClientRoutine,\\
                         int restorePreemptionAfterClientRoutine,\\
                         char thread name[4])\*O
.br
This is a routine or macro to create a new kernel thread. A similar macro/routine
exists to create user threads as well, but that is platform independent. The
\*LclientRoutine(\|)\*O function is called with \*LclientArgs\*O as an argument.
.P
The \*LblockPreemptionBeforeClientRoutine\*O and \*LrestorePreemptionBeforeClientRoutine\*O
flags are used in the kernel to respectively block preemption of the client thread and to
restore preemption on return from the client thread if non-zero.
.P
These flags are obsolete and do not need to be implemented by a porter.
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 5 "Vnode, Vfs, and Directory Name Lookup Cache Routines"
...\" ----------------------------------------------------------------------
...\" 
.P
The following functions or macros must be defined in the \*Losi_port_mach.h\*O
file.
.P
A number of macros and functions need to be defined to deal with vfs/vnode
fields.
...\" 
...\" 
.VL .5i
...\" 
.LI
\*Losi_vfs\*O
.P
Osi Vfs structure. Other OSI functions manipulate this structure.
...\" 
.LI
\*Losi_vfs_op\*O
.P
Pointer to vfs ops vector in \*Losi_vfs\*O structure.
...\" 
.LI
\*Losi_vnodeops\*O
.P
This is the type of the extended vnode ops vector.
...\" 
.LI
\*Losi_ufsVnodeops\*O
.P
Ufs's vnode operations.
...\" 
.LI
\*Losi_vfsops\*O
.P
Vfsops structure.
...\" 
.LI
\*Lvoid osi_vSetVfsp(struct vnode *, struct osi_vfs *)\*O
.P
Sets pointer to vfs for this vnode.
...\" 
.LI
\*Losi_vInitLocks(struct vnode *)\*O
.P
Initializes vnode locks.
...\" 
.LI
\*Lint osi_vType(struct vnode *)\*O
.P
Gets type of vnode.
...\" 
.LI
\*Lvoid osi_vSetType(struct vnode *, int)\*O
.P
Sets type of vnode.
...\" 
.LI
\*Lint osi_IsAfsVnode(struct vnode *)\*O
.P
Returns 1 if this is a DFS vnode.
...\" 
.LI
\*Lvoid osi_SetAfsVnode(struct vnode *)\*O
.P
Makes this a DFS vnode. Obsolete. The use of the term AFS rather than
DFS in the macros above is a historic relic.
...\" 
.LI
\*Lvoid osi_SetVnodeOps(struct vnode *, void *)\*O
.P
Sets vnode ops for this vnode.
...\" 
.LI
\*Lvoid OSI_VN_INIT(struct vnode *, osi_vfs *, int, dev_t, int)\*O
.P
Called to initialize a vnode.
...\" 
.LI
\*Lvoid OSI_VN_RELE(struct vnode *)\*O
.P
Decrement reference count on vnode. Normally, this would fall through to
\*LVN_RELE(\|)\*O, but some additional locking might be required in a
multithreaded kernel. In SunOS, a global lock is used.
...\" 
.LI
\*Lstruct osi_vfs * OSI_VP_TO_VFSP(struct vnode *)\*O
.P
Takes a vnode as an argument and returns a pointer to the vfs for
the vnode.
...\" 
.LI	
\*Lint OSI_IS_MOUNTED_ON(struct vnode *)\*O
.P
Returns 1 if this vnode has a file system mounted on it.
...\" 
...\" 
...\" 
.LE
.VL 2i
...\" 
.LI "\*Lint OSI_ISFIFO(VP)\*O"
FIFO
...\" 
.LI "\*Lint OSI_ISVDEV(VP)\*O"
Device
...\" 
.LI "\*Lint OSI_ISDIR(VP)\*O"
Directory
...\" 
.LI "\*Lint OSI_ISREG(VP)\*O"
Regular File
...\" 
.LI "\*Lint OSI_ISLNK(VP)\*O"
Symlink
...\" 
.P
The above macros, when given a vnode pointer, will return a 1 if the vnode
is of the appropriate type: FIFO, Device, Directory, or link.
...\" 
.LE
...\" 
...\" 
.VL .5i
...\" 
.LI
\*Losi_lockvp(struct vnode *)\*O
.P
Lock vnode in order to manipulate reference count. No-op for HP/UX and AIX.
...\" 
.LI
\*Losi_unlockvp(struct vnode *)\*O
.P
Unlock vnode. No-op for HP/UX and AIX.
...\" 
...\" 
.LE
...\" 
...\" 
.P
A number of interfaces for manipulating the \*Ldnlc\*O (directory name lookup cache)
should be present. In systems where the \*Ldnlc\*O does not exist, these are null
functions or macros.
...\" 
.VL .5i
.LI
\*Lvoid osi_dnlc_enter(struct vnode *dvp, char *name, struct vnode *Vp)\*O
.P
Create an entry in the \*Ldnlc\*O. \*LVp\*O is the vnode the name refers
to while \*Ldvp\*O is its parent.
...\" 
.LI
\*Lstruct vnode *osi_dnlc_lookup(struct vnode *dp, char *name)\*O
.P
Return an entry in the \*Ldnlc\*O given a name and a pointer \*Ldp\*O to vnode of
parent of \*Lname\*O.
...\" 
.LI
\*Lint osi_dnlc_fs_purge1(struct vnodeops *vop)\*O
.P
This purges one entry from the \*Ldnlc\*O that is part of the filesystem(s)
represented by \*Lvop\*O. If such a vode is found, 1 is returned, else 0.
...\" 
.LI
\*Lint osi_dnlc_fs_purge(struct vnodeops *vop, int reclaim)\*O
.P
Purge entries of a specific type in the \*Ldnlc\*O. The \*Lreclaim\*O parameter
in \*Losi_dnlc_fs_purge(\|)\*O is an indication of how badly the caller wants a
purge to take place. When reclaim is 1, the standard \*Lpurge1(\|)\*O is issued.
When \*Lreclaim\*O = 2, \*Lref.count\*O is ignored to allow directories to get
reclaimed more easily, when \*Lreclaim\*O = 3, any vnode of the right type (in
this case, Episode) will be taken regardless of whether it has pages or not.
...\" 
.LI
\*Lvoid osi_dnlc_purge_vp(struct vnode *)\*O
.P
Purge all entries corresponding to a particular vnode.
...\" 
.LI
\*Lint osi_dnlc_purge_vfsp(struct vfs *vfsp, int count)\*O
.P
Purge all entries corresponding to a particular file system. Caller supplies a
\*Lcount\*O of entries to purge. A \*Lcount\*O of zero indicates that all such
entries should be purged. Returns the number of entries that were purged.
...\" 
.LI
\*Lvoid osi_dnlc_remove(struct vnode *dp, char *name)\*O
.P
Remove a specific \*Vdirectory, name pair\*O entry from the \*Ldnlc\*O.
...\" 
...\" 
.LI
\*Lint osi_lookupname( char *namep,\\
                       enum uio_seg seg,\\
                       enum symfollow followlink,\\
                       struct vnode **dirvpp,\\
                       struct vnode **compvpp)\*O
.P
This function takes a name and returns a held vnode pointer to the corresponding file.
The \*Lseg\*O field indicates the address space that the name is in. If \*LOSI_UIOSYS\*O,
the name is in kernel space, if \*LOSI_UIOUSER\*O, it is in user space. If \*Lfollowlink\*O
is \*LFOLLOW_LINK\*O, then the function follows links, else if \*LNO_FOLLOW\*O, it does not.
The vnode of the directory containing the filename is returned in \*Ldirvpp\*O while the
vnode of the file itself is returned in \*Lcompvpp\*O. Returns 0 if succesful, error number
otherwise.
...\" 
.LI
\*Losi_vattr_init(struct vattr *vap, long mask)\*O
.P
Set mask for vnode attributes to  mask.
...\" 
.LI
\*Losi_vattr_add(struct vattr *vap, long mask)\*O
.LI
\*Losi_vattr_sub(struct vattr *vap, long mask)\*O
.P
Allow bits to be added or removed from the mask.
...\" 
.LI
\*Llong osi_setting_mode(struct vattr *vap)\*O
.LI
\*Llong osi_set_mode_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_mode_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_uid(struct vattr *vap)\*O
.LI
\*Llong osi_set_uid_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_uid_flag(struct vattr *vap\*O
...\" 
.LI
\*Llong osi_setting_gid(struct vattr *vap)\*O
.LI
\*Llong osi_set_gid_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_gid_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_uid_or_gid(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_size(struct vattr *vap)\*O
.LI
\*Llong osi_set_size_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_size_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_atime(struct vattr *vap)\*O
.LI
\*Llong osi_set_atime_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_atime_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_mtime(struct vattr *vap)\*O
.LI
\*Llong osi_set_mtime_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_mtime_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_times_now(struct vattr *vap)\*O
.LI
\*Llong osi_set_times_now_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_times_now_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_ctime(struct vattr *vap)\*O
.LI
\*Llong osi_set_ctime_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_ctime_flag(struct vattr *vap)\*O
...\" 
.LI
\*Llong osi_setting_link(struct vattr *vap)\*O
.LI
\*Llong osi_set_link_flag(struct vattr *vap)\*O
.LI
\*Llong osi_clear_link_flag(struct vattr *vap)\*O
.P
The above macros can be used to manipulate various vnode attributes. An
\*Losi_set_xxx_flag(\|)\*O macro will set the appropriate flag, an
\*Losi_clear_xxx_flag(\|)\*O will clear the appropriate flag. The
\*Losi_setting_xxx_flag(\|)\*O returns 0 if the appropriate flag
is not set and a non-zero number otherwise.
.P
The \*Losi_setting_xxx_flag(\|)\*O macros are for use by implementors of
\*LVOP_SETATTR\*O, such as the cache manager and Episode. The
\*Losi_set_xxx_flag(\|)\*O and \*Losi_clear_xxx_flag(\|)\*O macros are
for use by callers of \*LVOP_SETATTR\*O.
.P
In SunOS, the \*Lvattr\*O structure has a member that is a bitmask indicating
which attributes are supposed to be changed by \*LVOP_SETATTR\*O. On other
platforms, there is no bit mask; instead, to indicate that some attribute is
\*Vnot\*O supposed to be changed, its value is set  to -1.
.P
In the list of macros above, the relevant flags being manipulated (from top to
bottom) in the vnode attributes are : protection mode, uid, gid, uid or gid,
size, access time, modified time, changed time, flag indicating whether the
vnode is a link or not.
...\" 
...\" 
...\" 
.LI
\*Losi_hz\*O
.LI
\*Losi_sec\*O
.LI
\*Losi_subsec\*O
.LI
\*Losi_timeval_t\*O
.LI
\*Losi_SubUnit\*O
.LI
\*Losi_SubIncr\*O
.P	
The above macros are necessary because different platforms may use different
structures in the vnode \*Lattr\*O field. The SunOS structure has a second field
and a nanosecond field, while HP/UX and AIX have a second field and a microsecond
field. The \*Losi\*O field provides wrappers around this, using the \*Lsubsec\*O field
to indicate a subsecond field, while \*Losi_SubUnit\*O indicates what fraction of
a second this is. For a nanosecond, \*Losi_Subunit\*O = 10^9; for a microsecond,
10^6. \*Losi_SubIncr\*O is the minimum increment for the subsecond field.
.P
\*Losi_timeval_t\*O is the structure with \*Lsubsec\*O as a field and is manipulated
by all the other macros.
...\" 
...\" 
...\" 
.LI
\*Losi_UTimeFromSub(afsTimeval, osi_timeval_t)\*O
.P
Converts from \*Lsubsec\*O notation to \*LafsTimeval\*O.
...\" 
.LI
\*Losi_SubFromUTime(osi_timeval_t, afsTimeval)\*O
.P
Converts \*LafsTimeval\*O to \*Lsubsec\*O notation.
...\" 
.LI
\*Losi_TvalFromSub(afsTimeval, osi_timeval_t)\*O
.P
Converts from \*Lsubsec\*O notation to \*LafsTimeval\*O.
...\" 
.LI
\*Losi_SubFromTval(osi_timeval_t, afsTimeval)\*O
.P
Converts \*LafsTimeval\*O to \*Lsubsec\*O notation.
...\" 
.LI
\*Lint osi_ToUTime(osi_timeval_t)\*O
.P
This function takes the local time structure, and
returns the local sub-second time field in microseconds.
...\" 
.LI
\*Lint osi_ToSubTime(osi_timeval_t)\*O
.P
This takes the structure with microseconds stored in its
sub-second field, and converts it back to the local time.
.P
There are two sets of these macros for historical reasons.
...\" 
...\" 
.LE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "File System Operations"
...\" ----------------------------------------------------------------------
...\" 
.P
The following kernel file system related functions and macros need to be ported.
The \*Losi.h\*O file in the \*Lfile/osi\*O subdirectory must contain definitions
for these.
.P
These routines are used by the cache manager to manipulate the dcache, and can be
used to access the dcache independent of where it is located (disk or memory).
...\" 
...\" 
...\" 
.VL .5i
...\" 
.LI
\*Lstruct osi_file *osi_Open(vfsp, fhandle)
   struct osi_vfs *vfsp;
   struct osi_fid_t *fhandle;\*O
.P
This is used to open a file from within the kernel. Returns \*LNULL\*O on
failure.
...\" 
...\" 
.LI
\*Lstruct osi_stat {
    long size;		/* file size in bytes */
    long blocksize;	/* optimal transfer size in bytes */
    long mtime;		/* modification date */
    long atime;		/* access time */
}\*O
.P
Status representation of a file object: Only the fields shown above
are actually utilized.
...\" 
...\" 
.LI
\*Lint osi_Stat(afile, astat)
   struct osi_file	*afile;
   struct osi_stat	*astat;\*O
.P
Used to stat a file from within the kernel. Returns 0 if succesful,
otherwise returns error number.
...\" 
...\" 
.LI
\*Lint osi_Close(afile)
   struct osi_file	*afile;\*O
.P
Used to close a file from within the kernel. Always returns 0.
...\" 
...\" 
.LI
\*Lint osi_Truncate(afile, asize)
   struct osi_file	*afile;
   long			asize;\*O
.P
Used to truncate a file from within the kernel. Returns 0 if succesful,
otherwise returns error number.
...\" 
...\" 
.LI
\*Lint osi_Read(afile, aptr, asize)
   struct osi_file	*afile;
   char			*aptr;
   long			asize;\*O
.P
Used to read a file from within the kernel. Returns 0 if succesful,
otherwise returns error number.
...\" 
...\" 
.LI
\*Lint osi_Write(afile, aptr, asize)
   struct osi_file	*afile;
   char			*aptr;
   long			asize;\*O
.P
Used to write a file from within the kernel. Returns 0 if succesful,
otherwise returns error number.
...\" 
...\" 
.LI
\*Llong osi_MapStrategy(aproc, bp)
   int		(*aproc)();
   struct buf	*bp;\*O
.P
Takes a pointer \*Lbp\*O to a user memory location, maps it into kernel
memory, and calls the procedure \*Laproc(\|)\*O with the pointer \*Lbp\*O
as a parameter. After the procedure returns, this pointer is unmapped.
Returns the return value of the function \*Laproc(\|)\*O.
...\" 
...\" 
.LI
\*Lint osi_rdwr(
    enum uio_rw		rw,
    struct vnode	*vp,
    caddr_t		base,
    int			len,
    off_t		Offset,
    enum uio_seg	seg,
    int			ioflag,
    long		ulimit,
    osi_cred_t		*cr,
    int			*residp)\*O
.P
Performs reads or writes on a vnode. Rarely called directly. Typically,
\*Losi_Write(\|)\*O or \*Losi_Read(\|)\*O are called, both of which call
the present routine. The \*Lrw\*O field indicates whether this is a read
or a write call (\*LUIO_READ\*O and \*LUIO_WRITE\*O respectively), \*Lvp\*O
is the vnode on which the I/O operation is to be performed, \*Lbase\*O is a
pointer to the area to be read, \*Llen\*O is the number of bytes to be read
or written. \*LOffset\*O is the offset in the vnode, and the \*Lseg\*O field
indicates whether the address base is in user space or kernel space. \*Lresidp\*O
passes back the number of bytes remaining to do I/O on, and \*Lcr\*O
contains credentials. Returns 0 if there is no error. Otherwise, returns
the error number.
...\" 
...\" 
.LI
\*Lint osi_vptofid(struct vnode *vp, osi_fid_t *fidp)\*O
.P
Takes a pointer to a vnode structure and returns a pointer to a \*Lfid\*O. 
...\" 
.LE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Locking functions"
...\" ----------------------------------------------------------------------
...\" 
.P
The standard lock is a ``data lock'', which is a readers/writer lock with an
additional ``shared'' state; logically equivalent to a r/w lock plus a mutex.
This is defined to be of type \*Losi_dlock_t\*O. Functions that atomically
release a lock and sleep provide synchronization.
.P
The file \*Losi_lock_mach.h\*O should contain definitions for these functions
and macros. All of the routines listed in this section are kernel functions.
Similar user level functions exist, but they are not machine dependent. 
...\" 
...\" 
...\" 
.VL .5i
...\" 
.LI
\*Lvoid lock_Init(osi_dlock_t *lock)\*O
.P
Creates a lock.
...\" 
.LI
\*Lvoid lock_Destroy(osi_dlock_t *lock)\*O
.P
Destroys a lock.
...\" 
.LE
.P
The locks can be manipulated with the following functions and macros.
...\" 
.VL .5i
.LI
\*Lvoid lock_ObtainRead(osi_dlock_t *lock)\*O
.P
Obtains a read lock.
...\" 
.LI	
\*Lvoid lock_ObtainWrite(osi_dlock_t *lock)\*O
.P
Obtains a write lock.
...\" 
.LI
\*Lvoid lock_ObtainShared(osi_dlock_t *lock);\*O
.P
Obtains a shared lock, i.e. a read lock which can be upgraded to a write
lock later.
...\" 
.LI
\*Lint lock_ObtainWriteNoBlock(osi_dlock_t *lock);\*O
.P
Obtains a write lock without blocking. Returns 0 if unsuccessful.
...\" 
.LI
\*Lint lock_ObtainSharedNoBlock(osi_dlock_t *lock);\*O
.P
Obtains a shared lock without blocking. Returns 0 if unsuccessful.
...\" 
.LI
\*Lvoid lock_UpgradeSToW(osi_dlock_t *lock);\*O
.P
Upgrades a shared lock to a write lock.
...\" 
.LI
\*Lvoid lock_ConvertWToS(osi_dlock_t *lock)\*O
.P
Converts a write lock to a shared lock.
...\" 
.LI
\*Lvoid lock_ConvertWToR(osi_dlock_t *lock)\*O
.P
Converts a write lock to a read lock.
...\" 
.LI
\*Lvoid lock_ConvertSToR(osi_dlock_t *lock)\*O
.P
Converts a shared lock to a read lock.
...\" 
.LI
\*Lvoid lock_ReleaseRead(osi_dlock_t *lock)\*O
.P
Releases a read lock.
...\" 
.LI
\*Lvoid lock_ReleaseWrite(osi_dlock_t *lock)\*O
.P
Releases a write lock.
...\" 
.LI
\*Lvoid lock_ReleaseShared(osi_dlock_t *lock)\*O
.P
Releases a shared lock.
...\" 
.LI
\*Lvoid osi_SleepR(opaque addr, osi_dlock_t *lock);\*O
.P
Releases a read lock and sleeps on an address, atomically.
...\" 
.LI
\*Lvoid osi_SleepW(opaque addr, osi_dlock_t *lock);\*O
.P
Releases a write lock and sleeps on an address, atomically.
...\" 
.LI
\*Lvoid osi_SleepS(opaque addr, osi_dlock_t *lock);\*O
.P
Releases a shared lock and sleeps on an address, atomically.
...\" 
.LI
\*Lvoid osi_Sleep2(opaque addr, osi_dlock_t *lock, int lockType); \*O
.P
Releases a generic lock and sleeps on an address, atomically.
\*LlockType\*O may be either \*LREAD_LOCK\*O, \*LWRITE_LOCK\*O or
\*LSHARED_LOCK\*O.
...\" 
.LI
\*Lint osi_SleepRI(opaque addr, osi_dlock_t *lock);\*O
.P
Releases a read lock and sleeps interruptibly on an address, atomically.
...\" 
.LI
\*Lint osi_SleepWI(opaque addr, osi_dlock_t *lock);\*O
.P
Releases a write lock and sleeps interruptibly on an address, atomically.
...\" 
.LI
\*Lint osi_SleepSI(opaque addr, osi_dlock_t *lock);\*O
.P
Releases a write lock and sleeps interruptibly on an address, atomically.
...\" 
...\" 
.LE
...\" 
.P
The following functions and macros should be defined in \*Losi_port_mach.h\*O:
...\" 
...\" 
.VL .5i
.LI
\*Lint osi_Sleep(opaque)\*O
.P
Sleeps on an address.
...\" 
.LI
\*Lint osi_SleepInterruptably(opaque)\*O
.P
Sleeps interruptibly on an address.
...\" 
.LI
\*Lint osi_Wakeup(opaque)\*O
.P
Wakes up anyone sleeping on an address.
...\" 
.LI
\*Lint osi_PreemptionOff(void)\*O
.P
Prevents mutual concurrent access. Returns current level.
...\" 
.LI
\*Lvoid osi_RestorePreemption(int s)\*O
.P
Restores mutual concurrent access.
...\" 	
.P
\*Losi_PreemptionOff(\|)\*O and \*Losi_RestorePreemption(\|)\*O are used in the
kernel to ensure mutual exclusion. Throughout DFS there is code that assumes that
it is running in an old-style UNIX kernel environment, that is, an environment in
which a process cannot be preempted and in which only one processor is accessing
memory. Rather than adapt this code to different environments, DFS temporarily
relies on a global locking mechanism. In AIX, the existing ``kernel lock'' is used.
In SunOS, DFS defines its own global lock.
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "System Buffer handling facilities"
...\" ----------------------------------------------------------------------
...\" 
.P
These should be defined in the \*Losi_buf_mach.h\*O file.
...\" 
...\" 
...\" 
.VL .51
.LI
\*Ldev_t osi_bdev(struct buf *bp)\*O
.P
Gets device number.
...\" 
.LI
\*Losi_set_bdev(struct buf *bp, dev_t) \*O
.P
Sets device number.
...\" 
.LI
\*Losi_bio_init(struct buf *bp)\*O
.P
Initializes \*Lbuf\*O structure. (Initializes semaphores in SunOS, does nothing
elsewhere.)
...\" 
.LI
\*Losi_bio_wakeup(struct buf *bp)\*O
.P
Wakes up anyone waiting for this buffer.
...\" 
.LI
\*Losi_biodone(struct buf *bp)\*O
.P
Called from the asevent \*Liodone\*O function to do whatever processing
is necessary to complete a \*Lbuf\*O I/O.
...\" 
.LI
\*Losi_iodone_t\*O
.P
The return type of the \*Liodone\*O function (\*Lint\*O or \*Lvoid\*O).
...\" 
.LI
\*Losi_iodone_ret(int n)\*O
.P
A macro used to return from the \*Liodone\*O function. Returns \*Ln\*O if
\*Losi_iodone_t\*O is \*Lint\*O; returns nothing if \*Losi_iodone_t\*O is
\*Lvoid\*O.
...\" 
.LI
\*Lvoid osi_bio_cleanup(struct buf *bp)\*O
.P
Performs system dependent processing to clean up, after I/O on a \*Lbuf\*O
struct completes.
...\" 
.LI
\*Losi_bio_wait(struct buf *bp, osi_dlock_t *lockp)\*O
.P
Releases a write lock and waits for the \*Lbuf\*O I/O to complete.
...\" 
...\" 
.LE
...\" 
...\"  
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Credentials"
...\" ----------------------------------------------------------------------
...\" 
.P
The functions, macros and constants necessary for the definition and accessing of
credentials are found in \*Losi_cred_mach.h\*O.
.P
The credential structure is of type \*Losi_cred_t\*O. The following constants
pertain to this structure:
...\" 
...\" 
.VL .5i
.LI
\*LOSI_CR_GROUPS_COUNTED\*O
.P
If \*LTRUE\*O, the credential structure contains a count of the number of groups
in the \*Lcr_groups\*O list; otherwise, the list is terminated by a special value
(\*LOSI_NOGROUP\*O). Used in \*Losi_pag.c\*O.
...\" 
.LI
\*LOSI_HAS_CR_PAG\*O
.P
If \*LTRUE\*O, the credential structure has a member that records the PAG.
...\" 
.LI
\*LOSI_MAXGROUPS\*O
.P
Maximum number of groups.
...\" 
.LI
\*Lint osi_SetNGroups(osi_cred_t *, int)\*O
.LI
\*Lint osi_GetNGroups(osi_cred_t *)\*O
.LI
\*Lint osi_SetUID(osi_cred_t *, u)\*O
.LI
\*Luid_t osi_GetUID(osi_cred_t *)\*O
.LI
\*Luid_t osi_GetRUID(osi_cred_t *)\*O
.LI
\*Lint osi_SetGID(osi_cred_t *, gid_t)\*O
.LI
\*Lgid_t osi_GetGID(osi_cred_t *)\*O
.P	
The above functions/macros help to get or set the number of groups,
group id, uid, or real uid.
...\" 
.LI
\*Losi_cred_t * osi_getucred(void)\*O
.P
Gets user credentials. Both kernel and user level.
...\" 
.LI
\*Lvoid osi_setucred(osi_cred_t *);\*O
.P
Sets user credentials. Both kernel and user level.
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Error encoding and decoding"
...\" ----------------------------------------------------------------------
...\" 
.P
The file \*Losi_dfserrors.h\*O in the \*Lfile/osi\*O directory contains the
definitions for these functions. However, the functions themselves are not
platform independent, and need to be ported to each new platform.
...\" 
...\" 
.VL .5i
.LI
\*Lvoid osi_initDecodeTable(\|)\*O
.P
This routine is used to initialize the Error Mapping Table which is used by
the CM to perform the decoding work. This routine is invoked when the CM is
initialized.
...\" 
.LI
\*Lvoid osi_initEncodeTable(\|)\*O
.P
This routine is used to initialize the Error Mapping Table used by the DFS server
to map a native (AIX/SUNOS/HPUX) KERNEL error to a DFS plaform independent error.
This routine is called during the DFS server initialization process.
...\" 
.LE
...\" 
.P
Note that those kernel errors that are either very platform (SUNOS/AIX/HPUX)
specific or should never occur to (or be visible to ) remote callers will be
treated as \*LDFS_ENOTDEFINED\*O during the conversion and will be ignored by
the CM.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Miscellaneous Kernel and User level utilities"
...\" ----------------------------------------------------------------------
...\" 
.P
The osi_port_mach.h file should contain definitions for the following
functions and macros:
...\" 
...\" 
.VL .5i
.LI
\*Lint osi_wait3(int *childStatus, int wait3options, void *rvp)\*O
.P
This user level function provides the equivalent of the user level \*Lwait3(\|)\*O
function, which not all operating systems support.
...\" 
.LI
\*Losi_Pause(int seconds)\*O
.P
Kernel procedure/macro for suspending execution for a given number of seconds.
...\" 
.LI
\*Lopaque osi_kalloc(size_t size);\*O
.LI
\*Lint osi_kfree(opaque, size_t )\*O
.P
Kernel OSI \*Lmalloc\*O and \*Lfree\*O definitions.
...\" 
.LI
\*Lint osi_GetTime(struct timeval *)\*O
.P
Gets system time. (Both kernel and user level.)
...\" 
.LI
\*Lint osi_SetTime(struct timeval *)\*O
.P
Sets system time. (Both kernel and user level.)
...\" 
.LI
\*Lint osi_getfh(char *path, osi_fhandle_t *fidp)\*O
.P
Converts a pathname into a file handle. Formerly used by \*Ldfsd\*O,
now obsolete. 
...\" 
.LI
\*Lint osi_GetMaxNumberFiles(void)\*O
.P
Returns the maximum number of files that can be opened by a process.
...\" 
.LI
\*Lvoid osi_Invisible(void)\*O
.P
Kernel procedure for making DFS processes invisible to process listings.
...\" 
.LI
\*Lcaddr_t osi_caller()\*O
.P
Returns address of calling function.
...\" 
.LI
\*Lint osi_GetMachineName(char *buf, int len);\*O
.P
Kernel procedure that returns machine name. Returns 0 if succesful.
...\" 
.LI
\*Lint osi_setjmpx(jmp_buf)\*O
.LI
\*Lint osi_clrjmpx(jmp_buf)\*O
.LI
\*Lvoid osi_longjmpx(jmp_buf, int)\*O
.P
The above three routines set long jump, clear long jump and actually do the
longjump, respectively. These routines are in-kernel equivalents of \*Lsetjmp(\|)\*O,
\*Lclrjump(\|)\*O and \*Llongjump(\|)\*O.
...\" 
.LI
\*Lvoid osi_setuerror(int)\*O
.P
Sets current user error in the kernel.
...\" 
.LI
\*Lint osi_getuerror()\*O
.P
Gets current user error in the kernel.
...\" 
.LI
\*Lint osi_suser(struct osi_cred_t *)\*O
.P
Returns 1 if super-user.
...\" 
.LE
...\" 
.P
Finally, the \*Losi_port_mach.h\*O file makes the following functions/macros
available to any code that might use them. These should be ported only if
necessary.
...\" 
.BL
.LI
\*Lbcopy(src, dest, n)\*O
.LI
\*Lbcmp(s1, s2, n)\*O
.LI
\*Lbzero(s, n)\*O
.LI
\*Lindex(s, c)\*O
.LI
\*Lrindex(s, c)\*O
.LI
\*Lgetwd(b)\*O
.LI
\*Lstatfs(p, s)\*O
.LI
\*Lcrhold(struct osi_cred_t)\*O
.LI
\*Lint gettimeofday(struct timeval *tp)\*O
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "osi_uio_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Losi_uio_mach.h\*O file contains uio vector related definitions.
...\" 
...\" 
.VL .5i
.LI
\*Losi_uiomove(caddr_t addr, int len, int op, struct uio * uiop)\*O
...\" 
.P
where:
.VL 1i
.LI "\*Laddr\*O"
is a pointer to the buffer containing data to be copied from/to.
...\" 
.LI "\*Ln\*O"
specifies the number of bytes of data to copy.
...\" 
.LI "\*Lop\*O"
is a mode indicating the direction of copy; if \*LUIO_READ\*O, \*Luiomove(\|)\*O
will copy data from \*Laddr\*O to area specified by \*Luiop\*O, if \*LUIO_WRITE\*O,
\*Luiomove(\|)\*O will copy data from the area specified by \*Luiop\*O to the
area pointed to by \*Laddr\*O.
...\" 
.LI "\*Luio\*O"
is a pointer to the data structure that maintains the ``state'' of the transfer
operation. It is updated by every call to \*Losi_uiomove(\|)\*O.
...\" 
.LE
...\" 
.P
There is also a user level version of this function/macro, which is
a stub intended to ensure that user level episode can build correctly.
...\" 
...\" .P
...\" The \*Luio\*O data structure is assumed to consist of the following
...\" fields.
...\" 
...\" 
.LI
\*Lstruct uio {
         struct		iovec *osi_uio_iov;
         int		osi_uio_iovcnt;
         <type>		osi_uio_offset;
         osi_uio_seg_t	osi_uio_seg;
         int		osi_uio_resid;
};\*O
...\" 
.LI
\*Lstruct iovec {
         caddr_t	iov_base;
         int		iov_len;
};\*O
...\" 
...\" 
.P
where:
...\" 
...\" 
...\" 
.VL .5i
.LI
\*Losi_uio_iov\*O
.P
points to an array of I/O vector elements. It supports the notion of
scatter/gather I/Os. Each element has a base pointer (\*Liov_base\*O)
and a length (\*Liov_len\*O).
...\" 
.LI 
\*Losi_uio_iovcnt\*O
.P
specifies the size of the \*Liovec\*O array.
...\" 
.LI 
\*Losi_uio_offset\*O
.P
is incremented on every call to osi_uiomove by the number of bytes
copied. It is a \*Llong\*O on platforms like AIX and HP/UX, but a
\*Llonglong\*O on SunOS.
...\" 
.LI 
\*Losi_uio_seg\*O
.P
is a segment flag, indicating whether the data described by the \*Luio\*O
structure is in the kernel space or user space. This is of type
\*Losi_uio_seg_t\*O, which is an enumerated type. Possible values are
\*LOSI_UIOSYS\*O and \*LOSI_UIOUSER\*O.
...\" 
.LI 
\*Losi_uio_resid\*O
.P
indicates the data remaining to be copied. Initially, \*Luio_resid\*O is
set to be the total number of bytes described by the entire I/O vector.
...\" 
...\" 
.LE
...\" 
...\" ?????
.P

...\" ?????
...\" 
.LI
\*Lint osi_InitUIO(struct uio *)\*O
.P
Initializes the given \*Luio\*O structure. 
...\" 
.LI
\*Lint osi_uio_set_offset(struct uio u, hyper hyp)\*O
.P
Sets the offset field in the \*Luio\*O structure to \*Lhyp\*O. Returns 0 if
successful, 1 otherwise.
...\" 
.LI
\*Lvoid osi_uio_get_offset(struct uio u, hyper hyp)\*O
.P
Gets the offset field from the \*Luio\*O structure and returns it in \*Lhyp\*O.
.P
The \*Lhyp\*O structure is defined by DFS in \*Lfile/config/stds.h\*O, and
represents a 64 bit signed integer.
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "osi_intr_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Losi_intr_mach.h\*O should contain definitions for the following:
...\" 
...\" 
.VL .5i
.LI
\*Lint osi_splbio(\|)\*O
.P
Raises interrupt level above that for buffer I/O.
...\" 
.LI
\*Lvoid osi_splx(int)\*O
.P
Lowers interrupt level.
...\" 
.LI
\*Losi_spl_notify(opaque)\*O
.P
Wakes up anyone sleeping on this address.
...\" 
.LI
\*Losi_spl_wait(opaque)\*O
.P
Waits on this address.
...\" 
.LI
\*Losi_spl_init(\|)\*O
.P
Initializes.
...\" 
.LE
...\" 
...\" 
.P
These functions or macros are used by the asynchronous event code to
provide mutual exclusion between the interrupt-time \*Lasevent_Iodone\*O
code and other asevent code that does not run at interrupt time. Their
names are historic and are unfortunate, since they imply a general
purpose spl mechanism. Actually, as defined here, they  will only work
correctly when called from the asevent code; they cannot be used
for general interrupt-time mutual exclusion. In singlethreaded kernels,
these tend to be calls to the regular \*Lspl(\|)\*O functions. In SunOS,
which is multithreaded, these tend to be calls to mutex functions.
.P
Note that \*Losi_Sleep(\|)\*O and \*Losi_Wakeup(\|)\*O calls cannot be used
here because they cannot be used in interrupt routines.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "osi_kvnode_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Losi_kvnode_mach.h\*O file should contain the definitions for various public
vnode manipulation functions and macros such as \*Lvn_open(\|)\*O, \*Lvn_create(\|)\*O,
\*LVOP_OPEN(\|)\*O, \*LVN_HOLD(\|)\*O, \*LVN_RELE(\|)\*O, etc.
.P
This is necessary because on several platforms the public files define these
functions only for the kernel. Parts of Episode are built in user space for
testing, and \*Losi_kvnode_mach.h\*O contains the definitions which allow this
build to take place.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "osi_device_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Losi_device_mach.h\*O file should contain definitions for the following:
...\" 
...\" 
.VL .5i
.LI
\*Lint osi_strategy(struct buf *)\*O
.P
Calls the appropriate driver strategy function, with appropriate locking.
...\" 
.LI
\*Lint osi_major(dev_t)\*O
.LI
\*Lint osi_minor(dev_t)\*O
.P
Returns major and minor number of device, respectively.
...\" 
.LI
\*Ldev_t osi_makedev(int maj, int min)\*O
.P
Makes device with the given major and minor numbers.
...\" 
.LI
\*Losi_inc_ru_oublock(n)\*O
.LI
\*Losi_inc_ru_inblock(n)\*O
.P
Adjusts the \*Lru_oublock\*O and \*Lru_inblock\*O resource usage statistics.
...\" 
.LE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "osi_user_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Losi_user_mach.h\*O file should contain:
...\" 
...\" 
.VL .5i
.LI
\*Llong osi_getufilelimit(\|)\*O
.P
Get limit on file size.
...\" 
.LE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "osi_ufs_mach.h"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Losi_ufs_mach.h\*O file should contain definitions for OSI wrappers for
various UFS functions used directly by the cache manager:
...\" 
...\" 
.VL .5i
.LI
\*Lint osi_iget(struct vfs *vfsp, long inodenumber, struct inode **ipp)\*O
.P
Looks up an inode with a given number and file system and return a pointer to
the inode in \*Lipp\*O.
...\" 
.LI
\*Lint osi_iput(struct inode *)\*O
.P
Decrements reference count of an inode.
...\" 
.LI
\*Lint osi_iupdat(struct inode *ip, int wait)\*O
.P
Flushes inode to disk. The \*Lwait\*O parameter indicates whether to wait for
the update or not (if 1, the routine waits).
...\" 
.LI
\*Lint osi_irele(struct inode *);\*O
.P
Unlocks an inode.
...\" 
.LI
\*Lint osi_ilock(struct inode *);\*O
.P
Locks an inode.
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "File Layout"
...\" ----------------------------------------------------------------------
...\" 
.P
All platform independent files are in the \*Lfile/osi\*O directory. Most platform
dependencies are in a subdirectory with that platform's system type (\*LHPUX\*O for
HP/UX, \*LRIOS\*O for AIX 3.x). However, there are some files in the \*Lfile/osi\*O
directory which contain some minor platform dependencies, mostly in the form of
\*L#ifdef\*Os for specific environments.
.P
C files in this directory contain platform independent code. Some of these files
are not necessary on all platforms, and as a result are not included in the default
Makefile in the \*Lfile/osi\*O directory. Should one of these files be necessary,
it should be compiled by \*VMACHINE_TYPE\*L/Makefile\*O.
.P
Header files defined in the \*Lfile/osi\*O directory are used throughout the DFS
code base. Each header file maintains the following structure:
...\" 
...\" 
.oS
    /* copyright */
    /* log */

    #ifndef <FILENAME_IN_CAPS>_H
    #define <FILENAME_IN_CAPS>_H

    /* machine independent includes go here */
    #include <dcedfs/param.h>

    /* machine independent definitions go here */

    /* Now include machine dependent additions if there are any */
    #include <dcedfs/<filename>_mach.h>

    /* machine independent function prototypes go here */

    #endif /* <FILENAME_IN_CAPS>_H */
.oE
...\" 
...\" 
.zZ "enh,2757,R1.1,Improved OSI Material"
...\" 
...\" ...END OF MATERIAL ADDED FROM 1.0.3a RELEASE NOTES.
...\" 
...\" FOLLOWING SECTION, PRE-1.1 (i.e., of 1.0.3) VINTAGE, SUPERSEDED BY THE
...\"  ABOVE MATERIAL...
...\" ----------------------------------------------------------------------
...\" .H 4 "OSI Code for Reference Platforms"
...\" ----------------------------------------------------------------------
...\" 
...\" .P
...\" The code which implements the OSI layer for the reference platforms resides
...\" in system-specific subdirectories of:
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/osi/\*O
...\" .DE
...\" .P
...\" It contains error code and syscall return mappings between the
...\" platform's operating system and the DFS model, as well as defines and
...\" include files for kernel and user functions, and a \*LMakefile\*O.
...\" When porting DFS to a new platform, expect to create an analogous
...\" subdirectory for your target operating system and populate it with
...\" files performing similar functions.
...\" .P
...\" Since the OSI layer involves interaction among various subcomponents
...\" of DFS, as well as other DCE functions and the host environment,
...\" the code in these subdirectories is not complete. The remaining
...\" system-specific OSI code is spread among various portions of the
...\" source, including the Cache Manager and other DFS subcomponents.
...\" These subcomponents are mentioned in the comment areas of the OSI
...\" source files, though the subcomponent listings may not be exhaustive
...\" for your system.
...\" .P
...\" You can \*Lgrep\*O for relevant symbols and macro names in the sources to
...\" be sure of capturing all mentions of those items. However, doing so will
...\" not necessarily identify all the locations where you may have to add
...\" OSI-related code to bring up DFS on your system.
...\" 
...\"	You can improve your grasp of what additional OSI changes may
...\"	be necessary by studying the differences among the reference
...\"	platform specific files.
...\" 
...\" ...END OF SUPERSEDED SECTION.
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS vnode and VFS Operations"
...\" ----------------------------------------------------------------------
...\" 
.P
When porting DFS to a new UNIX platform, you must port DFS extensions
to that platform's \*Lvnode\*O and \*LVFS\*O operations. To start with,
you must decide how to manage the \*Lvnode\*Os required for DFS files.
There are two methods for allocating \*Lvnode\*Os for DFS files,
namely private and public pools.
.P
If DFS has a private \*Lvnode\*O pool, it does not compete with other
kernel functions, because it has a protected quota of file and address
space. In addition, erroneous operations in DFS will not exhaust the kernel's
entire \*Lvnode\*O supply and cause a panic. However, with a private pool,
space allocation can be less efficient than with a public pool, since unused
\*Lvnode\*Os on either side of the DFS/non-DFS barrier are unavailable to the
other side.
.P
With a public \*Lvnode\*O pool, all kernel processes, including DFS,
tap the same supply of memory space and the same mechanism for accessing
it. This scheme offers the simplicity of a single \*Lvnode\*O allocation
mechanism, and the efficiency of not holding any \*Lvnode\*Os in reserve.
However, implementing this mechanism requires that you integrate the
operations of DFS and other kernel file handling functions precisely.
Also, your system must be able to adjudicate filesystem resource
contention gracefully, among disparate kernel functions.
.P
If you are porting to a platform that does not run UNIX, or where the
virtual file system differs significantly from the models used in
DFS, the work of building an interface between your system and DFS can
be considerable. In particular, the tradeoffs between public and private
\*Lvnode\*O pools can become significant.
.P
The DFS \*Lvnode\*O and \*Lvfs\*O operations are based on the SunOS
3.5 \*Lvnode\*O and \*Lvfs\*O models. In addition, DFS supports the extended
\*Lvnode\*O operations defined in the DCE Virtual File System Extension (VFS+)
interface.
...\" 
.nS "Note"
The specifications for VFS+ are available in the \*LDCE Technical
Supplement\*O.
.nE
...\" 
...\"	For platforms whose interface differs from VFS+, you must provide a
...\"	\*Vglue\*O layer that maps the native \*Lvnode\*O operations to the
...\"	DCE Distributed File System operations.
...\"	
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Source Files for vnode and VFS Operations"
...\" ----------------------------------------------------------------------
...\" 
.P
Source files for managing \*Lvnode\*Os and the kernel Virtual File
System reside in
.DS
    \*Vdce-root-dir\*L/dce/src/file/xvnode\*O
.DE
.P
and its subdirectories, which are specific to the reference ports.
All filenames are preceded with \*Lxvfs_\*O, denoting mappings to and
from the VFS+ \*Lvnode\*O interface, as well as the appropriate
\*LMakefile\*Os for building and linking the object.
.P
At the top level of the
.DS
    \*Vdce-root-dir\*L/dce/src/file/xvnode\*O
.DE
.P
directory, the include files define the VFS+ \*Lvnode\*O structures
(\*Lxvfs_vnode.h\*O) and operations (\*Lxvfs_genvnode.h\*O), as well
as the source code for manipulating the \*Lvnode\*Os and volumes.
It also includes a \*LMakefile\*O that incorporates specifics of the
operating system and the physical machine to produce the correct
executables.
.P
The platform-specific subdirectories provide the forward and reverse
mappings between the VFS+ and local file system \*Lvnode\*O
structures, and the (platform-specific) token synchronization
necessary to permit direct server access to its local DFS fileset(s),
as well as a \*LMakefile\*O specific to that platform's operating system.
.P
When you port DFS to your system, you will produce a new subdirectory
of
.DS
    \*Vdce-root-dir\*L/dce/src/file/xvnode\*O
.DE
.P
for your operating system, and either copy and modify, or write your own,
headers and source files to accomplish the same functions.
...\" 
...\"	 Makefile
...\"	 xvfs_genvnode.h
...\"	 xvfs_vnode.c
...\"	 xvfs_vnode.h
...\"	 xvfs_volume.c
...\"	
...\"	OSF1
...\"	 Makefile
...\"	 xvfs_osf2vfs.c
...\"	 xvfs_osfglue.c
...\"	 xvfs_osfvnode.h
...\"	 xvfs_vfs2osf.c
...\"	
...\"	RIOS:
...\"	 Makefile
...\"	 xvfs_aix2vfs.c
...\"	 xvfs_aixglue.c
...\"	 xvfs_aixvnode.h
...\"	 xvfs_vfs2aix.c
...\" 
.P
When reading the sample code, note that the \*Lvnode\*O mapping
functions for AIX are provided in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/xvnode/RIOS/xvfs_aix2vfs.c\*O
.DE
.P
This code is quite similar to the DFS VFS+ model.
...\" 
...\" 
...\" .P
...\" However, the VFS on the OSF/1 reference platform differs significantly
...\" from the DFS model, so extensive changes were required for that port.
...\" Creating the three sets of routines which serve as the mapping code
...\" for VFS+, namely
...\" 
...\" .BL
...\" .LI
...\" conversion from VFS+ to the OSF/1 \*Lvnode\*O model
...\" .LI
...\" the reverse conversion (from OSF/1 \*Lvnode\*Os to VFS+)
...\" .LI
...\" token synchronization code for local (server) access to DFS-exported files
...\" .LE
...\" 
...\" .P
...\" constituted a significant portion of the porting effort. You can use it as
...\" an indication of the type of work you need to do for your own port, and you
...\" can use the AIX reference port to indicate the basic structures of VFS+.
...\" .P
...\" The mapping functions for the OSF1 platform are provided in the
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/xvnode/OSF1\*O
...\" .DE
...\" .P
...\" subdirectory. They show the types and amount of changes needed to align
...\" kernel-level file management between DFS and an OSF/1-like UNIX system.
...\" 
...\" 
...\" 
.P
The files in
.DS
    \*Vdce-root-dir\*L/dce/src/file/xvnode/RIOS\*O
.DE
...\" 
.P
should be examined closely before porting to a new platform. Even where the code
is not applicable to your environment, it points out issues you will have to resolve.
Also, since you are strongly recommended to obtain a reference platform when you
undertake to port DCE, the code in these subdirectories lets you observe direct
interactions between the mapping functions and DFS.
.P
Additional code for \*Lvnode\*O and \*LVFS\*O operations can be found
in the Cache Manager
.DS
    \*Vdce-root-dir\*L/dce/src/file/cm/\*O
.DE
.P
directory, primarily in \*Lcm_vfsops.c\*O and \*Lcm_vnodeops.c\*O. Also, if
you port DCE LFS, you will have to examine the
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops/\*O
.DE
.P
subdirectory for \*Lvnode\*O and \*LVFS\*O code. Primarily, this code exists
in \*Lefs_vnodeops.c\*O and \*Lefs_vfsops.c\*O.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Notes From the DFS Port to the AIX Reference Platform"
...\" ----------------------------------------------------------------------
...\" 
.P
This section describes changes made to the DFS code in its original port, to
the AIX reference platform.
...\" 
...\" Since that platform's structures are close to the
...\" DFS model, the changes made represent a lower bound on the work you can expect
...\" to do to port to a different system.
...\" 
...\" 
.P
These notes provide sample solutions to problems involved with porting DFS.
To the extent that your system shares the features listed below with AIX, you
may be able to replicate or incorporate these changes into your port.
Conversely, where your system differs from AIX, you can identify these
items as points requiring attention as you develop your own code.
...\" 
.BL
.LI
The device switch (\*Ldevsw\*O) is not directly accessible by kernel
extensions in AIX. Instead, the font service is used to call the device
switch. This change is reflected in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/async/asevent.c\*O
.DE
.P
.LI
AIX requires that you provide an \*Liodone\*O routine through the
\*Vbuf struct b_iodone\*O field when calling the \*Ldevstrat\*O routine,
which provides access to the device switch. This requirement differs from
BSD, where a default \*Liodone\*O is provided if the \*Liodone\*O field is
NULL. This change is contained in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/async/asevent.c\*O
.DE
.P
.LI
AIX does not support the \*Lsplbio(\|)\*O function. This function has been
redefined as a macro which uses \*Li_disable\*O to disable interrupts to
\*LINTIODONE\*O.
.LI
Since the storage obtained for the kernel heap with \*Lmalloc\*O
is pageable, storage that is accessed during critical sections must be
pinned in physical memory by the \*Lpin(\|)\*O service, to prevent page
faulting with interrupts disabled. Specifically, this constraint applies
to the data buffers associated with \*Lbuf\*O structures in the asynchronous
I/O layer. Also, \*Lmalloc\*Os and \*Lfree\*Os should not be performed while
interrupts are disabled, since the AIX  memory management code may
take a page fault.
.LI
AIX does not support the \*Llookupname(\|)\*O interface.
Instead, it supports a similar interface, \*Llookupvp(\|)\*O.
.LI
You can use the \*Lvmount(\|)\*O system call to mount file systems other
than the native local file system (Journaled File System). This interface
allows passage of VFS-specific arguments to the VFS implementation.
To achieve this, a mounthelper which uses the \*Lvmount(\|)\*O call is
provided for AIX.
...\" 
...\" .nS "Note"
...\" The mounthelper is not coded yet. Instead, a simple command which calls
...\" \*Lvmount(\|)\*O has been written for testing. This command is expected
...\" to develop into the mounthelper (\*Lefsmnthelp\*O).
...\" .nE
...\" 
.LI
Because the AIX kernel is preemptible, the DFS kernel code is written with
the assumption that critical sections of code are protected by locks. The locks
are obtained by the Protocol Exporter at the start of \*LSAFS_\*O calls in
.DS
    \*Vdce-root-dir\*L/dce/src/file/px/px_intops.c\*O
.DE
.P
and by the Cache Manager via calls to the \*Llockl\*O and \*Lunlockl\*O
subroutines.
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
...\" .H 4 "Notes From the DFS Port to the OSF/1 Reference Platform"
...\" ----------------------------------------------------------------------
...\" 
...\" .P
...\" Additional changes were required to port to the OSF/1 platform; these may
...\" represent additional areas that should be considered when doing other ports:
...\" 
...\" .BL
...\" .LI
...\" DFS assumes specific calling sequences for a number of internal kernel
...\" interfaces. If your kernel interfaces differ, you will have to modify DFS to
...\" add conditional code for your target. For example, calls to the \*Luiomove\*O,
...\" and \*Lgetf\*O interfaces had to be changed for the OSF/1 port. Compare the
...\" AIX and OSF/1 versions in your source tape to get representative treatments
...\" of these interfaces. If you can modify the existing code to satisfy your
...\" requirements, you may be able to save time and effort over rewriting the
...\" interfaces.
...\" .LI
...\" DFS assumes specific calling sequences for system calls. The OSF/1 system
...\" call preamble differs from the AIX version; DFS system call codes needed to
...\" be adjusted as a result.
...\" .LI
...\" DFS assumes that it knows the names of many fields in kernel data structures,
...\" including \*Lu.u_error\*O and \*Lvnode\*O fields. On OSF/1, these structures
...\" differ from the DFS model. Therefore, mappings between these structures had to
...\" be implemented.
...\" .P
...\" Some of these operating system specific differences have been eliminated over
...\" previous DFS releases with OSI layer extensions. Consult the \*VOSF DCE Release
...\" Notes\*O for current information on code differences among the reference platforms.
...\" 
...\" 
...\" .zA "def,4844,R1.0.2,access control consistency"
...\" 
...\" .LI
...\" 
...\" Certain filesystem permissions checks above the \*Lvnode\*O layer may need
...\" to be deleted or changed to implement a consistent access control model
...\" in DFS that is based only on DCE identities (and not on local UNIX uids).
...\" 
...\" .zZ "def,4844,R1.0.2,access control consistency"
...\" 
...\" 
...\" 
...\" .LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Platform-Specific Files"
...\" ----------------------------------------------------------------------
...\" 
.P
The DFS reference ports contain a large number of files with
conditional code sections. To obtain a list of these files for the AIX platform,
\*Lgrep\*O each subdirectory of the
.DS
    \*Vdce-root-dir\*L/dce/src/file\*O
.DE
.P
tree for occurrences of \*LRIOS\*O, \*LAFS_AIX_ENV\*O, and \*LAFS_AIX31_ENV\*O.
Also, check for \*LAFS_AIX32_ENV\*O in the same directories.
...\" .P
...\" To obtain a list of these files for the OSF/1 platform, \*Lgrep\*O for
...\" occurrences of the macro \*LAFS_OSF_ENV\*O. You should examine these files
...\" and add \*L#ifdef\*O statements for your operating system, if necessary.
.P
As noted previously, you can generate a partial listing of code
locations that must be checked, simply by defining \*LAFS_DEFAULT_ENV\*O in
.DS
    \*Vdce-root-dir\*L/dce/src/file/config/\*VTARGET_MACHINE\*L/param.h\*O
.DE
.P
and building the code, using an ANSI-compatible C compiler. If you perform
the build on a reference platform, selecting \*LTARGET_MACHINE\*O to match
your platform, you will not need to make any other code modifications to
create the listing.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Fileset and Aggregate Operations"
...\" ----------------------------------------------------------------------
...\" 
.P
You will have to port the files in
.DS
    \*Vdce-root-dir\*L/dce/src/file/ufsops\*O
.DE
.P
to provide fileset and aggregate operations on your target platform.
(The ``\*Lufs\*O'' stands for UNIX File System, on which the DFS system is
modeled.)
.P
If your target platform does not run UNIX, or its native file system
differs significantly from the standard UNIX models, this porting
effort could be far more extensive. In that case, you should give serious
consideration to porting DCE LFS to substitute for your existing file system,
for purposes of time and compatibility with DFS and the remainder of DCE.
...\" 
...\" 
...\" 
...\" .zA "def,6733,R1.0.2,KRPC Helper porting info"
...\" ----------------------------------------------------------------------
.H 3 "DCE 1.0.2 Modifications to the dfsbind-Kernel RPC Helper Mechanism"
...\" ----------------------------------------------------------------------
...\" 
.P
Major changes have been made in DCE 1.0.2 to the \*Ldfsbind\*O-KRPC helper
subsystem, which will have a significant impact on those porting DFS. In
addition to architectural changes to the subsystem itself, the interface
to this subsystem has also been changed to use a pseudo-device driver model.
The changes made to the helper interface are not compatible with the
previously existing interface; therefore, kernel and user-space components
must be upgraded simultaneously.
.P
The following sections describe the new model, its interface, and various
porting considerations.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Overview"
...\" ----------------------------------------------------------------------
...\" 
.P
The kernel RPC (KRPC) helper mechanism, coupled with the user space \*Ldfsbind\*O
daemon, provides the means by which the DFS kernel acquires needed information
from user space, such as CDS and Security-related information.
.P
The \*Ldfsbind\*O daemon is a process which serves as the intermediary between
the DFS client kernel and user space servers. ``Upcalls'' from the DFS kernel
client to the user space \*Ldfsbind\*O are triggered, for example, whenever
such events as a lookup by name in the CDS namespace is requested, or when the
kernel RPC wishes to resolve a process's credentials from its PAG (Process
Access Group).
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Motivation for the Changes"
...\" ----------------------------------------------------------------------
...\" 
.P
Previously to DCE 1.0.2, \*Ldfsbind\*O was a single-threaded process.
Several problems were observed with this model, the most severe one being
that, since \*Vall\*O upcalls were being served by one single-threaded process,
the mechanism tended to become a bottleneck for processes which had to make
upcalls. The bottleneck became even more evident during cross-cell operations
or over a slow network link, where all local upcalls would be delayed while
a lengthy upcall (e.g., to another cell) was being serviced. These delays
were sometimes as long as two minutes, which is the timeout period for upcalls
to complete within.
.P
The new design reduces the likelihood that a process will experience an
unacceptable delay in service, by employing a threaded \*Ldfsbind\*O process
to service the requests and ensuring that all request types are fairly
serviced. The design uses one request queue in the kernel for upcalls
for all types of upcall requests. A dedicated listener thread of \*Ldfsbind\*O
fetches the requests (one request per fetch) and passes them on to its user space
service threads. The user space process maintains two queues, one for high
priority requests and one for the remaining requests. Each queue is allocated
a dedicated thread pool whose size is configurable via the \*Ldfsbind\*O
command line. These threads process the requests and return responses
back to the kernel.
...\" 
...\" 
...\" 
.P
Changes were made to both the user space \*Ldfsbind\*O daemon and the kernel
space \*Lkrpc_helper\*O code. In addition to the multi-threading of user space
and request prioritization, the interface between kernel and user space was
reworked. The new interface is file descriptor-based, using the \*Lread(\|)\*O
and \*Lwrite(\|)\*O system calls to fetch requests and send responses. This takes
advantage of the selecting which DCE Threads already performs for other I/O.
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "dfsbind Modifications"
...\" ----------------------------------------------------------------------
...\" 
.P
\*Ldfsbind\*O was modified to create a configurable number of service threads
for each of the two request pools, plus one dedicated listener thread. The
listener thread is responsible for fetching the upcall requests from the kernel,
while the service threads are responsible for executing the requests and
returning the responses back to the kernel.
...\" 
...\" 
.P
The get request and process request/send response functions, both of which
were performed in a single loop in \*Ldfsbind\*O in the previous implementation,
have been separated in the new model so that the listener thread now performs
only the get request operation, and the executor threads perform the process
request/send response operations. The listener thread fetches the next
upcall request from the kernel via the \*Lread(\|)\*O system call, passes the
request to the service thread pool, and wakes up any idle threads to begin
processing; the listener thread then goes on with its loop, to fetch another
request. The service threads sleep if the work queue is empty, waiting for a
wakeup from the listener thread to indicate that there is work to do; otherwise,
they process pending requests in the work queue, send the responses back
to the kernel via the write system call, and then await new requests.
.P
The \*Ldfsbind\*O program was modified to accept additional arguments to specify
the number of service threads to create for each of the high priority and no
priority queues, and to use a default value in the event that such arguments
are not supplied.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Data Structures"
...\" ----------------------------------------------------------------------
...\" 
.P
The new implementation maintains a work request queue for each request category,
and a shared free request list of upcall requests slots. The listener thread
places requests on the appropriate work request queue, and wakes up any idle
threads servicing that queue. Service threads remove a request from the queue,
service the request, and then replace the work request structure on the free
list for reuse.
.P
The work request structure has the following definition:
...\" 
.oS
    struct upcall_request {

        struct upcall_request *next;     /* next structure on queue   */
        long opcode;
        long size;
        char *bufptr;
        long tid;                        /* transaction ID            */
        char inbuf[AFSBIND_BUFSIZE];     /* buffer for request data   */
        char outbuf[AFSBIND_BUFSIZE];    /* buffer for response data  */
    }
.oE
...\" 
.P
The structure is defined in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/dfsbind/main_helper.c\*O
.DE
...\" 
.P
\*Ldfsbind\*O initially allocates a configurable number of these structures
for each service request queue and places them on the free list. Access to both
the work queues and the free list is synchronized via mutexes.
.P
Condition variables have also been created, one per queue type, which the
idle worker threads sleep on while waiting for something to do.
...\" 
.oS
    pthread_cond_t wcond;    /* work queue condition variable */
.oE
...\" 
.P
In addition to these new data structures, there were other data
structures in the \*Ldfsbind\*O daemon which had to be made thread-safe.
This included such things as synchronization for shared data structures
which should not be accessible by all threads.
.P
The \*Lauthhelper\*O portion of \*Ldfsbind\*O caches information regarding
process credentials in its own private context cache. This cache is
implemented as a small linear array of structures containing credential
information associated with process PAGs. A cache hit occurs when there
is a match between the process PAG and \*Leuid\*O and a cache entry with the
same PAG and \*Leuid\*O. Helper thread access to this array was
synchronized via locking. One lock is employed for the entire cache: helper
threads must acquire this lock before and relinquish it after accessing the
credentials cache.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "dfsbind Algorithms"
...\" ----------------------------------------------------------------------
...\" 
.P
The listener thread operates according to the following algorithm:
...\" 
.VL .5i
.LI "1."
Initialize work queues, free lists, mutexes, and create thread pools.
.LI "2."
Issue \*Lopen(\|)\*O to get a file descriptor for the \*Lkrpc_helper\*O subsystem.
.LI "3."
Issue \*Lioctl(\|)\*O to configure the \*Lkrpc_helper\*O subsystem.
.LI "4."
Issue \*Lioctl(\|)\*O to enable the \*Lkrpc_helper\*O subsystem.
.LI "5."
Loop forever:
.VL .5i
.LI "6."
Take a free request buffer from the free list; sleep if none are available.
.LI "7."
Issue the \*Lread(\|)\*O system call to obtain the next request.
.LI "8."
Set up work request based on the information obtained from kernel.
.LI "9."
Queue request into appropriate work queue.
.LI "10."
Wake up any sleeping work threads for that queue.
.LI "11."
End of loop.
.LE
.LE
...\" 
...\" 
.P
The executor threads operate according to the following algorithm:
...\" 
.VL .5i
.LI "1."
Perform initialization.
.LI "2."
Loop forever:
.VL .5i
.LI "3."
If work queue is empty, then \*Lpthread_cond_wait(free_thread_cond)\*O
.LI "4."
Dequeue request.
.LI "5."
Service request.
.LI "6."
Issue \*Lwrite(\|)\*O to send response back to kernel.
.LI "7."
Free request by placing it on the free request queue.
.LI "8."
End of loop.
.LE
.LE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Interface Changes"
...\" ----------------------------------------------------------------------
...\" 
.P
The interface into the \*Lkrpc_helper\*O subsystem has been changed from using
the old \*Lcm_pioctl(\|)\*O subfunctions and now uses instead a pseudo-device
driver interface. The new interface required the addition of the following
routines to the \*Lkrpc_helper\*O subsystem:
...\" 
...\" 
.BL
...\" 
.LI
\*Lclose(\|)\*O (\*Lkrpch_close(\|)\*O)
.P
Aborts all operations in progress and marks the system as closed, so that new
requests to \*Lkrpc_GetHelper(\|)\*O will fail.
...\" 
.LI
\*Lioctl(\|)\*O (\*Lkrpch_ioctl(\|)\*O)
.P
Provides a facility for configuring the size of the kernel request handle
queues; also activates the helper subsystem.
...\" 
.LI
\*Lopen(\|)\*O (\*Lkrpch_open(\|)\*O)
.P
Performs the one-time initialization operations for the \*Lkrpc_helper\*O and
kernel RPC subsystems.
...\" 
.LI
\*Lread(\|)\*O (\*Lkrpch_read(\|)\*O)
.P
Reads the next request from the pending queue and copies it into user space.
...\" 
.LI
\*Lselect(\|)\*O (\*Lkrpch_select(\|)\*O)
.P
Indicates whether there are any transactions on the pending queue.
...\" 
.LI
\*Lwrite(\|)\*O (\*Lkrpch_write(\|)\*O)
.P
Writes the response into the kernel request handle buffer with the matching
transaction ID.
...\" 
.LE
...\" 
...\" 
.P
The addition of the device driver interface requires that the host operating
system be configured to contain knowledge of the \*Lkrpc_helper\*O pseudo-device
driver. The \*Ldce_config\*O script was also modified to create the
\*Lkrpc_helper\*O device in the AIX port.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Changes to the Kernel Helper"
...\" ----------------------------------------------------------------------
...\" 
.P
The changes to the kernel helper involved replacing the state-machine request
queue with queues implemented as linked lists. This now allows requests to be
queued while the helper is busy or while it idles in user-space, rather than
delaying the queueing of the request until a helper is available to service it.
.P
The previous \*Lkrpc_helper\*O mechanism operated by maintaining an array of
request slots which were used by KRPC or DFS when they had to make upcalls to
user space. The \*Ldfsbind\*O process would begin (via a \*Lcm_pioctl(\|)\*O
\*LVIOC_GETREQUEST\*O call) by looking for an unused request slot, mark the
slot as \*LKRPC_HELPER_WAITING\*O, and sleep waiting for an upcall request.
Callers from KRPC or DFS would attempt to locate a slot where a \*Ldfsbind\*O
process was waiting, place its upcall request in that request slot, and
subsequently awaken the waiting \*Ldfsbind\*O process. When an upcall client
could not find a slot where a helper process was waiting it would sleep,
waiting for a helper to become available.
.P
This method was not possible in the new model, where processes do
not sleep in the kernel waiting for a request to arrive. As a result,
the algorithms which govern the interaction between the \*Ldfsbind\*O
process and kernel upcallers were modified. The new model employs three
lists: one for pending requests, one for requests currently being serviced,
and one free list. The major difference between the new model and the former
implementations is that many of the state transitions for the request slots
have been replaced with transfers of a request from one queue to another.
.P
The kernel will also ensure that a certain percentage of the request handles
are allocated to high-priority requests, so that the no-priority requests do not
consume all of the available request handles. This is accomplished when handles
are dispensed in \*Lkrpc_GetHelper(\|)\*O. The call to \*Lkrpc_GetHelper(\|)\*O
has been modified to take an opcode as an argument. If the specified opcode
represents a high priority request, then the \*Lkrpc_helper\*O subsystem will always
dispense a handle if there is one available; otherwise, if the opcode represents
a no-priority request, a handle will be dispensed only if the combined number
of free handles, plus the number of handles allocated to high priority requests,
exceeds 25 per cent of the total number of request handles.
.P
Each request receives a transaction ID, which is included in the byte
stream transferred between kernel and user space. This enables the \*Ldfsbind\*O
process to locate the proper request handle when it returns a response to the
kernel.
.P
A process desiring to make an upcall acquires a request slot from the free
list via the \*Lkrpc_GetHelper(\|)\*O routine. The process then fills in
the request slot with its data via \*Lkrpc_WriteHelper(\|)\*O, and subsequently places
the request on the pending request queue for processing via \*Lkrpc_ReadHelper(\|)\*O,
and then sleeps awaiting the response. At this time, the \*Lkrpc_ReadHelper(\|)\*O
function performs select event notification to inform DCE Threads that there is data
on this I/O channel. The \*Ldfsbind\*O process reads the request (indirectly)
via the \*Lkrpch_read(\|)\*O function, and subsequently hands it off to a user space
thread for service. The user space thread servicing the request transfers
the request back to the kernel, and awakens the sleeping process \*Lkrpc_write\*O.
After the kernel client process has transferred the response, it places the
request slot back on the free list via \*Lkrpc_PutHelper(\|)\*O.
.P
The implementation of this model required modification to the following routines,
all of which are found in:
.DS
    \*Vdce-root-dir\*L/dce/src/rpc/kruntime/krpc_helper.c\*O
.DE
...\" 
...\" 
.BL
...\" 
.LI
\*Lkrpch_close(\|)\*O
.P
A new routine. It interfaces with the host operating system's \*Lclose(\|)\*O
system call. It aborts any upcalls which are in progress and marks the
\*Lkrpc_helper\*O subsystem as closed.
...\" 
.LI
\*Lkrpch_ioctl(\|)\*O
.P
A new routine. It interfaces with the host operating system's \*Lioctl(\|)\*O
system call. Current options support: configurable number of kernel request handles,
and enabling of the KRPC subsystem (i.e., notification that the system is now
actively processing requests). The configuration of kernel request handles
can only occur during the initialization phase.
...\" 
.LI
\*Lkrpch_open(\|)\*O
.P
A new routine. It interfaces with the host operating system's \*Lselect open(\|)\*O
system call. It performs one-time initialization functions and sets the state
of the system to indicate that the \*Lkrpc_helper\*O subsystem is currently in
the initialization phase.
...\" 
.LI
\*Lkrpch_read(\|)\*O
.P
Replaces \*Lkrpc_ReadHelper(\|)\*O. This routine checks the pending request
queue and, if it finds a request, transfers the information to user space.
This routine places the request header on the in-service queue. It
will also return \*LEWOULDBLOCK\*O if a read is attempted with no data on the
pending queue. This supports the method used by DCE Threads for non-blocking
I/O on character special devices. The routine will assign a transaction ID
to the request handle so that it can be located by the \*Lkrpch_write(\|)\*O call.
...\" 
.LI
\*Lkrpch_select(\|)\*O
.P
A new routine. It interfaces with the host operating system's \*Lselect(\|)\*O
system call to indicate whether there are any requests on the pending queue.
...\" 
.LI
\*Lkrpch_write(\|)\*O
.P
Replaces \*Lkrpc_WriteHelper(\|)\*O. This routine searches the in-service queue
for the appropriate transaction ID, copies in the response, changes the state of
the response to \*LKRPC_HELPER_RESPREADY\*O, and subsequently wakes up any process
waiting for this particular transaction. The acknowledge-wait which was part of
the previous routine is no longer performed.
...\" 
.LI
\*Lkrpc_GetHelper(\|)\*O
.P
Acquires a request header from the free list, allocates
a buffer for the header, and returns to the caller. This routine will sleep
if the free list is exhausted or if the request is a no-priority request and the
low-water mark has been reached. If the helper subsystem is not active, this
routine will return NULL to its caller. The routine also takes a parameter
specifying the opcode that the caller will be using for the request.
...\" 
.LI
\*Lkrpc_PutHelper(\|)\*O
.P
Frees the buffer associated with the request header, resets
the flags field in the request header, places the request header on the free
list, and wakes up any processes waiting for a free request header.
...\" 
.LI
\*Lkrpc_ReadHelper(\|)\*O
.P
Places the request on the pending request queue if the state
flag in the buffer indicates that this has not been done already
(\*LKRPC_HELPER_REQREADY\*O). If the state flag in the request header indicates
that a response has not yet been returned (\*LKRPC_HELPER_RESPREADY\*O), then
the process will sleep; otherwise, it reads the response from the buffer.
This routine performs any operations necessary to provide select event
notification to any process which might be waiting when the request is
enqueued to the pending queue.
...\" 
.LI
\*Lkrpc_gc_helper(\|)\*O
.P
Searches the in-service queue for requests which have not received responses
after a given time period, and aborts them.
...\" 
.LE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Kernel Helper Glue Layer Code"
...\" ----------------------------------------------------------------------
...\" 
.P
There is a ``glue'' layer which lies between the \*Lkrpc_helper\*O driver
routines (\*Lkrpc_read(\|)\*O, etc.) and the host operating system device
driver interface (used by AIX). The purpose of this layer
is to map the native operating system device driver interface to the kernel
helper driver routines. In porting DFS to a new platform, you may have to
re-create these glue routines if your device driver interface does not match
the kernel helper device routines.
.P
The layer contains routines which support the following operations:
...\" 
.BL
.LI
\*Lclose(\|)\*O
.LI
\*Lioctl(\|)\*O
.LI
\*Lopen(\|)\*O
.LI
\*Lread(\|)\*O
.LI
\*Lselect(\|)\*O
.LI
\*Lwrite(\|)\*O
.LE
...\" 
.P
Note that \*Lselect(\|)\*O, unlike the other routines, does not have an
operating system-independent counterpart in:
.DS
    \*Vdce-root-dir\*L/dce/src/rpc/kruntime/krpc_helper.c\*O
.DE
.P
Thus you will have to implement the entire \*Lselect(\|)\*O operation yourself.
Note also that some of the routines in \*Lkrpc_helper.c\*O invoke the macro
\*LSELECT_NOTIFY\*O. You should define this macro in your machine dependent
header file (e.g., \*Lkrpc_helper_mach.h\*O) if your system needs to perform
any operations whenever a selectable event's status has changed. In the case
of \*Lkrpc_helper.c\*O, this would be whenever an upcall request is added to
the pending queue; when this happens, any process which might be waiting for
data has to be notified that data is available to read.
.P
The glue routines reside in the kernel helper machine dependent directory
...\" 
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/rpc/kruntime/OSF1_TEMPLATE/krpc_helper_mach.c\*O
...\" .DE
...\" 
...\" .P
...\" (for the OSF/1 reference platform), and
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/rpc/kruntime/RIOS/krpc_helper_mach.c\*O
.DE
...\" 
.P
for the AIX reference platform.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Modifications to the Data Byte Stream Format"
...\" ----------------------------------------------------------------------
...\" 
.P
Because the new \*Lkrpc_helper\*O has been integrated under the \*Lread(\|)\*O and
\*Lwrite(\|)\*O system calls, the 4-byte value which (in the previous version of
the mechanism) indicated the length of the data passed is no longer needed.
Instead, the first element of the data stream is now a 4-byte value representing
the transaction ID for the request. This element is followed by the actual data
for the request. When the response is returned, the data for the response will
also be preceded by the 4-byte transaction ID value.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Kernel Helper Modules Affected"
...\" ----------------------------------------------------------------------
...\" 
.P
The following modules required modification to implement the new design:
...\" 
.BL
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/rpc/kruntime/krpc_helper.h\*O
.P
Support for linked list queues was added.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/rpc/kruntime/krpc_helper.c\*O
.P
Support added for: linked list queues; transaction ID management; asynchronous
helper notification.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/rpc/kruntime/sec_authn_krpc.c\*O
.P
Opcode was added to \*Lkrpc_GetHelper(\|)\*O calls.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm_vdirent.c\*O
.P
Opcode was added to \*Lkrpc_GetHelper(\|)\*O call.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm_pioctl.c\*O
.P
\*Lcm_PReadHelper(\|)\*O and \*Lcm_PWriteHelper(\|)\*O were removed;
\*Lcm_InitHelper(\|)\*O was added.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/dfsbind/main_helper.c\*O
.P
Support was added for: creating and supporting multiple threads; helper
interface changes; and new command line options.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/security/helper/auth_helper.c\*O
.P
Support for synchronization of credential cache access was added.
...\" 
.LE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "User Interfaces"
...\" ----------------------------------------------------------------------
...\" 
.P
The number of \*Ldfsbind\*O helper threads is now configurable. As a result
new arguments have been added to \*Ldfsbind\*O. For further information on
the \*Ldfsbind\*O command line arguments, refer to the \*Ldfsbind(8dfs)\*O
reference page in the \*VOSF DCE Administration Reference\*O.
...\" 
...\" 
...\" 
...\" 
...\" .zZ "def,6733,R1.0.2,KRPC Helper porting info"
...\" 
...\" 
...\" 
...\" 
...\" 
...\" 
...\" 
...\" 
...\" .zA "Added DFS-to-UNIX information"
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Notes on Porting DFS to a UNIX Platform"
...\" ----------------------------------------------------------------------
...\" 
.P
The following sections describe some of the detailed low-level porting work
required to make DFS run on and in a UNIX-like Operating System. Note that
DFS LFS is not discussed here.
.P
The major areas of work in porting are:
...\" 
.VL 1.5i
.LI "OSI"
Operating System Independent layer. All calls from DFS into the native OS are
implemented here, either as macros or as subroutine calls.
.LI "xvnode"
The VFS-to-VFS+ mapping layer.
.LI "Miscellaneous"
Other topics.
.LE
...\" 
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "OSI (Operating System Independent Layer)"
...\" ----------------------------------------------------------------------
...\" 
...\" 
.P
The following items describe the porting work necessary for various modules
in the OSI layer.
...\" 
...\" 
...\" 
.BL
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/osi.h\*O
.P
This is the primary header which is included by source modules to provide
some degree of operating system independence. It attempts to provide an API
into the OSI layer, and macros. It is largely portable as is, although it
requires extensive review\(emyou must make sure it is doing the right thing
for your platform.
.P
In cases where it is doing the wrong thing, you are supposed to \*L#undef\*O
and then re-\*L#define\*O the offending macro in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/osi_port_mach.h\*O
.DE
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/osi_port_mach.h\*O
.P
You should clone this from the RIOS or other reference platform code. Modify as
necessary.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/osi_pag.c\*O
.P
This code is somewhat portable, but you must understand what it is doing.
The \*LPAG\*O value (undefined for now) has to be stored in the \*Lucred\*O
structure somewhere. The RIOS platform inserts this longword value into
the group list in the \*Lucred\*O structure, and then intercepts calls such as
\*Lsetgroups(2)\*O and \*Lgetgroups(2)\*O to make this transparent to user space.
...\" The OSF/1 platform defines a new field in the \*Lucred\*O, called \*Losi_pag\*O.
...\" This approach is simpler, if you have the flexibility to extend the native \*Lucred\*O.
...\" .P
...\" The \*LAFS_PAG_IN_CRED\*O in \*Lparam.h\*O, if \*L#define\*Od to 1, indicates that
...\" you took the OSF/1 approach.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/*\*O
.P
You must either clone or mix-and-match the platform-dependent
files in \*Losi\*O and port them to your platform. A general description follows:
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/sysincludes.h\*O
.P
The system-dependent headers for all modules in DFS. This file is included by
\*Losi.h\*O.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/osi_port_os.c\*O
.P
Any platform-dependent subroutines for \*Losi\*O for your platform go here. For example,
if \*Losi.h\*O defines a macro which is wrong for you, and which you must \*L#undef\*O in
\*Losi_port_mach.h\*O and cannot re-\*L#define\*O simply as a macro, you should implement
it as a subroutine here. See the RIOS code for examples.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/osi_fio.c\*O
.P
This module implements a set of \*Losi\*O calls for generic file I/O. Note especially
that the signature for \*Losi_Open\*O varies depending on whether or not \*LAFS_VFSCACHE\*O
is turned on in \*Lparam.h\*O. The RIOS platform currently turns on
\*LAFS_VFSCACHE\*O. The code for \*Losi_fio.c\*O may still show the non-\*LAFS_VFSCACHE\*O
code in an \*L#else\*O clause.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/osi/\*VTARGET_OS\*L/osi_\*Vxxx\*L_mach.h\*O
.P
These headers are included by the corresponding headers in \*L./osi/osi_\*Vxxx\*L.h\*O,
if the corresponding switch of the form \*LHAS_OSI_\*Vxxx\*L_MACH_H\*O has been turned
on in \*Lparam.h\*O. You include whatever platform-dependent \*L#define\*Os you need for
the appropriate facility in this header.
...\" 
.LE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "xvnode (Glue Code)"
...\" ----------------------------------------------------------------------
...\" 
...\" 
.P
This is the ``glue'' code, which mates the VFS+ interface that DFS uses to 
your operating system's native VFS interface. You will have to extensively
review these modules, and do something of a merge between the RIOS and other
reference platform code in order to arrive at the right code for your platform.
If your VFS interface derives from the SunOS VFS, you will find that many of the
operations are trivial pass-throughs to the next layer.
.P
DFS intercepts calls to \*LVOP_\*Voperation\*O in the glue code, by rearranging
the VFS operations in the \*Lvnode\*O operations vector and then marking the
\*Lvnode\*O ``converted''. This is generally transparent to the host operating
system. The VOP call through VFS at the VFS switch is intercepted by the glue
code, and eventually the glue code calls through to the native VFS that handles
the local file system. 
.P
However, there is a potential problem with this scheme. If a native VFS file
system turns around and tries to call through the \*Lvnode\*O operations vector
a second time, DFS will again intercept this call in the glue code and as a
result will deadlock. The problem is that you cannot call through the glue
twice in the same call stack. In practical terms, this means that any code
of the form \*LVOP_\*Voperation\*O which appears in a \*Lvnode\*O operation
in a native local file system must be recoded.
...\" OSF/1 1.1 licensees have access
...\" to the OSC1.1-DFS porting modifications which may show this type of change in
...\" the OSF kernel. The \*Lxafs_writeop(\|)\*O and \*Lafs_readop(\|)\*O routines
...\" in \*Lxvnode/OSF1/xvfs_vfs2osf.c\*O are part of the workaround to this problem.
...\" 
...\" 
.BL
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/xvnode/xvfs_vnode.h\*O
.P
You must look for any VFS operations that may have different signatures
than expected, and modify the macros accordingly. You also have to allocate
a \*LV_CONVERTED\*O bit in your \*Lvnode\*O flags field.
...\" the default is 0x1000.
You must make sure this bit is not already in use in your \*Lvnode\*O flags field.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/xvnode/xvfs_vnode.c\*O
.P
You may need to write code in \*Lxvfs_InitFromXFuns(\|)\*O and \*Lxvfs_InitFromOFuns(\|)\*O
that properly initializes your extended \*Lvnode\*O operations vector. However, the
\*Lbcopy(\|)\*O calls under \*L#ifdef AFS_DEFAULT_ENV\*O should port to most platforms,
making this module easy to port.
.P
The use of the extended \*Lvnode\*O operations vector is described in block comments above
\*Lxvfs_InitFromXFuns(\|)\*O and \*Lxvfs_InitFromOFuns(\|)\*O.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/xvnode/\*VTARGET_OS\*L/xvfs_osglue.c\*O
.P
This is the glue code, which implements native VFS operations in terms of the native
VFS interface. It is called to ensure that local access to exported file sets
goes through the token mechanism.
.P
The porting work here is basically to copy the code from one of the reference platforms,
but account for differences between your native VFS and that platform's native VFS
interface. Signatures may differ, and some operations may be extra or missing.
You will probably want to refer to different reference platforms' code for various
operations.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/xvnode/\*VTARGET_OS\*L/xvfs_os2vfs.c\*O
.P
This module implements native VFS operations in terms of the extended VFS+ operations.
In the simpler cases, this code just calls through to the corresponding VFS+ operation.
In some cases, signatures of routines differ slightly, and this layer maps the
parameters supplied to the correct arguments to the corresponding VFS+ call.
In more difficult cases, there may be missing operations from one or the other interface.
Then you must implement the VFS operation in terms of several VFS+ operations.
...\" .P
...\" The OSF/1 reference platform has very different signatures for the corresponding
...\" calls in VFS and VFS+, so the RIOS code may be closer to what you need.
.P
The use of this code, and how it is called, is described in block comments in
\*Lxvfs_vnode.c\*O.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/xvnode/\*VTARGET_OS\*L/xvfs_vfs2os.c\*O
.P
This module implements VFS+ operations in terms of native VFS operations. As in
\*Lxvfs_os2vfs.c\*O, signature mismatches, or even missing operations, may cause
more porting difficulty.
.P
Note that the RIOS calls to \*LSwapCred(\|)\*O and \*LRestoreCred(\|)\*O are used to
manage the credential which appears in the \*Luarea\*O. The problem is that native
RIOS VFS does not allow passing a credential through the interface, so these operations
must save the existing credential, install the proper one, call through VFS, and restore
the original credential on the way out. You can safely ignore all this if you can pass a
credential through your VFS.
.P
Note also that \*Lxufs_getattr(\|)\*O requires non-trivial porting.
...\" 
.LE
...\" 
...\" 
...\" 
...\" 	
...\" 	
...\" ----------------------------------------------------------------------
.H 4 "Miscellaneous Topics"
...\" ----------------------------------------------------------------------
...\" 
...\" 
.P
The following items describe the considerations involved in porting various
miscellaneous DFS modules to a UNIX platform.
...\" 
.BL
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/afsd/afsd.c\*O
.P
The \*Lmount(2)\*O call must be checked.
...\" (\*L#error\*O).
This is where the DFS filesystem is mounted (usually on \*L/...\*O).
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/ufsops/ufs.h\*O
.P
The macros for \*LVOL_UFSFIRSTINODE\*O and \*LUFSMAXINO\*O should be reviewed
for accuracy on your UFS implementation.
.P
There are also other \*L#ifdef\*Os in this module that need review.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/ufsops/ufs_volops.c\*O
.P
This is where the extended \*LVNODE\*O operations are implemented for UFS.
The current state of this code produces \*L#error\*O and/or runtime calls to
\*Lpanic(\|)\*O for the operations which are difficult to implement in UFS,
such as \*Lvol_ufsScan(\|)\*O, \*Lvol_ufsDelete(\|)\*O, and
\*Lvol_ufsCreate(\|)\*O.
.P
However, these operations are not called for in a rough port of DFS, so
the \*L#error\*Os can be removed and a rough port should work acceptably.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/ufsops/ufs_agops.c\*O
.P
You should review \*Lag_ufsVolInfo(\|)\*O; you may need to supply a line or
two of code in order to make \*Lgetmount(2)\*O work. As is, the code produces
an \*L#error\*O.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/kutils/syscall.c\*O
.P
This module implements \*Lafs_syscall(\|)\*O, which is the special syscall into DFS.
You may have to adjust the signature for this routine to match the syscall calling
conventions for your platform.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm.h\*O
.P
You must \*L#define CM_VGET\*O, \*LCM_GETMOUNT\*O as appropriate for your platform.
\*LCM_VGET\*O is defined if your VFS operations include a \*LVGET\*O operation.
\*LCM_GETMOUNT\*O is defined if your VFS operations include a \*LGETMOUNT\*O operation.
.P
Also, you should check that \*LCM_MAXFIDSZ\*O is a reasonable value for your platform.
It should be less than or equal to the maximum FID size allowed by your NFS server code.
...\" This is related to the NFS Exporter.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm_scache.c\*O
.P
\*Lcm_NewSCache(\|)\*O may require a line or two of code related to allocating a new
\*Lvnode\*O for an \*Lscache\*O entry.
.P
You may have to alter \*LFlushSCache(\|)\*O for VM integration:
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm_subr.c\*O
.P
\*Lcm_FlushText(\|)\*O may require work. Also, \*Lcm_Active(\|)\*O, \*Lcm_FlushPages(\|)\*O.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm_vfsops.c\*O
.P
This module currently requires extensive review during porting.
...\" It should be cleaned up by OSF, however.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/cm/cm_vnodeops.c\*O
.P
This module currently requires extensive review during porting.
...\" It should be cleaned up by OSF, however.
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/config/\*VTARGET_MACHINE\*L/param.h\*O
.P
This header needs to define a few global switches, such as:
...\" 
...\" 
.VL 1.5i
.LI "\*LAFS_DYNAMIC\*O"
Turn this switch on if you are dynamically loading DFS as kernel extensions. You
should probably consider porting first to a monolithic, fully bound DFS kernel,
and turn this switch on later.
.P
Currently only RIOS turns this switch on.
...\" 
.LI "\*LAFS_VFSCACHE\*O"
Turn this switch on if you want UFS independence in the Cache Manager. Usually
it is easier to not turn this switch on for an initial port.
.P
Currently RIOS turns this switch on.
...\" 
.LI "\*LAFS_PAG_IN_CRED\*O"
Refer to porting notes on \*Losi_pag.c\*O.
...\" .P
...\" Currently only OSF/1 turns this switch on.
...\" 
.LE
...\" 
...\" 
.P
You must also assign a syscall number (\*LAFS_SYSCALL\*O) for the DFS syscall.
Use an unused syscall number in your existing kernel, or allocate a new one.
...\" 
.P
Note that the \*Lsetpag(\|)\*O call in \*Llibdce.a\*O must be a DFS syscall
in order for DFS to work correctly.
...\" 
...\" 
.LE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Porting the DFS/NFS Gateway Server"
...\" ----------------------------------------------------------------------
...\" 
.P
The DFS/NFS gateway server is the portion of the gateway that runs on a DFS
client and translates NFS requests into DFS requests. There are three components
of the gateway server:
...\" 
.BL
.LI
A kernel interface which handles incoming NFS requests.
.LI
\*Ldfsgwd\*O, which maintains the authentication translation table for use
by the kernel interface.
.LI
\*Ldfsgw\*O, which is the command line interface used on the gateway server to
manipulate the authentication table maintained by \*Ldfsgwd\*O.
.LE
...\" 
.P
Refer to the \*VPSF DCE DFS Administration Guide and Reference\*O for more
information about the gateway functionality.
...\" 
...\" 
.nS "Note"
For DCE 1.1, only the HP-UX 9.03 kernel contains support for the DFS/NFS Gateway
Kernel Interface. The AIX kernel does not support the needed kernel interface, so
the translator was not tested on the AIX platform.
.nE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Gateway Kernel Interface"
...\" ----------------------------------------------------------------------
...\" 
.P
The gateway kernel interface intercepts incoming NFS requests, uses the UID
and host address in the request to lookup credentials in an in-memory table,
and grants (or refuses) permission for operations accordingly. Operation of
the interface requires a hook to be inserted into the NFS server code through
which the translation routine will be called. The translation routine,
\*Ldfs_translate_creds(\|)\*O, has the following signature:
...\" 
.oS
    int at_translate_creds(struct sockaddr *addr, uid_t uid, struct ucred *cred);
.oE
...\" 
.P
A possible code sequence that could be found in the NFS server code might be
as follows:
...\" 
.oS
    /* kernel module nfs/nfs_server.c...                               */

    /* Here's the kernel NFS code hook...                              */ 
    int (*dfs_translate_creds)();

    /* In checkauth case for AUTH_UNIX...                              */

    /* HP_DFS - Pick up DFS credentials.                               */
    /* If the hook has been initialized, the call will occur...        */

    if ((fsid == VFS_AFSMAGIC) && (dfs_translate_creds))
    {
        if ((*dfs_translate_creds)((struct sockaddr*)caller_addr,
            caller_uid, cred) == 0)
        {

            /* No authentication found - invalidate user/group ID...   */

            caller_uid = anon;
            caller_gid = anon;
        }
    }
.oE
...\" 
.P
You will have to ensure that your machine's NFS server incorporates this hook
and lookup mechanism when it is processing requests for access to DFS filesystems.
.P
The definition of the hook appears in the NFS server code. However, it is
initialized in the DFS client kernel code in the routine \*Lcm_post_config(\|)\*O:
...\" 
.oS
    cm_post_config()
    {
        xdfs_vn_rele_op = xglue_vn_rele;
        xglue_cm_dcache_delete_op = cm_dcache_delete;

        /* Link credential translator mechanism...                     */
        dfs_translate_creds = at_translate_creds;  

        /* Add NFS Xlator syscall to afs_sysent table...               */
        at_configure();           
        return 0;
    }
.oE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Gateway Server Daemon Well Known Port"
...\" ----------------------------------------------------------------------
...\" 
.P
The gateway server daemon has been assigned a well known port by the IANA to
listen for client login/logout requests on. The port number is 438, and is
defined in
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/file/gateway/dfsgwd/dfsgw.h\*O
.DE
...\" 
.P
If you need to override this value for your site, you should either change
the contents of this file or place an entry in your \*L/etc/services\*O file
(or its equivalent) on the machines running the gateway.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Porting the DFS/NFS Gateway Client"
...\" ----------------------------------------------------------------------
...\" 
.P
The gateway client is a Kerberos client that is not dependent on any of the
DCE services. It uses Kerberos routines to contact the DCE security server
and authenticate the user.
.P
You will need to make sure that you have a compiled Kerberos Version 5 Beta 4
(\*Lkrbv5b4\*O) source tree on your platform in order to successfully use
the gateway client. Information about Kerberos can be obtained via anonymous
ftp from:
...\" 
.DS
    \*Lathena-dist.mit.edu\*O
.DE
...\" 
.P
Kerberos is provided from the Massachusetts Institute of Technology, and is
not supported by OSF.
.P
To build \*Ldfs_login\*O and \*Ldfs_logout\*O, you must copy the following
files:
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/file/gateway/dfs_login/*\*O
    \*Vdce-root-dir\*L/dce/src/file/gateway/dfsgwd/dfsgw.h\*O
    \*Vdce-root-dir\*L/dce/src/file/gateway/dfsgwd/sendrcv.h\*O
    \*Vdce-root-dir\*L/dce/src/file/gateway/dfsgwd/tgt_pack.h\*O
.DE
...\" 
.P
from the DCE source tree into
...\" 
.DS
    \*Lsrc/clients/dfs_login\*O
.DE
...\" 
.P
in the Kerberos source tree and build them there.
.P
Consult the
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/file/gateway/dfs_login/README\*O
.DE
...\" 
.P
file for more information on building the \*Ldfs_login\*O command.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 2 "DCE Local File System (LFS)"
...\" ----------------------------------------------------------------------
...\" 
.P
DCE LFS provides a token-based (logging) local file system which supports
POSIX Access Control Lists (ACLs), fileset replication, and remote cell
ownership of local files. It has been integrated with DFS and other DCE
components, and lets you take full advantage of DCE's integrated nature.
.P
You can build DCE LFS without having the other DCE components in place.
Therefore, you can port DCE LFS in parallel with porting the other DCE
components, whereas DFS cannot be ported until the components listed
under ``Considerations and Dependencies'' at the start of this chapter
have been brought up.
.P
Porting DCE LFS is a simpler task than porting DFS. However, since DCE LFS
deals with some of the same data structures and procedures as DFS, porting
DCE LFS provides a good introduction to porting DFS.
.P
The following sections provide guidelines for porting DCE LFS to new
platforms.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Relationships with Other DFS Subcomponents"
...\" ----------------------------------------------------------------------
...\" 
.P
For AIX, DCE LFS requires the subcomponents of DFS included in the
\*Ldfscore.ext\*O kernel extension, and the
.DS
    \*Vdce-root-dir\*L/dce/src/file/security\*O
.DE
.P
subdirectories. \*Ldfscore.ext\*O consists of the following libraries:
...\" 
.ne 2i
.TS
center;
lBe lBe lBe.
libagfs.a	libkicl.a	libkxcred.a
libaggr.a	libkolddacl.a	libkzlc.a
libaixexport.a	libkosi.a	libufsops.a
libkdacl.a	libktkc.a	libvolreg.a
libkdacllfs.a	libktkm.a	libvolume.a
libkdfskutil.a	libktpq.a	libxvnode.a
.TE
...\" 
.P
These components should be ported prior to porting the
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode\*O
.DE
.P
subdirectories.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DCE LFS vnode and VFS Operations"
...\" ----------------------------------------------------------------------
...\" 
.P
Like their counterparts in DFS, the DCE LFS \*Lvnode\*O and \*LVFS\*O
operations are based on the SunOS 3.5 \*Lvnode\*O and \*LVFS\*O models.
Code should be added to accommodate variations from this model. For example,
for the AIX operating system port, changes were required to the \*Lmount\*O
VFS operation. As noted in the previous (DFS) description about porting
\*Lvnode\*O and \*LVFS\*O operations, these changes are contained in
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops/efs_vfsops.c\*O
.DE
.P
and are within conditionally compiled code, using the \*LAFS_AIX31_ENV\*O
compiler constant. The changes primarily involve setting and reading
additional fields in the VFS structure. Similarly,
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops/efs_vnodeops.c\*O
.DE
.P
codes the DCE LFS handling of \*Lvnode\*Os.
...\" 
...\" .nS "Note"
...\" When you port DCE LFS to your platform, you must resolve differences
...\" in supported kernel interfaces. Since the AIX interface is quite close
...\" to the DCE LFS model, it is a good baseline for evaluating differences
...\" in kernel interface handling.
...\" .nE
...\" 
There are several AIX-specific sections in:
.DS
    \*Vdce-root-dir\*L/dce/src/episode/vnops/efs_misc.c\*O
.DE
.P
These sections account for the \*Lgnode\*O abstraction in the AIX VFS
implementation and the absence of block \*Lvnode\*O operations.
.P
In the \*Lvnax_CanAccess(\|)\*O function, located in
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/vnops/efs_access.c\*O
.DE
.P
an \*Lifdef\*O section is added that shifts the mode bits up six places.
This shift moves them from the ``others'' position to the ``user'' position,
as found in BSD. The same file also contains AIX-specific sections to account
for the larger number of supported groups.
...\" 
...\" 
...\" 
...\" .zA "def,7013,R1.0.2,agfs porting info"
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Porting the LFS Aggregate Filesystem Code"
...\" ----------------------------------------------------------------------
...\" 
.P
Traditional UNIX filesystems depend on periodic execution of the \*Lsync(2)\*O
system call to cause them to flush buffered data to permanent storage. The
\*Lsync(2)\*O system call works by traversing the list of \*Vmounted\*O
filesystems and executing each one's \*Lsync\*O VFS operation.
.P
Usually a \*Lsync(2)\*O is done by a system daemon every 30 seconds or so;
the actual time depends on the target operating system. The difficulty is
that LFS filesets are not mounted, in the usual sense of the word. They
have no local mount-point, and (more to the point) they are not in the
system's table of mounted filesystems, thus giving \*Lsync(2)\*O no chance
to find them. If they are not \*Lsync\*O'd, the LFS filesets build up
buffered data which is infrequently committed to
permanent storage. This causes serious problems if the machine crashes,
since all the buffered updates will be lost. The periodic \*Lsync\*Os
performed by the operating system are an attempt to limit such loss to
the last 30 seconds' worth of updates, but this is of no help to LFS since,
as we have seen, LFS filesets will not be \*Lsync\*O'd this way.
...\" 
...\" ----------------------------------------------------------------------
.H 4 "How the Aggregate Filesystem Code Works"
...\" ----------------------------------------------------------------------
...\" 
.P
The aggregate filesystem (\*Lagfs\*O) solves this problem by adding an entry
to the system's mount table for each attached LFS aggregate. The \*Lagfs\*O
is a simple virtual filesystem whose sole purpose is to flush its filesets'
buffered data to permanent storage whenever it receives a \*Lsync\*O request
from the operating system. Only a few of the VFS and \*Lvnode\*O operations are
implemented by \*Lagfs\*O. Specifically, only those that need to exist in
order to \*Lmount\*O, \*Lunmount\*O, \*Lstat\*O, and \*Lsync\*O the filesystem
are implemented.
.P
The aggregate mounting is done in user space immediately after an aggregate
is attached. The user space code (\*Lftu_AttachAggrWithAstab(\|)\*O) makes
sure that the intended mount point, which is currently:
.DS
    \*Vdcelocal\*L/var/dfs/aggr/\*Vaggrname\*O
.DE
.P
exists and then mounts the aggregate using VFS type \*LMOUNT_AGFS\*O. Only LFS
aggregates are mounted at this point, since UFS aggregates are implicitly mounted
and do not need \*Lagfs\*O in order to \*Lsync\*O their buffered data.
.P
In order to accommodate the mount request, the \*Lagfs\*O (kernel code) needs
to maintain a small pool of \*Lvnode\*Os. Vnodes from this pool used as root \*Lvnode\*Os,
one for each mounted aggregate.
.P
A side effect of having the \*Lagfs\*O is that attached aggregates now show up
in the output of the \*Ldf(1)\*O command.  The numbers given for ``kbytes'',
``used'', and ``free'' are: the size of the aggregate; the space used by all
filesets on the aggregate; and the amount of remaining space on the aggregate,
respectively.
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Considerations When Porting the Aggregate Filesystem Code"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Lagfs\*O is a largely OS-dependent piece of code. Specifically, it
requires OS-specific solutions for initialization, VFS and \*Lvnode\*O operation
definition, and \*Lvnode\*O pool management.
.P
The \*Lagfs\*O needs to be initialized by a call to \*Lagfs_Init(\|)\*O. Under
AIX, this is accomplished by using \*Lagfs_Init(\|)\*O as the initialization
routine when constructing the \*Lgfs\*O structure for \*Lagfs\*O at extension
(\*Ldfscore.ext\*O) initialization time.
...\" Under OSF/1, the \*Lvfsops\*O structure
...\" contains a slot for an initialization routine, so the \*Lagfs\*O is initialized
...\" when calling \*Lvfssw_add(\|)\*O in \*Lpx_configure(\|)\*O. 
Most operating
systems require slightly different sets of VFS and \*Lvnode\*O operations, so the
definitions of these structures will have to be tailored to meet the target
operating system's requirements.
.P
Finally, the definition and management of \*Lvnode\*Os differs greatly among various
UNIX operating systems. Different operating systems define \*Lvnode\*Os to contain
various idiosyncratic fields and flag values. Care must be taken to initialize
the correct fields and set the right flags when preparing a \*Lvnode\*O for use.
Another major difference is that some operating systems require the use of
private \*Lvnode\*O pools while others encourage the use of a single, public \*Lvnode\*O
pool. For example, under AIX, each VFS manages a private pool of \*Lvnode\*Os.
...\" , while
...\" OSF/1 provides a public pool. In both cases, however, 
The \*Lagfs\*O manages
its own, private pool of \*Lvnode\*Os. This is implicit under AIX.
...\" , but under OSF/1
...\" the \*LV_PRIVATE\*O \*Lvnode\*O flag is taken advantage of to tell the operating
...\" system that \*Lagfs\*O \*Lvnode\*Os are privately managed.
...\" 
...\" 
...\" 
...\" 
...\" 
...\" .zZ "def,7013,R1.0.2,agfs porting info"
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Kernel Configuration Considerations"
...\" ----------------------------------------------------------------------
...\" 
.P
On traditional UNIX implementations, the DCE LFS libraries must be linked into
the UNIX kernel. For DCE LFS on the AIX reference platform, a \*Ldcelfs.ext\*O
extension was created. This extension consists of the DFS subcomponent libraries
needed to build DCE LFS and \*Lconfig\*O procedures that are called when the
\*Lsysconfig(\|)\*O call is used to load and initialize the file system
extension. The \*Lconfig\*O procedures register DCE LFS and initialize the
file system components. The AIX \*Lconfig\*O procedures for DCE LFS are contained
in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/libefs/RIOS/efsconfig.c\*O
.DE
.P
In addition, the AIX \*Lcfgefs\*O command, is needed to make the \*Lsysconfig\*O
call load the kernel extension. The source file for this command is:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/libefs/cfgefs.c\*O
.DE
.P
The \*Lafscall_episode\*O system call is made available by exporting it in
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/libefs/RIOS/efs.exp\*O
.DE
.P
which is provided as input to the linker when the \*Ldcelfs.ext\*O extension is
built.
...\" .P
...\" For OSF/1, a kernel including DCE LFS is built by linking in the relevant
...\" kernel libraries. The \*LMakefile\*O in
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/file/libefs/OSF1\*O
...\" .DE
...\" .P
...\" builds a kernel including DCE LFS, called \*Lsefsvmunix\*O, or a kernel
...\" including both DFS and DCE LFS, called \*Lefsvmunix\*O.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 2 "Building and Linking DFS"
...\" ----------------------------------------------------------------------
...\"	
...\" 
.P
The
.DS
    \*Vdce-root-dir\*L/dce/src/file\*O
.DE
.P
directory contains the source code for building DFS.
.P
The compiler flags for building DFS can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/\*VTARGET_MACHINE\*L/machdep.mk\*O
.DE
.P
This file contains machine-specific compiler flags that affect compilation of
the whole component. Any machine-specific libraries that must be used to link
binaries should be set in this file.
.P
At this time, no object modules are archived into \*Llibdce.so\*O.
.P
The C flags for the DFS test cases in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file\*O
.DE
.P
directory are also set by including
.DS
    \*Vdce-root-dir\*L/dce/src/file/\*VTARGET_MACHINE\*L/machdep.mk\*O
.DE
.P
DFS uses the \*Llibdce.so\*O global library to resolve many subroutines
from other components.
.P
For a listing of directories where libraries and executables are built,
see the table of DFS File Locations at the beginning of this chapter.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 2 "Setup, Testing, and Verification"
...\" ----------------------------------------------------------------------
...\" 
.P
Since DFS interacts with various other DCE components, functional testing for
it is  not necessarily simple, particularly with a port of DCE to a new platform.
The detailed operation of other DCE components may not be known, and there will
not be a baseline of component behavior under different conditions of usage and
loading. Therefore, testing interactions between DFS and the other components
may indicate a need for modifications in those other components as well as DFS,
and necessitate a cyclical or incremental approach to functional testing, as well
as system test.
.P
When you start testing DFS, a reference platform is particularly useful, since
the code on it has been tested to known standards of functionality and robustness.
In addition, the reference platform lets you address interoperability issues with
a partner that works correctly.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
...\" .zA "dcetest_config information added"
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Installing DFS Functional Tests with dcetest_config"
...\" ----------------------------------------------------------------------
...\" 
.P
You can install the functional tests described in the following sections
by running the menu-driven \*Ldcetest_config\*O script described in Chapter
13 of this guide. \*Ldcetest_config\*O will install the tests you select at
the path you specify, and will create a softlink (called \*L/dcetest/dcelocal\*O)
to that location. The functional tests for a given component will thus be
installed under a:
.DS
    \*L/dcetest/dcelocal/test/\*Vcomponent_name\*L/\*O
.DE
...\" 
.P
directory, where the \*Ltest/\*Vcomponent_name\*O elements of this path are
equivalent to the \*Ltest/\*Vcomponent_name\*O elements in the pathnames given in
the sections below, which refer to the tests' source or build locations.
.P
Note that \*Ldcetest_config\*O will prompt you for the location \*Vfrom which\*O
the tests should be installed (in other words, the final location of the built
test tree). For the DFS functional tests, this path should be the location, on
your machine, of:
.DS
    \*Vdce-root-dir\*L/dce/install\*O
.DE
...\" 
...\" 
.P
\(emwhich is the DCE \*Linstall\*O tree (for more information on the structure
of the DCE tree, see Chapter 12 of this guide).
.P
Thus, \*Ldcetest_config\*O will install the DFS functional tests at:
.DS
    \*L/dcetest/dcelocal/test/file/\*O
.DE
...\" 
.P
where \*L/dcetest/dcelocal\*O is the link to whatever path you supplied as
the install destination. 
.P
The advantage in using \*Ldcetest_config\*O to install the functional tests
is that it will install \*Vall\*O that is needed and \*Vonly\*O what is needed
out of the DCE build, thus avoiding the mistakes that can occur with manual
installation.
.P
Note that you can only \*Vinstall\*O (if you choose) functional tests with
\*Ldcetest_config\*O; for test configuration and execution you must follow
the instructions in the sections below.
.P
Refer to Chapter 13 of this guide for further information on using
\*Ldcetest_config\*O.
...\" 
...\" 
...\" .zZ "dcetest_config information added"
...\" 
...\" ----------------------------------------------------------------------
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Debugging Notes"
...\" ----------------------------------------------------------------------
...\" 
.P
DFS involves the interaction of many different programs, which operate on
different machines (servers, clients) in both kernel and user space. It uses
the services of various other DCE components, such as RPC, Threads, DTS and
Security. It also uses the services of non-DCE components, such as the native
file services of at least one and possibly more host platforms.
.P
Therefore, porting DFS to a new platform presents a broad set of challenges.
The subcomponents must be built and integrated in a distributed and possibly
heterogeneous environment, interactively with other development efforts.
Porting and development work in different areas of DCE can proceed
asynchronously, and the DFS port effort must bridge changes in the
software environment.
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Running Tests on the HP/UX Platform"
...\" ----------------------------------------------------------------------
...\" 
.P
Note following before running the DFS functional tests on the HP/UX platform:
...\" 
...\" 
.BL
...\" 
.LI
You should not use \*L/bin/sh\*O, but rather the \*L/bin/posix/sh\*O shell,
when running the DFS functional tests. Otherwise errors will occur as a result
of the way \*L/bin/sh\*O handles arguments when function calls are made.
...\" 
...\" 
.LI
The \*Ldiff\*O command supplied with HP/UX 9.0.1 will not perform
...\" 
.iS
    diff -r
.iE
...\" 
.P
correctly under certain circumstances, returning a non-zero exit code even when
there are no differences in the directory trees specified. Functional tests such
as \*Llow\*O and \*Lfs\*O which use the \*Ldiff\*O command will incorrectly
report failures.
...\" 
...\" 
...\" 
...\" Fix is to use a patched diff
...\" supplied to us by the HP dfs group. They say that this problem
...\" will be fixed in a later release.  
...\" 
...\" 
...\" 
.LE
...\" 
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Distributed Development Environments"
...\" ----------------------------------------------------------------------
...\" 
.P
Often, multiple versions of a particular source file are in use simultaneously,
which complicates the debugging process when responsibilities are divided among
developers. Distributed development environments, such as OSF \*VOpen Development
Environment\*O (ODE), packaged with the DCE sources, can support this type of work.
The source control software included in such environments provides a handle for
managing distributed development with tracing tools to find the filenames, file
revisions and line of code affecting a particular variable or data object.
.P
If you define the \*LAFSL_USE_RCS_ID\*O preprocessor directive on the command
line when compiling a file, \*Losi_assert\*O failures return the source code
file, its version number, the assertion's line number, and (if possible) the
results of the assertion. Otherwise, the compiler's version of the filename
...\" , along with its last modification date,
is returned.
.P
The DFS code implements the file and version information with Revision Control
System (RCS). RCS is available from the Free Software Foundation. It is packaged
with the \*VOSF Development Environment\*O (ODE), which is provided on the DCE
source tape.
.P
However, the package is general enough that you can apply it to your own source
code control system, if you use a different development environment than ODE. To
modify the code which lets the AFSL_USE_RCS_ID construct return information in a
form appropriate to your source code control conventions, check and update the
following files under the
.DS
    \*Vdce-root-dir\*L/dce/src/file\*O
.DE
.P
directory:
...\" 
.TS
center;
lbe lbe lbe.
config/stds.h	osi/afsl_trace.c	osi/osi.h
.TE
...\" 
.nS "Note"
The code in the
.DS
    \*Vdce-root-dir\*L/dce/src/file/osi\*O
.DE
.P
directory contains various debugging aids for porting the \*Losi\*O layer.
Some of this code may be applicable to other portions of DFS.
.nE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Kernel Debugging Considerations"
...\" ----------------------------------------------------------------------
...\" 
.P
You need a kernel debugger as well as user space debugging facilities to bring
up DFS on your platform. At a minimum, such a debugger must be able to set
breakpoints and execute stack traces. Increasing the debugger's capabilities
and its integration into your computational environment can improve your
debugging efficiency. Specific desiderata for a debugging environment include
the following:
...\" 
.BL
.LI
Remote debugging, where the machine running the code differs from the machine
doing the testing.
.LI
Source code debugging.
.LI
Structure format conversion facilities (dumpers): DFS kernel code includes
multiple layers of nested structures. Written out in raw hexadecimal format,
they can be tedious to interpret. Format conversion facilities which cast the
information into a readable format, and trace out succeeding nested substructures,
can speed the debugging process significantly.
.LE
...\" 
.P
If your kernel debugging tools have any shortcomings, you may find that an
investment in improving them, particularly to provide the facilities listed
above, will be repaid in shortened debugging time as you bring up DFS.
...\" 
.nS "Note"
When you plan the porting process, you should evaluate the costs and
benefits of investing in improved development tools before you begin
working with DFS.
.nE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Debugging Facilities in the DFS Source Code"
...\" ----------------------------------------------------------------------
...\" 
.P
The DFS source code provides several built-in debugging tools, particularly
in the
.DS
    \*Vdce-root-dir\*L/dce/src/file/osi\*O
.DE
.P
directory.
...\" 
...\" 
...\" >     2.  afs_debug (macro?)
...\" 
...\" AFS_DEBUG enables debugging code within the DFS components.
...\" 
...\" >     3.  mbz macro.
...\" 
...\" Must Be Zero.  Specific type of assertion - typically used to verify
...\" that the return code of a function is zero (meaning the function did
...\" what it was supposed to do.)
...\" 
...\" >     5.  Apparently osi_debug.h can set several levels of debug output.
...\" > 		What are these levels and what is their usefulness -
...\" 
...\" The file osi.h documents this reasonably well.  The lowest level is zero
...\" (0) which leaves only truly critical invariant checks alone - the types
...\" of checks which if they fail could cause data corruption.  The
...\" intermediate level is three (3) which turns on general debugging but
...\" only if the cost is not "too high".  This means expressions which do not
...\" require substantial CPU time.  The highest level is five (5) which turns
...\" on all debugging, including debugging operations which may have
...\" significant CPU runtime costs associated with them.  For normal
...\" development, (5) would be the rule, while for production (0) will be the
...\" correct choice.
...\" >     6.  Once people have "finished" the port and are building a
...\" > 		production system, do they (should they) have a
...\" > 		unified mechanism for switching off the kernel
...\" > 		osi_asserts used in building DFS?
...\" 
...\" > 		Are there certain sets of such asserts, or any other
...\" > 			data objects, that require special attention
...\" > 			when transitioning to production?
...\" Do not define AFS_DEBUG OR AFSL_DEBUG_LEVEL.  The default then is that
...\" only critical asserts are turned on.  Once again, there is a table in
...\" osi.h which describes this.  Indeed, you may find this table is a useful
...\" addition to the porting guide.
...\" 
...\" 
.P
For example, \*Losi_assert\*O, which checks for internal consistency, and
debugging-related compiler switches can be found in
.DS
    \*Vdce-root-dir\*L/dce/src/file/osi/osi.h\*O
.DE
.P
If an \*Losi_assert\*O fails, the program uttering it restarts, typically
dumping core. You may wish to build a soft restart facility into your kernel
code, so such \*Losi_assert\*O failures do not cause a kernel panic. Doing so
can speed up code development and testing. However, in production systems,
\*Losi_assert\*O failures are normally only associated with critical problems
and possible data corruption. You must decide how to handle such failures in
your final product.
.P
Note that some debugging features must be ported separately for the
different libraries in which they run, once for kernel and once for
user-space code.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Debug Levels"
...\" ----------------------------------------------------------------------
...\" 
.P
You can select the level of debugging feedback with a numerical value for
\*LAFSL_DEBUG_LEVEL\*O, defined in \*Losi.h\*O. Currently, three values of
\*LAFSL_DEBUG_LEVEL\*O are implemented:
...\" 
.BL
.LI "0"
Only critical code reports errors.
.LI "3"
Consistency checks are reported if they are not computationally expensive.
For example, Boolean expressions of simple variables are checked, function
calls or complex macros are not.
.LI "5"
All consistency checks are performed, regardless of expense.
.LE
...\" 
.P
You can tune the debugging level, including definition of intermediate
levels for \*LAFSL_DEBUG_LEVEL\*O, to suit your needs, depending where
you are in the porting process.
.P
Because DFS code involves interaction among many modules on different
machines, expect to maintain a fairly high level of debugging reporting
through most of the development process. Typically, \*LAFSL_DEBUG_LEVEL\*O
will remain at 5, even for modules already built and separately functionally
tested.
.P
Once you have finished the debugging, and do not intend to trace operations
again, do not define \*LAFS_DEBUG\*O or \*LAFSL_DEBUG_LEVEL\*O in:
.DS
    \*Vdce-root-dir\*L/dce/src/file/osi/osi.h\*O
.DE
.P
Then only critical \*Losi_assert\*Os, where failures are associated with
possible data corruption, are turned on.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Test Types"
...\" ----------------------------------------------------------------------
...\" 
.iX "Test types"
...\" 
.P
There are several functional test suites available for DFS. Some are packaged
with DCE, and some which are not, but are probably already present on your
system. There are three sets of tests of overall DFS functionality, namely:
...\" 
.BL
.LI
Basic tests, such as the NFS connectathon suite, which are not packaged with
DCE.
.LI
The low-level functionality tests, in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/low\*O
.DE
.P
directory.
.LI
More extensive tests, in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs\*O
.DE
.P
directory.
.LE
...\" 
.P
At least with the latter two sets of tests, you can modify the stress level by
changing various parameters, such as the sizes and numbers of objects created,
listed, modified or removed.
.P
Besides testing basic DFS functionality, the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file\*O
.DE
.P
directory has subdirectories for tests of specific functions associated with
DFS.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Basic Testing with External Test Packages"
...\" ----------------------------------------------------------------------
...\" 
.P
If your platform also supports NFS, you can use tests packaged with it,
particularly the \*Vconnectathon\*O test suite, to check basic DFS functions,
such as creating, deleting, listing, reading and writing files and directories.
Alternatively, you may be able to modify other low-level external filesystem
test suites to test DFS during the porting process.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "The Standard DFS Test Suites"
...\" ----------------------------------------------------------------------
...\" 
.P
Once your implementation passes such basic tests, you can begin stress tests,
from the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/low\*O
.DE
.P
and
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs\*O
.DE
.P
directories.
.P
These tests let you specify sizes and number of objects to be manipulated, and
the mix of operations on those objects, so you can increment the stress on your
code along various parameters. In addition, the context in which the tests are
run, for example heterogeneous machines or split servers, let you generate a
matrix of performance stresses.
.P
Beside the basic tests listed above, the following types of function-specific
DFS tests are shipped with DCE:
...\" 
.BL
.LI
DFS kernel modification tests
.LI
DCE Local File System tests
.LI
DFS server process tests
.LI
DFS command interface tests
.LI
DFS administrative tool tests
.LE
...\" 
.P
These tests are contained in subdirectories of the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file\*O
.DE
.P
directory and are described in the DFS Test Plan.
.P
Before executing the test cases, you must configure DFS for testing, using the
instructions in the following section of this chapter (``DFS Test Setup''). You can
run tests on the configurations described in that section.
.P
Because some DFS code runs in kernel space, many of the interfaces cannot be
called directly in order to test them. Therefore, testcases have been written
using user interfaces that in turn access and exercise the kernel space code.
In addition, tests are included to exercise those subroutines not tested through
traditional UNIX interfaces.
.P
User-level code is tested using shell scripts that exercise the interfaces.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS Test Setup"
...\" ----------------------------------------------------------------------
...\" 
.P
Before running any DFS tests, you must first configure a DCE cell. Refer to
the \*VOSF DCE Administration Guide\(emIntroduction\*O for information on
configuring a DCE cell, specifically Chapter 6 ``Overview of The DCE
Installation and  Configuration Script,'' Chapter 7 ``Phase One:  Initial
Cell Configuration,'' and Chapter 8 ``Phase Two: Configuring a DCE Client
and Other DCE Services.''
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DCE Distributed File Service Tests"
...\" ----------------------------------------------------------------------
...\" 
...\" .zA "Additional information added for DCE 1.0.2"
...\" 
.P
The following sections describe functional tests for the DCE Distributed File
System. These tests are packaged on the distribution tape, in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/\*O
.DE
.P
directory. In addition, many of the DFS source subdirectories include test
programs for individual functions and and subcomoponents.
...\" 
.nS "Note"
Before building and running the test programs packaged with the DFS
sources, check them for platform and operating system dependencies.
They may need to be modified to operate correctly in your target
environment,  and to exercise ported code.
.nE
...\" 
...\" 
...\" ----------------------------------------------------------------------
...\" .H 4 "Additional Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
In addition to the DFS system call tests described in the section immediately
following, other following sections describe a number of development level
tests which are built in the individual subcomponent directories. These can be
used to test various phases of your port. Included are tests for the token
manager, aggregate operations, free pool management, system calls, and others.
...\" 
...\" 
...\" 
.P
For information on DFS system testing, refer to the ``DFS System Tests'' section
of Chapter 13 (``DCE System Testing'') of this guide.
...\" 
...\" ----------------------------------------------------------------------
.H 4 "System Call Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/low\*O
.DE
.P
and
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs\*O
.DE
.P
subdirectories contain testcases for testing the file system-related system
calls affected by DFS. Once your ported DFS code passes all tests in these two
subdirectories, it can provisionally be considered ready for integration with
other DCE functions.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 5 "The low Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The tests in
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/low\*O
.DE
.P
are C programs with shell script drivers that use DFS to exercise low-level
system calls. Brief descriptions of the \*Llow\*O tests are listed below.
...\" 
.nS "Note"
Tests 2 and 4 are not listed. They exist, but are computationally expensive
and are not considered necessary for testing DFS functionality.
.P
These tests are specific to UNIX platforms. If you are porting to a different
operating system, you will have to rewrite them, using your target environment's
system calls.
.nE
...\" 
.VL 1.0i
.LI "Test\ 1"
Performs \*Lstat(\|)\*O calls to check for existence of two test
files, one of 16 bytes, one of half a megabyte. Does repeated \*Lopen(\|)\*Os
and \*Lclose(\|)\*Os on each file, then repeated cycles of
\*Lopen(\|)\*O-\*Lwrite(\|)\*O-\*Lread(\|)\*O-\*Lclose(\|)\*O on each.
It then performs cycles of \*Llseek(\|)\*O and
\*Lopen(\|)\*O-\*Lread(\|)\*O-close(\|) on as many as three files.
Does not check data.
.LI "Test\ 3"
Performs sequential and random \*Lwrite(\|)\*Os to a file, then 
a \*Lclose(\|)\*O followed by \*Lfsync(\|)\*O.
Then it \*Lopen(\|)\*Os and \*Lread(\|)\*Os the file, and compares the
data with what it wrote.
.LI "Test\ 5"
Writes out a file, marches through the file with successive read(\|)
and lseek(\|) calls.
Compares the first byte of each buffer for data integrity.
.LI "Test\ 6"
File and directory manipulation:
Performs \*Lmkdir(\|)\*O and \*Lchdir(\|)\*O system calls.
Uses \*Lopendir(\|)\*O and \*Lreaddir(\|)\*O to confirm that what it
created actually exists.
.LI "Test\ 7"
Creates symbolic links, performs \*Llstat(\|)\*Os on them.
.LI "Test\ 8"
Creates different files with all permission modes, opens, renames, and
unlinks them.
Checks whether the modes stay correct on \*Lopen(\|)\*O.
.LI "Test\ 9"
Creates a file, manipulates its mode and time with \*Lchmod(\|)\*O,
\*Lfchmod(\|)\*O with the file open, and \*Lutimes(\|)\*O.
It then checks the file's status with \*Lstat(\|)\*O, and unlinks the
file.
.LI "Test\ 10"
File descriptor status manipulation: creates a file, \*Lopen(\|)\*Os
it, performs \*Lfcntl_sets\*O and \*Lfcntl_gets\*O on it, does some
\*Lread(\|)\*Os and \*Lwrite(\|)\*Os.  
It then calls \*Lfcntl_sets\*O again.
Then if truncates the file with \*Lftruncate(\|)\*O.
Finally, it checks the file's status flags with \*Lstat(\|)\*O, and
unlinks the  file.
.LI "Test\ 11"
Deadlock testing: a parent process forks a child, then both processes
lock and unlock a file.
.LI "Test\ 12"
Creates a file, reads and writes vectors of data to it with 
\*Lreadv(\|)\*O and \*Lwritev(\|)\*O.
.LI "RTest\ 1"
Tests the  \*Lchroot\*O command.
.LI "RTest\ 2"
Tests the \*Lchown\*O command.
.LE
...\" 
.P
Information on running these tests can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/low/READ_ME\*O
.DE
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 5 "The fs Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The tests in
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs\*O
.DE
.P
are shell scripts that execute a number of common UNIX commands relating to
files. These tests exercise the Cache Manager and Protocol Exporter functionality,
as well as verify that UNIX filesystem semantics are maintained. These tests
check that the DFS implementation adjudicates filesystem contention among multiple
processes, as happens in a multi-user environment. They are summarized below.
(Note that test 5 has been removed from the suite.)
.P
As with the low-level tests described above, these tests are specific to UNIX
systems, and will have to be rewritten for other target environments.
...\" 
.VL 1.0i
.LI "err1"
Tests file error conditions by issuing incorrect commands.
For instance, this test attempts to \*Lcp\*O to a directory, \*Lcd\*O
to a file, and perform invalid \*Lchmod\*O and \*Lchgrp\*O commands.
.LI "Test\ 1"
Run up to 9 simultaneous copies of a program, which modify different
parts of the same file at the same time.
.LI "Test\ 2"
Creates a new subdirectory, then spawns multiple processes which
performs various standard file operations in that subdirectory.
.LI "Test\ 3"
Performs hundreds of file creations and removals in the current
directory, then checks that all the correct files (and no others) are
present at the end of the process.
.LI "Test\ 4"
Concatenates files:  multiple processes \*Lcat\*O sets sixteen 1K files
into 16K files, then repeat the process with the larger files, forming
256 kilobyte files.
.LI "Test\ 6"
Tests process contention:  one process attempts to delete a file while
another has the file open.
.LI "Test\ 7"
Tests directory management integrity: creates a directory structure
containing a variable number of directories, each of which contains a
variable number of  16-kilobyte files.
The tree is repeatedly created and then removed.
.LI "RTest\ 1"
Checks the \*Lchgrp\*O, \*Lchmod\*O and \*Lchown\*O commands.
.LE
...\" 
...\" 
.P
Information on running these tests can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs/READ_ME\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "DFS Cache Consistency Tests"
...\" ----------------------------------------------------------------------
...\" cache consistency
.P
The DFS cache consistency tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/cache_mgr\*O
.DE
...\" 
.P
Descriptions of the tests and instructions on how to run them can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/cache_mgr/README\*O
.DE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "UNIX Filesystem Tests"
...\" ----------------------------------------------------------------------
...\" rtests  (rtest1)
.P
UNIX filesystem tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs\*O
.DE
...\" 
.P
Descriptions of the tests and instructions on how to run them can be found
in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fs/README\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "DFS ACL Tests"
...\" ----------------------------------------------------------------------
...\" acl
.P
The DFS ACL tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/acl\*O
.DE
...\" 
.P
Descriptions of the tests and instructions on how to run the tests can be found
in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/acl/README\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "DFS Token State Recovery Tests"
...\" ----------------------------------------------------------------------
...\" tsr (hand)
.P
The DFS token state recovery hand tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/tsr\*O
.DE
...\" 
.P
Descriptions of the tests and instructions on how to run them can be found
in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/tsr/TSR_README\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "DFS File Exporter Stress Tests"
...\" ----------------------------------------------------------------------
...\" file exporter stress
.P
The DFS file exporter stress tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fx\*O
.DE
...\" 
.P
A descriptions of the test script and instructions on how to run the tests
can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fx/README\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "ubik Failure Recovery Tests"
...\" ----------------------------------------------------------------------
...\" ubik failure recovery (hand) 
.P
The \*Lubik\*O failure recovery hand tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/ubik\*O
.DE
...\" 
.P
Descriptions of the tests and instructions on how to run them can be found
in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/ubik/READ_ME\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DCE Local File System Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The following sections describe tests for the DCE Local File System.
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "System Call Tests for LFS"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Llow\*O and \*Lfs\*O tests described in the ``System Call Tests''
section earlier in this chapter can also be run on the DCE Local File
System to test file system-related calls affected by DCE LFS.
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "LFS Fileset Operations Tests"
...\" ----------------------------------------------------------------------
...\" fset
.P
The fileset (``ftutil'') test tools for testing DCE LFS fileset operations are
located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fset\*O
.DE
...\" 
.P
Instructions on running the tests can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fset/README\*O
.DE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "LFS Authorization Salvage Test"
...\" ----------------------------------------------------------------------
...\" salvage (hand) 
.P
The LFS authorization salvage hand test is located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file\*O
.DE
...\" 
.P
A description of the test and instructions on how to run it can be found in
the comment at the top of the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/salvage/AuthCheckTest\*O
.DE
...\" 
.P
file. Test tools for the LFS salvager are located in
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/ravage\*O
.DE
.P
and:
.DS
    \*Vdce-root-dir\*L/dce/src/file/episode/scavenge\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "LFS ACL and LFS Recovery Tests"
...\" ----------------------------------------------------------------------
...\" recovery 
...\" PCTS==POSIX compliance test suite
.P
The LFS ACL and LFS recovery and associated POSIX compliance tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/recovery\*O
.DE
...\" 
...\" .P
...\" Descriptions of the tests and instructions on how to run them can be found
...\" in:
...\" 
...\" .DS
...\"     \*Vdce-root-dir\*L/dce/src/test/file/recovery\*O
...\" .DE
...\" 
.P
A description of the \*Lcheckaggr\*O tool, which is used by these tests, and
which is located in this directory, can be found in the comments at the top of
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/recovery/checkaggr\*O
.DE
...\" 
.P
and in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/recovery/README.checkaggr\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Other DCE LFS Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The tests in the following directories test additional functions specific to
the DCE LFS:
...\" 
.BL
.LI
\*Vdce-root-dir\*L/dce/src/file/episode/anode/test_anode.c\*O
.P
Described in:
.DS
\*Vdce-root-dir\*L/dce/src/file/episode/vnops/README\*O
.DE
.P
and:
.DS
\*Vdce-root-dir\*L/dce/src/file/episode/anode/README\*O
.DE
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/episode/async/astest.c\*O
...\" 
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/episode/dir/test_dir.c\*O
...\" 
...\" .LI
...\" \*Vdce-root-dir\*L/dce/src/file/episode/test\*O
...\" 
.LI
\*Vdce-root-dir\*L/dce/src/file/episode/vnops/test_vnodeops.c\*O
.P
Described in:
.DS
\*Vdce-root-dir\*L/dce/src/file/episode/vnops/README\*O
.DE
...\" 
...\" 
...\" 
.LE
...\" 
.P
Many of these tests are porting tests that run in user space. It is recommended
that these tests only be used before placing your ported code into kernel space
to help verify that the basic function is working correctly. In most cases, the
tests accept scripts that tell them which subroutines or operations to perform
in sequence. Functions covered include the following:
...\" 
.BL
.LI
Initializing aggregates
.LI
Creating aggregates
.LI
Verifying aggregates
.LI
Creating filesets
.LI
Closing filesets
.LI
Mounting and unmounting tests
.LI
Checking mode bit settings and access times
.LI
Testing \*Lvnode\*O operations
.LI
Testing locks (\*Lfile\*O and \*Lrecord\*O)
.LE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS Server Process Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
DFS server processes are exercised both by the cache manager and protocol
exporter operations described previously, and through DFS command tests.
These tests are described in the ``DFS Command Interface Tests'' section of
this chapter.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Ubik Database-Replication Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
A test server and client process, \*Lutst_server\*O and \*Lutst_client\*O,
are provided for testing replicated database functionality. These tests are
described in the DFS Test Plan and are in the
.DS
    \*Vdce-root-dir\*L/dce/src/file/ncsubik\*O
.DE
.P
directory. You must create entries in the CDS namespace in order to run
these tests.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS Command Interface Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
Tests for the \*Lbos\*O command are located in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/bos\*O
.DE
.P
directory. Information on setting up and running these tests can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/bos/READ_ME\*O
.DE
...\" 
...\" 
.P
Tests for the \*Lcm\*O command are located in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/cm\*O
.DE
.P
directory. Information on setting up and running these tests can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/cm/README\*O
.DE
...\" 
...\" 
.P
Tests for the \*Lfts\*O commands are located in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fts\*O
.DE
.P
directory. Information on setting up and running these tests can be found in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/fts/README\*O
.DE
...\" 
...\" 
...\" 
...\" 
.P
The DFS Test Plan describes these tests and explains how to execute them.
The \*Lruntests\*O script for the \*Lcm\*O and \*Lfts\*O tests contains
a number of variables which should be configured for the environment
being tested. For the \*Lfts\*O tests, two DCE LFS aggregates should
be available to test against, and two more DCE LFS aggregates should
be exported.
.P
The \*Lfts\*O tests for fileset replication are more effective if two
fileserver machines are available for use. However, basic replication
can be tested with a single file server.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS Administrative Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
Tests for the DFS administrative tools are available in the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file\*O
.DE
.P
directory. Details about the separate tests appear in the following sections.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Update Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Lupserver\*O and \*Lupclient\*O distribution tools should be
tested with the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/update\*O
.DE
.P
tests. Comments at the beginning of
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/update/uptest\*O
.DE
.P
explain how to run these tests.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Scout Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The Scout interactive monitoring tool is tested manually. Descriptions of the
manual tests are located in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/scout/READ_ME\*O
.DE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Backup System Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The DFS backup system is tested using the scripts in:
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/backup\*O
.DE
.P
A comment at the top of the
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/backup/runtests\*O
.DE
.P
script explains the necessary configuration and how to run the tests.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "DFS Gateway Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
Tests for the DFS Gateway are located in the
...\" 
.DS
    \*Vdce-root-dir\*L/dce/test/file/gateway\*O
.DE
...\" 
.P
directory. Details about the separate tests appear in the following sections.
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Gateway Daemon Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Ldfsgwd\*O should be tested using the tests in:
...\" 
.DS  
    \*Vdce-root-dir\*L/dce/src/test/file/gateway/dfsgwd\*O
.DE
...\" 
.P
Information on setting up and running these tests can be found in:
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/gateway/dfsgwd/README\*O
.DE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Gateway Administration Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
The \*Ldfsgw\*O command line interface should be tested using the tests in:
...\" 
.DS  
    \*Vdce-root-dir\*L/dce/src/test/file/gateway/dfsgw\*O
.DE
...\" 
.P
Information on setting up and running these tests can be found in:
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/gateway/dfsgw/README\*O
.DE
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 4 "Gateway Client Tests"
...\" ----------------------------------------------------------------------
...\" 
.P
\*Ldfs_login\*O and \*Ldfs_logout\*O should be tested using the tests in:
...\" 
.DS
    \*Vdce-root-dir\*L/dce/src/test/file/gateway/dfs_login\*O
.DE
...\" 
.P
Information on setting up and running these tests can be found in:
...\" 
.DS
    \*Ldce-root-dir\*L/dce/src/test/file/gateway/dfs_login/README\*O
.DE
...\" 
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
.H 3 "Test Plans"
...\" ----------------------------------------------------------------------
...\" 
.P
Refer to Chapter 1 of the \*VOSF DCE Release Notes\*O for the location of the
DCE DFS test plans, describing the DFS test cases and how to execute them, on
the DCE distribution tape.
...\" 
...\" 
...\" 
...\" .zZ "Additional information added for DCE 1.0.2"
...\" 
...\" 
...\" .zZ "def,8422,R1.0.3,alphabetized symbol lists"
...\" 
...\" 
...\" 
...\" ----------------------------------------------------------------------
