...\" @OSF_COPYRIGHT@
...\" COPYRIGHT NOTICE
...\" Copyright (c) 1990, 1991, 1992, 1993, 1994 Open Software Foundation, Inc.
...\" ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
...\" the full copyright text.
...\" 
...\" 
...\" HISTORY
...\" $Log: overview.gpsml,v $
...\" Revision 1.1.2.2  1994/06/10  21:08:14  devobj
...\" 	cr10872 - fixed copyright
...\" 	[1994/06/10  20:48:02  devobj]
...\"
...\" Revision 1.1.2.1  1994/04/05  15:14:26  rom
...\" 	{enh, 10283, R1.1}
...\" 	Moved from dce_books/app_gd/dfs/1.oview.dfs.gpsml to
...\" 	supp_docs/redistrib/dfs_app_gdref/chapters/overview.gpsml
...\" 	[1994/04/04  20:54:20  rom]
...\" 
...\" $EndLog$
...\"
...\" (c) Copyright 1991, Open Software Foundation, Inc. ALL RIGHTS RESERVED
..."Copyright (C) 1989, 1991, 1992, 1993 Transarc Corporation
..."The Gulf Tower
..."707 Grant Street
..."Pittsburgh, PA  15219
.H 1 "DCE Distributed File Service Overview"
.P
This chapter describes the architecture of the DCE
Distributed File Service (DFS), a high-performance distributed file
system that provides transparent local and remote file access, and
also provides background information for writing DFS applications. DFS is
designed to maximize reliability and interoperability with other file
systems. When accessing remote data, DFS uses DCE Remote Procedure
Calls (RPCs) to communicate with participating systems, to exchange
access requests, authentication information, and file and directory data, and
to synchronize information.
.P
DFS also includes a high-performance, log-based local file
system, the DCE Local File System (DCE LFS). The DCE LFS offers
capabilities not generally available in conventional
UNIX File Systems (UFSs),
such as support for logical groups of files (filesets) within a disk
partition, and support for DCE Access Control Lists (ACLs). The DCE LFS
maintains a log (on disk) of all actions that affect
file and directory metadata, such as file creation and
modification dates, file sizes, and directory entries. Maintaining a
log of file system operations provides a fast, robust mechanism for
recovering from computer system problems that do not actually involve
damage to the physical storage media.
.P
This chapter and the ones that follow it assume that you are
familiar with the
\*EIntroduction to OSF DCE\*O.
.H 2 "Writing DFS Applications"
.P
The capabilities of the DFS are
used in writing applications that depend
heavily on fileset manipulation, file server process management, and
client management. Such applications include backup systems, mail and
bulletin board programs, and graphical fileset management tools for system
administrators. In addition, customers may
wish to replace stock applications with customized versions;
this is made possible by the programming interface provided.
.H 3 "Related DCE Components"
.P
Because DFS is built on top of other DCE components, a complete understanding
of the other components is necessary for an understanding of DFS.
The information in this section is intended only as an overview of
the other components; it is assumed that you have read about
and understand the following DCE components:
.ML
...\'wH UNDEFINED{tightenlist}
.LI
DCE Remote Procedure Calls
.LI
DCE Security Service, especially how to use and interpret Access
Control Lists (ACLs)
.LI
DCE Directory Service, especially details about the namespace
.LI
DCE Distributed Time Service, especially client and server machine
synchronization
.LI
DCE Threads
.LE
.P
For more information about these components, consult the appropriate
parts of this guide and the \*EOSF DCE Application Development Reference\*O.
.H 4 "DCE Remote Procedure Calls"
.P
DCE Remote Procedure Calls are central to the DFS client/server model.
All communications between server machines and client machines happen
through RPCs. DFS functions generally take handles as arguments,
which direct the functions to use the correct RPC interfaces.
An application programmer
will need to know how to obtain these handles, and should have a
thorough understanding of the use of RPCs in general.
.H 4 "DCE Security Service"
.P
The DCE Security Service is composed of several different components
that are used in conjunction with DFS: the Authentication Service, the
Privilege Service, the Access Control List component, and the Registry
Service.
.P
The DCE Authentication Service component performs several security
functions that interact with DFS. It ensures that only certified users
can log in and use the system, and it ensures that only authorized
machines can access the system.
.P
The DCE Privilege Service component ensures that those people using
the system have the correct access rights to perform the operations
they request.
.P
The DCE Access Control List component provides an interface that allows
you to set different levels of protection on file system objects such
as directories and files. The different ACLs interact with the
UNIX
file system protection mode bits. You can grant permissions to
individuals, or you can define groups of users and grant permissions
to the groups. For specific information on ACLs and file system objects,
see the \*EOSF DCE Administration Guide\*O and the
\*Lafs_syscall(\|)\*O description in this guide and in the \*EOSF 
DCE Application Development Reference\*O.
.P
The DCE Registry Service maintains a Registry Database. This database
contains information similar to that stored in
UNIX password files,
such as user, group, and account information. An account defines
who can log in to the system and includes information about passwords
and home directories.
.P
The DCE Security Service operates through principals. A principal
is a representation of a user or process that can
read and/or alter user or system
data. Principals have keys associated with them, which are, roughly,
passwords (see Part 6 of this guide); a process must know
a principal's key to exercise the principal's access rights.
Typically, there is a one-to-one mapping between people and principals.
DFS server processes on a single machine all share a single principal,
while some other DCE components assign separate principals to each
server process.
.H 4 "DCE Directory Service"
.P
The DCE Directory Service provides a consistent way to identify and
locate users and resources, including files and directories, anywhere
in a networked computing environment. The Directory Service has three
main components: the Cell Directory Service, the Global Directory
Service, and a Global Directory Agent (a gateway between the local
and the global naming environments).
.P
The Cell Directory Service (CDS) manages names within a cell. The
Global Directory Service (GDS) supports the global naming environment
between cells and outside of cells. The Global Directory Agent (GDA)
makes cell interoperability possible by allowing CDS to participate in
the global naming environment.
.P
DFS uses the Directory Service to locate fileset, backup, and other servers.
.H 4 "DCE Distributed Time Service"
.P
The DCE Distributed Time Service (DTS) provides precise clock
synchronization for system clocks in a network. It is used to
keep DFS client and server machines synchronized.
.P
DTS is important for communications between client machines using the
Cache Manager and server machines running the File Exporter and other
server processes. Clients must remain in contact with server machines
whose tokens they hold to ensure that they have the most recent copies
of cached data. Clients and servers must refer to a common time
standard for communications to remain constant and data to remain
current.
.P
DTS is also important for replicated Fileset Location Databases (FLDBs),
which must be coordinated on different server
machines. Like clients and servers, machines housing replicated
databases must remain in constant contact to ensure that each server
has the current copy of the database. Failure to maintain
synchronization can result in unnecessary disruption of database
access.
.H 4 "DCE Threads"
.P
DCE Threads provides a parallel processing-like environment.
DFS uses DCE Threads, and
DFS programmers need to understand DCE Threads to use any of the DFS application
programming interfaces, except the \*Lpioctl(\|)\*O and \*Lafs_syscall(\|)\*O
interfaces. Applications
are required to coexist with DCE Threads.
It is possible that nonserver applications can be written
without DCE Threads if the applications do not make remote procedure calls.
.H 3 "The DFS Application Programming Interface (API)"
.P
The DFS Application Programming Interface (API) consists of the following parts:
.ML
.LI 
Cache Manager (\*Lpioctl(\|), ioctl(\|), afs_syscall(\|)\*O):
Provides calls to manipulate system
information, client cache information, ACLs, and related
items
.LI 
General Fileset Functions (\*LVC_...(\|)\*O):
Manipulates filesets and the Fileset Location Database at a high level
.LI 
Fileset Location Server (\*LVL...(\|)\*O):
Provides functions to manipulate
Fileset Location Database entries
.LI 
\*LFileset Server\*O (\*LFTSERVER\*O):
Provides functions to manipulate filesets on servers
.LI 
\*LBOS Server\*O (\*LBOSSVR\*O):
Provides functions to manipulate processes on File Server machines
.LE
.P
Some parts of the DFS API can be called by any user, but those functions
that involve altering fileset, FLDB, process, or Cache Manager information
are restricted to users with the appropriate privileges, usually
a DFS administrator or root on the local machine. More information about
these authorization issues is given in later sections that decribe these
functions. In general,
if the application is going to perform these operations, it must
be installed with the appropriate privileges.
.P
The rest of this chapter describes the DFS architecture in detail.
While an interface is not provided for each component,
some level of understanding
of the various pieces and how they fit together greatly assists
in understanding DFS. The chapters following this one
describe the Cache Manager,
the functions for manipulating filesets,
and the Basic OverSeer Server.
.H 2 "Overview of the DCE Distributed File Service Architecture"
.iX "File Server machine"
.iX "client" "machine"
.P
Each node, or system, in a DFS installation is either a server
machine, running a DFS server process or
maintaining local file systems on disk and making
them available to other nodes (exporting them), a client machine,
running applications that access files that are exported by File Server
machines, or both.
A client node can act as a server if it exports a local file
system. An exported local file system can either be a DCE
LFS or a conventional
UNIX
File System (UFS).
.P
To expedite file system response, DFS clients maintain a local cache
of recently requested file and directory information. To synchronize
locally cached file and directory information with requests made by
other systems for that same information, DFS uses a token-based
synchronization system. When a client requests file or directory
information from a File Server machine, the File Exporter
returns that information and grants an access token that describes the
remote system's capabilities with respect to that file or directory.
Subsequent requests made by other clients for conflicting operations
on that same information are not granted until the first token can be
revoked. On systems with a local hard disk, cache information is
stored on disk. On diskless client machines, cache information is
stored in system memory.
.iX "VFS+"
.iX "Virtual File System interface"
.P
DFS uses an enhanced Virtual File System interface (called VFS+) to ensure
consistency between requests for remote files (from File Server
machines) and requests for local files present in exported physical file
systems on the client machine. To guarantee consistency, all exported
DFS file systems types share the same access mechanism. The VFS+ interface
guarantees that local
requests for files located on the local file system generate
the same types of
access tokens as requests for these files that originate from a remote
system.
.P
The three main components of DFS are the Virtual File System, the
Cache Manager, and the File Exporter. These components
have the following responsibilities:
.ML
.iX "filesets"
.iX "DFS" "file system component"
.LI
The enhanced
Virtual File System (VFS+) component supports both local and remote
file access. Part of the server VFS+ interface is the concept of DCE LFS
filesets, which are logical groups of files present in
single disk partitions.
.iX "DFS" "Cache Manager"
.LI
The Cache Manager runs on client machines, and maintains a copy of
data that has been obtained from a server in the ``recent'' past. If the
file or information requested is already present in this cache, no
remote call is necessary. If the file or information requested by the
application is not already present in the local cache, the Cache
Manager makes calls to the File Exporter on the appropriate File Server
machine to retrieve that information, and stores it. How long information
is kept in the local cache depends on the size of the cache and the amount of
data retrieved.
.iX "DFS" "File Exporter"
.LI
The File Exporter makes any exported VFS that is
physically located on the File Server machine available to client
machines. This process also synchronizes requests for access to files
and directories to prevent conflicting
access to files and directories by clients. (These clients may include
the machine on which the files and directories reside, if a client is also
acting as a server.) The synchronization
mechanism is called the Token Manager. 
.iX "DFS" "Token Manager"
.iX "Token Manager"
Access rights are
granted through tokens that summarize the current read, write, and
access capabilities of a client with respect to each cached file or
directory.
.LE
.P
These primary components are supported by three other modules: the
Fileset Location Server, the Fileset Server, and the
Replication Server. These components have
the following responsibilities:
.ML
.iX "Fileset Location Database"
.iX "-: Fileset Location Database" "FLDB"
.LI
The Fileset Location Server, which maintains the Fileset Location
Database (FLDB), records the location of all
available filesets and the File Server machines associated with those filesets.
Fileset location lookups are necessary when translating new references
to different logical file systems when, for example, file system ``mount
points'' (see the glossary in the \*EIntroduction to the OSF DCE\*O) 
are encountered.
.iX "Fileset Server"
.LI
The Fileset Server performs operations that involve entire filesets.
These include making entire filesets available to DFS requests,
producing incremental dumps of filesets, and moving filesets from one
File Server machine to another to aid in load and resource balancing.
.iX "Replication Server"
.LI
The Replication Server makes it possible to dynamically create
read-only replicas of an active fileset.
.LE
.H 2 "Component Overview"
.P
DFS provides a transparent distributed file system environment to
both users and applications. However, developing system-level DFS
applications requires an understanding of the purpose of each of
the DFS modules and their interactions.
The following subsections discuss each DFS module in detail, examining the
functions provided by each module, its role in the DFS architecture,
and the interactions with the various services required and
provided by DFS. Not all of these components are visible via the API.
.H 3 "The DCE Local File System"
.iX "DCE Local File System"
.iX "local file system"
.iX "LFS"
.P
DCE LFS is a fast-restarting
UNIX file system that integrates the
capabilities needed for a large-scale distributed system with a
sophisticated recovery mechanism, providing performance equal to or
better than most existing physical file systems. DCE LFS offers
several capabilities not generally available in
UNIX file
systems, such as support for logical groups of files (filesets) within
a disk partition, support for ACLs, and the ability to
recover quickly from system failures.
.P
DCE LFS is based on
UNIX
disk partitions and is integrated
into the kernel. DCE LFS is designed to
take advantage of a multithreaded environment and asynchronous I/O.
DCE LFS can be accessed both as a local file system when individual
filesets are mounted and as a remote file system exported from File
Server machines. To provide uniform local and remote access, DCE LFS
implements a compatible extension of the standard VFS functions.
.P
The next subsections describe the DCE LFS capabilities that represent
advances over older physical file systems. These include the
fileset and aggregate concepts, the log-based file system, and support for
ACLs. These subsections also introduce some
implementation issues, such as how DCE LFS and conventional
UNIX File
Systems are integrated, and the anode abstraction, which is
comparable to the inode structure in
UNIX File Systems.
.H 4 "Filesets and Aggregates"
.P
In many
UNIX environments, a file system is associated with a
partition, which is a disk or logical portion of a disk that can be
mounted, addressed, and administered as a single unit. DCE LFS provides
an additional logical level of organization by introducing filesets, 
.iX "filesets"  "definition"
which are mountable subtrees within a standard disk or
partition. Filesets can be administered and referenced individually.
To avoid confusion with conventional
UNIX terminology, DFS uses the term
``aggregate'' (see the glossary in the \*EIntroduction to OSF DCE\*O) 
.iX "aggregates"  "definition"
to refer to a unit of disk storage (equivalent to a
UNIX
partition) because it can be logically composed of multiple
filesets. The use of the term aggregate also provides a reminder that
partitions composed of filesets enable DFS-specific administrative
operations such as moving and cloning, which are not possible (or
meaningful) on
UNIX file systems.
.P
The distinction between mountable logical filesets and physical
partitions allows DCE LFS filesets to be moved among partitions on a
single server or from one server to another. This provides the system
administrator with a mechanism for balancing the system load across
File Server machines by redistributing frequently accessed filesets
among the available file servers. DCE LFS supports dynamic fileset
motion, which does not require taking down any servers or prohibiting
the use of any standard facilities when moving filesets. During such
a move, the fileset itself is temporarily unavailable. This absence
is fairly transparent to application programs, which are temporarily blocked
from accessing the files and applications in that fileset.
.P
In addition to the standard interfaces required of any
VFS, DCE LFS provides higher-level fileset and
aggregate interfaces. Although each VFS is a mountable fileset, these
fileset interfaces are separate from the VFS interface. This ensures
that fileset operations such as moving, cloning, and replicating can
be performed on filesets that are not mounted.
.H 4 "The Log-Based File System"
.iX "log-based file system"
.P
Conceptually, the data available in filesets and aggregates can be
divided into two general types: 
.iX "filesets"  "types of data"
user data and metadata.
User data 
.iX "user data"  "definition"
is the data in a fileset, such as applications and data
files, that is created and referenced by users of the system.
.P
Metadata 
.iX "metadata"  "definition"
is the data in any part of a file system that is used to
describe and organize the files and directories in that fileset or
aggregate. Metadata is logged in DCE LFS, but user data is not.
.iX "logs"  "definition"
.P
Tracking the actions of a program or system is generally
referred to as keeping a log of that program or system. DCE LFS
is a log-based file system because all changes
to metadata in a given aggregate are recorded in the log for
that aggregate. To provide a method for organizing the changes
recorded in these log records, associated changes to metadata
are grouped together into ``atomic transactions.''
.iX "transactions"
.iX "transaction" "atomic"
The term ``transaction'' indicates that all of the individual changes
in that group are related to a larger logical operation. The term
``atomic'' 
.iX "atomic transactions"
means that no single change that is a part of a transaction
takes effect unless all the changes associated with that
transaction are performed. The log entry for each change records the
old and new values for all changed metadata and the
identity of the transaction of which the change is part.
.iX "transactions"  "committing"
.iX "committing" "definition"
.P
A transaction has ``committed'' once all of the log records associated
with that transaction are written to the log on disk. A separate log
entry notes when a transaction commits. If a system failure occurs, the
file system recovery procedure replays the log, completing
transactions that have been recorded as committed, and undoing any
effects of transactions that did not commit. Log-based file systems
are frequently referred to as ``recoverable'' file systems because,
.iX "recoverable file systems"
even with system problems, any file system changes that have
been logged are easily recovered by simply replaying the operations
recorded in the log on disk.
.H 5 "DFS Logging Versus Conventional Logging"
.P
The log maintained by DCE LFS records the most recent changes to file
system metadata in a given aggregate. In conventional
UNIX file systems,
system failures that interrupt certain operations can leave the file
system in an inconsistent state, making it potentially unsafe to use
and requiring the use of the
\*Lfsck\*O command after reboot to check
for and repair any damage.
Maintaining a record of the changes made to metadata enables
the DCE LFS to resume normal operation as quickly as possible after a
system failure; it eliminates the need to call \*Lfsck\*O,
unless the medium that stores the file system
is physically damaged. After a system crash, DCE LFS applies
recovery techniques that either undo operations that have begun but
not finished, or complete operations that have finished but have not yet been
written to the file system. The time spent in recovery is
proportional to the size of the active portion of the log instead of the
size of the file system.
.P
In conventional
UNIX file systems, file system operations that can
introduce file system inconsistencies must be written to disk as soon
as possible after they have been performed. Although these writes are
generally performed asynchronously, they generate so much disk traffic
that they can affect the overall performance of the system. In
addition, many operations must wait for one disk write to complete before
being permitted to make a related update.
.P
In comparison, a log-based file system does not need to force modified
metadata to the disk in order to guarantee consistency in the
event of a crash. The file system is always consistent, although perhaps
slightly out of date, after the log is replayed. The file system
may periodically commit all pending transactions, but fidelity to the
spirit of the UNIX
file system requires this to occur only every 30
seconds. For this reason, the DCE LFS log usually maintains log records
for changes to metadata within only the last 30 to 45 seconds of
file system activity.
.P
All pending transactions are automatically written to the log whenever
a \*Lsync(\|)\*O or \*Lfsync(\|)\*O system call is executed. These ``batch''
commits have very low overhead because of the sequential organization of
the log, and because disks are especially efficient at performing
these types of writes. This means that the DCE LFS actually generates
considerably fewer disk updates than a
UNIX
file system,
especially when performing operations that primarily change file
system metadata, such as file creation, deletion, and truncation.
.H 5 "How Logging Works"
.P
\ 
.P
Each aggregate has its own log, which is simply a dedicated portion of a
physical disk.
The size of the log for each aggregate is fixed when that aggregate is
initialized.
When the amount of data held in the log approaches the limits of the
space allocated to the log for that aggregate, the processing of new
file system requests is halted to allow pending transactions to
commit. File system changes are then flushed to disk, ensuring that
the log space previously required for records of those operations can
be freed.
.P
DCE LFS only logs file system metadata, such as ACLs,
directory entries,
UNIX
protection modes, and file and directory
status information. It does not keep a log of the changes to the user
data in a partition. Changes to metadata
can be logged quickly in small log records. The overhead
introduced by writing these small log records does not affect the
performance of the file system. On the other hand, maintaining a log
of changes to user data would require larger, longer
transactions that could easily require gigabytes of storage for log
records.
.P
To optimize the efficiency of the log and to minimize its size, some
restrictions are placed on the scope and type of operations that can
occur during a single transaction. DCE LFS restricts transactions to
a maximum of one VFS interface call. This is especially important
when a large number of log records are pending and their size nears
the physical limits of the log. In this case, new file system
operations are temporarily blocked to allow pending operations to
complete, reducing the number of records that must be retained in the
log. If transactions were allowed to span VFS interface calls, all
pending transactions could end up waiting for additional high-level VFS
operations to complete. In this case, none of these
operations could complete successfully, no log space could be freed,
and all subsequent system operations would be blocked. Bounding the
scope of a transaction eliminates this possibility, guaranteeing
smaller, more easily completed transactions.
.P
Similarly, long file system operations are broken into sequences of
short-lived transactions, each of which leaves the file system in a
consistent state. For example, a single operation that truncates a
file may be logged as a number of smaller transactions, each of which
truncates only a few blocks at a time. This guarantees that
transactions are short-lived, minimizing the size of the log
without requiring complex algorithms for log truncation, and identifying
the parts of the log that it is ``safe'' to reuse.
.P
The logging system is also integrated into the management of the disk
cache. File system functions must not directly modify data held in
the disk cache because all file system functions must use the logging
primitives in order to be recoverable. To guarantee this, most
higher-level functions simply release cached data when done, leaving
the logging system with the responsibility for writing that data back
to the file system or File Server machine. The log mechanism keeps a
record of the most recent log entry for each unit of locally cached
data. The cached data cannot be written back to a file system until
all relevant records are logged.
.H 4 "Access Control Lists"
.iX "ACL"
.iX "file" "protection"
.P
DCE ACLs are an enhancement to
UNIX
file protection and access
control mode bits. ACLs are not supported by conventional
UNIX
file
systems, although some vendors modify their file systems to
support specific ACL implementations. ACLs expand the control that
the owner of a file has over granting or denying file and directory
access to specific users or groups of users. ACLs allow a finer granularity of
access control for specific files and directories than do
UNIX
protection
bits.
.P
The DCE LFS supports both file and directory ACLs. A
description of the access control mechanisms provided by ACLs, and
the use of ACLs when developing applications, are presented in the ACL
section of Chapter 50 
of this guide and in the security sections of the \*EOSF DCE
Administration Guide\*O and Part 6 of this guide.
.H 4 "Anodes"
.P
For an applications programmer, one of the most important abstractions
provided by a file is its open-endedness. Programs can write to files
without having to directly allocate the storage associated with the
file. In conventional
UNIX
file systems, this open-endedness is
implemented at the level of the inode. 
.iX "inodes"
Other file-oriented
abstractions, such as authorization (mode bits and ACLs), status
information (access and update times), position in the directory
hierarchy, and reference (link) counts are also associated with
UNIX
inodes.
.P
Bundling all of these abstractions into a primary file system
structure can be inconvenient to the kernel programmer. When these
high-level abstractions are irrelevant, the additional overhead
of having them can discourage taking advantage of a file's
open-endedness. This
encourages either abandoning open-endedness in favor of fixed limits,
or reimplementing a primary form of open-ended resource allocation.
.iX "anodes"
.P
The DCE LFS's anode abstraction provides a convenient descriptor for
referencing an open-ended address space of storage, and nothing more.
All DCE LFS objects that require disk storage (for example, files, ACLs, 
filesets, aggregates, transaction logs, and low-level disk allocation bitmaps)
are implemented as anodes. Files are implemented as anodes
with additional information including a set of status bytes, a pointer
to an ACL, and a position in the directory hierarchy.
.P
Because both user and metadata are referenced through anodes,
disk utilities can access the disk uniformly, regardless of the level
of file system access that is required. This simplifies the
construction of utilities, such as the logging system and disk
recovery mechanisms, which might otherwise have to distinguish between
anode and nonanode disk areas.
.H 4 "DCE LFS and  UNIX File System (UFS) Differences"
.iX "UNIX file system"  "accessing from DFS"
.iX "DCE LFS versus UFS"
.P
In DCE,
UNIX
file systems can be accessed from remote machines if
they are exported to the DFS filespace. The only practical way to
export a UFS to the DCE environment is to export it as a unit at the lowest
level for which a parallel DFS organization exists. This means that a UFS
partition is exported as a single fileset. Once exported, this partition is
referred to as an aggregate that contains at most one fileset. The
Virtual File System interface (VFS+) 
provides a consistent mechanism for accessing the UFS files
either via DFS or by local processes running on the machine exporting the UFS.
.P
Not all fileset operations are supported for these non-LFS filesets because DCE
LFS functionality is a superset of the functionality provided by standard UFSs.
DCE LFS aggregates can accommodate multiple DCE LFS filesets;
UNIX
disk partitions can store only a single fileset. Because of this, DCE LFS
filesets are usually smaller than non-LFS filesets and easier to manage.
Lone fileset operations, such as dump and restore, can be supported for non-LFS
filesets. The cloning, move, and replication fileset operations are not
supported for non-LFS filesets because these require the capability of
accommodating multiple filesets in an aggregate.
.H 3 "The Virtual File System Interface (VFS+)"
.iX "Virtual File System interface"
.iX "VFS+"
.P
The VFS+ interface used in DFS is an enhancement of the
Virtual File System interface found in most
UNIX
kernels, or the Generic
File System (GFS) found in Ultrix. The VFS+
interface extends the VFS interface in several ways, adapting it to
provide the operational requirements of a distributed computing
environment.
.P
The VFS+ interface adds the following enhancements to standard
VFS interfaces:
.ML
.LI 
Generalized Credentials: 
The VFS+ interface implements an
open credential mechanism that allows the use of a variety of
authentication mechanisms. The new credential can therefore carry
both conventional
UNIX
authentication information and any other form of
authentication information required, such as authentication tickets.
.LI
Synchronization: A synchronization package is
incorporated into the VFS+ interface operations, implementing a mechanism
for 2-way communications between servers and their clients. When an
object in any type of VFS is changed, the Token Manager
notifies all interested parties.
.LI
Fileset/Aggregate Interface: The VFS+ interface supports
operations on DFS filesets and aggregates, including creating new
filesets within an aggregate, putting those filesets online, taking
existing filesets offline, iterating through the files in a fileset,
and enumerating existing filesets.
.LE
.P
The enhancements implemented by the VFS+ interface ensure compatibility
among local and remote processes simultaneously accessing files. For
example, a VFS+ server can make guarantees about when an exported file
changes while, at the same time, clients
attempt to modify these files. All of these types of access are
synchronized through the VFS+ interface.
.P
The VFS+ interface does not require that changes be made to external
servers or software in order to access standard VFS or vnode-level
functions. To eliminate potential problems with non-DCE software and
servers, the standard VFS vnode-level functions are redefined to
invoke the synchronization package that keeps track of the guarantees
made by the various servers. These updated function definitions are
implemented by writing ``wrapper'' functions for existing virtual file
system operations. These wrapper functions perform the appropriate
synchronization calls before and after the original VFS call.
.P
When using the VFS+ interface, calling a VFS function initially
executes an internal locking routine that provides a consistent
interface to all VFS types. The call declares the
operation to be performed and also identifies the file IDs involved
in that operation. As part of this function, other virtual file
systems may be called upon to relinquish conflicting access guarantees
(such as tokens) that they may already have issued for the same files.
Once the required access guarantees are obtained, the original
virtual file system operation is performed. When that operation is
complete, the locks are released. Releasing these locks does not
directly return the access guarantees required by the VFS operation,
but simply notifies the local synchronization package that the access
guarantees for this operation may now be released whenever necessary.
.H 3 "The Cache Manager"
.iX "DFS" "Cache Manager"
.iX "caching"  "definition"
.P
The DFS Cache Manager is a kernel-resident part of DFS that is
responsible for the local caching of file and directory data on
machines used as file system clients. Caching means that
once any portion of a file is requested and retrieved by a client, a
copy of that data is kept on the client machine. If the same part of
the file is requested again, the locally cached copy is immediately
available; it is not necessary to re-retrieve the data. The tokens
associated with the data the first time it was retrieved ensure that
the file has not changed on the server since it was first cached. If
the file has changed, a new copy is retrieved from the File Server
machine. Caching provides a substantial performance improvement over
using remote operations to fetch file and directory information each
time it is required.
.P
The client's Cache Manager presents a VFS+ interface to the
UNIX
kernel. Logically, the Cache Manager sits between the File Exporter,
from which it receives file services, and the kernel, to which it
provides files. Neither the kernel nor the processes that use the
kernel to perform file operations need to know the physical location
of the files referenced. The VFS+ interface ensures that all
files are obtained through the same mechanism, regardless of whether
their source was a conventional UNIX file system or a DFS file system. 
.P
The Cache Manager is divided into several layers, as shown in
Figure 49-1.  The lowest
layer is the Operating System Independent (OSI) layer,
which is responsible for isolating all low-level operating system
calls such as basic I/O and network, physical file system,
and virtual memory calls. This layer helps isolate higher-level Cache
Manager functions from vendor-specific or system-specific issues.
.PP
.ne 3.5i
.FG "The Organization of the DFS Cache Manager"
.pI chapters/figures/1_fig_cache_manager.ps 0 0 1
.P
The next layer is the DCE Resource Layer, which is responsible for
maintaining RPC connections, authentication, fileset location
information, and other information about the state of the DCE
environment. The resource layer is divided into several modules:
.ML
.LI
A User Authentication Cache Module maintains a per-user list of
tickets and additional per-user credential information.
.LI
A Server Module maintains a set of per-server structures
for tracking the status of recently contacted servers.
.LI
A Fileset Module maintains a list of accessed filesets, their mounted
positions in the global file system tree, and their physical locations
on one or more File Server machines.
.LI
A Cell Module
maintains a list of the administrative DCE cells that have been
accessed by the particular Cache Manager.
.LE
.P
Above the resource layer is the Caching Layer itself.
This layer maintains the set of cached file information, separated
into status and data components. The basic operations performed at
the caching layer are fetching and storing the data and status
information associated with files.
A client uses the directory package to keep its
cached directories synchronized with the server's originals. When an
RPC call to a file server causes the contents of a remote
directory to be updated, the Cache Manager's directory package can
update the cached directory. This avoids having to refetch
directory information each time a directory is updated.
.P
The highest levels of the DFS Cache Manager are the VFS modules, which
implement the functions exported to the VFS+ interface, and therefore
offer the functions required by the kernel in order to treat DCE DFS
as a true external file system. This layer also contains the VFS
functions that provide the OS-specific code needed to support different
VFS/kernel interfaces. This isolates the Cache Manager from
OS-related differences in virtual file systems in the same way the
Cache Manager's OSI layer isolates the Cache Manager from OS
differences in lower kernel layers.
.H 3 "The File Exporter"
.iX "DFS" "File Exporter"
.P
The File Exporter is the kernel-resident part of a DFS File Server machine that
evaluates client requests and sends file and directory information in
response to those requests. The File Exporter actually consists of
several components:
.ML
.LI
Token Manager
.LI
Host Module
.LI
VFS+ Interface
.LI
Fileset Registry
.LI
Server Procedure
.LE
.iX "DFS" "Token Manager"
.iX "Token Manager"
.P
The Token Manager, called by the File Exporter, maintains the set of
file and directory tokens that have been granted to existing clients of that
File Server machine. Among these tokens is a binary compatibility
relation telling which other tokens can be simultaneously granted to
other clients. The Token Manager is discussed in more detail in
the next section.
.iX "DFS" "Host Module"
.iX "Host Module"
.P
The File Exporter's Host Module associates a pair of structures
with every client request. One structure describes the state of the
client Cache Manager that made the call and contains such information
as the delivery status of any token revocation messages issued to the
client. The other structure contains
authentication information about the user who made the file system
request on the client machine. This includes the user's authentication system
identity and group memberships.
.iX "VFS+ Interface"
.P
The File Exporter's VFS+ Interface interacts with the Cache Manager,
File Exporter, and DCE LFS to access the data stored at the server.
.iX "DFS" "Fileset Registry"
.iX "Fileset Registry"
.P
The File Exporter's Fileset Registry is a table that enumerates the
filesets residing locally on the server. It is the ``glue'' that sits
between the File Exporter and the Cache Manager and UFS.
Given a fileset name or ID,
the fileset registry must be able to identify and locate the
mount-level directory of that fileset. The fileset registry fills two
important functions in the processing of an incoming DFS request.
First, it tells the File Exporter whether a fileset is local, which
determines whether the request can be filled by that File Exporter.
Second, it maps the fileset specified in the request into a VFS
identifier, so that the appropriate file system anode can
be located for a given request.
.iX "DFS" "Server Procedure"
.iX "Server Procedure"
.P
Finally, the File Exporter's Server Procedure implements the
RPC interface in terms of calls to the previous components.
.iX "getting"  "fileset data"
.iX "filesets"  "getting data"
.P
Processing a request for file data or status information starts with
the host module, which keeps track of the information about each
client that is needed by the file server. To expedite client
requests, the host module maintains a cache of authenticated users on
that File Server machine and the RPC connections they have previously
used to contact the server.
.P
The information maintained by the host module for each user is
associated with a credential structure before the VFS file system
layer is called. This allows the File Exporter to invoke various
types of virtual file systems while still allowing each file system
to use its own authentication information.
.P
Once a request is associated with a host module client structure, the
File Exporter checks with the fileset registry for the fileset
referenced in the request, and passes the file IDs associated with the
request to the Token Manager. This gives all other DFS File Exporters
that may be holding tokens for the fileset
an opportunity to revoke the appropriate tokens or other forms of
promises they may have made to external clients. 
.P
The File Exporter converts the
incoming file IDs into vnode structures in a local fileset by calling
the appropriate VFS layer function. At this time, the actual VFS
operation is invoked to perform the desired function.
.H 4 "The File Exporter's Token Manager"
.iX "DFS" "Token Manager"
.iX "Token Manager"
.P
In order to guarantee that file or directory information in use by a
client machine is not simultaneously being modified, each
DCE File Server kernel includes a Token Manager to guarantee file access
synchronization. The Token Manager keeps track of the clients that
are referencing files located on that File Server machine. A Token
Manager is local to each File Server machine, and maintains
tokens for, and monitors access to, only files and directories on that
particular File Server machine. For example, the File Exporter
for a given File Server machine may grant a read token for a specific
file, allowing a client to read the contents of that file until
otherwise notified. The File Exporter records that the client has
received a guarantee, and does not allow any other client to write
data to the file without first revoking that guarantee by notifying
the client that its cached data must no longer be used.
.iX "callbacks"  "definition"
.P
When a File Exporter obtains any tokens on behalf of a client, it
registers a procedure to be called in the event that the token later
has to be revoked. Calls registered for later communications with
other processes are referred to as ``callbacks'' or ``revocations''.  When an
incompatible token must be revoked, the Token Manager executes that
callback to the client to revoke the token.
.P
The Token Manager is invoked by all calls through the VFS+ interface,
and ensures that any access incompatible with synchronization permissions
already granted through existing tokens are revoked before the operation
continues. The Token Manager is invoked by the VFS+ layer because
both non-DFS File Exporters and locally executed system calls can
perform various operations on local physical file systems that must
be synchronized with the guarantees exported by the DFS File Exporter.
.P
Using the Token Manager to control physical file and directory access
rights suggests expanding the term ``client'' from simply being a
remote user of files exported by a file server, to being any system
(local or remote) that requests tokens on a file.
This is a more flexible and appropriate definition of a client,
because the potential clients of a Token Manager can range from the
local UNIX kernel to any number of remote Cache Managers.
.H 4 "Types of Tokens"
.P
DFS File Exporters support a number of different types of tokens.
These different token types reflect the different types of access to files
and directories that are required in a distributed computing
environment. Because tokens reflect the type of file or directory
access requested by a client, some types of tokens are incompatible.
Before granting a new token, the Token Manager may have to revoke some
existing tokens. The following types of tokens can be granted by a
File Server machine's File Exporter:
.ML
.iX "tokens"  "data"
.iX "data tokens"
.LI
Data Tokens:
Data Tokens
grant the client the right to access a range
of bytes in a file. Both read and write data tokens are
available. A read data token allows the client to cache and use a
copy of the relevant file data without repeatedly performing RPCs to
the appropriate file server (either for validating the data or for
re-reading it). A write data token allows the client to update the
data in a cached copy of the file without storing the data back to the
server or notifying the server.
.iX "tokens"  "status"
.iX "status tokens"
.LI
Status Tokens: 
Status Tokens 
allow the client to access the status
information associated with a file or directory. Both read and
write status tokens are available. A read status token allows
the client to refer to cached copies of the status information without
calling the server to check the status. A write status token
allows the client to update its cached copy of the file's status
without notifying the server. 
.iX "tokens"  "lock"
.iX "lock tokens"
.LI
Lock Tokens:
Lock Tokens
allow the client to set a lock on a
particular range of bytes within a file. Both read and write lock tokens are available. With a lock token, the client is
assured that other clients cannot set conflicting locks on
the file without first revoking the token. 
Lock tokens behave in the same way as open tokens with respect to revocation;
see the following.
.iX "tokens"  "open"
.iX "open tokens"
.LI
Open Tokens: 
Open Tokens
grant the holder the right to open a file.
Different types of open tokens are available, corresponding to
different possible open modes: normal reading, normal
writing, executing, shared reading (same as executing),
and exclusive writing.
.iX "tokens"  "preserve"
.iX "preserve tokens"
.iX "tokens"  "delete"
.iX "delete tokens"
.LI
Preserve and Delete Tokens: used for preserving zero-link-count files without
interfering with other aggregate operations.
.LE
.iX "tokens"  "compatibility"
.P
Different types of data or lock tokens can be held simultaneously for the same
file because they refer to separate byte ranges in the file. Table 49-1
describes the compatibility matrix of open tokens; a value of ``C'' indicates
that the tokens are compatible, and ``I'' that they are incompatible.
Tokens of the same type may also be incompatible with each other, as in the
following examples:
.ML
.LI
Read and write data tokens are incompatible if the
byte ranges associated with those tokens overlap, but otherwise are compatible.
.LI
All write data or lock tokens are incompatible with all others for the same
byte range.
.LI
Read and write lock tokens are incompatible if the
byte ranges associated with those tokens overlap, but otherwise are compatible.
.LE
Preserve tokens only conflict with delete tokens.  Delete tokens conflict
with everything, and revoke all other open tokens.
.PP
.ne 3.0i
.TB "Compatibility Between Open Tokens"
.TS
center, box, tab(@);
lb | lb | lb | lb | lb | lb | lb
lb | lb | lb | lb | lb | lb | lb
lw(1.05i) | c  | c  | c  | c  | c  | c.
\^@Normal@Normal@Shared@Exclusive@@@
Access@Reading@Writing@Reading@Writing@Preserve@Delete
_
T{
Open for normal
.br
reading
T}@C@C@C@I@C@I
.sp .5v
T{
Open for normal
.br
writing
T}@C@C@I@I@C@I
.sp .5v
T{
Open for shared
.br
reading
T}@C@I@C@I@C@I
.sp .5v
T{
Open for
.br
exclusive writing
T}@I@I@I@I@C@I
T{
Open for
.br
preserve
T}@C@C@C@C@C@C
T{
Open for
.br
delete
T}@I@I@I@I@I@I
.TE
.P
DFS tokens are managed by the VFS+ wrapper functions, as described
previously. To summarize, the VFS+ wrapper functions modify all
standard VFS functions to first call the Token Manager, obtaining the
appropriate tokens for the operations they will perform before
actually performing those operations.
The functions that compose this part of the VFS+ interface are
frequently referred to as the ``glue layer.''
.P
When the Token Manager wishes to revoke a token, the File Exporter
notifies the client that holds the token. If the token was granted
for 
.iX "tokens"  "revocation"
.ML
.LI
Reading (status or data), the client has only to return it.
.LI
Writing, the client must write
back any status or data that it modifies before returning the
token.
.LI
Locking or opening, the client cannot return the token if the file
is still open, or if the corresponding lock is still held.
This is the normal action if the client has already locked or opened the file.
.LE
.P
For remote clients, token management requires two-way RPC
communications. Clients must call File Server machines to access files and
obtain tokens,
and File Exporters must call clients to request token revocation. Token
revocation requests for file systems that are local to the client
making the requests do not require RPC communications. When handling
requests to local file systems, the Token Manager and VFS+ interface
communicate directly via the Host Module.
.H 3 "The Fileset Server"
.iX "filesets"
.P
Filesets are the basic storage and administrative unit for data
in DFS. The abstraction provided by filesets is similar to that
provided by UNIX disk partitions, although DCE LFS filesets are actually
implemented as logical subsets of disk partitions (aggregates). The entire
contents of a DCE LFS fileset must physically be located within a
single aggregate, and its growth is limited by a per-fileset quota.
Non-LFS filesets are equal in size to the exported partition.
Keeping filesets small compared to aggregates
simplifies load balancing. Load balancing involves
moving filesets from one aggregate to another when the first aggregate
becomes nearly full or when equalizing load across various
File Server machines. It is easier to find room on other aggregates
for a fileset if it is relatively small. DCE LFS filesets can be moved
between aggregates on the same File Server machine, or to an aggregate
on another server. The files stored together in a fileset form an
entire subtree of the file system, which can be separately mounted and
administered. Filesets are mounted so that they create the illusion
of a seamless hierarchy of files, even though that hierarchy is
generally distributed across multiple File Server machines. Storing
filesets on multiple File Server machines, or dynamically moving
filesets between different machines, does not affect the transparency
of user access.
.iX "filesets"  "headers"
.P
A fileset header is associated with each fileset, and is stored
on the same aggregate as the fileset it describes.
The fileset header contains the fileset name, fileset ID number,
type, and status. The fileset header also contains the anode
indexes of all of the files and directories present in that fileset.
.P
The Fileset Server allows system administrators to create, delete,
duplicate (clone), move, back up, or restore entire filesets with a single
operation. The administration of DCE LFS filesets is covered in detail in the
\*EOSF DCE Administration Guide.\*O
.P
Creating, deleting, and moving filesets are standard administrative
functions that are required to effectively manage any subset of a file
system. 
.iX "filesets"  "cloning"
.iX "cloning"
Cloning
allows an administrator to dynamically produce an inexpensive
read-only copy of a DCE LFS fileset, recording its exact state at
the time of the copy. Uses of fileset clones include
serving as online backups from which users can retrieve a former
version of a file they accidently change or delete, or as
fileset replicas, which can be distributed to multiple File Server
machines. 
.iX "replication"
.iX "filesets"  "replication"
Replication
increases the availability of the contents of a
fileset, and can reduce the overhead associated with accessing
filesets that contain frequently used system binaries and files.
When copies of these files are available at multiple locations, the
requests for these files are distributed across the File
Server machines on which these files are available. An incidental benefit of
replicated filesets is that the File Exporter does not need to revoke tokens
because (by definition) files in a
read-only clone cannot change. Replication and cloning require features of
DCE LFS and thus are not supported for non-LFS filesets.
.P
It is important to understand the difference between cloning and
replication. 
.iX "cloning"  "versus replication"
.iX "replication"  "versus cloning"
Cloning means to create a read-only copy of an existing
fileset on the same aggregate. Replication, which uses the
cloning operation as a primitive, means making an exact copy of a
fileset, including all of the data blocks associated with the files
and directories in that fileset. This copy may or may not be located
on the same aggregate or File Server machine as the parent
fileset.
.P
Filesets can be cloned only within the aggregate on which the original
fileset is located, because the cloning operation initially copies only
the anode indexes for the files on the specified fileset, rather than
copying the files themselves. Once these are copied, the
original indexes are updated to point to the clone's copied indexes,
and a bit is set in each parent anode to indicate that this
indirection is occurring. Subsequent write operations on parts of the fileset
first check the
status of the indirection bit for the anode associated with a file.
If this bit is set, the original data must be copied to other blocks,
the indirection must be undone, and the indirection bit must be
cleared before the write actually occurs. This ensures that cloning a
fileset initially involves copying the minimum amount of information.
Initially, the parent fileset consists of a skeleton fileset that
simply points to the blocks in the clone. As more writes occur in a
cloned fileset, the parent fileset comes to own an
increasing number of ``new'' blocks. Only the blocks that change
are copied; a small change to a large file does not result in the entire
file being copied.
.iX "filesets"  "dumping"
.iX "filesets"  "restoring"
.P
Dumping and restoring files are standard administrative
functions necessary for any computer system. Dumping a fileset refers
to the process of copying a fileset to a data stream. The
target of this data stream is usually some archival device,
such as a tape drive. Restoring refers to the process of converting
a data stream back into proper fileset format. The
ability to dump and restore logical portions of disk storage is
necessary to create backups for long-term storage on tape. Backups
are done both as a precaution against loss of data due to hardware
failure and as a way of protecting users against the accidental
deletion of their own data. Backups are also routinely performed when
deleting a fileset from the file system. Dumping and restoring are
also used when moving filesets between machines.
.H 3 "The Fileset Location Database and Server"
.iX "Fileset Location Database"
.P
The Fileset Location Database (FLDB) is a cell-wide replicated
database that maps fileset names to the servers on which they are actually
located. The FLDB is accessed only via a collection of Fileset Location
Server processes, one at each machine on which the FLDB is replicated.
These RPC server processes provide calls to examine and change information
about the filesets located in the cell.
.P
An individual fileset location entry, an FLDB entry, contains the fileset's
name, its type, the File Server machines where the fileset is located,
the numeric identifier of the aggregate on the specified File Server machine
on which the fileset is located, any status flags specific to that file
server location, any flags associated with the fileset, and
the numeric identifiers of this and any associated filesets such as backups and
read-only copies.
.P
For more detailed information about legal fileset types and the
contents of fileset location entries, see the \*EOSF DCE
Administration Guide.\*O
.H 3 "The Replication Server"
.nS note
Replication only operates on DCE LFS filesets; it cannot be applied
to the UNIX File System.
.nE
.P
The organization of DCE LFS filesets into aggregates provides a number of
features not found on ordinary
UNIX
file systems. First, the concept of
DCE LFS filesets as logical subsets of aggregates means that filesets are
not tied to a physical location on the storage media and can
therefore be manipulated independently. This allows filesets to be
moved among partitions or moved from one server to another.
.P
Similarly, a read-only copy of a fileset (a clone)
can be created within the same aggregate where the original
fileset is located.  Fileset cloning can be used as an intermediate
backup mechanism or as a part of the replication process. 
Another way to make read-only copies of a fileset is to use replication
on the fileset.  Cloned copies of filesets
cannot exist outside the aggregate on which the parent fileset is
located, while replicas of filesets can be located on any file server
machine. This is similar to the distinction between the \*Lcp\*O (copy)
and \*Lln\*O (link) commands in UNIX type operating 
systems; that is, files can be physically copied
across partitions because the \*Lcp\*O command creates a copy of
both file and user data at the destination of the copy, but files can
never be hardlinked across partitions.
Similarly, filesets cannot be cloned outside the aggregate that holds
their parent fileset because they share storage directly.
.P
The DFS replication servers provide for both scheduled replication and
release replication of filesets.
.iX "replication"  "scheduled"
In scheduled replication,
a fileset can be copied and updated at
specified intervals by the replication server. A replica is therefore
maintained permanently and is guaranteed to be out-of-date by no more
than a time interval specified by the system administrator. For practical
reasons, the existing implementation is unable to keep up with short
intervals of time (less than 10 minutes).
Additionally, a fileset may be replicated via release replication, in
which case the administrator chooses the moment at which a snapshot is
taken as a source for replicas.
.iX "replication"  "release"
.P
Any client of a replicated fileset is guaranteed to always see a
consistent snapshot of the fileset and is guaranteed that the data in
the replica is never replaced by older data. When a replication
server must update the replica, it obtains only those files that have
changed during the replication interval from the master fileset.
.P
Both cloning and replication are useful tools for system
administration because the basic DFS backup unit is the fileset and not
the aggregate. A system administrator can back up a fileset by first
cloning it and then copying the clone to removable media whenever
convenient. Cloning is also an efficient backup mechanism because of
the small initial size of the cloned fileset, and because it initially
copies only the fileset's metadata. Cloned filesets can continue
to exist on disk indefinitely, during which time files can be directly
restored from the cloned fileset. This can eliminate, in some cases, the most
time-consuming part of restoring files from backups, which is the time
required to mount and search removable backup media.
.H 3 "The BOS Server"
.P
The BOS Server (Basic OverSeer Server) 
.iX "BOS Server"
runs on every DFS server machine and monitors the other user-space DFS server
processes on the machine. It restarts processes automatically
if they fail. The BOS Server also provides
an interface through which processes can be started, stopped, and monitored,
and through which binaries for software can be maintained.
.P
The programming interface to the BOS Server allows the creation, deletion,
and modification of processes. Processes are managed by the use of
bnodes, which keep track of process parameters, start times, and
frequency of execution. 
.iX "bnodes"
.H 2 "An Example of DFS File Access Synchronization"
.P
This section provides an example of the interaction between the
various DFS support modules, showing the roles of the Token Manager
and File Exporters in synchronizing access to a file. The file in
this example is stored in a conventional
UNIX
file system, and is being
written by both a local user (User1), who is issuing both read and
write system calls, and a remote user (User2), who is attempting to
access the same file through a client Cache Manager.
.P
Initially, User2's remote application issues a write system call to
the file. This operation is handled by the client's Cache Manager,
which requires a token guaranteeing that it is permitted to
update the cached copy of the file locally.
This token is requested from the File Exporter of
the File Server machine where the master copy of the file is located.
The File Exporter registers the client with the File Server machine's
Token Manager as having a data write token. Once User2 receives the
data write token, the client can handle all remote writes to the
cached copy of the file without contacting the File Server machine further.
.P
At some point User1, through a process local to the File Server
machine and therefore accessing the file locally (not through a DFS
File Exporter), decides to read some data from the server's copy of the
file. Before reading the server's copy, the VFS+ interface calls the
local Token Manager, requesting a read data token for the file.
Because there is a conflicting write data token already granted to User2
by the File Exporter, the read token for User1 cannot be granted
immediately;
User2's incompatible token must be revoked before User1's local
process can be granted its own token.
.P
At this point, the Token Manager attempts to resolve the access synchronization
conflict by requesting that the File Exporter revoke the conflicting
token. As part of the revocation procedure, the File Exporter makes
an RPC back to User2's client Cache Manager, asking it to return the write
token. Before the client returns the write token, it also stores
its modifications back to the fileset on the File Server machine in a
separate RPC.
Once this occurs, the File Exporter returns from the revocation
call made by the Token Manager. The new read data token can be
granted to the VFS+ interface call.
Once the new token is granted, the VFS+ interface can then proceed with
User1's read operation by calling the original virtual
file system's read data function.
.P
The read data token can be returned at any time after
the read data call has completed, although it is usually held as long as
possible to avoid unnecessary RPCs. Remote clients always hold
on to tokens as long as they can to avoid unnecessary RPCs. If
User2 issues another call to write the specified data, the client
system must contact the File Server machine again to get
another guarantee, and the File Exporter must call the
local Token Manager again to obtain another write data token.
Thus, the VFS+ layer provides a consistent file system image regardless of
the dispersion of clients making reference to any one piece of it.
