...\" @OSF_COPYRIGHT@
...\" COPYRIGHT NOTICE
...\" Copyright (c) 1990, 1991, 1992, 1993, 1994 Open Software Foundation, Inc.
...\" ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE for
...\" the full copyright text.
...\" 
...\" 
...\" HISTORY
...\" $Log: topics.gpsml,v $
...\" Revision 1.1.4.2  1996/03/27  13:07:14  wardr
...\" 	{edit R1.2.1}
...\" 	More Release Edits
...\" 	[1996/03/27  13:06:46  wardr]
...\"
...\" Revision 1.1.4.1  1996/03/21  20:31:39  wardr
...\" 	{edit R1.2.1}
...\" 	Release Edits
...\" 	[1996/03/21  20:31:07  wardr]
...\" 
...\" Revision 1.1.2.20  1995/06/07  14:38:56  rcb
...\" 	PRENTICE HALL reformat; final edits and changes
...\" 	[1995/06/05  19:56:21  rcb]
...\" 
...\" 	PRENTICE HALL reformat
...\" 	[1995/04/26  13:50:27  rcb]
...\" 
...\" 	incorporated 1.1 edits
...\" 	[1995/04/20  18:48:12  rcb]
...\" 
...\" Revision 1.1.2.19  1994/11/15  20:47:53  neilson
...\" 	Converted book title references to macro form.
...\" 	[1994/11/15  18:57:58  neilson]
...\" 
...\" Revision 1.1.2.18  1994/10/27  19:43:05  jshirley
...\" 	Final edits.
...\" 	[1994/10/27  19:42:12  jshirley]
...\" 
...\" Revision 1.1.2.17  1994/10/20  01:53:32  zahn
...\" 	{edt,9262,R1.1}
...\" 	(edt,7699,R1.1}
...\" 
...\" 	Added change bars around rpc_ss/sm note.
...\" 	[1994/10/20  01:53:15  zahn]
...\" 
...\" Revision 1.1.2.16  1994/10/19  21:07:07  zahn
...\" 	{edt,10526,R1.1}
...\" 
...\" 	Added technical review comments on
...\" 	memory management section.
...\" 	[1994/10/19  21:06:48  zahn]
...\" 
...\" Revision 1.1.2.15  1994/10/18  17:32:58  zahn
...\" 	{edt,10526,R1.1}
...\" 
...\" 	Consolidated memory management information
...\" 	from app_gd/rpc/idl.gpsml into this chapter,
...\" 	rewrote the entire section based on input
...\" 	from DEC, OSF, and HP.
...\" 	[1994/10/18  17:32:37  zahn]
...\" 
...\" Revision 1.1.2.14  1994/10/13  19:36:59  zahn
...\" 	{edt,????,R1.1}
...\" 
...\" 	Working on IDL encoding services memory
...\" 	management and other memory management
...\" 	chapters.  Checking back in so others
...\" 	can use.
...\" 	[1994/10/13  19:36:47  zahn]
...\" 
...\" Revision 1.1.2.13  1994/10/13  00:10:32  jshirley
...\" 	{def,10160,R1.1} Fixed pipe documentation.
...\" 	[1994/10/13  00:09:37  jshirley]
...\" 
...\" Revision 1.1.2.12  1994/10/12  13:04:41  jshirley
...\" 	{def,10526,R1.1} Reorg.  RPC chapters.  Consolidated Context Handle discussion.
...\" 	[1994/10/12  13:04:04  jshirley]
...\" 
...\" Revision 1.1.2.11  1994/09/14  19:21:13  jshirley
...\" 	{def,10526,R1.1,Reorg RPC chapters.
...\" 	[1994/09/14  19:20:40  jshirley]
...\" 
...\" Revision 1.1.2.10  1994/08/22  19:35:02  zahn
...\" 	{edt,9601,R1.1}
...\" 
...\" 	Added corrections to the example in the
...\" 	IDL encoding services section of this
...\" 	chapter from Rico A. at DEC.
...\" 	[1994/08/22  19:34:49  zahn]
...\" 
...\" Revision 1.1.2.9  1994/08/11  20:36:35  jhh
...\" 	expand copyright
...\" 	[1994/08/11  19:58:41  jhh]
...\" 
...\" Revision 1.1.2.8  1994/07/27  18:47:50  zahn
...\" 	{review,9601.R1.1}
...\" 
...\" 	Added more information about memory management
...\" 	to IDL encoding services subsection.
...\" 	[1994/07/27  18:47:11  zahn]
...\" 
...\" Revision 1.1.2.7  1994/07/26  19:53:36  zahn
...\" 	{review, 9601,R1.1}
...\" 
...\" 	Rewrote the section on memory management in
...\" 	the IDL encoding services section of this chapter
...\" 	to explain how to allocate and free memory when
...\" 	using the IDL encoding services.  Used material
...\" 	from email correspondence between Greg Carpenter (HP),
...\" 	John Bowe (OSF), and Rico A. (DEC).
...\" 	[1994/07/26  19:53:23  zahn]
...\" 
...\" Revision 1.1.2.6  1994/07/20  22:37:37  zahn
...\" 	(edt,9601,R1.1}
...\" 
...\" 	Moved sentence about using comm_status with
...\" 	IDL encoding services to this chapter, in
...\" 	IDL encoding services section, from acf.gpsml
...\" 	general discussion of comm_status.
...\" 	[1994/07/20  22:37:21  zahn]
...\" 
...\" Revision 1.1.2.5  1994/07/18  20:32:16  zahn
...\" 	{enh,9951,R1.1}
...\" 
...\" 	Removed references to rpc_status_codes(7rpc) and
...\" 	replaced with reference to Problem Determination Guide.
...\" 
...\" 	{def,10526,R1.1}
...\" 
...\" 	Removed an unnecessary section head "Defining Exceptions
...\" 	in the IDL" and moved the discussion of the exceptions interface
...\" 	attribute syntax and example to the description
...\" 	of the attribute in app_gd/rpc/idl.gpsml.
...\" 
...\" 	Cleaned up the section on IDL encoding services and removed
...\" 	information that is redundant to the idl_es* reference pages.
...\" 	[1994/07/18  20:31:57  zahn]
...\" 
...\" Revision 1.1.2.4  1994/07/13  19:42:08  zahn
...\" 	{def,10526,R1.1}
...\" 
...\" 	Eliminated redundancy of encode, decode attributes
...\" 	in this file and in topics.gpsml.
...\" 	[1994/07/13  19:41:33  zahn]
...\" 
...\" Revision 1.1.2.3  1994/07/12  22:21:00  zahn
...\" 	{def,10526,R1.1}
...\" 
...\" 	Moved all of app_gd/rpc/10a_encode.gpsml
...\" 	to section "Creating Portable Data with
...\" 	the IDL Encoding Services". Cut routine
...\" 	descriptions because they were redundant
...\" 	to the routine reference pages in the app_ref.
...\" 	[1994/07/12  22:20:46  zahn]
...\" 
...\" Revision 1.1.2.2  1994/07/11  19:45:28  jshirley
...\" 	{def,10526,R1.1}
...\" 	Fixed the names of some headings.
...\" 	[1994/07/11  19:44:16  jshirley]
...\" 
...\" 	{def,10526,R1.1}
...\" 	Created Headings and inserted some info. from old files
...\" 	4_remoteness.gpsml, 8_err_hndling.gpsml, and 6_advanced_topics.gpsml.
...\" 	[1994/07/11  19:23:01  jshirley]
...\" 
...\" Revision 1.1.2.1  1994/07/08  15:04:16  jshirley
...\" 	{def,10526,R1.1}
...\" 	Created new file for the RPC reorganization.
...\" 	Other information to be inserted from older files later.
...\" 	[1994/07/08  15:03:05  jshirley]
...\" 
...\" $EndLog$
...\"
.H 1 "Topics in RPC Application Development"
.P
This chapter describes special features of DCE RPC for application
development.  The topics include
.ML
.LI
Memory management
.LI
Error handling
.LI
Context handles
.LI
Pipes
.LI
Nested calls and callbacks
.LI
Routing RPCs
.LI
Portable data and the IDL encoding services
.LE
.H 2 "Memory Management"
...\"
.iX "IDL" "memory management"
.iX "memory" "management"
.iX "allocating memory"
.iX "freeing memory"
.iX "memory" "freeing"
.iX "memory" "allocating"
...\"
.PP
When called to handle a remote operation, RPC client
stubs allocate and free memory by using whatever
memory management scheme is currently in effect.
The \*Eclient code\*O\(emthe generic code that
can be called from either RPC clients or RPC 
servers\(emcan use DCE RPC stub support routines to
control which memory management scheme
the stubs will use.
.P
If client code has not explicitly set the memory
management routines, the RPC client stubs use the
following defaults:
.ML
.LI
When called from manager code, and the operation contains
one or more parameters that are full or unique pointers, or
the ACF \*Lenable_allocate\*O attribute has been applied,
the client stubs use the \*Lrpc_ss_allocate(\|)\*O and
\*Lrpc_ss_free(\|)\*O routines.
.LI
When called from any other context, the RPC client stubs
use the operating system allocation and free routines
(for example, \*Lmalloc(\|)\*O and \*Lfree(\|)\*O) on POSIX
platforms.
.LE
.P
Note that the memory management scheme established, whether
explicitly or by default, is on a per-thread basis.
.P
RPC server stubs do not allocate memory.  Instead, they rely
on the \*Emanager code\*O\(emthe code that the server
stubs call\(emto allocate it for them.
.P
The following sections gives guidelines for how client code
and manager code should use the the various allocation and
free routines provided with DCE.
.nS note
DCE provides two versions of DCE RPC stub support routines.
The \*Lrpc_ss_\*V*\*L(\|)\*O routines raise an exception, while the 
\*Lrpc_sm_\*V*\*L(\|)\*O
routines return an error status value.  In all other ways, the
routines are identical.  It is generally recommended that you
use the \*Lrpc_sm_\*V*\*L(\|)\*O routines instead of the 
\*Lrpc_ss_\*V*\*L(\|)\*O routines
for compliance with the \*(Dc.
.nE
.H 3 "Using the Memory Management Defaults"
.PP
If it does not matter to the client code which memory allocation
routine the RPC client stubs use, the client code
should call the \*Lrpc_ss_client_free(\|)\*O routine
to free any memory that the client stub allocates and returns.
The \*Lrpc_ss_client_free(\|)\*O routine uses the current
free routine that is in effect.  Client code that uses
\*Lrpc_ss_client_free(\|)\*O must use caution if it calls
other routines before it frees all of the pieces of allocated
storage with \*Lrpc_ss_client_free(\|)\*O, because it is possible
that the called code has been written so that it swaps in a
different allocation/free pair without reestablishing the
previous allocation/free pair on exit.
.H 3 "Using rpc_ss_allocate and rpc_ss_free"
.PP
Both client code and manager code can use \*Lrpc_ss_allocate(\|)\*O
and \*Lrpc_ss_free(\|)\*O.  The next sections describe how.
.H 4 "Using \*Lrpc_ss_allocate\*O and \*Lrpc_ss_free\*O in Manager Code"
.P
Manager code uses either the \*Lrpc_ss_allocate(\|)\*O and
\*Lrpc_ss_free(\|)\*O routines or the operating system 
allocation and free routines to allocate and free memory.
.P
Manager code uses \*Lrpc_ss_allocate(\|)\*O to allocate
storage for data that the server stub is to send back to
the client.  Manager code can either use \*Lrpc_ss_free(\|)\*O to
free the storage explicitly, or it can rely on the server stub
to free it.  After the server stub marshalls the output parameters,
it releases any storage that the manager code has allocated 
with \*Lrpc_ss_allocate(\|)\*O.
.P
Manager code can also use the \*Lrpc_ss_free(\|)\*O routine to release
storage pointed to by a full pointer in an input parameter and
have the freeing of the memory reflected on return to the calling
application if the \*Lreflect_deletions\*O attribute has been
specified as an operation attribute in the interface definition.
See Chapter 18 for instructions on how to declare the
\*Lreflect_deletions\*O operation attribute.
.P
.ne 5
Manager code uses the operating system allocation routine
to create storage for its internal data.
The server stub does not automatically free memory that
operating system allocation routines have allocated.
Instead, manager code must use the operating system
free routine to deallocate the memory explicitly
before it exits.
.P
When manager code makes a remote call, the default
memory management routines are
\*Lrpc_ss_allocate(\|)\*O and \*Lrpc_ss_free(\|)\*O.
.H 4 "Using \*Lrpc_ss_allocate\*O and \*Lrpc_ss_free\*O in Client Code"
.P
Client code may also want to use the \*Lrpc_ss_allocate(\|)\*O
and \*Lrpc_ss_free(\|)\*O routines as the stub memory management
scheme.  However, before client code can use \*Lrpc_ss_allocate(\|)\*O and
\*Lrpc_ss_free(\|)\*O, it must first call the
\*Lrpc_ss_enable_allocate(\|)\*O routine, which enables
the use of \*Lrpc_ss_allocate(\|)\*O.  If client code calls
\*Lrpc_ss_enable_allocate(\|)\*O, it must also call the
\*Lrpc_ss_disable_allocate(\|)\*O routine before it exits
its thread to disable use of \*Lrpc_ss_allocate(\|)\*O.
This routine releases all of the memory allocated by calls to
\*Lrpc_ss_allocate(\|)\*O in that thread since the call to 
\*Lrpc_ss_enable_allocate(\|)\*O was made.
As a result, client code can either free each piece of allocated storage
with \*Lrpc_ss_free(\|)\*O, or it can have \*Lrpc_ss_disable_allocate(\|)\*O
free it all at once when it disables the \*Lrpc_ss_allocate/free\*O memory
management scheme.
.P
Before calling \*Lrpc_ss_enable_allocate(\|)\*O, client code
must ensure that it has not been called by code that
has already set up the \*Lrpc_ss_allocate/free\*O
memory management scheme.  As a result, if the client code
can ensure that it has not been called from a manager
routine, \*Vand\*O it can ensure that any previous
calls to \*Lrpc_ss_enable_allocate(\|)\*O
have been paired with calls to \*Lrpc_ss_disable_allocate(\|)\*O,
it can safely call \*Lrpc_ss_enable_allocate(\|)\*O.
.P
If client code cannot ensure that these conditions are true,
it should check to make sure the \*Lrpc_ss_allocate/free\*O
scheme has not already been set up.  For example:
.oS
.ne 22
/* Get RPC memory allocation thread handle */

    rpc_ss_thread_handle_t thread_handle;
    idl_void_p_t (*p_saved_alloc)(unsigned long);
    void (*p_saved_free)(idl_void_p_t);

    TRY
        thread_handle = rpc_ss_get_thread_handle();
    CATCH(pthread_badparam_e)
        thread_handle = NULL;
    ENDTRY

    if (thread_handle == NULL)    {

     /* Set up rpc_ss_allocate environment */

        rpc_ss_enable_allocate();
    }

    rpc_ss_swap_client_alloc_free(
        appl_client_alloc,appl_client_free,
        &p_saved_alloc,&p_saved_free);
.oE
.P
After control returns from the client stub, the client code
should again check to see whether \*Lrpc_ss_allocate/free\*O has
already been enabled before it calls \*Lrpc_ss_disable_allocate(\|)\*O:
.oS
    rpc_ss_set_client_alloc_free(p_saved_alloc,p_saved_free);

/* If we set up rpc_ss_allocate environment, disable it now */

    if (thread_handle == NULL)
        rpc_ss_disable_allocate();
.oE
.ne 15
.H 3 "Using Your Own Allocation and Free Routines"
.PP
At times it might be necessary for client code to change
the routines that the client stubs use to allocate and free memory.
For example, client code that is making an RPC call might want to
direct the RPC client stubs to use special ``debug'' versions
of \*Lmalloc(\|)\*O and \*Lfree(\|)\*O that check
for memory leaks.  Another example might be an application
that uses DCE RPC but needs to preserve its users' ability
to free memory returned from the application by using the
platform's memory management scheme (rather than exposing the
user to DCE).
.P
Client code that wants to use its own memory allocation and free
routines can use the \*Lrpc_ss_swap_client_alloc_free(\|)\*O routine
to exchange the current client allocation and freeing mechanism
for one supplied in the call.  The routine returns pointers to the
memory allocation and free routines formerly in use.
Before calling \*Lrpc_ss_swap_client_alloc_free(\|)\*O, client code
must ensure that it has not been called from a manager routine.
.P
Deallocation of allocated storage returned from the client stubs
is not automatic.  Therefore, client code must ensure that it
uses the free routine that it specified in the call 
to \*Lrpc_ss_swap_client_alloc_free(\|)\*O to deallocate
each piece of allocated storage.
.P
Client code that swaps in memory management routines with
\*Lrpc_ss_swap_client_alloc_free(\|)\*O should use the
\*Lrpc_\%ss_\%set_\%client_\%alloc_\%free(\|)\*O routine before
it exits to restore the old allocation and free routines.
.H 3 "Using Thread Handles in Memory Management"
.PP
.iX "memory" "management" "server threads"
.iX "server" "threads" 
.iX "thread" "memory management for"
There are two situations where control of memory
management requires the use of thread handles.  The more
common situation is when the manager thread spawns additional threads.
The less common situation is when a program transitions from being a
client to being a server, then reverts to being a client. 
.H 4 "Spawning Threads"
.iX "spawning server threads"
.iX "memory" "server threads"
.P
When a remote procedure call invokes the manager code, 
the manager code may wish to spawn additional threads to complete the
task for which it was called.  To spawn additional threads that are
able to perform memory management, the manager code must first call
the \*Lrpc_\%ss_\%get_thread_handle(\|)\*O routine to get its thread handle
and then pass that thread handle to each spawned thread.  Each spawned
thread must call the \*Lrpc_\%ss_\%set_thread_handle(\|)\*O routine
with the handle received from the manager code.
.PP
These routine calls allow the
manager and its spawned threads to share a common memory management
environment.  This common environment enables memory allocated by the
spawned threads to be used in returned parameters and causes all
allocations in the common memory management environment to be released
when the manager thread returns to the server stub. 
.PP
The main manager thread must not return control to the server stub
before all the threads it spawned complete execution; otherwise,
unpredictable results may occur.
.PP
The listener thread can cancel the main manager thread if the
remote procedure call is orphaned or if a cancellation occurs on the
client side of the application.  You should code the main manager thread
to terminate any spawned threads before it exits.  The code should
anticipate exits caused by an unexpected exception or by being canceled.
.PP
Your code can handle all of these cases by including a \*LTRY/FINALLY\*O
block to clean up any spawned threads if a cancellation or other exception
occurs.  If unexpected exceptions do not concern you, then your code can
perform two steps.  They are
disabling cancelability before threads are spawned followed by enabling
cancelability after the join operation finishes and after testing for any
pending cancel operations.  Following this disable/enable sequence
prevents routine \*Lpthread_join(\|)\*O from producing a cancel point in a
manager thread that has spawned threads which, in turn, share thread
handles with the manager thread.
.H 4 "Transitioning from Client to Server to Client"
.iX "clients becoming servers"
.P
Immediately before the program changes from a client to a
server, it must obtain a handle on its environment as a client by calling
\*Lrpc_ss_get_thread_handle(\|)\*O.  When it reverts from a server to
a client, it must reestablish the client environment by calling
the \*Lrpc_ss_set_thread_handle(\|)\*O routine, supplying the previously
obtained handle as a parameter.
...\"
...\"------------------------------------------------------------
.H 2 "Guidelines for Error Handling"
.PP
During a remote procedure call, server and communications errors may
occur.  These errors can be handled using any or all of the following
methods:
.iX "errors"
.iX "exceptions"
.iX "failures"
.iX "server" "failure"
.iX "communication failure"
.iX "exceptions" "handler"
.iX "routines" "error"
.iX "comm_status attribute"
.iX "ACF"
.ML
.LI
Writing exception handler code to recover from the error or to exit
the application
.LI
Using the \*Lfault_status\*O attribute in the ACF to report an RPC 
server failure
.LI
Using the \*Lcomm_status\*O attribute in the ACF to report a 
communications failure
.LE
.P
Use of exceptions, where the procedure exits the program due to an error,
tends to improve code quality.  It does this by making errors
obvious because the program exits at that point,
and by lessening the amount of code needed to detect
error conditions and handle them. 
When you use 
the \*Lfault_status\*O attribute, an exception that occurs on the server 
is not reported to the client as an exception.
The variable to which the
\*Lcomm_status\*O attribute is attached contains
error codes that report errors that would not have 
occurred if the
application were not distributed over a communications network. 
The \*Lcomm_status\*O 
attribute provides a method
of handling RPC errors without using an exception handler.
.H 3 "Exceptions"
.PP
Exceptions report either RPC errors or errors in
application code.  Exceptions have the following characteristics:
.P
.ML
.LI
You do not have to adjust procedure declarations between local and 
distributed code.
.LI
You can distribute existing interfaces without changing code.
.LI
You do not have to check for failures.  This results in more 
robust code because errors are reported even if they are not checked.
.LI
Your code is more efficient when there is no recovery coded for failures.
.LI
You can use a simpler coding style.
.LI
Exceptions work well for coarse-grained exception handling.
.LI
If your application does not contain any exception handlers and the 
application thread gets an error, the application thread is 
terminated and a system-dependent error message from the threads 
package is printed.
.LE
.P
.nS note
.iX "exception codes, RPC exceptions"
RPC exceptions are equivalent to RPC status codes.  To identify the
status code that corresponds to a given exception, replace the
\*L_x_\*O string of the exception with the string \*L_s_\*O.  For
example, the exception \*Lrpc_x_comm_failure\*O  is equivalent to the
status code \*Lrpc_s_comm_failure\*O.  The RPC exceptions are defined
in the \*Ldce/rpcexc.h\*O header file, and the equivalent status codes
are described in the
\*(Tg.
.nE
.iX "IDL" "user-defined exceptions"
.iX "extern_exceptions attribute"
.iX "exceptions" "attribute"
.iX "system exceptions"
.P
The set of exceptions that can always be returned from the server to
the client (such as the \*Lrpc_x_invalid_tag\*O exception) 
are referred to as \*Vsystem exceptions\*O. 
These exceptions are defined in \*Ldce/rpcexec.h\*O and
\*Ldce/exec_handling.h\*O. 
...\".sp
...\".iS
...\"exc_e_arritherr
...\"exc_e_fltdiv
...\"exc_e_fltovf
...\"exc_e_fltund
...\"exc_e_illinstr
...\"exc_e_intdiv
...\"exc_e_intovf
...\"pthread_cancel_e
...\"rpc_x_invalid_bound
...\"rpc_x_invalid_tag
...\"rpc_no_memory \*O(Raised to client as \*Lrpc_x_remote_no_memory)
...\"rpc_x_ss_context_mismatch
...\"rpc_x_ss_pipe_closed
...\"rpc_x_ss_pipe_comm_error
...\"rpc_x_ss_pipe_discipline_error
...\"rpc_x_ss_pipe_empty
...\"rpc_x_ss_pipe_memory
...\"rpc_x_ss_pipe_order
...\"rpc_x_ss_remote_comm_failure
...\".iE
.P
An interface definition can also specify a set of user-defined
exceptions that the interface's operations can return
to the client.  You can declare user-defined exceptions in
an interface definition by using the \*Lexceptions\*O interface
attribute, which is described in Chapter 17.
.P
.ne 6
If a user-defined exception in the implementation of a
server operation occurs during server execution, the server
terminates the operation and propagates the exception to
the client in a manner similar to the way system exceptions
are propagated.  If a server implementation of an operation raises an
exception that is neither a system exception nor a user-defined exception,
the exception returned to the client is \*Lrpc_x_unknown_remote_fault\*O.
.P
By default, the IDL compiler defines and initializes all exceptions
under a ``once block'' in the generated stubs.  If you want to
share exception names in multiple interfaces or you desire greater
control over how these exceptions are defined and initialized,
you can use the ACF \*Lextern_exceptions\*O attribute to
disable the automated mechanism that the IDL compiler
uses to define and initialize exceptions.  See Chapter 19
for more information on the \*Lextern_exceptions\*O attribute.
.P
Because exceptions are associated with operation implementation, they are not
imported into other interfaces by way of the \*Limport\*O declaration.
For more information about using exceptions to handle errors, see 
Part 2 of this guide.
.H 3 "The fault_status Attribute "
.PP
The \*Lfault_status\*O attribute requests that errors 
occurring on the server due to incorrectly specified parameter values, 
resource constraints, or coding errors be reported by a
designated status parameter instead of by an exception. 
.P
If a user-defined exception is returned from a server to a client that has
specified \*Lfault_status\*O on the operation in which the exception
occurred, the value given to the \*Lfault_status\*O parameter is
\*Lrpc_s_fault_user_defined\*O.
.PP
The \*Lfault_status\*O attribute 
has the following characteristics:
.PP
.ML
.LI
Occurs where you do not want transparent 
local/remote behavior
.LI
Occurs where you expect that you may be passing 
incorrect data to the server or the server is not coded robustly, or both
.LI
Works well for fine-grained error handling
.LI
Requires that you adjust procedure declarations between local 
and distributed code
.LI
.ne 4
Controls the reporting only of errors that come from the server 
and that are reported via a fault packet 
.LE
.PP
For more information on the \*Lfault_status\*O attribute, see 
Chapter 19.
.H 3 "The comm_status Attribute "
.PP
The \*Lcomm_status\*O attribute requests that RPC 
communications failures be reported through a designated status parameter 
instead of by an exception.  The \*Lcomm_status\*O attribute 
has the following characteristics:
.PP
.ML
.LI
Occurs where you expect communications to 
fail routinely; for instance, no server is available, the
server has no resources, 
and so on
.LI
Works well for fine-grained error handling; for example, trying
a procedure many times until it succeeds
.LI
Requires that you adjust procedure declarations between local 
and distributed code to add the new status parameter
.LI
Controls the reporting of errors only from RPC runtime 
error status codes
.LE
.PP
For more information on the \*Lcomm_status\*O attribute, see
Chapter 19.
.H 3 "Determining Which Method to Use for Handling Exceptions"
.PP
Some conditions are better for using the \*Lcomm_status\*O or 
\*Lfault_status\*O attribute on an operation, rather than the 
default approach of handling exceptions.
.PP
.ne 11
The \*Lcomm_status\*O attribute is 
useful only if the call to the operation has a 
specific recovery action to perform for one or more communications
failures; for example, \*Lrpc_s_comm_failure\*O or 
\*Lrpc_s_no_more_bindings\*O.  The \*Lcomm_status\*O attribute is recommended 
only when the application knows
that it is calling a remote operation.
If you expect communications to fail often because the server
does not have enough resources to execute the call, you can use this
attribute to allow the call to be retried several times.
If you are using an implict or explicit binding, you can use the 
\*Lcomm_status\*O attribute if you want to try another server because the 
operation cannot be performed on the one you are currently using.
You can also use an exception handler for each of the two previous
instances.
.P
In general, the advantange of using \*Lcomm_status\*O if the recovery is 
local to the routine is that the overhead is less.
The disadvantage of using 
\*Lcomm_status\*O is that it results in two different operation signatures.
Distributed calls contain the \*Lcomm_status\*O attribute,
however; local calls do not.
Also, if all of the recovery cannot be done locally
(where the call is made),
there must be a way to pass the status to outer layers of code to 
process it.
.PP
The \*Lfault_status\*O attribute is 
useful only if the call to the operation has a
specific recovery action to perform for one or more server faults;
for example, \*Lrpc_\%s_\%invalid_\%tag\*O, \*Lrpc_\%s_\%fault_\%pipe_\%comm_\%error\*O,
\*Lrpc_\%s_\%fault_\%int_\%overflow\*O, or \*Lrpc_\%s_\%fault_\%remote_\%no_\%memory\*O.  
Use \*Lfault_\%status\*O only when the application calls a remote operation
and wants different behavior than if it calls the same operation locally.
If you are requesting an operation on a large data set, you can use
this attribute to trap \*Lrpc_\%s_\%fault_\%remote_\%no_\%memory\*O and retry the
operation to a different server, or you may break your
data set into two smaller sections.
You can also handle the previous case with exception handlers.  The
advantange of using \*Lfault_\%status\*O if the recovery is local is that
the overhead is less.  The disadvantage of
\*Lfault_\%status\*O is that the operation is different between the local
and distributed case.  Also, if all of the recovery cannot be done
locally, there must be a way to pass the status to outer layers
of code to process it.
.H 3 "Examples of Error Handling"
.PP
The following subsections present 
two examples of error handling.  The first example 
assumes that the \*Lcomm_status\*O attribute is in use in the ACF.
The second example assumes that the \*Lcomm_status\*O attribute is not in use.
.H 4 "The Matrix Math Server Example"
.PP
Assume that you have an existing local interface that provides matrix math
operations.  Since it is local, errors such as floating-point 
overflow or divide by zero are returned to the caller of a matrix
operation as exceptions.  It is likely that these exceptions are
caused by providing data to the operation in an improper form.  
.PP
In this case, the exceptions are part of the interface, so 
\*Lfault_status\*O
changes the way the application calls the matrix interface and
probably is undesirable.  Depending on the environment, finding a
server may not be difficult (if the network is relatively stable and
has enough resources), and adding \*Lcomm_status\*O serves only 
to introduce differences between the local and distributed
applications.
.PP
If a decision as to what action to take is
based upon a communications
failure, then you may try to add the conditional code
\*Lcomm_status\*O requires. 
Otherwise, using
\*Lauto_handle\*O allows an attempt on each available server.  If no
server is available, the application terminates because it cannot
proceed.  You can add an exception handler to the main program to
report the error in a user-friendly manner.
.H 4 "The Stock Quote Application Example"
.PP
Assume that you have an application that reads from stock
quote servers and displays graphs of the data.  Since
you do not expect to get server failures because it is a commercial-quality
server, you are not interested in writing code 
to handle values returned from \*Lfault_status\*O.  
If high availability and robustness is
important, you may have a list of recovery plans to make sure a 
stock analyst can get the necessary information as quickly as
possible.  For example:
.PP
.oS
retry_count = 10;
do  {
    query_stock_quote(h, ...,&st);
    switch (st)         /* st parameter can be used because */
    {                   /* [comm_status] is in the ACF */
        case rpc_s_ok:
            break;
        case rpc_s_comm_failure:
            retry_count -= 1;
            break;
        case rpc_s_network_unreachable:
            h = some_other_handle;
            break;
        case
            .
            .
            .
        default:
            retry_count -= 1;
    }
}
while ((st == rpc_s_ok) || (retry_count <= 0))
.oE
.PP
If this is not a critical application, you 
may only report that the server is currently unavailable.  
Depending upon the design of the application, there may be 
several places to put the exception handler 
to report the failure but continue processing.  For example:
.PP
.oS
TRY
    update_a_quote(...);
CATCH_ALL
    display_message("Stock quote not currently available");
ENDTRY
.oE
.PP
This example assumes that \*Lupdate_a_quote(\|)\*O 
eventually calls the remote operation \*Lquery_stock_quote(\|)\*O 
and that this call may raise an exception that is
detected and reported here.
.PP
.ne 6
The advantage of using exceptions in this
case is that all of the work done in \*Lupdate_a_quote(\|)\*O 
has the same error
recovery and it does not need to be repeated at every call to a remote
operation.  Another advantage is that, if one of the remote operations
does have a recovery for one exception, it can handle that one 
exception and allow the rest to propagate to the more general handler 
in an outer layer of the code.
.H 2 "Context Handles"
.P
During a series of remote procedure calls, the 
client may need to refer to a context maintained by a specific
server instance.
Server application code can maintain information it needs for a
particular client (such as the state of RPC the client is using) 
as a context.  To provide a client with a means of referring to its context, 
the client and server pass back and forth an RPC-specific parameter
called a \*Econtext handle\*O.
...\" .gL "context handle"
.iX "context handle" "definition of"
A context handle is a reference (a pointer) to the server instance and the 
context of a particular client. 
A context handle ensures that subsequent remote
procedure calls from the client can reach the server instance that is
maintaining context for the client. 
.PP
On
completing the first procedure in a series, the server passes a context handle
to the client.  The context handle identifies the context that the server
uses for subsequent operations.  
The client is not supposed to do anything with the context handle; it merely
passes it to subsequent calls as needed, and it is used internally by
the remote calls.  This allows applications to have such things as remote
calls that handle file operations much as local calls would; that is,
a client application can remotely open a file, get back a handle to it,
and then perform various other remote operations on it, passing the
context handle as an argument to the calls.  A context handle can be used
across interfaces (where a single server offers the multiple interfaces),
but \*Ea context handle belongs only to the client that caused it to be
activated\*O. 
.P
The server maintains the context for a client until the client calls
a remote procedure that terminates use of the context or
communications are lost.  In the latter case, the server's runtime can
invoke a context rundown procedure.
This application-specific routine is called by the server stub 
automatically to reclaim (rundown) the pointed-to resource in the
event of a communications break between the server and client.  
For example, in the case of the remote file pointer just mentioned, 
the context rundown routine would simply close the file.
.P
.ne 4
As usual with RPC, you need to apply indirection operators in a variety of
ways to maintain the correct \*L[in]\*O and \*L[out]\*O semantics.  Typical
declarations for a context handle are as follows.  In the \*L.idl\*O file,
declare a named type such as
...\" 
.oS
typedef [context_handle] void* my_handle_t;
.oE
...\" 
.P
A manager routine that returns a context handle as an \*Lout\*O parameter
declares it as
...\" 
.oS
my_handle_t *h;
.oE
...\" 
.P
The routine then sets the value of the handle as follows:
...\" 
.oS
*h = &context_data;
.oE
...\" 
.P
A routine that refers to a context handle as an \*Lin\*O parameter
declares it as
...\" 
.oS
my_handle_t h;
.oE
...\" 
.P
and dereferences the handle as follows:
...\" 
.oS
context_data = (my_handle_t*)h;
.oE
...\" 
.P
For the \*Lin,out\*O case, the routine uses the same declaration as in the
\*Lout\*O case, and dereferences the handle as follows:
...\" 
.oS
context_data = (my_handle_t*)*h;
.oE
.P
The following extensive example shows a simple use of context handles.
In the sample code, the client requests a unit of storage from the
server, using the \*Lstore_open(\|)\*O call, and receives a handle to the
allocated storage.  The \*Lstore_read(\|)\*O, \*Lstore_write(\|)\*O, and
\*Lstore_set_ptr(\|)\*O routines allow the client to read from and write
to specific locations in the allocated storage.  The \*Lstore_close(\|)\*O
routine releases the server resources.
.nL
.ne 12
.H 3 "Context Handles in the Interface"
.P
The \*L.idl\*O file declarations for the \*Lstore\*O interface are as
follows:
.oS
.ps 10
.vs 12
/*
 * store.idl
 * A sample interface that demonstrates server maintained context.
 * The client requests temporary storage of a specified size, 
 * and the server returns a handle that can be used to read and
 * write to storage.  The interface doesn't care how the
 * server implements the storage. 
 */
[
uuid(0019b8c5-e8b5-1c84-9a41-0000c0d4de56),
pointer_default(ref),
version(1.0)
]
interface store
{

    /* A context handle used to access remote storage:                 */
    typedef [context_handle] void* store_handle_t;

    /* A storage object name string:                                   */
    /*  typedef [string] char* store_name_t; */

    /* A buffer type for data:                                         */
    typedef byte store_buf_t[*];

    /* Note that the context handle is an [out] parameter of the open  */
    /*  routine, an [in, out] parameter of the close routine, and an   */
    /*  [in] parameter of the other routines.  If the context handle   */
    /*  were treated as an [in] parameter of the close routine, the    */
    /*  stubs would never learn that the context had been set to NULL, */
    /*  and would consider the context to still be live.  This would   */
    /*  result in the rundown routine's being called when the client   */
    /*  terminated, even though there would be no context to run down. */

    void store_open(
	[in] handle_t binding,
	[in] unsigned32 store_size,
	[out] store_handle_t *store_h,
	[out] error_status_t *status
    );

    void store_close(
	[in,out] store_handle_t *store_h,
	[out] error_status_t *status
    );

    void store_set_ptr(
	[in] store_handle_t store_h,
	[in] unsigned32 offset,
	[out] error_status_t *status
    );

    void store_read(
	[in] store_handle_t store_h,
	[in] unsigned32 buf_size,
	[out, size_is(buf_size), length_is(*data_size)] \\
	  store_buf_t buffer,
	[out] unsigned32 *data_size,
	[out] error_status_t *status
    );

    void store_write(
	[in] store_handle_t store_h,
	[in] unsigned32 buf_size,
	[in, size_is(buf_size)] store_buf_t buffer,
	[out] unsigned32 *data_size,
	[out] error_status_t *status
    );

}
.ps 12
.vs 14
.oE 
.H 3 "Context Handles in a Server Manager"
.P
Server manager code to provide a rudimentary implementation of the \*Lstore\*O
interface is as follows:
...\" 
.oS
.ps 10
.vs 12
/* context_manager.c -- implementation of "store" interface.            */
/*                                                                      */
/* The server maintains a certain number of storage areas, only one of  */
/*  which can be (or should be) opened by a single client at a time.    */
/*  More than one client can, however, apparently be invoked (up to the */
/*  number of separate storelets == store handles available, defined by */
/*  the value of NUM_STORELETS).  Each client keeps track of its store  */
/*  (and likewise enables the server to do the same) by means of the    */
/*  context handle it receives when it opens its store.                 */
/*                                                                      */
/************************************************************************/

.ne 6
#include <stdio.h>
#include <string.h>
#include <malloc.h>
#include <pthread.h>
#include <dce/dce_error.h>
#include <dce/daclif.h>

#include "context.h"

#define NUM_STORELETS 10

/************************************************************************/
/* The actual "storelet" structure...                                   */

typedef struct store_hdr{
    pthread_mutex_t ref_lock;
    unsigned32 size;
    unsigned32 refcount;
    idl_byte *storage;
} store_hdr_t;

store_hdr_t headers[NUM_STORELETS]; /* There's an array of these.       */

/************************************************************************/
/* The store specification structure; note that it is equivalent to the */
/*  handle; the pointer to it is returned as the handle by the          */
/*  store_open() routine below...                                       */
/*  The assumption is that all access to a given handle is serialized   */
/*  in a single thread, so no locking is needed for these.              */

typedef struct store_spec{
    unsigned32 number;     /* The storelet number we've opened.         */
    unsigned32 offset;     /* The current read/write position.          */
} store_spec_t;  /* There's only one of these; it's the handle that     */
                 /*  gives access to one of the NUM_STORELETS set of    */
                 /*  "storelets".                                       */


/* The server entry name:                                               */
extern unsigned_char_p_t entry;


/* Initialization control block:                                        */
pthread_once_t init_once_blk = pthread_once_init;

.ne 14
/******
*
* store_mgmt_init -- Zeroes out all the storelet structures; executed 
*                    only once per server instance, as soon as a client 
*                    has called the store_open() routine.
*
******/
/************************************************************************/
void 
store_mgmt_init(
)
{
    int i;
    store_hdr_t *hdr;

    fprintf(stdout, "Store Manager: Initializing Store\n");
    memset(headers, 0, sizeof(store_hdr_t) * NUM_STORELETS);
    for (i = 0; i < NUM_STORELETS; i++)
    {
	hdr = headers + i;
	pthread_mutex_init(
	    (pthread_mutex_t *)hdr,
	    pthread_mutexattr_default);
    }

}

/******
*
* store_open -- Opens a store and returns a handle to it.  Store consists
*               of one "storelet" selected from array of NUM_STORELETS.
*
******/
/************************************************************************/
void 
store_open(
    handle_t binding,
    unsigned32 store_size,    /* Size specified for actual storage.     */
    store_handle_t *store_h,  /* To return the store handle in.         */
    error_status_t *status
)
{
    int i;                 /* Index variable.                           */
    store_spec_t *spec;    /* Store specification == handle.            */
    store_hdr_t *hdr;      /* Storelet structure.                       */

    /* Do the store initialization if this is the first open call...    */
    /* Zero out the store headers...                                    */
    pthread_once(&init_once_blk, store_mgmt_init);

.ne 10
    /* The following loop goes through all the storelets, looking for   */
    /*  one whose reference count is zero.  As soon as one such is      */
    /*  found, a handle is allocated for it, storage is allocated for   */
    /*  its store structure, and the loop (and the call) terminates. If */
    /*  no unreferenced storelet is found, a status of -1 is returned   */
    /*  and no handle is allocated...                                   */
    for(i = 0; i < NUM_STORELETS; i++)
    {
	/* Go to the next storelet...                               */
	hdr = headers + i;

	/* Is it unreferenced?...                                   */
	if (hdr->refcount == 0)
	{
	    /* If so, lock the header...                        */
	    *status = pthread_mutex_lock((pthread_mutex_t *)hdr);
	    if (*status != 0)
	    {
		return;
	    }

	    /* ...and check the reference count again...        */
	    if (hdr->refcount == 0)
	    {
		/* Now we know we "really" have this one.   */
		/* Only one open is allowed, so lock only   */
		/*  the reference count...                  */
		hdr->refcount++;

		/* Now unlock the header so other threads   */
		/*  can continue to check it...             */
		*status = pthread_mutex_unlock((pthread_mutex_t *)hdr);
		if (*status != 0)
		    return;

		/* Now allocate space for the specifica-    */
		/*  tion structure...                       */
		spec = (store_spec_t *)malloc(sizeof(store_spec_t));
		spec->number = i;
		spec->offset = 0;
		*store_h = spec;

		/* Allocate space for the storage part of   */
		/*  the header...                           */
		hdr->storage = (idl_byte *)malloc(store_size);
		hdr->size = store_size;

.ne 5
	    	/* Finally, set the return status to OK,    */
	    	/*  and return...                           */
		*status = error_status_ok;
		return;
	    }

	    /* If the reference count turned out to have        */
	    /*  been accessed between our first check and our   */
	    /*  locking the mutex, we must now unlock the mutex */
	    /*  preparatory to looping around to check the next */
	    /*  storelet...                                     */
	    *status = pthread_mutex_unlock((pthread_mutex_t *)hdr);
	    if (*status != 0)
	    {
		return;
	    }
	}
    }

    /* The following is reached only if we never found a free           */
    /*  storelet...                                                     */
    *store_h = NULL;
    *status = -1;

}

/******
*
* store_set_ptr -- Insert a new value into the store buffer pointer.
*
*******/
/************************************************************************/
void store_set_ptr(
    store_handle_t store_h,     /* The store handle.                    */
    unsigned32 offset,    /* Value to insert into store buffer pointer. */
    error_status_t *status
)
{
    store_spec_t *spec;              /* Our pointer to store handle.    */

    spec = (store_spec_t *)store_h;  /* Get the store spec.             */
    spec->offset = offset;     /* Copy in the new buffer pointer value. */
    *status = error_status_ok;
}

.ne 14
/******
*
* store_close -- Close the opened storelet.
*
******/
/************************************************************************/
void 
store_close(
    store_handle_t *store_h,                /* Store handle.            */
    error_status_t *status
)
{
    store_spec_t *spec;             /* Our pointer to store handle.     */
    store_hdr_t *hdr;               /* Pointer to a storelet.           */

    printf("Store Manager: Closing Store\n");

    spec = (store_spec_t *)*store_h; /* Get the store spec.             */
    hdr = headers + spec->number;    /* Point to the correct storelet.  */

    /* If the thing is actually opened, close it...                     */
    if (hdr->refcount > 0)
    {
	/* Lock the header first...                                 */
	*status = pthread_mutex_lock((pthread_mutex_t *)hdr);
	if (*status != 0)
	{
	    printf("Close: lock failed\n");
	    return;
	}

	/* Check the reference count to make sure no one slipped in */
	/*  before we could lock the header, and already closed the */
	/*  critter...                                              */
	if (hdr->refcount > 0)
	{
	    /* The store is open, and it's locked by us, so we  */
	    /*  can safely close it. So do it. First, decrement */
	    /*  the reference count...                          */
	    hdr->refcount--;

	    /* Is it completely closed now?                     */
	    if (hdr->refcount == 0)
	    {
		/* If so, get rid of its storage space...   */
		hdr->size = 0;
		free(hdr->storage);
	    }
	}

.ne 9
	/* If the store turned out to be closed before we could     */
	/*  close it, we have nothing to do but release the lock... */
	*status = pthread_mutex_unlock((pthread_mutex_t *)hdr);
	if (*status != 0)
	{
	    printf("Close: unlock failed\n");
	    return;
	}
    }

    /* And free our handle space...                                     */
    free(spec);

    /* Be sure to NULL the context handle.  Otherwise, the context      */
    /*  will be considered to be live as long as the client is run-     */
    /*  ning...                                                         */
    *store_h = NULL; 
    *status = error_status_ok;
}

/******
*
* store_read -- Read a certain number of bytes from the opened store.
*
******/
/************************************************************************/
void
store_read(
    store_handle_t store_h,   /* Store handle.                          */
    unsigned32 buf_size,      /* Number of bytes to read.               */
    store_buf_t buffer,       /* Space to return data read in.          */
    unsigned32 *data_size,    /* To return number of bytes read in.     */
    error_status_t *status
)
{
    store_spec_t *spec;       /* Our handle pointer.                    */
    store_hdr_t *hdr;         /* Pointer to a storelet.                 */

    spec = (store_spec_t *)store_h;  /* Get the storelet spec.          */
    hdr = headers + spec->number;    /* Point to the correct storelet.  */

    /* If the amount we're to read is less than the amount left to be   */
    /*  read, then read it...                                           */
    if (buf_size <= hdr->size)
    {

	/* Copy bytes from the storelet storage, beginning at off-  */
	/*  set, into the return buffer, up to the size of the      */
	/*  buffer...                                               */
	memcpy(buffer, hdr->storage + spec->offset, buf_size);

	/* Update the storelet buffer pointer past what we've just  */
	/*  read...                                                 */
	spec->offset += buf_size;

	/* Show return size of data read...                         */
	*data_size = buf_size;
	*status = error_status_ok;
	return;
    }

    /* If there's less data left than has been specified to read, don't */
    /*  read it...                                                      */
    *data_size = 0;
    *status = -1;
}

/******
*
* store_write -- Write some data into the opened store.
*
******/

void 
store_write(
    /* handle_t IDL_handle,*/  /* If the server ACF declares            */
                               /*  [explicit_handle]                    */
    store_handle_t store_h,    /* Store handle.                         */
    unsigned32 buf_size,       /* Number of bytes to write.             */
    store_buf_t buffer,        /* Data to be written.                   */
    unsigned32 *data_size,     /* To return number of bytes written.    */
    error_status_t *status
)
{
    store_spec_t *spec;            /* Our pointer to store handle.      */
    store_hdr_t *hdr;              /* Pointer to a storelet.            */

    /* Do an access check on IDL_handle here...                         */
    /* [--ORIGINAL NOTE] -- I don't know what the above means.          */

    spec = (store_spec_t *)store_h;  /* Get the storelet spec.          */
    hdr = headers + spec->number;    /* Point to the correct storelet.  */

    /* If the amount of unused room left in the storelet is greater     */
    /*  than what we're supposed to write in it, write it...            */
    if ((hdr->size - spec->offset) > buf_size)
    {

	/* Copy bytes from the buffer into the storelet storage,    */
	/*  beginning at the current read/write position...         */
	memcpy(hdr->storage + spec->offset, buffer, buf_size);

	/* Update the storelet buffer pointer to point past what    */
	/*  we've just written...                                   */
	spec->offset += buf_size;

	/* Add a null in case we want to read the store as a        */
	/*  string...                                               */
	*(hdr->storage + spec->offset) = 0; 

	/* Show return size of data written...                      */
	*data_size = buf_size;
	*status = error_status_ok;
	return;
    }

    /* If we don't have room to write the whole buffer, don't write     */
    /*  anything...                                                     */
    *data_size = 0;
    *status = error_status_ok;
}

/******
 *
 * print_manager_error-- Manager version.  Prints text associated with 
 * bad status code.
 *
 *
 ******/
void
print_manager_error(
char *caller,  /* String identifying routine that received the error. */
error_status_t status) /* status we want to print the message for.    */
{
    dce_error_string_t error_string;
    int print_status;

    dce_error_inq_text(status, error_string, &print_status);
    fprintf(stderr," Manager: %s: %s\n", caller, error_string);

}
.ps 12
.vs 14
.oE
.P
The sample implementation of the store interface is obviously too
limited for any practical use, but it does demonstrate the application
of context handles in a straightforward way.  A context handle
returned by the \*Lstore_open(\|)\*O routine is opaque to the client.
To the server, it is a pointer to the server's representation of a
storage unit.  In this case, it points to a structure that keeps track
of the client's current location within a specific piece of 
server-maintained storage.
.P
.ne 8
Aside from deallocating the actual storage, the \*Lstore_close(\|)\*O
routine sets the context handle to NULL. The NULL value
indicates to the server stub that the context is no longer active, and
the stub, in turn, tells the RPC runtime not to maintain the context.
For example, after the \*Lstore_close(\|)\*O routine has been invoked,
the rundown routine will not be invoked if communication ends between
client and server.  The context rundown routine takes care of closing
the client's storage in case of a communication failure while the
context is active.
.P
The global array of \*Lstore_hdr\*O structures that keeps track of
allocated storage, obviously servers no practical purpose in the
example. (Presumably the operating system is already doing this!)
However, it does provide a demonstration of the fact that global
server manager data is shared data in the implicitly multithreaded
server environment.  The routines that manipulate this shared data may
be called simultaneously by multiple server threads (in response to
multiple simultaneous client calls); therefore, locking must be
provided, in this case on the \*Vrefcount\*O field.  The sample also
demonstrates how the \*Lpthread_once(\|)\*O facility can be used to
provide one-time initialization of the shared data on the first
\*Lstore_open(\|)\*O call.
.P
As an exercise, the storage interface can easily be made more
interesting by providing multiple clients simultaneous access to a
given storage area.  To implement this, the application could add a
\*Vstore_name\*O parameter to the \*Lstore_open(\|)\*O routine and
replace the \*Vrefcount\*O field with counts of readers and writers.
The division of the storage management between the \*Vstore_hdr\*O
and the \*Vstore_spec\*O data structures is intended to facilitate
this; the \*Vstore_hdr\*O holds shared state relating to each store,
while the \*Vstore_spec\*O holds each thread's private state.
.H 3 "Context Rundown"
.P
Context handles typically point to some state maintained by a server
instance for a client over a series of RPC operations.  If the series
of operations fails to complete because communication is lost between
client and server, the server will probably have to take some kind of
recovery action such as restoring data to a consistent state and
freeing resources.
.P
.ne 16
The stub detects outstanding context when it marshals context handle
parameters.  Outstanding context is considered to exist from the point
at which a non-NULL pointer value is returned, until a NULL
pointer value is returned.  When outstanding context exists, the server
stub code will call a context rundown routine in response to certain
exceptions that indicate a loss of contact with the client.  You
should note that the exact timing of the call depends on the
transport.  In particular, with the connectionless protocol, servers
that maintain context for clients expect clients to indicate
periodically that they are still running.  If the server fails to hear
from the client during a specified timeout period, the server will
assume that the client has stopped and call the context rundown
routine.  This can mean a substantial delay between the time the
client actually fails and the time at which context maintained for the
client is actually cleaned up.  If the context being held represents a
scarce resource on the server, one consequence of the delayed rundown
may be that failed calls continue to hold the scarce resource for some
time before it is made available again.
.P
Since a context handle may be freely shared among threads of the
calling client context, it is possible for outstanding context to
exist for more than one call simultaneously.  Such shared context is
considered to be outstanding as long as it is outstanding for any of
the participating threads.  Also, any communications failures are
likely to be detected at different times for each such call thread,
and the difference in timing may be especially noticeable in the case
of the connectionless protocol.  Context rundown occurs only after all
server call threads have been terminated.  This means that call
operations in progress on the server need not be concerned that the
context they are operating on will be changed unexpectedly.  Imagine a
situation in which context handles represent open file descriptors, and
the rundown routine closes the files.  A manager thread that shares these
descriptors via a context handle is guaranteed that the files will remain
open even if a communications failure is detected in another thread that
also is using the same context handle.
...\" 
...\" .P
...\" [Don't forget to mention rpc_sm_destroy_client_context()]
...\" 
.oS
.ne 16
/******
*
* store_handle_t_rundown -- Closes the opened storelet.
*
******/
/************************************************************/
void
store_handle_t_rundown(
    store_handle_t store_h
)
{
    error_status_t st;

    printf("Store Manager: Running down context.\n");
    store_close(&store_h, &st);
}
.oE
.H 3 "Binding and Security Information"
.P
One element that is clearly missing from the context handle sample
code is any access checking.  To do this, it is necessary to get the
client binding, although it may not be immediately obvious how to do
this with a context handle.  The answer is actually quite simple but,
to understand it, it helps to have a clear idea of how binding
parameters operate in RPC.
.P
Every call requires binding information, whether this is supplied
explicitly as a binding parameter or not.  When a call is made with a
binding handle, the client uses cached binding information associated
with the binding handle.  When no binding handle parameter is passed,
the client derives the binding information it needs by some other
means.  For example, with a context handle, the client uses cached
binding information associated with the context handle.
.P
Even when an explicit binding handle parameter is present, the handle
is not marshalled as call data in the same way other call parameters
are.  Similarly, on the server side, when a binding handle parameter
is present in a manager operation, it is \*Eunmarshalled\*O simply as a
reference to the binding information cached by the server runtime for
the call.  It is irrelevant whether the call was made with an explicit
binding handle parameter on the client side.  
.P
.ne 3
Therefore, it is
perfectly possible for a server manager operation to have a binding
handle as a parameter even when the client RPC call is made without an
explicit binding parameter.
.P
The mechanics of this are to use different \*L.acf\*O declarations on
the client and server sides.  The \*L.idl\*O file declaration for the
operation does not declare an explicit binding handle parameter, but
the server \*L.acf\*O file applies the \*L[explicit_handle]\*O attribute
to the operation.  This results in a server stub that expects to unmarshal
a binding handle as the first parameter of the operation, while the client
stub does not expect an explicit binding handle parameter for the call.
.P
An example of a server-side \*L.acf\*O file for the store interface is as
follows:
...\" 
.oS
/* store.acf - server side
 * Unmarshal a client binding handle on each call
 */

interface store
{
    store_open();
    [explicit_handle]store_close();
    [explicit_handle]store_set_ptr();
    [explicit_handle]store_read();
    [explicit_handle]store_write();
}
.oE
.P
You could achieve the same effect by using different \*L.idl\*O files
for the client and server, but this is not recommended.  The \*L.idl\*O
file serves as the canonical representation of an interface and hence
should be the same for all clients and servers.
...\" 
...\" (Of course, the IDL
...\" compiler makes this difficult if the server and client code are located
...\" in the same directory.  Since the compiler derives the \*L.acf\*O filename
...\" from the \*L.idl\*O filename, you need \*L.idl\*O files with different
...\" pathnames (although they have the same content) for client and server.
...\" The compiler really needs a switch to define a nondefault \*L.acf\*O
...\" filename!)
...\" 
.P
This technique can be used in a number of ways; for example, to permit
the client to use implicit binding while the server manager operations
extract authorization information from a client binding handle.  In the
case of a context handle, the principle is the same.  You use the server
\*L.acf\*O declarations to add a binding parameter to the call on the
server side.  The client continues to call using the context handle, while
the server manager receives the client binding as a first extra parameter.
.P
.ne 3
In the case of the sample code, the client calls to the store interface
remain the same, but the server manager implementations now contain an
extra parameter.  For example:
.oS
.ne 12
void 
store_write(
    handle_t IDL_handle,
    store_handle_t store_h,
    unsigned32 buf_size,
    store_buf_t buffer,
    unsigned32 *data_size,
    error_status_t *status
)
{
    store_spec_t *spec;
    store_hdr_t *hdr;

    if (check_access(IDL_handle, sec_acl_perm_write) == 0)
    {
        *status = str_s_no_perms;
        return;
    }
    .
    . 
    .
}
.oE
.H 2 "Pipes"
.P
Pipes are a mechanism for efficiently handling large quantities of
data by overlapping the transfer and processing of data.  
Input data is transferred in chunks to the server for processing, 
and output data is processed by the server in chunks and transferred
to the client.
A pipe is declared in a type definition of an interface definition,
and the data type is used as parameters in the operations of the interface.
The server manager calls stub pipe support routines in a loop, 
and the client stub calls pipe support routines that the client application
must provide.  
.P
.ne 9
One of the pipe support routines that the client must provide is
an \*Lalloc\*O routine, which allocates a buffer for each chunk of pipe data. 
Given that pipes are intended to process data asynchronously,
consuming it as it arrives, the \*Lalloc\*O routine should not just blindly
allocate a new buffer each time it is called, since the net effect
would be to allocate space for the whole stream.  A reasonable
approach is either to declare a buffer statically or allocate it on
the first call (per thread), and thereafter simply return the same
buffer.  The following code example shows the form an \*Lalloc\*O routine
takes in client application code.
.P
.oS
#define CLIENT_BUFFER_SIZE 2048
idl_byte client_buffer[CLIENT_BUFFER_SIZE];

void client_alloc (state, bsize, buf, bcount)
    rpc_ss_pipe_state_t state;
    unsigned int bsize;
    byte **buf;
    unsigned int *bcount;
{
    *buf = client_buffer;
    *bcount = CLIENT_BUFFER_SIZE;
}
.oE
.H 3 "Input Pipes"
.P
In the following example, a client sends
the contents of a file to a server as a set of chunks allocated from the
same static buffer. 
The chunks are processed (in this case simply printed) as they arrive.
.P
The declaration in the interface definition is as follows:
...\" 
.oS
typedef pipe char test_pipe_t;

void pipe_test1(
    [in] handle_t handle,
    [in] test_pipe_t test_pipe,
    [out] error_status_t *status
);
.oE
...\" 
.P
.ne 4
Note that the pipe is declared as a \*Ltypedef\*O, resulting in an
IDL-generated C typedef for \*Ltest_pipe_t\*O, which is
a structure containing pointers to the pipe support 
routines and a pipe state field.  The server
manager and client code then implement the pipe in a complementary
fashion. 
.P
For an \*L[in]\*O pipe, the server manager code consists of a
cycle of calls to the \*Ltest_pipe.pull\*O routine (a server stub
routine) which terminates when a zero-length chunk is received:
.oS
void 
pipe_test1(
    handle_t binding_h,
    test_pipe_t test_pipe,
    error_status_t *status 
)
{
    char buffer[SBUFFSIZE];
    int count;
    char *cptr;
    do
    {
        (*(test_pipe.pull))(test_pipe.state, buffer, \\
	  SBUFFSIZE, &count);
        for (cptr = buffer; cptr < buffer + count; cptr++)
            putchar(*cptr);
    } while (count > 0);
}
.oE
.P
Using the buffer supplied by the manager, the \*Ltest_pipe.pull\*O routine 
unmarshals an amount of data that is nonzero, but not more than the 
buffer can hold.  There is no guarantee that the buffer will be filled.  
The actual amount of data in the buffer is indicated by the
\*Lcount\*O parameter returned in the \*Ltest_pipe.pull\*O routine.
This count equals the number of \*Ltest_pipe_t\*O data elements in the buffer.
.P
.ne 11
The \*Ltest_pipe.pull\*O routine signals the end of data in the pipe by
returning a chunk whose count is 0 (zero).  Any attempt to pull data from
the pipe after the zero-length chunk has been encountered will cause
an exception to be raised.  The \*Lin\*O pipes must be processed in the
order in which they occur in the operation signature.  Attempting to
pull data from an \*Lin\*O pipe before end-of-data on any preceding
\*Lin\*O pipe has been encountered will result in an exception being
raised.  If the manager code attempts to write to an \*Lout\*O pipe
or return control to the server stub before end-of-data has been
encountered on the last \*Lin\*O pipe, an exception will be raised.
(Note that there is no guarantee that chunks seen by the manager will
match the chunks supplied by the client's \*Lpull\*O routine.)
.P
The client application code must supply \*Lpull\*O and \*Lalloc\*O
routines and a pipe state.  
These routines must work together to produce a sequence 
of pointers to chunks, of which only the last is empty.
In the following example, the client code provides a
\*Ltest_pipe.pull\*O routine that reads chunks of the input file into
a buffer and returns a count of the chunk size, 
returning a zero count when the end of the file is reached.
The pipe state block is used here simply as a convenient way to make the file
state available to the \*Lpull\*O routine. 
Applications need not make any use of the pipe state.
.oS
/* Client declares types and routines */

typedef struct client_pipe_state_t {
    idl_char *filename;
    idl_boolean file_open;
    int file_handle;
} client_pipe_state_t;

client_pipe_state_t client_in_pipe_state = {false, 0};

void client_pull(state,buf,esize,ecount)
    client_pipe_state_t * state;
    byte *buf;
    unsigned int esize;
    unsigned int *ecount;
.ne 18
{
    if ( ! state->file_open )
    {
        state->file_handle = open(state->filename,O_RDONLY);
        if (state->file_handle == -1)
        {
            printf("Client couldn't open %s\n", state->filename);
            exit(0);
        }
        state->file_open = true;
    }
    *ecount = read( state->file_handle, buf, esize );
    if (*ecount == 0)
    {
        close(state->file_handle);
        state->file_open = false;
    }
}
.oE
.P
Finally, the client must do the following:
.P
.AL
.LI
Allocate the \*Ltest_pipe_t\*O structure.
.LI
Initialize the \*Ltest_pipe_t.pull\*O, \*Ltest_pipe_t.alloc\*O, 
and \*Ltest_\%pipe_\%t.state\*O fields.
.LI
Include code where appropriate for checking the \*Lpipe_t.state\*O field.
.LI
Pass the structure as the pipe parameter.  The structure can be 
passed either by value or by reference, as indicated by the signature 
of the operation that contains the pipe parameter:
.LE
.P
.oS
/* Client initializes pipe */
test_pipe_t test_pipe;

test_pipe.pull = client_pull;
test_pipe.alloc = client_alloc;
test_pipe.state = (rpc_ss_pipe_state_t)&client_in_pipe_state;

/* Client makes call */

pipe_test1(binding_h, test_pipe, &status);
.oE
.P
.ne 4
To transmit a large amount of data that is already in the proper 
form in memory (that is, the data is already an array of \*Ltest_pipe_t\*O), 
the client application code can have the \*Lalloc\*O routine allocate 
a buffer that already has the information in it.  In this case, the 
\*Lpull\*O routine becomes a null routine.
.H 3 "Output Pipes"
.P
An \*L[out]\*O pipe is implemented in a similar way to an input pipe, 
except that the client and server make use of the \*Lpush\*O routine instead
of the \*Lpull\*O routine. 
The following samples show an \*L[out]\*O pipe used to read
the output from a shell command executed by the server.
.P
The declarations in the interface definition are as follows:
.oS
typedef pipe char test_pipe_t;

void pipe_test2(
    [in] handle_t handle,
    [in, string]  char cmd[],
    [out] test_pipe_t *test_pipe,
    [out] error_status_t *status
);
.oE
.P
The server manager routines demonstrate a couple of possible
implementations.  In each case, the manager makes a cycle of calls to
the server stub's \*Lpush\*O routine, ending by pushing a zero-length chunk:
...\" 
.oS
#include <dirent.h>
#define SBUFFSIZE 256

void 
pipe_test2(
    handle_t binding_h,
    idl_char *cmd,
    test_pipe_t *test_pipe,
    error_status_t *status 
)
{
  
.ne 2
    DIR *dir_ptr;
    struct dirent *directory;

    char buffer[SBUFFSIZE];
    FILE *str_ptr;
    int n;

    /* An elementary mechanism to execute a command and get 
     * the output back.  Note that popen() and fread() are 
     * thread-safe, so the whole process won't block while 
     * the call thread waits for them to return.
     *
     * This is potentially a dangerous operation!
     * Here we'll only allow a couple of "safe" commands.
     */

    if (!strcmp(cmd, "ps") || !strcmp(cmd, "ls"))
    {
        if ((str_ptr = popen(cmd, "r")) == NULL)
            return;
        while ((n = fread(buffer, sizeof(char), \\
	  SBUFFSIZE, str_ptr)) > 0)
        {
            (*(test_pipe->push))(test_pipe->state, buffer, n);
        }
        (*(test_pipe->push))(test_pipe->state, buffer, 0);
        fclose(str_ptr);
    }

    /* Here's another method: list an arbitrary directory 
     * This time, we buffer the directory names as null-
     * terminated strings of various lengths.  The client 
     * will need to provide formatting of the output stream, 
     * for example, by substituting a CR for each NULL byte.
     */
 
.ne 14
    /*
    if ((dir_ptr = opendir(cmd)) == NULL)
    {
        printf("Can't open directory %s\n", cmd);
        return;
    }
    while ((directory = readdir(dir_ptr)) != NULL)
    {
        if (directory->d_ino == 0)
            continue;
        (*(test_pipe->push))(test_pipe->state, \\
	  directory->d_name,
                            strlen(directory->d_name)+1);
    }
    (*(test_pipe->push))(test_pipe->state, \\
      directory->d_name, 0);
    closedir(dir_ptr);
    */

    *status = error_status_ok;
}
.oE
.P
The stub enforces well-behaved pipe filling by the manager by raising 
exceptions as necessary.  After all \*Lin\*O pipes have been drained 
completely, the \*Lout\*O pipes must be completely filled, in order.
.P
The client code uses the same declarations as in the input pipe example,
except that instead of using a \*Lclient_pull\*O routine it uses a
\*Ltest_push\*O routine that prints out the contents of each received buffer:
.oS
/* 
 * Our push routine prints each received buffer-full.
 */
void test_push(
    rpc_ss_pipe_state_t *state,
    idl_char *buf,
    unsigned32 count
)
.ne 12
{
    unsigned_char_t *cptr;
    for (cptr = buf; cptr < buf + count; cptr++)
    {
        /* For the second, directory reading example, 
           uncomment the following:
        if (*cptr == 0)
            *cptr = '\n';
         */
        putchar(*cptr);
    }
}
.oE
.P
.ne 3
For an \*Lout\*O pipe, the client code must do the following:
.iX "pipes" "out"
.P
.AL
.LI
Allocate the \*Ltest_pipe_t\*O structure.
.LI
Initialize the \*Ltest_pipe_t.push\*O and \*Ltest_pipe_t.state\*O fields.
.LI
Pass the structure as the pipe parameter, either by value or 
by reference.
.LE
.P
.oS
test_pipe_t test_pipe;

test_pipe.alloc = (void (*)())client_alloc;
test_pipe.push = (void (*)())test_push;
test_pipe.state = (rpc_ss_pipe_state_t)&out_test_pipe_state;

pipe_test2(binding_h, cmd, &test_pipe, &status);
.oE
.P
The client stub unmarshals chunks of the pipe into a buffer 
and calls back to the application, passing a reference to the buffer.  
To allow the application code to manage its memory usage, and possibly 
avoid unnecessary copying, the client stub first calls back to 
the application's \*Ltest_pipe.alloc\*O routine to get a buffer.  In some 
cases, this may result in the \*Ltest_pipe.push\*O routine's not having 
any work to do.
.P
The client stub may go through more than one (\*Ltest_pipe.alloc\*O, 
\*Ltest_pipe.push\*O) cycle in order to unmarshal data that the server 
marshalled as a single chunk.    Note that there is 
no guarantee that chunks seen by the client stub will match the 
chunks supplied by the server's \*Lpush\*O routine.
.H 3 "Pipe Summary"
.P
The pipe examples show how the client and server tasks are
complementary.  The client implements the appropriate callback
routines (\*Ltest_pipe.alloc\*O and either \*Ltest_pipe.push\*O or
\*Ltest_pipe.pull\*O), and the server manager makes a cycle of calls
to either \*Ltest_pipe.push\*O or \*Ltest_pipe.pull\*O of the stub. 
The application code gives the illusion that the server manager is calling
the client-supplied callbacks.  In fact, the manager is actually
calling stub-supplied callbacks, and the client callbacks
are asynchronous: a server manager call to one of the callback
routines does not necessarily result in a call to the corresponding
client callback.
.P
One result of this is that the client and server should not count on
the chunk sizes being the same at each end.  For example, in the last
directory reading example, the manager calls the \*Ltest_pipe.push\*O
routine once with each NULL-terminated filename.  However, the
client \*Ltest_push\*O routine does not necessarily receive the data
stream one filename at at time.  For example, if the \*Ltest_push\*O
routine attempted to print the filenames using \*Lprintf("%s\\n",buf);\*O, 
it might fail.  An interesting exercise would be to add \*Lprintf(\|)\*O
routines to the client callbacks and the server manager to show 
when each callback is made.
.P
Note also that the use of the pipe \*Vstate\*O field by the client is
purely local and entirely at the discretion of the client.  The state
is not marshalled between client and server, and the server
stubs use the local \*Vstate\*O field in a private manner.  The server
manager should not alter the state field.
.P
Pipes may also be \*L[in,out]\*O, although the utility of this
construct is somewhat limited.  Ideally, a client would like to
be able to pass a stream of data to the server and have it processed and
returned asynchronously.  In practice, the input and output streams
must be processed synchronously; that is, 
all input processing must be finished
before any output processing can be done.  This means that \*L[in,
out]\*O pipes, while they can reduce latency within both the server and
the client, cannot reduce latency between server and client; the
client must still wait for all server processing to finish before it
can begin to process the returned data stream.
.P
For an \*Lin,out\*O pipe, 
both the \*Lpull\*O routine (for the \*Lin\*O
direction) and a \*Lpush\*O routine (for the \*Lout\*O direction) must
be initialized, as well as the \*Lalloc\*O routine and the state.
During the last \*Lpull\*O call (when it will return a zero count to 
indicate that the pipe is ``drained''), the application's \*Lpull\*O routine 
must reinitialize the pipe state so that the pipe can be used by the 
\*Lpush\*O routine correctly.
.H 2 "Nested Calls and Callbacks"
.P
A called remote procedure can call another remote procedure.  The call to the
second remote procedure is nested within the first call; 
that is, the second call is a
...\" .gL "nested remote procedure call"
.iX "nested remote procedure call"
nested remote procedure call.
A nested call involves the following general 
phases, as illustrated in Figure 17-1:
.AL
.LI
A client makes an initial remote procedure call to the first
remote procedure.
.LI
The first remote procedure makes a nested call
to the second remote procedure.
.LI
The second remote procedure executes the nested call and returns it to the
first remote procedure.
.LI
The first remote procedure then resumes executing the initial call. 
.LE
...\" .AL (numbered) list
.PP
.FG "Phases of a Nested RPC Call"
...\" DEC-file name .P! pictures/rpc-nested-call.ps 2.0i
.pI ../rpc/figures/7_advanced_topics_09.ps 1.5i 4.25i
.sp .5
.PP
.iX "nested remote procedure call"
A specialized form of a nested remote procedure call involves a called remote
procedure that is 
making a remote procedure call (callback) to the address space of the calling
client application thread.  Calling the client's address space requires that
a server application thread be listening in that address space. 
Also, the second remote procedure needs a server
binding handle for the address space of the calling client.  
.PP
.ne 10
The remote procedure can ask the local RPC runtime to convert the client
binding handle, provided by the server runtime, into a server binding handle.
This is done by calling the \*Lrpc_binding_server_from_client(\|)\*O routine.
This routine returns a partially bound binding handle (the server
binding information lacks an endpoint).  For a nested remote
procedure call to find the address space of the calling client, the
application must ensure that the partially bound binding handle 
is filled in with the endpoint of that address space.  The 
the \*Lrpc_binding_server_from_client(3rpc)\*O
reference page discusses alternatives for ensuring that the endpoint is
obtainable for a nested remote procedure call. 
.PP
Using the server binding handle, a remote procedure can attempt a
nested remote procedure call.  The nested call involves the general 
phases illustrated by Figure 17-2.
.P
.ne 4.3i
.FG "Phases of a Nested RPC Call to Client Address Space"
...\" DEC-file name .P! pictures/rpc-nested-call-to-client-appl.ps 4.5i
.pI ../rpc/figures/7_advanced_topics_10.ps 0 0 1
.sp .5
.PP
The application threads in the preceding figure are performing the
following activities:
.AL
.LI
A client application thread from a multithreaded RPC application makes an
initial remote procedure call to the first
remote procedure.
.LI
.ne 4
After converting the client binding handle into a server binding handle and
obtaining the endpoint for the address space of the calling client application
thread, the first remote procedure makes a nested call to the second remote
procedure at that address space. 
.LI
.ne 2
The second remote procedure executes the nested call and returns it to the
first remote procedure.
.LI
The first remote procedure then resumes executing the initial call
(the client). 
.LE
.H 2 "Routing Remote Procedure Calls"
The following section
discusses routing incoming remote procedure calls between their arrival
at a server's system and the server's invocation of the requested
remote procedure.  The following routing steps are discussed:
.AL
.LI
...\" step 1
If a client has a partially bound server binding handle, before sending a call
request to a server, the client runtime must get the endpoint of a compatible
server from the endpoint mapper service of the server's system.  This endpoint
becomes the server address for a call request. 
.LI
...\" step 2
When the request arrives at the endpoint, the server's system places it in a
request buffer belonging to the corresponding server. 
.LI
...\" step 3
As one of its scheduled tasks, the server gets the incoming calls from the
request buffer.  The server either accepts or rejects an incoming call,
depending on available resources.  If no call thread is available, an accepted
call is queued to wait its turn for an available call thread.
.LI
...\" step 4
The server then allocates an available call thread to the call.
.LI
...\" step 5
The server identifies the appropriate manager for the called remote procedure
and invokes the procedure in that manager to execute the call.
.LI
...\" step 6
When the call thread finishes executing a call, the server
returns the call's output arguments and control to the client.
.LE
...\" .AL (numbered) list
.PP
Figure 17-3 illustrates these steps.
.PP
.ne 6.6i 
.FG "Steps in Routing Remote Procedure Calls"
...\" DEC-file name .P! pictures/rpc-server-call-routing-steps.ps 7.0i
.pI ../rpc/figures/7_advanced_topics_11.ps 0 0 1
.sp .5
.PP
.ne 8
The concepts in the following 
subsections are for the advanced RPC developer. 
The first subsection discusses how clients obtain endpoints 
when using partially bound binding handles.  
Then we discuss how a system buffers call
requests and how a server queues incoming calls; 
this information is relevant mainly to advanced RPC developers.
The final subsection discusses how a server selects the manager to
execute a call; it is relevant for developing an application that
implements an interface for different types of RPC objects.
.H 3 "Obtaining an Endpoint"
The endpoint mapper service of \*Ldced\*O maintains the local endpoint map.
.iX "endpoint" "map"
The endpoint map is composed of elements which contain fully bound
server binding information for a potential binding and an associated interface
identifier and object UUID (which may be nil).  Optionally, a map element can
also contain an annotation such as the interface name. 
.P
Servers use the local endpoint mapper service to register their 
binding information.  Each interface for which a server must register 
binding information requires a separate call to an 
\*Lrpc_ep_register...(\|)\*O 
routine, which calls the endpoint map service.  The endpoint map
service uses a new map element for every combination of binding information
specified by the server.  Figure 17-4 shows the
correspondence between server binding information specified by a
server and a graphic representation of the resulting endpoint map elements. 
.P
.ne 6.6i
.FG "Mapping Information and Corresponding Endpoint Map Elements"
...\" DEC-file name .P! pictures/rpc-ep-map-elements.ps 7.0i
.pI ../rpc/figures/7_advanced_topics_12.ps 0 0 1
.sp .5
.PP
.ne 4
A remote procedure call made with server binding information that lacks an
endpoint uses an endpoint from the endpoint map service.   This endpoint must
come from binding information of a compatible server.  The map element of a
compatible server contains the following: 
.ML
.LI
.ne 6
A compatible interface identifier
.PP
The requested interface UUID and compatible version numbers are necessary.  For
the version to be compatible, the major version number requested by the client
and registered by the server must be identical, and the requested minor version
number must be less than or equal to the registered minor version number. 
.LI
The requested object UUID, if registered for the interface
.LI
A server binding handle that refers to compatible binding information
that contains the following:
.ML
.LI
A protocol sequence from the client's server binding information
.LI
The same RPC protocol major version number that the client runtime supports
.LI
At least one transfer syntax that matches one used by
the client's system
.LE
...\" .ML (bulleted) list
.LE
...\" .ML (bulleted) list
.PP
...\"
To identify the endpoint of a compatible server,
the endpoint service uses the following rules:
.AL
.LI
If the client requests a nonnil 
object UUID, the endpoint map service begins by
looking for a map element that contains both the requested interface UUID and
object UUID. 
.AL
.LI
On finding an element containing both of the UUIDs, 
the endpoint map service selects the endpoint from that
element for the server binding information used by the client. 
.LI
If no element contains both UUIDs, the endpoint map service 
discards the object UUID and starts over
(see rule 2).
.LE
...\" .AL (alpa) list
.LI
If the client requests the nil object UUID (or if 
the requested nonnil object UUID is not registered),
the endpoint map service
looks for an element containing the requested interface UUID
and the nil object UUID.
.AL
.LI
On finding that element,
the endpoint map service selects the endpoint from the
element for the client's server binding information.
.LI
If no such element exists, the lookup fails.
.LE
...\" .AL (alpa) list
.LE
...\" .AL (numbered) list
.PP
The RPC protocol service inserts the endpoint of the compatible
server into the client's server binding information.
.PP
Figure 17-5 illustrates the decisions the endpoint map
service makes when looking up an endpoint for a client.
.P
.ne 7.3i
.FG "Decisions for Looking Up an Endpoint"
...\" DEC-file name .P! pictures/rpc-ep-map-lookup.ps 8.0i
.pI ../rpc/figures/7_advanced_topics_13.ps 0 0 1
.sp .5
.PP
.ne 6
You can design a server to allow the coexistence on a host system of multiple
interchangeable instances of a server.
...\" .gL "interchangeable server instances"
.iX "server" "interchangeable instances"
.iX "instance" "of an RPC server"
Interchangeable server instances are identical,
except for their endpoints; that is, they offer the same RPC interfaces and
objects over the same network (host) address and protocol sequence pairs.
For clients, identical server instances are fully
interchangeable. 
.PP
Usually, for each such combination of mapping information, the endpoint map
service stores only one endpoint at a time.  When a server registers a new
endpoint for mapping information that is already registered, the endpoint map
service replaces the old map element with the new one.
.PP
For interchangeable server instances to register their endpoints in the
local endpoint map, they must instruct the endpoint map service not to replace
any existing elements for the same interface identifier and object UUID.
Each server instance can create new map elements for itself 
by calling the \*Lrpc_ep_register_no_replace(\|)\*O routine.
.PP
When a client uses a partially bound binding handle, load sharing among
interchangeable server instances depends on the RPC protocol the client is
using. 
.ML 
.LI
Connectionless (datagram) protocol
.PP
...\"
The map service selects the first map element with compatible server binding
information.  If necessary, a client can achieve a random selection among all
the map elements with compatible binding information.  However, 
this requires that, before making a remote procedure call, 
the client needs to resolve the binding by 
calling the \*Lrpc_ep_resolve_binding(\|)\*O routine. 
.LI
Connection-oriented protocol
.PP
The client RPC runtime uses the 
\*Lrpc_ep_resolve_binding(\|)\*O routine, and the
endpoint map service selects randomly among all the map elements of compatible
servers. 
.LE
.PP
For an alternative selection criteria, a client can call the
\*Lrpc_mgmt_ep_elt_inq_\*O{\*Lbegin\*O,\*Lnext\*O,\*Ldone\*O}\*L(\|)\*O
routines and use an application-specific routine to select from
among the binding handles returned to the client. 
.PP
.ne 6
When a server stops running, its map elements become outdated.  Although the
endpoint map service routinely removes any map element containing an outdated
endpoint, a lag time exists when stale entries remain.  If a remote
procedure call uses an endpoint from an outdated map element, the call fails to
find a server.  To avoid clients getting stale data from the endpoint map,
before a server stops, it should remove its own map elements. 
.PP
A server also has the option of removing any of its own elements from the local
endpoint map and continuing to run.  In this case, an unregistered endpoint
remains accessible to clients that know it. 
.H 3 "Buffering Call Requests"
Call requests for RPC servers come into the RPC runtime over the network.  For
each endpoint that a server registers (for a given protocol sequence), the
runtime sets up a separate request buffer.  A 
...\" .gL "request buffer"
request buffer
.iX "request buffer"
is a first-in, first-out queue where an RPC system temporarily stores call
requests that arrive at an endpoint of an RPC server.  The request buffers allow
the runtime to continue to accept requests during heavy activity.  However, a
request buffer may fill up temporarily, causing the system to reject incoming
requests until the server fetches the next request from the buffer.  In this
case, the calling client can try again, with the same server or a different
server.  The client does not know why the call is rejected, nor does the client
know when a server is available again. 
.PP
Each server process regularly dequeues requests, one by one, from all of its
request buffers.  At this point, the server process recognizes them as incoming
calls.  The interval for removing requests from the buffers depends on the
activities of the system and of the server process. 
.PP
How the runtime handles a given request depends partly on the
communications protocol over which it arrives, as follows:
.ML
.LI
A call over a connectionless transport
is routed by the server's system to the call
request buffer for the endpoint specified in the call.
.nL
.ne 11
.LI
A call over a connection-oriented transport may be routed by the server's system
to a request buffer or the call may go directly to the server process. 
.PP
Whether a remote procedure call goes to the request buffer depends on whether
the client sends the call over an established connection.  If a client makes a
remote procedure call without an established connection, the server's system
treats the call request as a connection request and places the call request
into a request buffer.  If an established connection is available, the client
uses it for the remote procedure call; the system handles the call as an
incoming call and sends it directly to the server process that owns the
connection. 
.LE
...\" .ML (bulleted) list
.PP
Whether a server gets an incoming call from a request buffer or over an
existing connection, the server process manages the call identically.  A server
process applies a clear set of call-routing criteria to decide whether to
dispatch a call immediately, queue it, or reject it (if the server is extremely
busy).  These call-routing criteria are discussed in Section 17.6.3.
.PP
When telling the RPC runtime to use a protocol sequence, a server specifies the
number of calls it can buffer for the specified communications
protocol (at a given endpoint).  Usually, it is best for a server to specify a
default buffer size, represented by a literal whose underlying value depends on
the communications protocol.  The default equals the capacity of a single
socket used for the protocol by the server's system. 
.PP
The default usually is adequate to allow the RPC runtime to accept all the
incoming call requests.  For a well-known endpoint, the size of a
request buffer cannot exceed the capacity of a single socket descriptor 
(the default size); specifying a higher number causes a runtime error.  For
well-known endpoints, specify the default for the maximum
number of call requests.
...\" but for other endpoints, you can specify any size you choose. 
.PP
For example, consider the request buffer at full capacity as represented
in Figure 17-6.  This buffer has the capacity to store five
requests.  In this example, the buffer is full, and the runtime 
rejects incoming requests, as is happening to the sixth request.
.PP
.ne 3i
.FG "A Request Buffer at Full Capacity"
...\" DEC-file name .P! pictures/rpc-server-call-req-buffer.ps 2.0i
.pI ../rpc/figures/7_advanced_topics_14.ps 0 0 1
.H 3 "Queuing Incoming Calls"
Each server process uses a first-in, first-out 
...\" .gL "call queue"
.iX "call queue"
call queue.  When the server is already executing its maximum number of
concurrent calls, it uses the queue to hold incoming calls. 
The capacity of queues for incoming calls is implementation dependent;
most implementations offer a small queue capacity, which
may be a multiple of the maximum number of concurrently executing calls. 
.PP
A call is rejected if the call queue
is full.  The appearance of the rejected call depends on the
RPC protocol the call is using, as follows:
.ML
.LI
Connectionless (datagram) protocol
.PP
The server does not notify the client about this failure.
The call fails as if the server does not exist,
.iX "status codes"
returning an \*Lrpc_s_comm_failure\*O communications status code 
(\*Lrpc_x_comm_failure\*O exception).
.LI
Connection-oriented protocol
.PP
The server rejects the call with an \*Lrpc_s_server_too_busy\*O
communications status code (\*Lrpc_x_server_too_busy\*O exception).
.LE
...\" .ML (bulleted) list
.PP
The server process routes each incoming call as it arrives.  Call routing is
illustrated by the server in Figure 17-7.  This server has the
capacity to execute only one call concurrently.  Its call queue has a capacity
of eight calls.  This figure consists of four stages (A through D) of call
routing by a server process.  On receiving any incoming call, the server begins
by looking at the call queue. 
.PP
.ne 6i
.FG "Stages of Call Routing by a Server Process"
...\" DEC-file name .P! pictures/rpc-server-call-queuing.ps 7.0i
.pI ../rpc/figures/7_advanced_topics_15.ps 6.15i 5.15i
.sp .5
.PP
The activities of the four stages in the preceding figure are
described as follows:
.AL
.LI
.ne 5
In stage \*LA\*O, call \*L1\*O arrives at a server that lacks any
other calls.  
When the call arrives, the queue is empty and a call thread is available.  The
server accepts the call and immediately passes it to a call thread.  The
requested remote procedure executes the call in that thread,
which becomes temporarily unavailable. 
.LI
.ne 2
In stage \*LB\*O, call \*L5\*O arrives.  The call queue is
partially full, so the server accepts the call and adds it to the
end of the queue.
.LI
In stage \*LC\*O, call \*L11\*O arrives.  The queue is full, so the
server rejects this call, as
it rejected the previous call, \*L10\*O.  (The caller can try again with
the same or a different server.)
.LI
In stage \*LD\*O, the called procedure has completed call \*L1\*O, making
the call thread available.  The server has removed call
\*L2\*O from the queue and is passing it to the call thread for
execution.  Thus, the queue is partially empty as call \*L12\*O
arrives, so the server accepts the call and adds it to the queue.
.LE 
.H 3 "Selecting a Manager"
Unless an RPC interface is implemented for more than one specific type
of object, selecting a manager for an incoming call is a
simple process.  When registering an interface with a single manager,
the server specifies the nil type UUID for the manager
type.\*F
.FS 1
The API uses NULL to specify a synonym to the address of
the nil UUID, which contains only zeros.
.FE
In the absence of any other manager, all calls, regardless of whether
they request an object, go to the nil type manager.
.PP
The situation is more complex when a server registers multiple
managers for an interface.  The server runtime must select from among the
managers for each incoming call to the interface.  
The DCE RPC dispatching mechanism requires a
server to set a nonnil type UUID for a set of objects and 
for any interface that will access the objects 
in order to register a manager with the same type UUID.
...\"***********comment**********
...\" A server can do its own dispatching, or it can use the dispatching 
...\" mechanism provided by the DCE RPC runtime. 
...\"***********endcomment**********
.PP
.ne 16
To dispatch an incoming call to a manager, a server does the following:
.AL
.LI
If the call contains the nil object UUID, the server looks for a manager
registered with the nil type UUID (the nil type manager).
.AL
.LI
If the nil type manager exists for the requested interface,
the server dispatches the call to that manager.
.LI
Otherwise, the server rejects the call.
.LE
.LI
If the call contains a nonnil object UUID, the server looks to see
whether it has set a type for the object (by assigning a nonnil
type UUID).
.PP
If the object lacks a type, the server looks for the nil type manager.
.AL
.LI
If the nil type manager exists for the requested interface,
the server dispatches the call to that manager.
.LI
Otherwise, the server rejects the call.
.LE
.LI
If the object has a type, the call requires a remote procedure 
of a manager whose 
type matches the object's type.  In its absence, the RPC runtime rejects
the call.
.LE 
.P
Figure 17-8 illustrates the decisions a server makes to
select a manager to which to dispatch an incoming call.
.PP
.ne 7i
.FG "Decisions for Selecting a Manager"
...\" DEC-file name .P! pictures/rpc-manager-selection.ps 7.5i
.pI ../rpc/figures/7_advanced_topics_16.ps 0 0 1
.H 2 "Creating Portable Data via the IDL Encoding Services"
.P
The IDL encoding services provide client and server RPC applications
with a method for encoding data types in input parameters into
byte stream format and decoding data types in output parameters
from a byte stream without invoking the RPC runtime.  Encoding
and decoding functions are just like marshalling and unmarshalling,
except that the data is stored locally and is not transmitted
over the network; the IDL encoding services separate the data
marshalling and unmarshalling functions from interaction with
the RPC runtime.
.P
Client and server applications can use the IDL encoding services
to ``flatten'' (or ``serialize'') a data structure, even binary data,
and then store it; for example, by writing it to a file on disk.
An RPC application on any DCE machine, regardless of its data type
size and byte endianess, is then able to use the IDL encoding
services to decode previously encoded data.  Without the IDL
encoding services, you cannot create a file of data on one
machine and then successfully read that data on another machine
that has different size data types and byte endianess.
.P
The IDL encoding services can generate code that takes
the input parameters to a procedure and places them in a standard form in one or
more buffers that are delivered to user code.  This process is called 
\*Eencoding\*O. Encoded data can be written to a file or forwarded by a
messaging system.
The IDL encoding services can also generate code that delivers,
as the output parameters of a procedure, data that has been
converted into the standard form by encoding.  Delivery of data
in this way is called \*Edecoding\*O. Data to be decoded can
be read from a file or received by a messaging system.
.P
Applications use the ACF attributes \*Lencode\*O and \*Ldecode\*O 
as operation attributes or as interface attributes
to direct the IDL compiler to generate IDL encoding
services stubs for operations rather than generating
RPC stubs.  See Chapter 19
for usage information on \*Lencode\*O and \*Ldecode\*O.
.H 3 "Memory Management"
.iX "memory management"
.P
IDL encoding services stubs handle memory management
in the same way as RPC client stubs: when you call an
operation to which the \*Lencode\*O and/or \*Ldecode\*O attributes
have been applied, the encoding services stub uses whatever
client stub memory management scheme is currently in effect.
See Section 17.1 for further details on client stub
memory management defaults and setting up memory management schemes.
.P
You can control which memory management scheme the stubs will
use by calling the \*Lrpc_ss_swap_client_alloc_free(\|)\*O
and \*Lrpc_\%ss_\%set_\%client_\%alloc_\%free(\|)\*O routines.
The first routine sets the memory management routines used by
both the encoding and decoding stubs, and the second routine
restores the previous memory management scheme after encoding
and decoding are complete. 
.P
Note that the memory management scheme established, whether
explicitly or by default, is on a per-thread basis.
.H 3 "Buffering Styles"
.iX "buffering styles"
.P
There are a number of different ways in which buffers containing encoded data
can be passed betweeen the application code and the IDL encoding services.
These are referred to as different \*Ebuffering styles\*O.
The different buffering styles are:
.ML
.LI
.iX "incremental encoding"
Incremental encoding
.P
The incremental encoding style
requires that you provide an \*Lallocate\*O routine which
creates an empty buffer into which IDL encoding services
can place encoded data, and a \*Lwrite\*O routine
which IDL encoding services will call when the buffer is full or all the
parameters of the operation have been encoded.
The IDL encoding services call the \*Lallocate\*O and \*Lwrite\*O routines
repeatedly until the encoding of all of the parameters has
been delivered to the user code.  See the 
\*Lidl_es_encode_incremental(3rpc)\*O 
reference page for a description of the required parameters for the
\*Lallocate\*O and \*Lwrite\*O routines.
.LI
.ne 9
.iX "fixed buffer encoding"
Fixed buffer encoding 
.P
The fixed buffer encoding style requires that the application
supply a single buffer into which all the encoded data is to be placed.
The buffer must have an address that is 8-byte aligned and must be
a multiple of 8 bytes in size.  It must also be large enough
to hold an encoding of all the data, together with an
encoding header for each operation whose parameters are being encoded;
56 bytes should be allowed for each encoding header.
.LI
.iX "dynamic buffer encoding"
Dynamic buffer encoding
.P
With the dynamic buffer encoding style,
the IDL encoding services build a single buffer containing all the
encoded data and deliver the buffer to application code.
The buffer is allocated by whatever client memory management
mechanism has been put in place by the application code.
The default for this is \*Lmalloc\*O(\|).  When the application
code no longer needs the buffer, it should release the
memory resource.
.P
The dynamic buffer encoding style has performance implications.
The IDL encoding services will usually allocate a number of
intermediate buffers, then allocate the buffer to 
be delivered to the application code, copy data into it
from the intermediate buffers, and release the intermediate buffers.
.LI
.iX "incremental decoding"
Incremental decoding
.P
The incremental decoding buffering
style requires that you provide a \*Lread\*O routine which, when
called, delivers to the IDL encoding services a buffer that contains
the next part of the data to be decoded.  The IDL encoding services will call
the \*Lread\*O routine repeatedly until all of the required data
has been decoded.  See the \*Lidl_\%es_\%encode_\%incremental(3rpc)\*O 
reference page for a description of the required parameters for the
\*Lread\*O routine.
.LI
.iX "buffer decoding"
Buffer decoding
.P
The buffer decoding style requires that
you supply a single buffer containing all the encoded data.
Where application performance is important, note that, if the
supplied buffer is not 8-byte aligned, the IDL encoding services allocate
a temporary aligned buffer of comparable size and copy data from the
user-supplied buffer into it before performing the requested decoding.
.LE
.H 3 "IDL Encoding Services Handles"
.iX "IDL" "encoding services handles"
.P
When an application's encoding or decoding operation
is invoked, the handle passed to it must be an IDL
encoding services handle (the \*Lidl_es_handle_t\*O type).
The IDL encoding services handle indicates whether encoding or decoding
is required, and what style of buffering is to be used.
The IDL encoding services provides a set of routines to enable
the application code to obtain encoding and decoding handles to
the IDL encoding services.  The IDL encoding services
handle-returning routine you call depends on the buffering
style you have chosen:
.ML
.LI
If you have selected the incremental encoding style, you call the
\*Lidl_es_encode_incremental(\|)\*O routine, which returns an
incremental encoding handle.
.LI
If you have selected the fixed buffer encoding style, you call the
\*Lidl_es_encode_fixed_buffer(\|)\*O routine, which returns a fixed
buffer encoding handle.
.LI
If you have selected dynamic buffer encoding, you call the
\*Lidl_es_encode_dyn_buffer(\|)\*O routine, which returns
a dynamic buffer encoding handle.
.LI
If you have selected incremental decoding as your buffering style,
you call the \*Lidl_es_decode_incremental(\|)\*O routine, which
returns an incremental decoding handle.
.LI
If you have selected the buffer decoding style, you call the
\*Lidl_es_decode_buffer(\|)\*O routine, which returns a buffer decoding
handle.
.LE
.P
When the encoding or decoding for which an IDL encoding services
handle was required is completed, the application code should release the
handle resources by calling the \*Lidl_es_handle_free(\|)\*O routine.
See the \*(Dr for a complete description of the IDL encoding service
routines.
.P
It is an error to call an operation for which \*Lencode\*O or
\*Ldecode\*O has been specified by using an RPC binding handle, and
it is an error to call an RPC operation by 
using an IDL encoding services handle.
.iX "restrictions on handle use"
.P
.ne 10
The following restrictions apply to the use of IDL encoding services handles:
.ML
.LI
An operation can be called with an encoding
handle only if the operation has been
given the \*Lencode\*O ACF attribute.
.LI
An operation can be called with a decoding
handle only if the operation has been
given the \*Ldecode\*O ACF attribute.
.LI
The \*Lauto_handle\*O ACF attribute cannot be
used with the IDL encoding services.
.LI
The \*Limplicit_handle\*O ACF attribute cannot be
used with the IDL encoding services.
.LI
Customized handles cannot be used with the IDL encoding services.
.LI
An \*Lin\*O context handle does not contain the handle information
needed by the IDL encoding services.
.LE
.H 3 "Programming Example"
.P
The following example uses the IDL encoding service
features described in the preceding sections.  The
example verifies that the results of a number of
decoding operations are the same as the parameters used to create the
corresponding encodings.
.P
The interface definition for this example is as follows:
.oS
.ne 24
[uuid(20aac780-5398-11c9-b996-08002b13d56d), version(0)]
interface es_array
{
    const long N = 5000;

    typedef struct
    {
        byte b;
        long l;
    } s_t;

    typedef struct
    {
        byte b;
        long a[7];
    } t_t;

    void in_array_op1([in] handle_t h, [in] long arr[N]);
    void out_array_op1([in] handle_t h, [out] long arr[N]);

    void array_op2([in] handle_t h, [in,out] s_t big[N]);

    void array_op3([in] handle_t h, [in,out] t_t big[N]);
}
.oE
.P
The attribute configuration file for the example is as follows:
.oS
interface es_array
{
    [encode] in_array_op1();
    [decode] out_array_op1();
    [encode, decode] array_op2();
    [encode, decode] array_op3();
}
.oE
.P
.ne 20
The test code for the example is as follows:
.oS
.ps 10
.vs 12
#include <dce/pthread_exc.h>
#include "rpcexc.h"
#include <stdio.h>
#include <stdlib.h>
#include <file.h>
#include <sys/file.h>
#include "es_array.h"

/*
 *  User state for incremental encode/decode
 */
typedef struct es_state_t {
    idl_byte *malloced_addr;
    int file_handle;
} es_state_t;

static es_state_t es_state;

#define OUT_BUFF_SIZE 2048
static idl_byte out_buff[OUT_BUFF_SIZE];
static idl_byte *out_data_addr;
static idl_ulong_int out_data_size;

/*
 *  User allocate routine for incremental encode
 */
void es_allocate(state, buf, size)
idl_void_p_t state;
idl_byte **buf;
idl_ulong_int *size;
{
    idl_byte *malloced_addr;
    es_state_t *p_es_state = (es_state_t *)state;

    malloced_addr = (idl_byte *)malloc(*size);
    p_es_state->malloced_addr = malloced_addr;
    *buf = (idl_byte *)(((malloced_addr - (idl_byte *)0) + 7) & (~7));
    *size = (*size - (*buf - malloced_addr)) & (~7);
}

.ne 14
/*
 *  User write routine for incremental encode
 */
void es_write(state, buf, size)
idl_void_p_t state;
idl_byte *buf;
idl_ulong_int size;
{
    es_state_t *p_es_state = (es_state_t *)state;

    write(p_es_state->file_handle, buf, size);
    free(p_es_state->malloced_addr);
}

/*
 *  User read routine for incremental decode
 */
void es_read(state, buf, size)
idl_void_p_t state;
idl_byte **buf;
idl_ulong_int *size;
{
    es_state_t *p_es_state = (es_state_t *)state;

    read(p_es_state->file_handle, out_data_addr, out_data_size);
    *buf = out_data_addr;
    *size = out_data_size;
}

static ndr_long_int arr[N];
static ndr_long_int out_arr[N];
static s_t sarr[N];
static s_t ref_sarr[N];
static s_t out_sarr[N];
static t_t tarr[N];
static t_t ref_tarr[N];
static t_t out_tarr[N];
static ndr_long_int (*oarr)[M];

#define FIXED_BUFF_STORE (8*N+64)
static idl_byte fixed_buff_area[FIXED_BUFF_STORE];

.ne 11
/*
 *  Test Program
 */
main()
{
    idl_es_handle_t es_h;
    idl_byte *fixed_buff_start;
    idl_ulong_int fixed_buff_size, encoding_size;
    idl_byte *dyn_buff_start;
    error_status_t status;
    int i,j;

    for (i = 0; i < N; i++)
    {
        arr[i] = random()%10000;
        sarr[i].b = i & 0x7f;
        sarr[i].l = random()%10000;
        ref_sarr[i] = sarr[i];
        tarr[i].b = i & 0x7f;
        for (j = 0; j < 7; j++) tarr[i].a[j] = random()%10000;
        ref_tarr[i] = tarr[i];
    }

    /* 
     *Incremental encode/decode
     */
    /* Encode data using one operation */
    es_state.file_handle = open("es_array_1.dat", \\
                                              O_CREAT|O_TRUNC|O_WRONLY,
                                                                 0777);
    if (es_state.file_handle < 0)
    {
        printf("Can't open es_array_1.dat\en");
        exit(0);
    }
    idl_es_encode_incremental((idl_void_p_t)&es_state, es_allocate, \\
                                                          es_write,
                                         &es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_encode_incremental\en", status);
        exit(0);
    }
    in_array_op1(es_h, arr);
    close(es_state.file_handle);
    idl_es_handle_free(&es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_handle_free\en", status);
        exit(0);
    }
.ne 24
    /* Decode the data using another operation with the same signature */
    out_data_addr = (idl_byte *)(((out_buff - (idl_byte *)0) + 7) & (~7));
    out_data_size = (OUT_BUFF_SIZE - (out_data_addr - out_buff)) & (~7);
    es_state.file_handle = open("es_array_1.dat", O_RDONLY, 0);
    if (es_state.file_handle < 0)
    {
        printf("Can't open es_array_1.dat for reading\en");
        exit(0);
    }
    idl_es_decode_incremental((idl_void_p_t)&es_state, es_read,
                                &es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_decode_incremental\en", status);
        exit(0);
    }
    out_array_op1(es_h, out_arr);
    close(es_state.file_handle);
    idl_es_handle_free(&es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_handle_free\en", status);
        exit(0);
    }

    /* Check the input and output are the same */
    for (i = 0; i < N; i++)
    {
        if (out_arr[i] != arr[i])
        {
            printf("out_arr[%d] - found %d - expecting %d\en",
                    i, out_arr[i], arr[i]);
        }
    }

    /*
     * Fixed buffer encode/decode
     */
    fixed_buff_start = (idl_byte *)(((fixed_buff_area - \\
                                                  (idl_byte *)0) + 7)
                                                             & (~7));
    fixed_buff_size = (FIXED_BUFF_STORE - \\
                                (fixed_buff_start - fixed_buff_area))
                                                             & (~7);
    idl_es_encode_fixed_buffer(fixed_buff_start, fixed_buff_size,
                                &encoding_size, &es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_encode_fixed_buffer\en", status);
        exit(0);
    }
.ne 7
    array_op2(es_h, sarr);
    idl_es_handle_free(&es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_handle_free\en", status);
        exit(0);
    }
    idl_es_decode_buffer(fixed_buff_start, encoding_size, &es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_decode_buffer\en", status);
        exit(0);
    }
    array_op2(es_h, out_sarr);
    idl_es_handle_free(&es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_handle_free\en", status);
        exit(0);
    }
    for (i = 0; i < N; i++)
    {
        if (out_sarr[i].b != ref_sarr[i].b)
        {
            printf("array_op2 - out_sarr[%d].b = %c\en", i, out_sarr[i].b);
        }
        if (out_sarr[i].l != ref_sarr[i].l)
        {
            printf("array_op2 - out_sarr[%d].l = %d\en", i, out_sarr[i].l);
        }
    }

    /*
     * Dynamic buffer encode - fixed buffer decode
     */
    idl_es_encode_dyn_buffer(&dyn_buff_start, &encoding_size, &es_h, \\
                                                              &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_encode_dyn_buffer\en", status);
        exit(0);
    }
    array_op3(es_h, tarr);
    idl_es_handle_free(&es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_handle_free\en", status);
        exit(0);
    }
.ne 6
    idl_es_decode_buffer(dyn_buff_start, encoding_size, &es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_decode_buffer\en", status);
        exit(0);
    }
    array_op3(es_h, out_tarr);
    rpc_ss_free (dyn_buff_start);
    idl_es_handle_free(&es_h, &status);
    if (status != error_status_ok)
    {
        printf("Error %08x from idl_es_handle_free\en", status);
        exit(0);
    }
    for (i = 0; i < N; i++)
    {
        if (out_tarr[i].b != ref_tarr[i].b)
        {
            printf("array_op3 - out_tarr[%d].b = %c\en", i, out_tarr[i].b);
        }
        for (j=0; j<7; j++)
        {
            if (out_tarr[i].a[j] != ref_tarr[i].a[j])
            {
                printf("array_op3 - out_tarr[%d].a[%d] = %d\en",
                        i, j, out_tarr[i].a[j]);
            }
        }
    }

    printf("Test Complete\en");
}
.ps 12
.vs 14
.oE
.ne 20
.H 3 "Performing Multiple Operations on a Single Handle"
.iX "multiple operations on a single IDL encoding services handle"
.P
Multiple operations can be performed using one 
encoding handle before the handle is released.  In this case,
all the encoded data is part of the same buffer system.
.P
A single decoding handle is used to obtain the contents of the
encoded data.  Decoding operations must be called in the same order the
encoding operations were called to create the encoded data.
.P
The definition of the user client memory management functions, and any memory
allocated by IDL encoding services using the client memory allocator, must not
be modified between operations for which the same encoding handle is used.
.P
.H 3 "Determining the Identity of an Encoding"
.iX "determining the identity of an encoding"
Applications can use the \*Lidl_es_inq_encoding_id(\|)\*O routine
to determine the identity of an encoding operation, for example,
before calling their decoding operations.
