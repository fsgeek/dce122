...\"
...\" @OSF_COPYRIGHT@
...\" COPYRIGHT NOTICE
...\" Copyright (c) 1990, 1991, 1992, 1993 Open Software Foundation, Inc.
...\" ALL RIGHTS RESERVED (DCE).  See the file named COPYRIGHT.DCE in the
...\" src directory for the full copyright text.
...\"
...\"
...\" HISTORY
...\" $Log: issues.gpsml,v $
...\" Revision 1.1.11.4  1996/09/16  20:11:07  wfl
...\" 	Added editorial changes
...\" 	[1996/09/16  20:10:19  wfl]
...\"
...\" Revision 1.1.11.3  1996/08/20  13:15:28  wfl
...\" 	{enh,13605,R1.2.2}
...\" 	Security enhancements
...\" 	[1996/08/20  13:14:55  wfl]
...\" 
...\" Revision 1.1.11.2  1996/07/16  19:50:48  wfl
...\" 	{enh, 13566, R1.2.2}
...\" 	Added multihomed servers
...\" 	[1996/07/16  19:50:17  wfl]
...\" 
...\" Revision 1.1.11.1  1996/05/14  19:52:32  wardr
...\" 	{enh,R1.2.2}
...\" 	Removed changebars
...\" 	[1996/05/14  19:51:18  wardr]
...\" 
...\" Revision 1.1.8.3  1996/01/23  16:57:53  weir
...\" 	Test checkin/out
...\" 	[1996/01/23  16:57:26  weir]
...\" 
...\" Revision 1.1.8.2  1995/10/04  16:02:48  wfl
...\" 	{def, 13140, R1.2.1}
...\" 	Fixed quotes in change markers
...\" 	[1995/10/04  16:02:19  wfl]
...\" 
...\" Revision 1.1.8.1  1995/09/16  16:08:39  wfl
...\" 	{enh,13093,R1.2.1}
...\" 	Add server preference
...\" 	[1995/09/16  16:08:17  wfl]
...\" 
...\" Revision 1.1.6.26  1995/07/07  19:06:00  buckler
...\" 	More 1.1 edits.
...\" 	[1995/07/07  19:04:43  buckler]
...\" 
...\" 	1.1 edits and Prentice Hall reformat
...\" 	[1995/07/07  16:43:11  buckler]
...\" 
...\" Revision 1.1.6.25  1994/10/14  19:34:40  jeff
...\" 	{defect, 12535, R1.1}
...\" 	Incorporate dcecp commands in DFS documentation.
...\" 	[1994/10/14  19:32:51  jeff]
...\" 
...\" Revision 1.1.6.24  1994/08/10  22:28:28  jeff
...\" 	Editorial work.
...\" 	[1994/08/10  22:27:24  jeff]
...\" 
...\" Revision 1.1.6.23  1994/07/18  23:41:31  jeff
...\" 	{defect, 11350, R1.1}
...\" 	Document root/self privileges.
...\" 	[1994/07/18  23:40:33  jeff]
...\" 
...\" Revision 1.1.6.22  1994/07/18  22:40:36  jeff
...\" 	Editorial work.
...\" 	[1994/07/18  22:40:22  jeff]
...\" 
...\" Revision 1.1.6.21  1994/06/01  11:48:20  jeff
...\" 	Minuscule editorial change.
...\" 	[1994/06/01  11:48:05  jeff]
...\" 
...\" Revision 1.1.6.20  1994/05/31  22:40:57  jeff
...\" 	Correct formatting.
...\" 	[1994/05/31  22:40:43  jeff]
...\" 
...\" Revision 1.1.6.19  1994/05/25  15:04:54  jeff
...\" 	{defect, 10752, R1.1}
...\" 	Minor technical clarification for database access with Ubik.
...\" 	[1994/05/25  15:04:35  jeff]
...\" 
...\" Revision 1.1.6.18  1994/05/24  22:00:52  jeff
...\" 	{defect, 10752, R1.1}
...\" 	Minor technical clarification for fileset names.
...\" 	[1994/05/24  22:00:26  jeff]
...\" 
...\" Revision 1.1.6.17  1994/05/19  20:49:49  jeff
...\" 	{defect, 8118, R1.1}
...\" 	Correct use of double quotes.
...\" 	[1994/05/19  20:48:56  jeff]
...\" 
...\" Revision 1.1.6.16  1994/05/18  14:47:36  jeff
...\" 	Fix more formatting problems.
...\" 	[1994/05/18  14:46:45  jeff]
...\" 
...\" Revision 1.1.6.15  1994/05/17  14:53:53  jeff
...\" 	Correcting small formatting errors.
...\" 	[1994/05/17  14:49:18  jeff]
...\" 
...\" Revision 1.1.6.14  1994/05/13  18:19:11  jeff
...\" 	{defect, 9472, R1.1}
...\" 	Document that database servers must be in each other's admin lists.
...\" 	[1994/05/13  18:18:51  jeff]
...\" 
...\" Revision 1.1.6.13  1994/05/06  13:52:18  jeff
...\" 	{defect, 10552, R1.1}
...\" 	Fix index entry inconsistencies.
...\" 	[1994/05/06  13:49:54  jeff]
...\" 
...\" Revision 1.1.6.12  1994/04/28  22:21:17  jeff
...\" 	{defect, 10439, R1.1}
...\" 	Correct cross-references for reorganization of DFS documentation.
...\" 	[1994/04/28  22:14:26  jeff]
...\" 
...\" Revision 1.1.6.11  1994/04/20  21:05:55  jeff
...\" 	{enh, 10306, R1.1}
...\" 	Confirm/correct removal of diskless references.
...\" 	[1994/04/20  21:01:34  jeff]
...\" 
...\" Revision 1.1.6.10  1994/04/07  19:56:26  rom
...\" 	{enh, 10306, R1.1}
...\" 	Remove diskless documentation from the DCE doc set.
...\" 	[1994/04/07  15:31:41  rom]
...\" 
...\" Revision 1.1.6.9  1993/11/11  16:36:29  zahn
...\" 	Checked out for possible Postscript problem.
...\" 	None found, no changes necessary.
...\" 	[1993/11/11  16:36:02  zahn]
...\" 
...\" Revision 1.1.6.8  1993/10/15  12:18:47  kdu
...\" 	{def,8393,R1.0.3}
...\" 	Document the udebug command.
...\" 	[1993/10/15  12:17:57  kdu]
...\" 
...\" Revision 1.1.6.7  1993/10/13  20:23:58  tmw
...\" 	Added index entries for second version of master index.
...\" 	[1993/10/13  15:25:30  tmw]
...\" 
...\" Revision 1.1.6.6  1993/09/23  13:18:11  kdu
...\" 	{def,7715,R1.0.3}
...\" 	Examples of sysname.
...\" 	[1993/09/23  13:17:31  kdu]
...\" 
...\" Revision 1.1.6.5  1993/09/17  13:33:54  kdu
...\" 	{def,8616,R1.0.3}
...\" 	Foreign groups cannot own server entries.
...\" 	[1993/09/17  13:33:01  kdu]
...\" 
...\" Revision 1.1.6.4  1993/09/15  21:28:01  kdu
...\" 	{def,8515,R1.0.3}
...\" 	Availability of read-only filesets.
...\" 
...\" 	{def,8590,R1.0.3}
...\" 	Configuring a fileset database machine.
...\" 	[1993/09/15  21:26:57  kdu]
...\" 
...\" Revision 1.1.6.3  1993/08/10  23:50:26  jeff
...\" 	Changed versions for defect fixes from 1.0.2A to 1.0.3.
...\" 	[1993/08/10  23:49:46  jeff]
...\" 
...\" Revision 1.1.6.2  1993/07/30  17:31:38  kdu
...\" 	{def,8386,R1.0.3}
...\" 	Incorporate OSF editorial comments into DFS Admin Guide and related
...\" 	documentation.
...\" 	[1993/07/30  17:18:47  kdu]
...\" 
...\" Revision 1.1.4.12  93/02/22  19:17:29  jeff
...\" 	Fix for defect 5795, update backup documentation.
...\" 	[1993/02/22  19:16:47  jeff]
...\" 
...\" Revision 1.1.4.11  1993/02/22  01:11:24  jeff
...\" 	Fix for defect 7219, review comments.
...\" 	[1993/02/22  01:10:40  jeff]
...\" 
...\" Revision 1.1.4.10  1993/02/19  18:38:14  jeff
...\" 	Fix for defect 6055, clarify root privileges (also
...\" 	some work for defect 7219).
...\" 	[1993/02/19  18:37:07  jeff]
...\" 
...\" Revision 1.1.4.9  1993/02/05  20:16:08  jeff
...\" 	Fix for defect 7135, change key file to keytab file.
...\" 	[1993/02/05  20:14:54  jeff]
...\" 
...\" Revision 1.1.4.8  1993/02/04  01:56:15  jeff
...\" 	Fix for defects 6888 and 4016.
...\" 	[1993/02/04  01:54:36  jeff]
...\" 
...\" Revision 1.1.4.7  1993/01/28  19:05:07  dbelch
...\" 	Embedding copyright notice
...\" 	[1993/01/28  18:29:00  dbelch]
...\" 
...\" Revision 1.1.4.6  1993/01/27  20:07:01  buckler
...\" 	Fixed cross-refs and figure calls for new book org
...\" 	[1993/01/27  20:04:52  buckler]
...\" 
...\" Revision 1.1.4.5  1993/01/27  19:10:23  jeff
...\" 	... TEMPORARY CHECK-IN TO ALLOW FOR OSF WORK....
...\" 	[1993/01/27  19:00:44  jeff]
...\" 
...\" Revision 1.1.4.4  1993/01/13  19:13:19  jeff
...\" 	Fix for defect 6811, -ioprocs of dfsd is AIX specific.
...\" 	[1993/01/13  19:12:31  jeff]
...\" 
...\" Revision 1.1.4.3  1993/01/09  18:46:36  jeff
...\" 	Fix for defects 6326 and 6594, update Ubik and related
...\" 	information and document dfsbind changes, respectively.
...\" 	[1993/01/09  18:46:04  jeff]
...\" 
...\" Revision 1.1.4.2  1992/08/26  12:09:22  weir
...\" 	Removed change bars
...\" 	[1992/08/26  11:51:46  weir]
...\" 
...\" Revision 1.1.2.8  1992/07/05  19:25:40  jeff
...\" 	Corrected Private File Server machine processes.
...\" 	[1992/07/05  19:24:24  jeff]
...\" 
...\" Revision 1.1.2.7  1992/07/04  15:48:35  jeff
...\" 	Documented that inclusion in the fxd admingroup and being
...\" 	root are not equivalent.
...\" 	[1992/07/04  15:48:07  jeff]
...\" 
...\" Revision 1.1.2.6  1992/06/06  16:48:43  jeff
...\" 	Updated descriptions of the privileges provided by inclusion
...\" 	in the group specified with the fxd -admingroup option.
...\" 	[1992/06/06  00:41:19  jeff]
...\" 
...\" Revision 1.1.2.5  1992/06/04  21:52:40  jeff
...\" 	Edited to emphasize that the fs junction is not well known.
...\" 	[1992/06/04  21:48:40  jeff]
...\" 
...\" Revision 1.1.2.4  1992/06/04  18:32:41  jeff
...\" 	Updated the information described as prerequisite for the
...\" 	Ubik synchronization mechanism.  A couple large chunks of
...\" 	text were added, and some existing facts were modified.
...\" 	[1992/06/04  18:30:02  jeff]
...\" 
...\" Revision 1.1.2.3  1992/05/12  15:42:17  jeff
...\" 	Verified and/or modified italics and other editorial
...\" 	aspects of the file.
...\" 	[1992/05/11  19:21:54  jeff]
...\" 
...\" Revision 1.1.2.2  1992/03/05  23:42:18  jeff
...\" 	Testing ODE.
...\" 	[1992/03/05  22:37:40  jeff]
...\" 
...\" Revision 1.1  1992/01/29  16:14:39  damon
...\" 	Initial revision
...\" 
...\" $EndLog$
...\"
...\" (c) Copyright 1991, Open Software Foundation, Inc.  ALL RIGHTS RESERVED
..."Copyright (C) 1989, 1991, Transarc Corporation
..."The Gulf Tower
..."707 Grant Street
..."Pittsburgh, PA  15219
...\" CHANGED
...\" 12-11-91:  Section 2.3.5:  Removed first two sentences from last par.
...\" 12-11-91:  Added references to Private File Server machine to Sections
...\"            2.1.1, 2.1.1.3, and 2.1.1.7.  References consist of merely
...\"            using the term.
...\" 12-11-91:  Section 2.1.1.7:  Added four index tags for Private File
...\"            Server machine.
...\" END CHANGED
...\"DOCUMENTSTYLE [12pt]{book}
.H 1 "DFS Configuration Issues"
.iX "-[" "Distributed File Service (DFS)" "configuration"
.P
This chapter provides summary information about the following DFS configuration
issues: choosing DFS machine roles, DFS server and client configuration issues,
setting up DCE LFS filesets, understanding DFS data access management, and
understanding the DFS distributed database technology.  Subsequent chapters
provide specific details about managing DFS server machines, processes,
filesets, and files.
.P
This chapter is intended as an overview of DFS configuration issues. It also
serves as a reference for the issues and considerations that go into the
configuration of a cell and its administrative domains.  You should read and 
become familiar with the information in this chapter before attempting to 
use any of the commands described later in this guide.
.H 2 "Choosing DFS Machine Roles"
.iX "-[" "machines" "roles in DFS"
.P
DFS server and client machines can run the following processes:
the BOS Server to monitor other processes; the Fileset Server, Fileset
Location Server, and Replication Server to manipulate DFS filesets
and their replicas; the Backup Server to contact the Backup Database;
the \*Lbutc\*O process to back up file system data to tape; and the
\*Ldfsd\*O process to initialize the Cache Manager on a client machine.
.P
Each DFS server or client machine must also run the RPC, CDS, and Security
processes necessary for configuration as a DCE client machine.  An RPC binding
must be created in CDS for the DCE pathname of each server machine, and a DFS
server principal and associated account must also be created in the Registry
Database for each server machine.  The following sections assume that these
requirements have been satisfied prior to configuring a machine as a DFS
server or client machine. (See your vendor's installation and configuration
documentation for more information about fulfilling these requirements.)
.P
The system administrator determines, at installation, which processes are to
be run on which machines.  A machine's role is determined by the types of
processes it runs.  The information in the following subsections details the 
different roles a machine can assume.
.P
Each DFS server process has an associated administrative list.  Users, groups,
and server machines included on a process's administrative list can issue
commands or calls that affect the process.  Members can be added to
administrative lists at any time.  Chapter 4 provides detailed information
about the procedures used to create and maintain administrative lists. (See
Part 2 of this guide and reference for complete information about the
administrative privileges and permissions required to issue each DFS command.)
.P
The Basic OverSeer (BOS) Server, or \*Lbosserver\*O process, is not
associated with any one machine role; it runs on every DFS server machine.
Its primary function is to minimize system outages.  It monitors other server
processes on the local machine and restarts failed processes automatically.
.P
.ne 10
By default, the BOS Server on each server machine stops and immediately
restarts all DFS processes (including itself) on the machine once a week, at
4:00 a.m. on Sunday.  It also checks for any newly installed binary files in
the \*Vdcelocal\*L/bin\*O directory every morning at 5:00 a.m. (Note that
these restart times can be configured.) If it finds any new files, which
it does by checking for time stamps later than the time at which the 
corresponding process last started, it restarts the corresponding process. 
Because restarting processes causes a service outage, the default times are 
in the early morning hours, when an outage disturbs the fewest number of 
users.  This brief suspension of services should have no effect on processes 
that are currently executing; the processes should continue normally once 
service resumes.
.P
Install the BOS Server on all server machines to assist in administrative tasks
on the machines.  The \*Ladmin.bos\*O list is used to designate administrative
users who can issue \*Lbos\*O commands that affect the \*Lbosserver\*O process
on a server machine.  Members of the \*Ladmin.bos\*O list can vary among
different DFS administrative domains.
.H 3 "Overview of DFS Machine Roles"
.P
Following is a brief summary of the DFS roles a machine can assume:
.ML
.LI
.iX "System Control machines" "about"
System Control machine: A single machine acts as the System Control
machine for a domain, updating the other machines in the domain with identical
versions of common configuration files such as administrative lists.
.LI
.iX "Binary Distribution machines" "about"
Binary Distribution machine: One Binary Distribution machine of each
CPU/operating system (OS) type is installed in a cell.  The Binary Distribution
machine updates other machines of its CPU/OS type with identical versions of
system binary files.
.LI
.iX "File Server machines" "about"
File Server machine: A File Server machine runs the basic set of
processes necessary for storing and exporting DCE LFS and non-LFS data.
.LI
.iX "Fileset Database machines" "about"
Fileset Database machine: This type of database machine runs the process
that maintains the Fileset Location Database (FLDB).
.LI
.iX "Backup Database machines" "about"
Backup Database machine: This type of database machine runs the process
that maintains the Backup Database.
.nL
.ne 10
.LI
.iX "client machines" "about"
DFS client machine: Any machine can run the Cache Manager and its associated
processes to act as a DFS client.  This machine serves primarily as a single or
multiuser workstation.  It can also be configured as a Private File Server
machine to export data.
.LE
.P
Depending on the number of machines in your cell, assign the following roles
to your server machines:
.ML
.LI
In a cell with only one server machine, the machine runs all processes and
fills all the necessary machine roles.  Note that the System Control machine 
and Binary Distribution machine roles are unnecessary in this configuration.
.LI
.iX "Fileset Database machines" 
.iX "Backup Database machines" 
In a cell with two server machines, both machines act as Fileset Database
machines and Backup Database machines to replicate the databases.  For each
database, one of the machines automatically assumes the role of the
synchronization site and houses the source copy of the database.  If one of the
machines becomes unavailable, the information in the database may not be able
to be changed. (See Section 2.5 for a detailed description of database
synchronization.)
.LI
In a cell with three or more server machines, three machines run as Fileset
Database machines and three machines run as Backup Database machines.  This
configuration allows the cell to benefit from the database replication
capabilities of DFS.  An odd number of database machines is best.
.LE
.P
The software for all server processes can be installed on every server machine,
even though a machine need not run every process.  To then change the role of
a machine, simply start or stop the appropriate processes.  Machine roles are
not mutually exclusive; that is, any server machine can assume multiple server machine
roles, any server machine can be configured as a client machine, and any client
machine can be configured as a server machine.
.nL
.ne 15
.H 4 "System Control Machines"
.iX "-[" "System Control machines" "about"
.iX "-[" "Update Server" "about"
.P
The System Control machine in a domain stores and distributes system
configuration information, such as administrative lists, shared by all DFS
server machines in the domain.  Configure the first server machine for
any new domain as the System Control machine for that domain.  It can then be 
used to distribute the administrative lists for that domain from its 
\*Vdcelocal\*L/var/dfs\*O directory to any subsequent server machines 
added to the domain.
.P
The following processes run on a System Control machine:
.ML
.LI
An \*Lupserver\*O process (the server portion of the Update Server), which
controls the distribution of common configuration files to all other server
machines in the domain.
.LI
An \*Lupclient\*O process (the client portion of the Update Server), which
retrieves binary files from the Binary Distribution machine of the proper
CPU/OS type. (See Section 2.1.1.2 for a description of the Binary Distribution
machine.)
.LI
A BOS Server (\*Lbosserver\*O process). (See Section 2.1 for more information
about the BOS Server.)
.LE
.P
The Update Server helps ensure that all server machines in a domain run
the same version of common configuration files such as administrative lists.
Configuration files are created and modified on the System Control machine,
which runs the server portion, or \*Lupserver\*O process, of the Update Server.
Other server machines in the domain run the client portion, or \*Lupclient\*O
process, of the Update Server.  The \*Lupclient\*O processes on the other
server machines in the domain frequently contact the \*Lupserver\*O process on
the System Control machine to verify that the most recent version of each
configuration file is in use.  If the most recent version of a file is not in
use, the \*Lupclient\*O process on each machine retrieves the most recent
version from the System Control machine and installs it locally.
.P
.iX "\*Ladmin.up\*O file" 
The server portion of the Update Server must be run on any machine that acts
as a System Control machine for a domain.  The \*Ladmin.up\*O list is used to
identify all server principals that can obtain updates from the System Control
machine.  The list should include the names of all of the server machines in a
domain.
.iX "-]" "Update Server" "about"
.iX "-]" "System Control machines" "about"
.H 4 "Binary Distribution Machines"
.iX "-[" "Binary Distribution machines"
.iX "binary files" "distributing"
.P
A Binary Distribution machine stores DFS binary files for processes and
command suites for distribution from its \*Vdcelocal\*L/bin\*O and related
directories to all other server machines of its CPU/OS type in a cell.  Each
server keeps a copy of server process binaries in a local directory; however,
all the machines must be running the same version of the process for the system
to perform correctly.  Therefore, the binaries are installed on a single Binary
Distribution machine, which acts as a source for the others.  Configure one
Binary Distribution machine for each CPU/OS type for which multiple machines
exist in the cell.
.P
A Binary Distribution machine runs the following processes:
.ML
.LI
An \*Lupserver\*O process (the server portion of the Update Server), which
controls the distribution of binary files to other server machines of the same
CPU/OS type in the cell.
.LI
An \*Lupclient\*O process (the client portion of the Update Server), which
retrieves configuration files from the System Control machine.
.LI
A BOS Server (\*Lbosserver\*O process). (See Section 2.1 for more information
about the BOS Server.)
.LE
.P
A second Update Server, different from the one used to distribute
configuration files from the System Control machine, helps ensure that
all server machines of the same CPU/OS type in a cell run the same
binary files.  Like System Control machines, Binary Distribution machines
run an \*Lupserver\*O process.  The \*Lupclient\*O processes on
the other server machines of the same CPU/OS type in the cell frequently
contact the \*Lupserver\*O process to verify that the most recent
version of each binary file is in use.  If it is not, the
\*Lupclient\*O processes on the other server machines retrieve the
most recent version from the Binary Distribution machine and install it
locally.  You do not have to install new software on each individual
server machine because the Update Server does it automatically.
.P
The server portion of the Update Server must be run on any machine that acts
as a Binary Distribution machine for a cell.  The \*Ladmin.up\*O list associated
with this Update Server is used to identify all server principals that can
obtain updates from the Binary Distribution machine.  The list should include
the names of all machines of the same CPU/OS type in a cell.
.P
.ne 8
Unless a server machine is fulfilling the roles of both System Control machine
and Binary Distribution machine, different Update Servers handle the
distribution of configuration and binary files.  A machine configured to
perform both roles runs only a single Update Server to distribute both common
configuration files and system binary files.
.iX "-]" "Binary Distribution machines"
.H 4 "File Server Machines"
.iX "-[" "File Server machines" "about"
.iX "Fileset Server" "about"
.P
A File Server machine is used to store and export DCE LFS or non-LFS
data for use in the global namespace.  Configure enough File Server machines to
contain the data to be exported from the domain.  A File Server machine must run
the following processes, most of which are necessary for storing filesets,
exporting data, and storing replicas of filesets:
.ML
.LI
A Fileset Server (\*Lftserver\*O process).
.LI
The File Exporter, which is initialized by the \*Lfxd\*O process, in the 
kernel.
.LI
The \*Ldfsbind\*O process.
.LI
The Replication Server (\*Lrepserver\*O process).
.LI
Two \*Lupclient\*O processes: one to retrieve configuration files from the
System Control machine, and one to retrieve binary files from the Binary
Distribution machine of the proper CPU/OS type.
.LI
A BOS Server (\*Lbosserver\*O process).  (See Section 2.1 for more information
about the BOS Server.)
.LE
.P
The Fileset Server, or \*Lftserver\*O process, provides an interface for
commands that affect filesets (commands that create, delete, or move 
filesets, and commands that prepare filesets for archiving to tape or other 
media).  The most common
occurrences of fileset creation and deletion are when you add or remove users
from the system.  Filesets are most often moved to provide load balancing among
File Server machines.
.P
.iX "\*Ladmin.ft\*O file"
The Fileset Server must run on any machine that exports data for use in the
global namespace.  The \*Ladmin.ft\*O list is used to designate administrative
users who can issue \*Lfts\*O commands that affect the \*Lftserver\*O process
on a machine and to designate other server machines from which the machine can
accept filesets.  Users, groups, and machines listed in the \*Ladmin.ft\*O list
can differ among DFS administrative domains.
.P
.iX "-[" "File Exporter" "about"
The File Exporter (sometimes called the \*EProtocol Exporter\*O) runs as part
of the kernel on each File Server machine.  It provides the same services across
the network that the local operating system provides on a local disk:
.ML
.LI
Delivering requested files and programs to clients; storing files and programs
when clients finish with them
.LI
Maintaining the directory hierarchy structure
.LI
Handling file-related or directory-related requests (creating, deleting, 
copying, and moving filesets)
.LI
Tracking status information (including size and modification status) about 
each file and directory
.LI
Creating symbolic links between files
.LE
.P
.iX "File Exporter" "administrative mechanisms"
Unlike the DFS server processes, the File Exporter is not associated with an
administrative list.  Instead, the command line for the \*Lfxd\*O process,
which is used to initialize the File Exporter and start related kernel
daemons, includes an \*L\-admingroup\*O option that specifies the administrative
group for the File Exporter on each File Server machine.  The group specified
with this option must be defined in the Registry Database, as must all groups
used with DFS.
.P
.iX "permissions" "changing on exported filesets"
Members of this administrative group can change the ACL and UNIX permissions
of \*Eall\*O data exported from the machine.  They have the equivalent of the
ACL \*Lc\*O permission on all of the files and directories in each exported DCE
LFS fileset, and they can effectively change the UNIX permissions on all of the
files and directories in each exported non-LFS fileset.  Members of the group
can also change the owner and owning group of all files and directories
exported from the machine.  Include only highly trusted system administrators
in this group.
.P
.iX "access control lists (ACLs)" "\*Lroot\*O permissions"
.iX "access control lists (ACLs)" "\*Lself\*O permissions"
.iX "\*Lself\*O principal" "ACL permissions"
.iX "\*Lroot\*O" "ACL permissions"
Though similar in many respects, inclusion in the administrative group
associated with the File Exporter and being logged in as \*Lroot\*O are
\*Enot\*O equivalent.  A user who is logged into the local machine as \*Lroot\*O
can perform different operations on a file or directory, depending on how the
user accesses the file or directory:
.nL
.ne 12
.ML
.LI
\*EWhen accessing a file or directory via its DCE pathname\*O, if the user
is logged into the local machine as \*Lroot\*O but is not authenticated to
DCE, DFS treats the user as the
\*L/.../\*Vcellname\*L/hosts/\*Vhostname\*L/self\*O principal of the local
machine; in this case, the \*Lroot\*O user receives the permissions associated
with the machine's \*Lself\*O principal, which is treated as an authenticated
user from the local cell.  If the user is also authenticated to DCE as
\*Lroot\*O, DFS treats the user according to the DCE identity \*Lroot\*O. (Note
that you do not have to be logged into the local machine as \*Lroot\*O to be
logged into DCE as \*Lroot\*O.)
.nS "note"
The DCE identity \*Lroot\*O effectively has \*Lroot\*O privileges for data in
all exported non-LFS filesets in the cell.  The identity is very powerful and
represents a serious security risk.  Either use the DCE \*Lroot\*O identity
\*Every\*O cautiously or disable it altogether.
.nE
.LI
\*EWhen accessing a file or directory via its local pathname\*O, the \*Lroot\*O
user has all of the privileges commonly associated with \*Lroot\*O.  For local
access, \*Lroot\*O can perform any file system operation on a file or
directory; for example, \*Lroot\*O can change the UNIX mode bits of a file or
directory, change the ACL permissions of a DCE LFS file or directory, change
the owner or owning group of a file or directory, or create or remove a file
or directory. (A file or directory in a non-LFS fileset can always be accessed
via a local pathname because a non-LFS fileset must always be mounted locally, 
as a file system on its File Server machine; a file or directory in a DCE LFS
fileset can be accessed via a local pathname only if its fileset is mounted
locally.)
.LE
.P
Being a member of the \*Lfxd\*O administrative group allows you to perform any
operation on a file or directory in an exported fileset, but you may have to
change the file's or directory's protections first.  Being logged into the local
machine as \*Lroot\*O lets you perform any operation on a file or directory in
a locally mounted fileset immediately, without first changing the protections.
Being authenticated as DCE \*Lroot\*O lets you perform any operation on a file
or directory in an exported non-LFS fileset immediately.
.P
The File Exporter also manages the distribution of tokens to clients.  It
maintains an inventory of outstanding tokens, including the clients to which
it has granted tokens, the data for which it has granted those tokens, and the
type of each token it has granted. (A token's type dictates the operations that
the client holding the token can perform on the data to which the token
applies.) (See Section 2.4 for more information about the File Exporter's
token-management mechanism.)
.P
.iX "\*Lfxd\*O process"
.ne 8
The \*Lfxd\*O process must be run on any machine used to export data to
the global namespace. (See Part 2 of this guide and reference for complete
information about the \*Lfxd\*O process.)
.P
.iX "\*Ldfsbind\*O process" "about"
The \*Ldfsbind\*O process on a File Server machine maintains user
authentication information required by the File Exporter on the machine.  The
File Exporter uses this information to ensure that only authenticated users
access data from the machine.  The \*Ldfsbind\*O process must be run on any
machine used to export data to the global namespace.
.P
The \*Ldfsbind\*O process must also be run on all client machines.  Its role on
client machines is described along with client machines and their processes in
Section 2.2.2. (See Part 2 of this guide and reference for complete information
about the \*Ldfsbind\*O process.)
.P
.iX "Replication Server" "about"
The Replication Server, or \*Lrepserver\*O process, manages replicas of
filesets on all File Server machines.  Depending on the replication method in
use, you either release a new version of a fileset for distribution by the
Replication Server, or the Replication Server automatically creates replicas
at specified intervals.  Install the Replication Server on all File Server
machines, which are the machines that can store read-only replicas of 
filesets.  No administrative list is associated with 
the \*Lrepserver\*O process.
.P
.zA "enh,13566,R1.2.2,Add multihomed server"
In addition, each File Server machine must have a server entry
registered in the FLDB before it can house filesets. Each File Server
machine can have up to four server entries, with each entry specifying
a different host name or IP address. The server entry
must exist before the \*Lfts create\*O or \*Lfts crfldbentry\*O
command can be used to create an entry in the FLDB for a DCE LFS or
non-LFS fileset from the machine. The following section discusses
server entries in more detail. (See Chapter 6 for more information
about creating server entries.)
.zZ "enh,13566,R1.2.2,Add multihomed server"
.P
A client machine can also be configured as a Private File Server machine to
export data to the global namespace. (See Section 2.1.1.7 for more information
about configuring a client machine to export data.)
.iX "-]" "File Exporter" "about"
.iX "-]" "File Server machines" "about"
.H 4 "Fileset Database Machines"
.iX "-[" "Fileset Database machines"
.iX "-[" "Fileset Location Database" "about" 
.P
A Fileset Database machine stores the Fileset Location Database.
Optimally, you should configure three or a larger, odd number of Fileset
Database machines sufficient to support the File Server machines in the cell.
.P
Each Fileset Database machine runs the following processes:
.ML
.LI
.iX "Fileset Location Server" "about"
A Fileset Location Server (\*Lflserver\*O process).
.LI
Two \*Lupclient\*O processes: one to retrieve configuration files from the
System Control machine, and one to retrieve binary files from the Binary
Distribution machine of the proper CPU/OS type.
.LI
A BOS Server (\*Lbosserver\*O process). (See Section 2.1 for more information
about the BOS Server.)
.LE
.P
The Fileset Location Server (FL Server), or \*Lflserver\*O process,
is used to track the locations of all filesets in a cell, making file access
transparent.  It tracks the locations of filesets and records changes to them
in the FLDB.  There is one master copy of the FLDB per cell.
.P
.iX "files" "locating"
The first time it needs to retrieve a requested file, the Cache Manager
contacts the FL Server to learn which File Server machine houses the fileset
containing the file.  Because of this dependency, the Cache Manager cannot
retrieve a requested file if the information in the FLDB is inaccessible, even
if the File Exporter on the machine that houses the fileset containing the file
is working properly.
.P
.iX "\*Ladmin.fl\*O file"
The \*Ladmin.fl\*O list is used to designate administrative users who can
issue commands that affect the \*Lflserver\*O process (operations that affect
the FLDB) on a Fileset Database machine.  The same \*Ladmin.fl\*O list should
be used for all FL Servers in a cell.
.P
A user can issue commands that affect FLDB entries for filesets on a server
machine without being listed in the \*Ladmin.fl\*O list, provided the user owns
the machine's server entry in the FLDB.  A user gains ownership of a server
entry in the FLDB by being included in the group specified as the owner of
that machine's entry with the \*Lfts crserverentry\*O command. (See Chapter 6
for more information about creating server entries in the FLDB.)
.iX "-]" "Fileset Database machines" 
.iX "-]" "Fileset Location Database" "about"
.H 4 "Backup Database Machines"
.iX "-[" "Backup Database machines"  
.P
A Backup Database machine houses the Backup Database. As with Fileset
Database machines, it is best to configure three or a larger, odd number of
Backup Database machines sufficient to back up the cell's data.
.P
Each Backup Database machine runs the following processes:
.ML
.LI
A Backup Server (\*Lbakserver\*O process).
.LI
Two \*Lupclient\*O processes: one to retrieve configuration files from the
System Control machine, and one to retrieve binary files from the Binary
Distribution machine of the proper CPU/OS type.
.LI
A BOS Server (\*Lbosserver\*O process). (See Section 2.1 for more information
about the BOS Server.)
.LE
.P
A Backup Database machine stores the Backup Database. The Backup Database
houses administrative information used in the DFS Backup System, such as the
dump schedule for backups and the groups of filesets to be dumped to tape in
each backup.  The information in the database can be used to restore data from
tape to the file system in the event of a system failure.  There is one master
copy of the Backup Database per cell.
.P
.iX "Backup Server" "about"
The Backup Server, or \*Lbakserver\*O process, maintains the Backup
Database.  The \*Lbakserver\*O process must run on all machines that store a
copy of the Backup Database.  The \*Ladmin.bak\*O list is used to designate
administrative users who can issue commands in the \*Lbak\*O suite, most of
which communicate with the Backup Server.  The same \*Ladmin.bak\*O list should
be used for all Backup Servers in a cell.
.P
Commands in the \*Lbak\*O suite are used to communicate with the DFS Backup
System.  They can be entered from any machine in the cell.  Data is physically
backed up and restored on a Tape Coordinator machine, which is a client or
server machine that has a tape drive and runs the \*Lbutc\*O process to manage
the drive.  Information stored in the Backup Database determines the data to
be backed up by a Tape Coordinator machine. (See Chapter 9 for more information
on configuring and using Tape Coordinator machines.)
.iX "-]" "Backup Database machines"  
.H 4 "DFS Client Machines"
.iX "-[" "client machines" "about"
.P
A DFS client machine serves primarily as a single or multiuser
workstation.  It communicates with File Server machines to access files for
application programs, provides local data storage, and provides computer
cycles.  A domain should include enough client machines to allow its users to
access exported data from the local or foreign cells.
.P
Each client machine must run
.ML
.LI
The Cache Manager, which is initialized by the \*Ldfsd\*O process, in 
the kernel
.LI
The \*Ldfsbind\*O process
.LE
.P
.iX "Cache Manager" "about"
.ne 12
The Cache Manager runs as part of the client machine's kernel.  It communicates
with server processes running on File Server machines to fetch data on behalf
of application programs.  When an application program on a client machine
requests data, the Cache Manager contacts the FL Server to learn the
location of the fileset that houses the data.  It then translates the
application program's data request into a Remote Procedure Call (RPC) to the
File Exporter running on the appropriate File Server machine.
.P
.iX "cache" "about"
.iX "tokens" "storing"
When the Cache Manager receives the requested data, it stores the data in its
local cache, which is an area reserved for data storage on disk or in memory on
the client machine.  It then passes the data to the application program.  The
Cache Manager also stores tokens it receives from the File Exporter on the File
Server machine. 
.P
Within limits, the Cache Manager attempts to make the most current data 
available to users.  The Cache Manager judges the currency of the data in its 
cache based on the type of fileset from which the data was retrieved:
.ML
.LI
If the data comes from a read/write fileset, the Cache Manager uses the
tokens to track the currency of the data.  The cached data remains current for
as long as the Cache Manager's tokens remain valid.  If the read/write source
of the data changes, the File Exporter revokes the tokens.  The next time the
data is requested, the Cache Manager retrieves the newer version to its
cache before providing it to the application program.
.LI
If the data comes from a read-only fileset, the Cache Manager compares the
amount of time since the data was last verified as being current with a
configurable time period associated with the fileset.  If the read-only copy of
the data changes, the Cache Manager continues to distribute the cached data
until the time since verification equals or exceeds the configurable time
period.  The next time data is requested, the Cache Manager retrieves the newer
version to its cache before providing it to the application program.
.LE
.P
.iX "\*Ldfsd\*O process" "about"
The \*Ldfsd\*O process initializes the Cache Manager on a client machine.  It
can be used to alter aspects of the Cache Manager's cache, such as its location
and size.  It also starts several background daemons, which help the Cache
Manager manage the data stored in its cache.
.P
.iX "\*Ldfsbind\*O process" "about"
.ne 10
The \*Ldfsbind\*O process, in addition to its role on File Server machines,
is used by the Cache Managers on client machines to help with the resolution
of DCE pathnames.  It also obtains authentication information about users that
Cache Managers require for RPC bindings to File Server machines. (See Section
2.2.2 for more information about the Cache Manager and the \*Ldfsd\*O and
\*Ldfsbind\*O processes.)
.H 4 "Exporting Data from a Client Machine (Private File Server Machine)"
.iX "-[" "client machines" "as File Servers"
.iX "Private File Server machine"
.P
The primary function of a client machine is to communicate with File Server
machines to access files for application programs.  However, a client machine
can also be configured as a Private File Server machine to export data 
from its local disk for use in the global namespace.  To export data as 
a Private File Server machine, a client machine must meet the following 
additional requirements:
.ML
.LI
Have an RPC binding in CDS
.LI
Have a DFS server principal and associated account in the Registry Database
.LI
Have a server entry in the FLDB
.LI
Run the Fileset Server (\*Lftserver\*O process)
.LI
Run the File Exporter, which is initialized with the \*Lfxd\*O process
.LI
Run the \*Lupclient\*O process to retrieve binary files from the proper Binary
Distribution machine
.LI
Run the BOS Server (\*Lbosserver\*O process)
.LI
Optionally, run the Replication Server (\*Lrepserver\*O process)
.LE
.P
Although meeting these requirements qualifies a client machine as a File
Server machine, that is not the machine's primary role.  The machine's local
disk is not to be used for data storage for an entire cell or domain.  A client
machine meets the previous requirements solely to allow users who administer
the machine to make data on its local disk available in the global namespace.
(See Section 2.1.1.3 for more information about these additional processes.)
.P
To prohibit other users from creating filesets on the client machine, the users
who administer the machine should be the only ones listed in the \*Ladmin.ft\*O
list and the \*Lfxd\*O administrative group for the machine.  They should 
also be listed in the group that is given ownership of the server entry for 
the machine in the FLDB.  These local privileges do not grant the owners of 
the workstation administrative privilege beyond the local machine.  However, 
the owners have all of the privileges required to administer the filesets on 
their machine and the entries for those filesets in the FLDB.  These 
privileges, and the ability to set the ACLs for any data that is exported 
from the workstation, allow the owners to prevent other users from storing 
data on the machine.
.P
.iX "\*Ladmin.up\*O file"
Because a client machine that exports data must run DFS server processes (such
as \*Lbosserver\*O, \*Lftserver\*O, and \*Lfxd\*O), it must also run the
\*Lupclient\*O process to retrieve current versions of binary files for the
processes from the Binary Distribution machine for its CPU/OS type in the
cell.  It must therefore be included in the \*Ladmin.up\*O list of the Binary
Distribution machine of its CPU/OS type.  Beyond that, neither the machine nor
its owners need to be included in the administrative lists used by the other
machines in their cell or administrative domain.
.iX "-]" "client machines" "about"
.iX "-]" "client machines" "as File Servers"
.H 3 "Summary of DFS Machine Roles"
.P
Table 2-1 summarizes the DFS machine roles described in the previous sections.
For each machine role, the table provides a brief description of its purpose
and lists the DFS processes that a machine filling the role must run.  The
table also provides suggestions for how to configure machines of a specific
type and other roles a machine of each type can assume.  A machine that is 
assuming any of the roles listed in the table must be configured as a DCE 
client machine.  A machine assuming a role as a DFS server must have both an 
RPC binding in CDS for its pathname and a DFS server principal in the 
Registry Database.
.P
Recall that any server machine can be configured to perform any of the other
server machine roles.  Also, a server machine can be configured as a client
machine, and vice versa.  For a machine to fill an additional role, it must run
the processes listed for that role in the third column of the table. (See
Section 2.1.1 for expanded descriptions of the machine roles.)
.nS "note"
Table 2-1 uses the numbers \*L1\*O and \*L2\*O to differentiate the
\*Lupserver\*O and \*Lupclient\*O processes running on the machines.  The
notations \*Lupserver\u\s-4\&1\s0\d\*O and \*Lupclient\u\s-4\&1\s0\d\*O
denote the Update Server that distributes common configuration files from
a System Control machine.  The notations \*Lupserver\u\s-4\&2\s0\d\*O and
\*Lupclient\u\s-4\&2\s0\d\*O denote the Update Server that distributes
binary files from a Binary Distribution machine.
.PP
.ne 6.5i
.TB "Summary of DFS Machine Roles"
.ps 11
.vs 12
.ad l
.in -.3i
.TS H
center, box, tab(@);
lB | lB | lB | lB
lw(.5i) | lw(.8i) | lBw(.6i) | lw(1.75i).
Machine Role@Purpose@Processes@Suggestions
=
.TH
T{
System Control machine
T}@T{
To distribute common configuration
files for a domain
T}@T{
.EQ
delim %%
.EN
bosserver
.br
%bold upserver sup bold 1 %
.br
%bold upclient sup bold 2 %
.EQ
delim off
.EN
T}@T{
Use a Binary Distribution machine
as the System Control machine for
a domain.
T}
_
T{
Binary Distribution machine
T}@T{
To distribute system binary
files for its CPU/OS type
T}@T{
.EQ
delim %%
.EN
bosserver
.br
%bold upserver sup bold 2 %
.br
% bold upclient sup bold 1 %
.EQ
delim off
.EN
T}@T{
Use the System Control machine for
a domain as a Binary Distribution
machine.
T}
_
T{
File Server machine
T}@T{
To export and store
DCE LFS and
non-LFS data
T}@T{
.EQ
delim %%
.EN
bosserver
.br
ftserver
.br
fxd
.br
dfsbind
.br
repserver
.br
%bold upclient sup bold 1 %
.br
%bold upclient sup bold 2 %
.EQ
delim off
.EN
T}@T{
A File Server machine must
also have a server entry in the
FLDB.  In a large cell, dedicate
one File Server machine to
housing read-only replicas.
T}
_
T{
Fileset Database machine
T}@T{
To store the Fileset Location
Database (FLDB)
T}@T{
.EQ
delim %%
.EN
bosserver
.br
flserver
.br
%bold upclient sup bold 1 %
.br
%bold upclient sup bold 2 %
.EQ
delim off
.EN
T}@T{
Configure three Fileset Database
machines.  Configure Fileset Database
machines as Backup Database machines.
T}
_
T{
Backup Database machine
T}@T{
To store the Backup Database
T}@T{
.EQ
delim %%
.EN
bosserver
.br
bakserver
.br
%bold upclient sup bold 1 %
.br
%bold upclient sup bold 2 %
.EQ
delim off
.EN
T}@T{
Configure three Backup Database machines.
Configure Backup Database machines as
Fileset Database machines.
T}
.nL
.ne 12
T{
DFS client machine
T}@T{
To serve as a single-user or
multiuser workstation; to access
files for application programs
T}@T{
dfsd
.br
dfsbind
T}@T{
.EQ
delim %%
.EN
Export DCE LFS and non-LFS data from
the machine by running \*Lbosserver\*O,
\*Lftserver\*O, \*Lfxd\*O,
%bold upclient sup bold 2 %, and optionally
\*Lrepserver\*O, creating an RPC binding in
CDS, registering a DFS server principal in
the Registry Database, and creating a server
entry in the FLDB.
.EQ
delim off
.EN
T}
.TE
.ps 12
.vs 14
.ad b
.in
.iX "-]" "machines" "roles in DFS"
.H 2 "DFS Server and Client Configuration Issues"
.iX "-[" "directories" "server machines (DFS)"
.iX "-[" "server machines" "configuring"
.P
The following subsections describe some general issues to consider before 
configuring DFS server and client machines.  They also provide additional 
information about the files that must reside on server and client machines 
and a few of the processes only briefly described in earlier sections of 
this chapter.  They also serve as an introduction to some issues to be 
considered before configuring a domain.
.H 3 "Server Machine Processes and Files"
.P
As mentioned previously, you should combine machine roles
for the machines in your cell and domains.  For example, you may wish to set up
a database server machine to house both the FLDB and the Backup Database.  A
machine that houses these databases needs to be stored in a secure location so
that unauthorized users cannot access and possibly damage fileset data or the
databases.
.P
.ne 12
In any cell, there is only one version of the FLDB and one version of the
Backup Database, even though these databases can be replicated at other sites.
The initial copies of these databases are created when the Fileset Location
and Backup Servers are first started in the cell.  They are automatically
replicated to other machines as additional instances of their respective
server processes are started on those machines.  When configuring a new domain
in an existing cell, do not attempt to create a new FLDB or Backup Database
for the domain; configure additional instances of the existing database as
necessary.
.P
Several directories contain files related to DFS server processes.  The
directories in the following list store files on a server machine's local
disk.  Files stored on the local disk are generally required for DFS to start
without accessing the global namespace. (See your vendor's documentation for
information about the files that reside on the local disk of a server machine.)
.ML
.LI
.iX "binary files" "directory"
The \*Vdcelocal\*L/bin\*O directory contains DFS binaries that are appropriate 
for the machine's CPU/OS type.  The binary files are for server processes, 
command suites, and other processes and programs.
.LI
.iX "administrative lists" "directory"
The \*Vdcelocal\*L/var/dfs\*O directory houses administrative lists for server
processes; for example, \*Ladmin.bos\*O and \*Ladmin.ft\*O.  It also contains
configuration files that are used by the BOS Server and the \*Ldfsexport\*O 
command.  If the machine is running the Fileset Location Server, this 
directory also contains the FLDB.
.LI
.iX "log files" "directory (DFS)"
The \*Vdcelocal\*L/var/dfs/adm\*O directory stores log files generated by
server processes.  These files detail events that occur during the operation of
server processes.  Server processes do not use these log files to reconstruct
failed operations because only completed events are recorded in them.  However,
because the information in the files is in human-readable format, examination
of these files is the first step in the troubleshooting procedure.  They can
help you evaluate process failures and related problems.
.P
.iX "image files" "directory"
The \*Vdcelocal\*L/var/dfs/adm\*O directory also contains the core image file
that is generated if a process being monitored by the BOS Server crashes.  The 
BOS Server adds an extension to the standard \*Lcore\*O name to indicate which
process generated the file; for example, \*Lcore.flserver\*O.  However, if two
processes abort at exactly the same time, the BOS Server may not be able to
assign the correct extension to the core file.
.LE
.P
In addition, the \*Vdceshared\*L/bin\*O directory also stores all of the binary
files that are housed in the \*Vdcelocal\*L/bin\*O directory.  Current 
versions of the files are always available from \*Vdceshared\*L/bin\*O for 
installation on the local disk of a server machine.  The directory also 
contains the binary files for a number of programs that are not integral to 
starting DFS, such as the \*Lscout\*O program and a number of programs 
related to the DFS Backup System.
.iX "-]" "server machines" "configuring"
.iX "-]" "directories" "server machines (DFS)"
.H 3 "Client Machine Processes and Files"
.iX "-[" "client machines" "configuring"
.iX "disk space" "saving on client machines"
.P
Client machines run the \*Ldfsd\*O process, which initializes the Cache
Manager, and the \*Ldfsbind\*O process.  You can save disk space on a client
machine by storing commonly used files in the DFS filespace.  You can then
create symbolic links on the local disk that refer to the files in the
filespace.
.P
.iX "chunks" "about"
When the Cache Manager retrieves a requested file, it caches the data before
passing it on to an application program.  It does not cache the entire file; it
instead caches "chunks," or pieces, of data.  By default, each chunk of
cached data contains 64 kilobytes of data in a disk cache or 8 kilobytes of
data in a memory cache.
.P
.iX "\*Ldfsd\*O process" "about"
The \*Ldfsd\*O process initializes the Cache Manager on a client machine by
transferring configuration information into kernel memory.  It also mounts the
root of the global namespace (\*L/...\*O).  You can use the options available
with the command line for the \*Ldfsd\*O process to alter the definitions for
the type of cache to be used (disk or memory), total cache size, cache chunk
size, the local disk directory to be used for caching, and other configuration
information.
.P
In addition, the \*Ldfsd\*O process starts several background daemons.  These
daemons include one or more maintenance daemons that perform routine
maintenance tasks such as garbage collection, background daemons that improve
performance by performing delayed writing of updated data, token daemons that
respond to token revocation requests from File Exporters, and (on the AIX
operating system) I/O daemons that move data between disk and memory.
.P
.iX "\*Ldfsbind\*O process" "about"
.ne 10
The \*Ldfsbind\*O process resolves CDS pathnames and returns information about
Fileset Database machines to the Cache Manager.  The information allows the
Cache Manager to contact the FL Server on an appropriate Fileset Database
machine in the cell to determine the locations of filesets that house data
requested by users.
.P
The \*Ldfsbind\*O process also returns user authentication information from the
Security Server to the kernel RPC Runtime of the client machine.  Authentication
information must be included in RPC bindings that request data from a File
Server machine for a user.  The Cache Manager uses the RPC bindings to access
data for the user from the File Server machine.
.P
(See Part 2 of this guide and reference for complete information about the
\*Ldfsd\*O and \*Ldfsbind\*O commands that start the respective processes and
the options available with the commands.)
.P
Two types of files must reside on the local disk of a client machine: boot
sequence files needed during reboot, and files that are useful during File
Server machine outages.
.P
During a reboot, DFS is inaccessible until the \*Ldfsd\*O process reinitializes
the Cache Manager; the \*Ldfsbind\*O process must be running before the
\*Ldfsd\*O process can be run.  Any files that are needed during reboot and
prior to the start of the \*Ldfsd\*O process must reside on the local disk.
Following is a list of recommended DFS files to store on a local disk. (See
your vendor's documentation for information about the files that reside on
the local disk of a client machine.)
.ML
.LI
The \*Vdcelocal\*L/bin/dfsbind\*O command is the start-up command for the 
\*Ldfsbind\*O process.
.LI
The \*Vdcelocal\*L/bin/dfsd\*O command is the start-up command for the Cache 
Manager.
.LI
The \*Vdcelocal\*L/etc/CacheInfo\*O file is a file that specifies aspects of 
Cache Manager configuration.
.LI
The \*Vdcelocal\*L/var/adm/dfs/cache\*O directory is a directory that 
contains cache-related files, such as \*LV\*Vn\*O files and the 
\*LCacheItems\*O file, generated and used by the Cache Manager.
.LE
.P
.iX "\*Lbos\*O command suite" "access on client machines"
.iX "\*Lcm\*O command suite" "access on client machines"
.ne 12
You may also wish to store diagnostic and recovery files on a local disk.
Certain commands in the \*Lbos\*O and \*Lcm\*O command suites can help users
diagnose problems caused by a File Server outage.  It is useful to have local
disk copies of the binary files for the \*Lbos\*O and \*Lcm\*O suites because
the File Server outage that requires their use can also make them inaccessible.
In addition, you may wish to keep the binaries for a text editor, such as
\*Led\*O or \*Lvi\*O, on the local disk for use during outages.
.P
.zA "enh,13566,R1.2.2,Add multihomed server"
Additionally, if you wish to modify the default Cache Manager 
preferences for accessing File Servers and FLDB machines, you may
wish to add \*Lcm setpreference\*O commands to the machine's
initialization file.  This ensures that such preferences are loaded
each time the machine is initialized.  For more information about
Cache Manager preferences for File Servers and FLDB machines, see 
the following section.
.zZ "enh,13566,R1.2.2,Add multihomed server"
.iX "-]" "client machines" "configuring"
.zA "enh,13566,R1.2.2,Add multihomed server"
.H 3 "Multihomed Server Configuration Issues"
.iX "multihomed servers" "configuring"
.P
Multihomed server capabilities allow administrators to specify up to
four interfaces (either host names or IP addresses) in the FLDB for
each File Server and FLDB machine. Servers can have more than four
network connections; however, the FLDB can accept only four entries
per server. This capability, coupled with server preference lists
maintained by the individual Cache Managers, allows you to configure
DFS to work optimally within your network.  For example, a single File
Server can have up to four IP addresses (specified for use by DFS), and
the various clients that use that server can have their Cache Manager
preference lists configured so that the preferred access to that
server is through the most efficient possible network connection.
Should a single connection to a File Server become unavailable, the
various clients that previously used that connection would consult
their Cache Manager's preference lists and reroute their requests to
another address for a File Server containing the required fileset.
This lets you configure DFS for the most efficient use of the network
while providing additional fail-over capabilities for the file system.
.H 4 "How Multihomed Servers and Preferences Work Together"
.iX "multihomed servers" "description"
.P
Each Cache Manager maintains a list of File Server and Fileset
Location (FL) Server preferences.  Each entry in that list contains
both the address of a server and a ranking. The ranking value
determines the order in which these servers are accessed, or their
"preference." The FLDB can contain up to four addresses for each server
machine; therefore, the preference list can also contain up to four
entries for each server (each with its own address and preference
rank).
.P
In operation, when a Cache Manager requires a particular fileset, it
first consults its list of FL Servers and attempts to contact an FL
Server at the address with the lowest ranking in the preference list.
The FL Server provides the addresses of the various File Servers that
contain that fileset. (The fileset location information is then cached
by the Cache Manager and is updated periodically.) If the fileset is
replicated, multiple File Servers may contain that fileset.  The Cache
Manager again consults its preference list and contacts a suitable
File Server at the address with the lowest ranking value. Should the
Cache Manager not be able to contact a server during this process, it
simply checks its preference list and attempts to contact a suitable
server at the next "most preferred" IP address.
.P
The preference list is automatically created each time a Cache Manager
is initialized.  It consists of the IP addresses of FL Servers and File
Servers and an automatically assigned preference value for each.  New
entries are added to the preference list as necessary when filesets
are first required. By default, the Cache Manager assigns preferences
that make sensible choices based on the location of servers.  The
default values make the Cache Manager try to connect to servers in the
following order:
.AL
.LI
The same machine as the client (default rank of 5000).
.LI
The same subnetwork as the client (default rank of 20000).
.LI
The same network as the client (default rank of 30000).
.LI
Different networks (default rank of 40000).
.LE
.P
Cache Manager preferences are explained in detail in Chapter 9.
.P
For example, a server on the same machine as the Cache Manager
receives a rank of 5000, while a server on the same subnetwork receives a
rank of 20000.  The entry with the lowest ranking value has the highest
"preference."  Thus, a server with a preference value of 5000 will be
chosen before a server with a rank of 20000.  
.P
You can change Cache Manager preferences by using the \*Lcm setpreferences\*O
command.  Additionally, you can create a file specifying server
preferences that is read each time a Cache Manager is initialized, thus
providing a method for overriding the default server preference values.
You can also load preference entries from standard input, or any combination
of all three sources. This is also explained in Chapter 9.
.P
Should two servers be assigned the same preference value, such as two
File Servers on the same subnetwork both receiving a default value of
20000, the server with the lowest round-trip value is chosen. Each
server is assigned a random round-trip value when the Cache Manager is
initialized. The assigned round-trip value is always higher than the
upper bound for stored actual round-trip values.  This ensures that an
actual round-trip value will always be chosen over assigned values.
.P
By judiciously providing multiple addresses for FL Servers and File
Servers and properly configuring the Cache Manager preference lists,
you can configure DFS to make the most efficient use of servers within
the network.  For example, you may wish to provide a connection from
commonly used File Servers and FL Servers to the same subnetworks that are shared
by the majority of the DFS clients.  This reduces cross-router and
gateway traffic through the network.  As a backup, you can provide
higher-ranking preference entries for server connections to other
areas of the network. This provides continued access to the servers
should a particular network connection become unavailable.
.P
The following simplified scenario illustrates how multihomed servers
can be configured to make the most efficient use of the local network.
In this example, a read-only fileset is replicated on two File
Servers.  The File Servers have connections to both subnetworks within
the network, and these connections are the preferred connections used
by DFS clients on each respective subnetwork.  When a DFS client must
fetch data from the read-only fileset, it first consults the list of
suitable Files Servers.  The Cache Manager then consults its list of
preferences and chooses the connection to a suitable File Server that
has the lowest rank.  Since both File Server connections on the local
subnetwork have the same rank, the connection with the lowest
round-trip value is chosen.  This is shown in Figure 2-1.
.P
.ne 4i
.FG "Cache Manager Contacting File Server Address With Lowest Rank
.pI gd/figures/pref1.ps 3.5i 5.25i
.P
Should the Cache Manager lose contact with the preferred File Server
connection (either through a network or server problem), the Cache
Manager again consults its preference list and attempts to
contact a suitable File Server at the address with the next lowest rank. 
This is shown in Figure 2-2. In this figure, when the Cache Manager can no longer contact a File Server at a given connection, it attempts to connect to the File Server address with the next-lowest preference value.
.P
.ne 4i
.FG "Cache Manager Connecting to File Server Address With Next Lowest Rank
.pI gd/figures/pref2.ps 3.5i 5.25i
.P
If the Cache Manager again loses contact with a File Server through
its current connection, it once more consults the preference list for
the address of a suitable File Server with the next lowest value.  In
this case, the Cache Manager must now establish a connection to
another subnetwork. There are two possible connections to suitable
File Servers in that subnetwork, both having equal ranks. The
Cache Manager therefore chooses the connection with the lowest round-trip
time value. This is shown in Figure 2-3. In this figure, should the Cache Manager again lose its connection, it checks the preference list for a connection to a suitable File Server with the next lowest ranking.
.PP
.ne 4i
.FG "Cache Manager Again Losing Connection and Contacting File Server Address in Another Subnet
.pI gd/figures/pref3.ps 3.5i 5.25i
.P
The entire process of changing connections as required is carried out 
automatically, without the DFS client users being aware that it has 
occurred.   
.H 4 "Tasks to Administer Multihomed Servers"
.iX "multihomed servers" "administering"
.P
The following are tasks to configure a multihomed File Server environment:
.ML
.LI
You must add each host name or IP address for each File Server to the Fileset
Location Database (FLDB).  The initial entry, along with one address
for that entry, is created using the \*Lfts crserverentry\*O command.
Additional addresses can be added or deleted from the entry by using the
\*Lfts edserverentry\*O command.  This is explained in Chapter 6.
.LI
Optionally, you can modify the preferences for each client's Cache
Manager to take advantage of the most efficient connections to the
File Servers.  However, you should only modify the preferences if
there are compelling reasons, as the Cache Manager's default preference
choices are generally the most efficient for any given network
configuration.
.P
Client preference lists are transient in that they are
reestablished at their default values each time the Cache Manager is
initialized.  However, a list of preferences can be loaded into the
Cache Manager at initialization through a preferences file. Chapter 9
explains how to both create such a file and ensure that it is loaded
each time the Cache Manager is initialized.
.LE
.H 4 "IP Layer Override of Preferences"
.iX "multihomed servers" "IP layer override"
.P
While the FLDB can only contain up to four addresses for a given File Server
or FL Server, such servers can have more than four connections to the
network. In such instances, the DCE RPC mechanism can allow the IP
layer to choose a source address for a server response that is
different, and presumably more efficient, than the specified
destination of the corresponding request.  In this case, the chosen
server address is likely to be a function of the client address to
which the response is being set; however, the exact algorithm for
choosing the address will differ for each operating system
vendor. Such a routing decision is observed by the Cache Manager as
a change in the server-binding's address.
.P
Should the IP layer select a different server address, then this
connection becomes the connection used by the Cache Manager,
regardless of its preference rank or whether or not it is one of the
addresses listed in the FLDB for a given server.  This scenario is
shown in Figure 2-4.
.PP
.ne 4.5i
.FG "An Example of the IP Layer Overriding the Cache Manager's
Preference
.pI gd/figures/override1.ps 4.25i 5.25i
.P
Should the IP layer select a different connection and override the
preference choice, the \*Lcm getpreferences\*O command returns the
address of the currently used connections (the connection selected by
the IP layer) as the entry in the preference list, even though it may
not be listed in the FLDB.
.H 4 "Creating Additional Default Entries in the Routing Table"
.iX "multihomed server" "server routing table entries"
.P
The preference list provides each Cache Manager with a list of
known connections to various File Servers and FL Servers.  This
allows the Cache Manager to select alternative connections to
communicate with the appropriate servers should a network or server
fault make a particular connection unavailable.  Similarly, each File
Server or FL Server that has multiple connections to the network should
have multiple default entries in its routing table that define the various
routers available to that server.  Thus, if a network fault makes a
particular router unavailable, that server has additional router
choices that would allow it to reply to Cache Manager requests.  Refer
to your operating system documentation for information concerning
adding default entries to a server's routing table.
.zZ "enh,13566,R1.2.2,Add multihomed server"
.H 2 "Setting Up Filesets"
.P
DCE LFS filesets are created with the \*Lfts create\*O command. Non-LFS
filesets are created in the local operating system and registered in DFS with
the \*Lfts crfldbentry\*O command.  Mount points to the global namespace for
both DCE LFS and non-LFS filesets are created with the \*Lfts crmount\*O
command.
.P
The following subsections discuss setting up a cell's root fileset, binary and
configuration filesets, and user filesets.  Information about fileset
replication and the \*L@sys\*O and \*L@host\*O variables, which simplify cell
administration, is also provided. (See Chapter 6 for complete information
about creating and mounting filesets; see Part 2 of this guide and reference
for complete information about \*Lfts\*O and other DFS commands.)
.H 3 "Setting Up the Root Fileset"
.iX "filesets" "root"
.P
.iX "\*Lroot.dfs\*O file" "creating"
The main read/write fileset, \*Lroot.dfs\*O, is required in every cell's file
system.  It is the first fileset created in a cell during DFS configuration.  It
is the implied fileset for the root of a cell's DFS filespace
(\*L/.../\*Vcellname\*L/fs\*O, by default).  It can be a DCE LFS fileset or it
can be a non-LFS fileset.  However, it must be a DCE LFS fileset if
functionality such as replication is to be available in the cell.
.P
.ne 12
To create \*Lroot.dfs\*O as a DCE LFS fileset, issue the \*Lfts create\*O
command to create the fileset on a specified server machine and exported DCE
LFS aggregate.  For example:
.iS
\*C$ \*Lfts create root.dfs -server \*Vmachine\*L -aggregate \*Vname\*O
.iE
.P
Once the root fileset is created, start the \*Ldfsd\*O process if it is not
already running.  The \*Ldfsd\*O process automatically mounts the root of the
global namespace (\*L/...\*O).  Once the global namespace is mounted, the
\*Lroot.dfs\*O fileset automatically resides at the top level of the cell's
DFS filespace.
.P
You must enter the \*Lfts crmount\*O command with the \*L\-rw\*O option to
create an explicit read/write mount point for the fileset below the top level
of the cell's DFS filespace.  For example:
.iS
\*C$ \*Lfts crmount /:/.rw root.dfs -rw\*O
.iE
.P
Once these steps are complete, you can replicate \*Lroot.dfs\*O. Replication
is then available for DCE LFS filesets created in the cell.  It is important
that you follow these instructions if you plan to replicate filesets in your
cell.  Due to the nature of mount points, if you replicate \*Lroot.dfs\*O
before creating its read/write mount point, you effectively make it impossible
to access the read/write version of \*Lroot.dfs\*O.
.P
(See Chapter 6 for more information about creating and mounting filesets,
using mount points, and creating and exporting aggregates.)
.nS "note"
.iX "junctions"
By default, the junction to the DFS filespace is defined at
\*L/.../\*Vcellname\*L/fs\*O.  However, the name of the junction is not
considered to be well known and can be changed during installation and
configuration of DCE. (See your vendor's installation and configuration
documentation for more information.) The examples in this part of the
guide use the default, \*Lfs\*O, as the junction of the DFS filespace.
.nE
.nL
.ne 12
.H 3 "Choosing Fileset Names"
.iX "-[" "filesets" "naming conventions"
.P
Each directory in \*L/.../\*Vcellname\*L/fs\*O usually corresponds to a
separate, mounted fileset (mounted filesets can also occur anywhere in the
file tree).  Subdirectories of \*L/.../\*Vcellname\*L/fs/\*Vdirectory_name\*O
can be either standard directories or mount points to separate filesets.  For
simplified administration, group the directories and their contents into small,
easily managed filesets.
.P
.iX "mount points" "fileset names"
.iX "filesets" "name and mount points" 
Each fileset has a name unique to the cell in which it resides.  Fileset names
are stored in the FLDB.  A fileset's name is not the same as the name of its
mount point, although you can assign the same name to a fileset and its mount 
point.
.P
There is a 111-character limit on the length of fileset names.  However,
because a nine-character \*L.readonly\*O extension is added when you replicate
a fileset, you need to specify fileset names that contain no more than 102
characters.  When creating filesets, do not add the \*L.readonly\*O and
\*L.backup\*O extensions yourself; DFS automatically adds the appropriate
extension when it creates a read-only or backup fileset. (DFS reserves the
\*L.readonly\*O and \*L.backup\*O extensions for use with read-only and backup
filesets, so you cannot create a fileset whose name ends with either of these
extensions.)
.P
You can give filesets any names that you feel are appropriate. For simplified
administration, however, a fileset's name needs to
.ML
.LI
Reflect the fileset's contents
.LI
Reflect the name of the fileset's mount point
.LI
Be consistent with other filesets that contain similar types of data so that 
you can easily manipulate groups of filesets when using the DFS Backup System
.LE
.P
You may find it helpful to use a common prefix for related filesets. The
following list summarizes this type of naming scheme:
.ML
.LI
Use the \*Lcommon.\*Vtype\*O prefix for common filesets.  For example, use
\*Lcommon.etc\*O for common configuration files (mounted at
\*L/.../\*Vcellname\*L/fs/common/etc\*O), and \*Lcommon.forms\*O for common
forms (mounted at \*L/.../\*Vcellname\*L/fs/common/forms\*O).
.LI
Use the \*Lsrc.\*Vtype\*O prefix for source filesets.  For example, use
\*Lsrc.dfs\*O for DFS source files (mounted at 
\*L/.../\*Vcellname\*L/fs/src/dfs\*O).
.LI
Use the \*Luser.\*Vusername\*O prefix for all user filesets.  For example, use
\*Luser.terry\*O for user \*Lterry\*O's fileset (mounted at 
\*L/.../\*Vcellname\*L/fs/usr/terry\*O).
.LI
Use the \*Lpublic.\*Vusername\*O prefix for each user's public fileset.  For 
example, use \*Lpublic.terry\*O for \*Lterry\*O's public fileset, which 
contains information the user wants to make available to everyone.  The
\*Lpublic.terry\*O fileset is mounted at 
\*L/.../\*Vcellname\*L/fs/public/terry\*O.
.LI
Use the \*Vsys_type\*L.\*Vdistribution_dir\*O 
prefix for operating-system-specific 
filesets.  For example, use \*Lpmax_osf1.bin\*O for OSF/1
binary files (mounted at \*L/.../\*Vcellname\*L/fs/pmax_osf1/bin\*O), and
\*Lpmax_osf1.lib\*O for OSF/1 library files (mounted at
\*L/.../\*Vcellname\*L/fs/pmax_osf1/lib\*O).  In DFS, symbolic links are often
created from the \*L/bin\*O and \*L/lib\*O directories (or their equivalents)
on the local disk of a workstation to these DFS mount points.
.LE
.P
(See Chapter 6 for more information on additional rules for naming filesets.)
.iX "-]" "filesets" "naming conventions"
.H 3 "Setting Up Binary and Configuration Filesets"
.iX "-[" "binary files" "storing"
.iX "-[" "filesets" "binary and configuration"
.P
You may find it convenient to store DCE binaries, system binaries, and
configuration files (for example, those commonly found in directories such as
\*L/bin\*O and \*L/etc\*O or their equivalents) in the DFS filespace, instead
of on the local disk of each machine.  Because binary files are 
operating-system specific, you may want to create a different fileset for each
system type (for example, \*Lpmax_osf1\*O or \*Lrs_aix32\*O) and distribution
directory (for example, \*L/etc\*O and \*L/bin\*O) and store the filesets on
a DFS File Server machine.  You can then create symbolic links from the local
disk to the fileset.
.P
Note that DFS simplifies the creation of such links by providing the \*L@sys\*O
variable, which is set on a per-Cache Manager basis.  When the Cache Manager
encounters the \*L@sys\*O variable in a pathname, it substitutes its system
name for the variable. (See Section 2.3.7 for a more detailed description of
the \*L@sys\*O variable.)
.P
.ne 12
For example, while it is a good practice to store the binary files for a single
text editor on the local machine, the binaries for other text editors do not
need to be stored on each machine.  A system administrator can create filesets
that store text editor binaries for each system type.  The administrator can
then construct a symbolic link from the local disk of each machine to the
appropriate fileset in DFS.  For instance, system administrators in the
\*Labc.com\*O cell, which runs the OSF/1 and AIX 3.2 operating systems, can
configure part of their file tree as shown in Table 2-2.
.iX "binary files" "fileset names and mount points (table)"
.TB "Examples of Fileset Names and Mount Points for Binary Files"
.TS H
center, tab(@), box;
lB | lB
lBw(1.125i) | lBw(2i).
Fileset Name@Mount Point
=
.TH
pmax_osf1@/.../abc.com/fs/pmax_osf1
pmax_osf1.bin@/.../abc.com/fs/pmax_osf1/bin
pmax_osf1.etc@/.../abc.com/fs/pmax_osf1/etc
rs_aix32@/.../abc.com/fs/rs_aix32
rs_aix32.bin@/.../abc.com/fs/rs_aix32/bin
rs_aix32.etc@/.../abc.com/fs/rs_aix32/etc
.TE
.P
Storing common files in a central location eliminates the need to store copies
on every client's local disk and, thus, saves local disk space.  Replication
further enhances the availability of common files. (Some binaries, however,
must remain on the local disk of every machine.)
.iX "-]" "filesets" "binary and configuration"
.iX "-]" "binary files" "storing"                       
.H 3 "Setting Up User Filesets"
.iX "-[" "filesets" "user"
.P
Each user has a unique DCE account.  You may also want to create a single,
separate fileset for each user and mount the fileset at
\*L/.../\*Vcellname\*L/fs/usr/\*Vusername\*O, where \*Vusername\*O is the
name of the user who owns the fileset.  For example, assign the name
\*Luser.terry\*O to the fileset for the user named \*Lterry\*O.  When you mount
the fileset at \*L/.../abc.com/fs/usr/terry\*O, the root directory of the
fileset (the user's home directory) is named \*L/.../abc.com/fs/usr/terry\*O.
The user's home directory contains all of the files, subdirectories, and mount
points in the fileset named \*Luser.terry\*O.
.P
As with any other fileset, you may want to create additional filesets
based on logical file groupings and mount them below
\*L/.../\*Vcellname\*L/fs/usr/\*Vusername\*O if the user's fileset becomes too
large.  For example, if \*Lterry\*O has 5000 kilobytes of data in the
\*Lproject1\*O subdirectory and 3000 kilobytes of data in the \*Lproject2\*O
subdirectory, you may want to create two smaller filesets organized below
\*L/.../abc.com/fs/usr/terry\*O.  Table 2-3 lists the organization and names 
of the filesets in this example.
.iX "filesets" "names and mount points (table)"
.iX "mount points" "fileset names" 
.PP
.TB "Examples of Fileset Names and Mount Points for User Data"
.TS H
center, tab(@), box;
lB | lB
lBw(1.125i) | lBw(2i).
Fileset Name@Mount Point
=
.TH
user.terry@/.../abc.com/fs/usr/terry
user.terry.project1@/.../abc.com/fs/usr/terry/project1
user.terry.project2@/.../abc.com/fs/usr/terry/project2
.TE
.H 3 "Moving Data from Non-LFS Directories to DCE LFS Directories"
.iX "filesets" "mounting non-LFS"
.P
The guidelines in Section 2.3.4 assume that the user does not have an existing
home directory in the file system.  A user who has data in an existing home
directory in a non-LFS fileset mounted in the global namespace can continue to
use that fileset.  However, if you choose to create and mount a DCE LFS fileset
for the user, you must be careful; DFS does not allow you to mount a fileset
at an existing directory.  You must move the user's data from the existing home
directory in the DCE namespace to a temporary directory.  You must then remove
the existing home directory before creating and mounting the user's DCE LFS
fileset.  You can then move the user's data to the new fileset.
.P
For example, suppose the user named \*Lterry\*O in the previous example has
an existing home directory in a non-LFS fileset mounted at
\*L/.../abc.com/fs/usr/terry\*O.  In this case, move the user's data from
\*L/.../abc.com/fs/usr/terry\*O to a temporary location (such as a subdirectory
of \*L/tmp\*O on the local disk), and remove the
\*L/.../abc.com/fs/usr/terry\*O directory and its contents.  Then create and
mount the user's DCE LFS fileset as described in Section 2.3.4.  Finally,
move the user's data from the temporary location into the new DCE LFS fileset
mounted at \*L/.../abc.com/fs/usr/terry\*O.  When these steps are complete,
the user can access the data as before.
.iX "-]" "filesets" "user"
.H 3 "Replicating DCE LFS Filesets"
.iX "filesets" "replicating"
.P
You replicate DCE LFS filesets by placing read-only copies of them on one or
more File Server machines in a cell.  If a machine that houses a read-only copy
of the fileset becomes unavailable, the information is usually still available
from a copy of the fileset on another machine.  However, for a fileset that uses
Release Replication, if the read-only fileset that resides at the same site as
the read/write fileset becomes unavailable, all other read-only versions of
that fileset become unavailable after a configurable amount of time.  Similarly,
for a fileset that uses Scheduled Replication, if the read-write fileset
becomes unavailable, all read-only versions of the fileset become unavailable
after a configurable amount of time. (See Chapter 6 for detailed information
on the availability of read-only filesets.)
.P
Replicate only those DCE LFS filesets that meet the following criteria:
.ML
.LI
The files in the fileset are read much more often than they are modified.
.LI
The files in the fileset are heavily used; for example, binary files for text
editors or other popular application programs.  Replicating the fileset lets
you distribute the load for the files that it contains across several machines.
.LI
The files in the fileset must remain available.  By replicating the fileset on
multiple File Server machines, even if one of the machines that houses a 
replica of the fileset becomes unavailable, replicas are usually still 
available from other machines.
.LI
The fileset is mounted at a high level in the cell's file tree; for example,
\*Lroot.dfs\*O and its subdirectories.
.LE
.P
.iX "disk space" "replicas"
If your cell is large, you may want to use a small set of File Server machines
to store just read-only filesets.  These machines can then distribute frequently
used data, lessening the load on other machines.  Keep in mind that each replica
not stored on the same aggregate as its read/write source fileset uses as much
disk space as its source fileset.  A read-only fileset created on the same
aggregate as its source fileset is created as a clone of its source and so
requires potentially much less space than a full read-only replica created on
a different aggregate.
Each Cache Manager maintains preferences in the form of numerical ranks that
bias its selection of File Server machines for read-only fileset access. When
accessing a read-only fileset, the Cache Manager consults its collection of
preferences and attempts to access the read-only fileset from the File Server
machine that has the lowest recorded rank. If the Cache Manager cannot access
the fileset from that machine, it tries to access the fileset from the machine
that has the next-lowest rank. It continues in this manner until it either
succeeds in accessing the fileset or determines that all of the machines that
house the fileset are unavailable.
.P
By default, the Cache Manager assigns preferences to File Server machines based
on IP addresses. You can set or change the Cache Manager's preferences to suit
your needs. (See Chapter 8 for more information about the Cache Manager and
File Server machine preferences.)
.H 3 "Using the @sys and @host Variables"
.iX "Distributed File Service (DFS)" "variables"
.P
DFS simplifies the administration of operating system-specific or host-specific
files by providing the \*L@sys\*O and \*L@host\*O variables.  When the Cache
Manager encounters \*L@sys\*O or \*L@host\*O in a pathname, it replaces the
variable with either the system name (defined with the \*Lcm sysname\*O
command) or the hostname (defined with the local operating system's
\*Lhostname\*O command or its equivalent).
.P
.iX "\*L@sys\*O variable"
.iX "\*L@host\*O variable"
.iX "system variable"
.iX "host variable"
.iX "-[" "symbolic links" "and variables"
.iX "variables" "\*L@host\*O and \*L@sys\*O"
The \*L@sys\*O and \*L@host\*O variables are especially useful when
constructing symbolic links from the local disk to the DFS filespace.  You
create identical links on all machines, yet each machine accesses the files
that are appropriate to its system type or hostname.  Use the \*L@sys\*O 
variable to access files that are organized on a per-system type basis; 
use \*L@host\*O to access files that are organized on a per-machine basis.
The following subsections provide examples of the \*L@sys\*O and \*L@host\*O 
variables.
.H 4 "The @sys Variable"
.iX "\*Lcm\*O command suite" "\*Lsysname\*O" 
.iX "-[" "Cache Manager" "interpretations of variables"
.P
The \*L@sys\*O variable is expanded to the name of a CPU/OS type. The \*Lcm
sysname\*O command sets and displays the current value of the \*L@sys\*O
variable.  The following examples show how the Cache Manager interprets the
same pathname differently, depending on the value of \*L@sys\*O.
.P
On a machine running OSF/1:
.iS
\*C$ \*Lcm sysname\*O
.iE
.oS
\*CCurrent sysname is `pmax_osf1'\*O
.oE
.iS
\*C$ \*Lcd /.../abc.com/fs/@sys\*O
\*C$ \*Lpwd\*O
.iE
.oS
\*C/.../abc.com/fs/pmax_osf1\*O
.oE
.P
On a machine running AIX 3.2:
.iS
\*C$ \*Lcm sysname\*O
.iE
.oS
\*CCurrent sysname is `rs_aix32'\*O
.oE
.iS
\*C$ \*Lcd /.../abc.com/fs/@sys\*O
\*C$ \*Lpwd\*O
.iE
.oS
\*C/.../abc.com/fs/rs_aix32\*O
.oE
.P
The \*L@sys\*O variable is commonly used in symbolic links from a DFS client
machine to a fileset in the DFS filespace.  A single copy of a binary file for
each system type is stored on a single File Server machine in DFS instead of
on the local disk of each client machine.  Links are then created from client
machines to the central copy of the binary file, eliminating the need to store
the same binary file on each client machine.  Accessing binary files this way
saves disk space on client machines and ensures that users on all client
machines are using the same version of the binary file.  It also eases system
administration by allowing administrators to update central copies of binary
files, rather than requiring them to update the copies stored on each client
machine.
.P
.iX "binary files" "access from client machines"
.iX "client machines" "use of \*L@sys\*O variable"
A link that includes the \*L@sys\*O variable can be created on each client
machine.  The Cache Manager on each machine interprets the \*L@sys\*O variable,
so each machine accesses the binary file for its system type from the global
namespace.  Symbolic links that include the \*L@sys\*O variable are commonly
used to access binary files for programs such as \*Lmake\*O and \*Lemacs\*O.
.P
The following examples create a symbolic link used to access the proper binary
files for programs traditionally stored in \*L/usr/local\*O.  In the examples,
the Cache Managers on two machines interpret the link differently, depending on
their respective values of \*L@sys\*O.
.P
.ne 8
On a machine running OSF/1:
.iS
\*C$ \*Lln -s /.../abc.com/fs/@sys/usr/local /usr/local\*O
\*C$ \*Lls -l /usr/local\*O
.iE
.oS
\*Clrwxrwxrwx 1 root 34 Nov 22 1991 /usr/local ->
\*C/.../abc.com/fs/@sys/usr/local\*O
.oE
.iS
\*C$ \*Lcd /usr/local\*O
\*C$ \*Lpwd\*O
.iE
.oS
\*C/.../abc.com/fs/pmax_osf1/usr/local\*O
.oE
.P
On a machine running AIX 3.2:
.iS
\*C$ \*Lln -s /.../abc.com/fs/@sys/usr/local /usr/local\*O
\*C$ \*Lls \-l /usr/local\*O
.iE
.oS
\*Clrwxrwxrwx 1 root 32 Aug 1 06:44 /usr/local ->
\*C/.../abc.com/fs/@sys/usr/local\*O
.oE
.iS
\*C$ \*Lcd /usr/local\*O
\*C$ \*Lpwd\*O
.iE
.oS
\*C/.../abc.com/fs/rs_aix32/usr/local\*O
.oE
.P
When creating links on server machines, do not use links to access binary files
for DFS server processes.  These files must reside on the local disk of each
server machine to avoid bootstrapping problems.
.P
(See Part 2 of this guide and reference for more information about the \*Lcm
sysname\*O command.)
.iX "-]" "Cache Manager" "interpretations of variables"
.iX "-]" "symbolic links" "and variables"
.H 4 "The @host Variable"
.iX "\*L@host\*O variable"
...\".iX "diskless machines" "use of \*L@host\*O variable"
...\".iX "machines" "diskless"
.P
The \*L@host\*O variable is expanded to the value defined by the \*Lhostname\*O
command (or its equivalent) of the local operating system.  The \*L@host\*O
variable is especially useful when configuring machines that must execute a
machine-specific set of start-up routines. 
.P
For example, suppose two machines, \*Lfs1.abc.com\*O and \*Lfs2.abc.com\*O,
use two different, machine-specific versions of an initialization file for
an application that they start following a reboot.  The name of the
initialization file is \*Lstart\*O.  The file \*Lstart\*O can be stored in
DFS and accessed on a machine-specific basis via the \*L@host\*O variable.
To access the proper copy of the file, both machines can have symbolic links
from \*L/etc/rc/start\*O to \*L/.../abc.com/fs/etc/@host/rc/start\*O.  On the
first machine, the symbolic link resolves to the file named
.iS
\*L/.../abc.com/fs/etc/fs1.abc.com/rc/start\*O
.iE
On the second machine, the symbolic link resolves to the file named
.iS
\*L/.../abc.com/fs/etc/fs2.abc.com/rc/start\*O
.iE
...\" .cS
...\" For this reason, the \*L@host\*O
...\" functionality is most frequently used in cells that support diskless machines.
...\" .P
...\" For example, all diskless machines in a cell could have symbolic links from
...\" the proper initialization file (\*L/etc/rc\*O or its equivalent) to
...\" \*L/.../abc.com/fs/diskless_config/@host/rc\*O.  A diskless machine with a
...\" hostname defined as \*Ldiskless1.abc.com\*O would execute the file named
...\" .iS
...\" \*L/.../abc.com/fs/diskless_config/diskless1.abc.com/rc\*O
...\" .iE
...\" .P
...\" A diskless machine with a hostname of \*Ldiskless2.abc.com\*O would execute the
...\" file named
...\" .iS
...\" \*L/.../abc.com/fs/diskless_config/diskless2.abc.com/rc\*O
...\" .iE
...\" .cE
.H 2 "Data Access Management in DFS
.iX "-[" "File Exporter" "access control by"
.iX "-[" "tokens" "about"
.P
All access to data and metadata on a File Server machine is managed by the File
Exporter.  Clients contact the File Exporter when they wish to access data.  The
Cache Manager is the client of the File Exporter most visible to the user, as
well as the one most frequently discussed in this guide, but other
clients do exist.  For example, the \*Lfts\*O program can become a client of the
File Exporter when a fileset is moved from one aggregate or machine to another,
and the Replication Server is a frequent client of the File Exporter as it
manages replicas of read/write filesets.
.P
.ne 8
The File Exporter uses tokens to manage the distribution of data and metadata
to clients.  A client that wants to access or change data must first request and
obtain the proper tokens for the data from the File Exporter on the machine on
which the data resides.  If the File Exporter can grant the client's request, it
passes the tokens to the client; otherwise, it either queues the request until
it can service it or just refuses to grant it.  A client that receives the
requested tokens can then use them to access the data it wants from the File
Exporter.
.P
The following subsections provide more detailed information about tokens, their
management by the File Exporter, and the token state recovery that occurs after
a communications failure between a File Exporter and its clients.
.H 3 "Tokens"
.iX "tokens" "types"
.P
Tokens and their distribution and management by the File Exporter are
completely transparent at the user level.  The File Exporter uses tokens to
.ML
.LI
Track the clients to which it has given data and the types of operations they
are permitted to perform on the data.
.LI
Ensure that multiple clients are not simultaneously accessing the same data in
a conflicting manner.
.LI
Guarantee that each client always has access to the most recent versions of
read/write data.  If data stored on a File Server machine changes while a 
client has a copy of it, the File Exporter on that machine notifies the 
client; the client then obtains the new version of the data the next time it 
needs it.
.LE
.P
Different operations require different types of tokens. DFS includes four
general classes of tokens:
.VL
.LI "Open Tokens"
.nL
Allow a client to open an entire file or fileset to read from it, write to it,
delete it, or prevent it from being deleted.  For example, a client that wants
to open a file for reading requests an open token for the file.
.LI "Status Tokens"
.nL
Allow a client to read or write file status information.  For example, a client
that wants to append data to a file, thus changing its size, needs a status
token for the file.
.LI "Data Tokens"
.nL
Allow a client to read from or write to a range of bytes in a file.  For
example, a client that wants to modify the first 10 bytes of a file requests
a data token for those bytes.
.LI "Lock Tokens"
.nL
Allow a client to read lock or write lock a range of bytes in a file.  For
example, a client that must ensure that only one process is locking the first
10 bytes of a file requests a lock token for those bytes.
.LE
.P
Each token class includes a number of token types; for example, the data class
includes the read data and write data types.  The different classes and types
of tokens combine to allow for the different kinds of data access required by
file system clients.  Most operations require that a client possess multiple
tokens for the data it wishes to manipulate; for instance, appending text to
a file requires open tokens to access the file, data tokens to modify the
contents of the file, and status tokens to change the size of the file.
.P
Some tokens can be granted to different clients simultaneously, while others
cannot.  Two tokens that can be granted simultaneously are said to be
\*Ecompatible\*O; two tokens that cannot be granted at the same time are said
to be \*Econflicting\*O.  A token is always compatible with tokens from other
classes, but it may conflict with other token types from within its class.
In general, the token types associated with read operations are mutually
compatible, while those associated with write operations conflict with other
tokens.
.H 3 "Token Management"
.iX "-[" "tokens" "management by File Exporter"
.iX "-[" "File Exporter" "managing tokens"
.P
To determine whether it can grant a client's request for tokens, the File
Exporter checks for outstanding tokens that conflict with those requested.  If
no other client has conflicting tokens, the File Exporter grants the requested
tokens.  If another client has conflicting tokens, the File Exporter takes the
action associated with the first condition met from the following list:
.ML
.LI
If the existing tokens can be revoked, the File Exporter revokes them and
grants those requested.  When its tokens are revoked, a client such as the
Cache Manager flushes cached data for which the tokens applied, writing any
modified data back to the File Server machine. (Information about token
revocation follows this list.)
.LI
If the existing tokens cannot be revoked, the File Exporter either places the
request in a queue, to be serviced as soon as possible, or refuses to grant
the requested tokens outright.  The client dictates the File Exporter's response
to this situation when it requests the tokens.
.LE
.P
In general, if a client's existing tokens conflict with those requested by
another client, the File Exporter attempts to revoke the existing tokens to
grant the request.  Many factors influence the File Exporter's ability to
revoke a client's tokens.  The File Exporter can usually revoke some types of
tokens, but clients can refuse to relinquish other types of tokens in various
situations.  In addition, lifetimes that the File Exporter assigns to the 
tokens it grants and to the clients to which it grants them also affect its 
ability to revoke tokens, as follows:
.VL 15
.LI "Token Lifetime"
Specifies the length of time for which a token is valid.  All tokens have a
fixed token lifetime.  Once its lifetime has elapsed, a token expires.  The File
Exporter needs to revoke only valid tokens.  Because expired tokens are no
longer valid, the File Exporter does not need to revoke them; it can simply
grant new tokens as if the expired tokens did not exist.  A client can contact
the File Exporter to request that its tokens' lifetimes be extended before
they expire.
.LI "Host Lifetime"
Indicates the length of time for which the File Exporter considers a client
to be alive.  Each client that has tokens from the File Exporter has a host
lifetime within which it must contact the File Exporter to let it know that
it is still alive, thus renewing its host lifetime.  The File Exporter needs
the client's permission to revoke tokens that are held by the client as long 
as the client's host lifetime has not expired.
.LI "Host RPC Lifetime
.nL
Defines the length of time for which the File Exporter guarantees to attempt
to make an RPC to a client before the File Exporter revokes its tokens.  If 
the client responds to the RPC (thus renewing its host lifetime), the 
File Exporter cannot revoke the client's tokens without the client's 
permission.  If the client fails to respond to the RPC but its host lifetime 
has not expired, the File Exporter cannot revoke the client's tokens; if 
the client fails to respond and its host 
lifetime has expired, the File Exporter can revoke any tokens the client holds 
without attempting to contact it further.  The File Exporter can revoke the 
tokens of any client whose host RPC lifetime has expired without contacting 
the client; the client needs to either reclaim its tokens or request new ones 
as necessary.
.LE
.P
Each File Exporter defines the lengths of its clients' host lifetimes and host
RPC lifetimes, so a client can have different lifetimes for different File
Exporters.  For any File Exporter, however, a client's host RPC lifetime must
be equal to or greater than its host lifetime. (By default, both lifetimes are
only a few minutes in length.)
.P
The following general rules govern the File Exporter's revocation of valid
tokens held by a client:
.ML
.LI
If the client's host lifetime has not expired, the File Exporter tries to
contact the client; the File Exporter must have the client's permission to 
revoke its tokens.
.nL
.ne 10
.LI
If the client's host lifetime has expired but its host RPC lifetime has not,
the File Exporter tries to contact the client one time.  If the client responds,
the File Exporter cannot revoke the client's tokens without its permission; 
otherwise, the File Exporter can revoke any tokens held by the client 
without contacting it further.
.LI
If the client's host RPC lifetime has expired, the File Exporter can revoke
the client's tokens without contacting it.
.LE
.iX "-]" "File Exporter" "managing tokens"
.iX "-]" "tokens" "management by File Exporter"
.H 3 "Token State Recovery"
.iX "tokens" "recovering"
.iX "File Exporter" "recovering tokens"
.P
Token state recovery refers to clients regaining their tokens following a
communications failure between themselves and a File Exporter.  The following
problems can interrupt communications between a File Exporter and its clients:
.ML
.LI
If a File Exporter is restarted (for example, after its File Server machine 
crashes), it loses all knowledge of the tokens it granted prior to the 
restart.  For a brief period after it first returns to service, the File 
Exporter refuses all requests for new tokens from all clients, accepting 
requests only to reestablish tokens from those clients that held them before 
the File Exporter became unavailable.  This is the first form of token state 
recovery.
.LI
If a network failure prevents a client from contacting a File Exporter, the
client may be unable to prevent its host lifetime from expiring.  Once
communications are restored, the client must either reclaim its tokens or, if
necessary, request new ones.  This is the second form of token state recovery.
.LI
If a client is restarted, it loses all knowledge of the tokens it possessed
prior to the restart; recovery of its tokens is not possible.
.LE
.P
During the first form of token state recovery, the File Exporter attempts to
preserve the state of its tokens across restarts by initially accepting
requests only to reestablish existing tokens.  While the File Exporter is
unavailable, clients that have tokens from it continue to probe it at regular
polling intervals until it returns to service.  When it is again available, the
File Exporter enters token state recovery to give these clients the opportunity
to recover their tokens without threat of conflicts with tokens that were
granted to new clients.
.P
Different File Exporters remain in token state recovery for different lengths
of time after a restart.  However, each File Exporter ensures that its recovery
period lasts long enough to give all of its clients the opportunity to
reestablish their tokens, basing the duration on the host lifetimes or polling
intervals that it assigns, whichever are greater.
.P
During the second form of token state recovery, the File Exporter does not
provide the client with an opportunity to reestablish its tokens without fear 
of conflicting tokens.  The client continues to poll the File Exporter until the
network outage is resolved.  However, if its host lifetime expires before it
can contact the File Exporter, the client may be unable to recover tokens that
it held prior to the network problem.
.P
Values that the File Exporter uses to determine the host lifetimes, host RPC
lifetimes, and polling intervals of its clients are specified with options of
the \*Lfxd\*O command. (See Part 2 of this guide and reference for complete
information about the \*Lfxd\*O command and its options.)
.iX "-]" "File Exporter" "access control by"
.iX "-]" "tokens" "about"
.zA "enh, 13605, R1.2.2, Security enhancements"
.H 2 "Data Access Security in DFS"
.iX "-[" "RPC authentication levels"
.P
DFS includes administrative commands to establish and modify RPC authentication levels for communications between Cache Managers and File Servers.  DFS provides very flexible tools for managing these RPC authentication levels, allowing you to set RPC authentication levels for each Cache Manager and RPC authentication bounds for each File Server.  You can also set advisory RPC authentication bounds for each fileset.
.P 
The default values for security settings at the Cache Manager and File Server result in communications between a Cache Manager and File Server being authenticated at the DCE packet integrity security level.  This means that all data received has been authenticated as originating at the expected host and has been verified to have not been modified during transmission.  However, you can choose to set higher or lower RPC authentication levels for each Cache Manager and File Server.  Note that higher authentication levels result in some degradation of performance (due to increased overhead).
.P
Each Cache Manager maintains a pair of initial RPC authentication level settings and RPC authentication lower bound settings.  One pair governs Cache Manager communications with File Servers in the same cell, while the second set governs communications with File Servers in foreign cells.  Similarly, each File Server maintains a pair of RPC authentication lower and upper bound settings.  Again, one pair governs communications with Cache Managers in the same cell, while the second pair controls communications with Cache Managers in foreign cells.
.P
When a Cache Manager must contact a File Server to access a given fileset the Cache Manager and File Server negotiate for a mutually acceptable RPC authentication level.  In operation, the process works as follows.
.P
The Cache Manager sends an RPC to the File Server that is using the Cache Manager's initial RPC authentication level.  The File Server checks the RPC and compares it to the authentication level range determined by the File Server's upper and lower authentication level bounds.  If the RPC falls within the authentication level range, communications between the Cache Manager and File Server are established.  However, if the RPC authentication level is above or below the File Server's range, the File Server responds with an instruction to increase or decrease the authentication level accordingly.  This negotiation continues until the Cache Manager and File Server arrive at a mutually agreeable RPC authentication level or until the File Server requests an authentication level below the minimum allowed for the Cache Manager (causing the Cache Manager to refuse communications with the File Server).
.P
After arriving at a mutually agreeable RPC authentication level, the Cache Manager stores that information so that it does not need to renegotiate an authentication level during further communications with that particular file server.
.P
Note that Cache Managers in versions of DFS previous to 1.2.2 cannot negotiate RPC authentication levels.  Thus, if  you set the minimum authentication level bound at a File Exporter higher than packet integrity, this will prevent that File Server from communicating with earlier-version Cache Managers.
.P
You can establish a Cache Manager's initial and lower bound RPC authentication levels by using the \*Ldfsd\*O command.  You must assume the \*Lroot\*O identity on the Cache Manager machine to issue this command. You can adjust these settings by using the \*Lcm setprotectlevels\*O command, and you can check the Cache Manager's current RPC authentication level settings with the \*Lcm getprotectlevels\*O command.
.P
You can establish the upper and lower File Exporter RPC authentication bounds by using the \*Lfxd\*O command. You cannot display a File Exporter's RPC authentication bound settings. For more information about setting the File Exporter's authentication bounds with the \*Lfxd\*O command, see Part 2 of this guide and reference.
.P
.H 3 "Fileset Advisory RPC Authentication Bounds"
.P
You can establish advisory minimum and maximum RPC authentication bounds for each fileset.  As with the File Server RPC authentication bounds, the FLDB holds a pair of bounds for each fileset.  One set of bounds governs communications with Cache Managers that are in the same cell as the File Server within which the fileset resides; the other set of bounds controls communications with Cache Managers in foreign cells. While these advisory bounds are not currently enforced (although they may be in a future release of DFS), they do serve to bias the initial RPC authentication level when a Cache Manager attempts to access that fileset. The advisory bounds work as follows.
.P
When the Cache Manager contacts a Fileset Location (FL) Server to ascertain the location (or locations) of a given fileset, the information returned by the FL Server includes that fileset's lower and upper RPC bounds.  The Cache Manager compares its initial RPC authentication level to the range set by the advisory bounds.  If the initial level falls within that range, the Cache Manager begins negotiations with a File Server using the initial level.  However, if the initial level is above or below the range, the Cache Manager adjusts its initial level to match the closest bound level. (If the File Server requests that the Cache Manager lower its authentication level below the minimum level specified for the Cache Manager, the Cache Manager refuses communications with that File Server.) The Cache Manager then uses the modified initial level to begin negotiations with a File Server.
.P
You establish the fileset advisory RPC authentication level bounds by using the \*Lfts setprotectlevels\*O command. You can check if a given fileset has advisory bounds and display the bound level values by using either the \*Lfts lsfldb\*O command or the \*Lfts lsft\*O command. 
.P
.iX "-]" "RPC authentication levels"
.zZ "enh, 13605, R1.2.2, Security enhancements"
.H 2 "DFS Distributed Database Technology"
.iX "-[" "Ubik"
.P
DFS includes two administrative databases: the Fileset Location Database
(FLDB) and the Backup Database.  You can increase system efficiency, file
availability, and system reliability by replicating (copying) these two
databases on multiple server machines.  If one machine housing a copy of a
database then becomes unavailable, the information can still be accessed from
a copy of the database on another machine.
.P
Unlike replicated filesets, replicated databases may change frequently. To
ensure consistent system behavior, all copies of a database must be identical.
DFS uses a library of utilities, \*EUbik\*O, as a mechanism for synchronizing
multiple copies of a replicated database. (Because Ubik is a subroutine
library, it does not appear in listings of the processes running on a server
machine.)
.P
.ne 10
In DFS, one server machine houses a master copy of a replicated database such
as the FLDB.  When a user alters information in the database, Ubik coordinates
the distribution of the change from the master copy to the copies of the
database on other machines; the distribution is automatic and nearly
instantaneous.  Ubik dynamically selects a master copy of a database from among
the servers that house it.  The selection process and the propagation of
changes to all copies of a database are managed entirely by Ubik and are
transparent to administrators and users.
.H 3 "Ubik Database Synchronization"
.iX "-[" "Distributed File Service (DFS)" "database synchronization"
.P
The Ubik library has a client portion and a server portion. Clients such as
the \*Lfts\*O and \*Lbak\*O programs call subroutines in the Ubik library's
client portion to contact the Fileset Location (FL) Server or Backup Server.
These database server processes in turn call subroutines in the server portion
of the Ubik library to access or modify information in the FLDB or Backup
Database.
.P
.iX "Ubik" "synchronization site"
.iX "Ubik" "coordinator"
The master copy of an FLDB or Backup Database is referred to as the
\*Esynchronization site\*O.  The other copies of the database are referred to
as \*Esecondary sites\*O.  A separate occurrence of Ubik, referred to as a \*EUbik
coordinator\*O, maintains the copy of the database at each site.  A database
server process makes a change to a database by issuing a call to the Ubik
coordinator at the synchronization site, which makes the change to that copy
of the database and distributes the change to the Ubik coordinators at the
secondary sites.  The coordinator at each secondary site then updates the copy
of the database at its site.
.P
Each copy of a database has a version number, which should always be the same
for all copies of the database.  Each change to a database increments the
version number of the database by one.  The coordinator at the synchronization
site uses the version number to determine whether each secondary site has a
copy of the most recent version of the database.
.P
.ne 10
For example, if a service outage isolates a secondary site from the
synchronization site, the secondary site no longer receives database updates
from the synchronization site.  When communications are restored, the
coordinator at the synchronization site examines the version number of the
database at the secondary site to determine whether the secondary site has the
most recent version.  If necessary, it sends the copy with the highest version
number to the secondary site.
.P
.iX "Ubik" "electing synchronization site"
The Ubik coordinator at the synchronization site periodically sends an RPC to
each secondary site.  A response to the RPC from the coordinator at a secondary
site serves as a \*Evote\*O to maintain the current synchronization site in its
role for a fixed amount of time.  Within that time, the synchronization site
sends a subsequent RPC to the secondary site in an attempt to retain its role.
.P
The coordinator at the synchronization site constantly tallies the votes it
receives from the secondary sites.  It continues in its role as synchronization
site, confident that the other sites have not chosen a new synchronization
site and begun making competing changes to the database, as long as it
receives the votes of a strict majority (more than 50%) of all database sites,
including itself.  The necessary majority of database sites is referred to as a
\*Equorum\*O.
.P
Because Ubik relies on the actions of a quorum, having an odd number of
database sites is helpful; in most cases, storing a replicated database at
three sites is sufficient.  Note, however, that the vote of the coordinator on
the database server machine with the lowest network address of all database
server machines of its type (those that house the FLDB or those that house the
Backup Database) carries more weight than the votes of the coordinators at the
other sites.  This allows Ubik to attain a quorum if an even number of sites
exist.
.P
The synchronization site stops sending RPCs to the secondary sites if hardware,
software, or network problems result in any of the following:
.ML
.LI
It stops receiving votes from a quorum of the database sites.
.LI
It cannot propagate changes to a quorum of the database sites.
.LI
It or its machine fails.
.LE
.P
.ne 10
If the coordinator at the synchronization site stops sending RPCs for any
reason, Ubik elects a new synchronization site.  In an election, each
coordinator is biased to vote for the site with the lowest network address
from among the sites it can contact, with the vote of the site with the lowest
network address of all database server machines of that type again carrying
slightly more weight than the votes of the other sites.  One site, usually the
one with the lowest network address, typically gathers the necessary majority
quickly and is elected the new synchronization site.
.P
Immediately following the election, the newly elected synchronization site
polls all sites to find the database with the highest version number.  It
adopts this version as the master copy and distributes it to the sites that
do not yet have it.  The election and database distribution are typically
brief, usually taking no longer than a few minutes.
.P
During the period while Ubik cannot obtain quorum and during the subsequent
election and database distribution, the affected database cannot be modified
in any way.  If the Backup Database is affected, information cannot be read from
the database; the database is completely unavailable.  If the FLDB is affected,
Cache Managers can still read information from the database about the locations
of filesets from which they need to access information; however, \*Lfts\*O
commands such as \*Lfts lsfldb\*O cannot be used to get information from the
database.  Because the FLDB is most often accessed by Cache Managers seeking
fileset location information, a Ubik election and ensuing database distribution
do not interfere with the database's primary purpose.
.iX "-]" "Distributed File Service (DFS)" "database synchronization"
.H 3 "Providing Information for Ubik"
.P
For the most part, Ubik operates without human intervention.  However, it does
depend on other DCE facilities and services for some things.  The following
list describes the interaction between Ubik and the remainder of DCE.  It also
provides an overview of the configuration information necessary for Ubik to
operate properly.  Section 2.5.3 discusses the database server configuration
steps required for Ubik to function properly.
.ML
.LI
.iX "Distributed Time Service (DTS)" "interaction with Ubik"
\*EUbik relies on DTS\*O to synchronize the clocks on server machines that
house copies of a replicated database.  Ubik coordinators must agree on the
time; clock differences among Ubik sites can cause them to believe they are no
longer in contact with each other, even if they are operating correctly.  If a
site falls out of touch, it may try to elect a new synchronization site or
refuse to give out information.  You can prevent such service outages by using
DTS to synchronize the clocks on database server machines.
.LI
.iX "Security Service" "interaction with Ubik"
\*EUbik relies on the Security Service\*O for secure communications between
all Fileset Database machines (machines that house the FLDB) and Backup
Database machines (machines that house the Backup Database).  Each type of
database server has its own security group, of which all machines that house a
copy of that type of database must be members.  A machine's membership in this
group enables the Ubik coordinator on that machine to communicate with the
Ubik coordinators on the other database servers of that type, thus allowing
the coordinator to participate in Ubik elections.
.P
Abbreviated forms of the DFS server principals of all Fileset Database machines
must be listed in the \*Lsubsys/dce/dfs-fs-servers\*O group in the Registry
Database.  Similarly, abbreviated forms of the DFS server principals of all
Backup Database machines must be listed in the \*Lsubsys/dce/dfs-bak-servers\*O
group in the Registry Database.  To view the members of either of these security
groups, use the \*Ldcecp group list\*O command.
.P
A machine's DFS server principal is of the form
\*L/.../\*Vcellname\*L/hosts/\*Vhostname\*L/dfs-server\*O.  The abbreviated
form of a machine's DFS server principal is of the form
\*Lhosts/\*Vhostname\*L/dfs-server\*O.  For example, in the cell named
\*Labc.com\*O, the abbreviated server principals of all Fileset Database
machines are listed in \*Lsubsys/dce/dfs-fs-servers\*O in the form
\*Lhosts/\*Vhostname\*L/dfs-server\*O.
.LI
.iX "Cell Directory Service (CDS)" "interaction with Ubik"
\*EUbik relies on CDS\*O for a complete list of all Fileset Database and Backup
Database machines.  Each type of database server has its own RPC server group in
CDS.  Ubik examines the machines listed in the appropriate RPC group to
determine how many sites constitute a majority and where to send votes in the
event of an election.
.P
.iX "Remote Procedure Call (RPC)" "interaction with Ubik"
The names of the RPC bindings of all Fileset Database machines must be listed
in the RPC group in CDS at \*L/.../\*Vcellname\*L/fs\*O, the junction to the
DFS filespace.  Likewise, the names of the RPC bindings of all Backup Database
machines must be listed in the RPC group in CDS at
\*L/.../\*Vcellname\*L/subsys/dce/dfs/bak\*O.  To view the members of either
of these RPC server groups, use the \*Ldcecp rpcgroup list\*O command.
.P
The name of a machine's RPC binding is of the form
\*L/.../\*Vcellname\*L/hosts/\*Vhostname\*L/self\*O.  For example, in the cell
named \*Labc.com\*O, the names of the RPC bindings of all Fileset Database
machines are listed in \*L/.../abc.com/fs\*O in the form
\*L/.../abc.com/hosts/\*Vhostname\*L/self\*O.
.LE
.nS "note"
.iX "directories" "well known names (DFS)"
In a server machine's DFS server principal or the name of its RPC binding, the
element that follows the \*Vcellname\*O component is not considered to be well
known; for example, \*Lhosts\*O could be \*Ldfs-hosts\*O.  However, the string
used for the element must be applied consistently to all such names in a cell.
.P
In addition, the names \*L/.../\*Vcellname\*L/fs\*O and
\*L/.../\*Vcellname\*L/subsys/dce/dfs/bak\*O in CDS are not considered
to be well known; either can be changed during installation and configuration
of a cell.  Conversely, the names of the \*Lsubsys/dce/dfs-fs-servers\*O and
\*Lsubsys/dce/dfs-bak-servers\*O groups are well known and cannot be changed.
.nE
.H 3 "Configuring Database Server Machines for Ubik"
.iX "-[" "Ubik" "configuring database server machines"
.iX "-[" "DFS servers" "configuring for Ubik"
.P
A cell's initial database server machines are configured when DFS is installed
and configured in the cell.  If it becomes necessary to add or remove database
server machines after initial cell configuration, perform the steps in Sections
2.5.3.1 and 2.5.3.2 to properly configure information for Ubik. (You may be
able to use the DCE installation and configuration program to modify the
database servers configured in your cell.  See your vendor's installation
and configuration documentation for more information.)
.P
.ne 8
When the Cache Manager on a DFS client machine needs information from the
FLDB in a cell, the \*Ldfsbind\*O process on the machine provides it with
information about the names and network addresses of the Fileset Database
machines for the cell.  The information is valid for a limited amount of time,
24 hours by default, at which time the Cache Manager requests refreshed
information from \*Ldfsbind\*O; the Cache Manager also needs to refresh the
information when it is restarted.  
.P
The \*Lfxd\*O process on a File Server
machine passes the same information about Fileset Database machines for the
local cell to the File Exporter on its machine, but only when it is restarted
(generally when the machine is rebooted).
.P
It is seldom necessary to restart client or server machines if you reconfigure
a cell's Fileset Database machines.  As long as at least one Fileset Database
machine remains the same after reconfiguration, all machines can continue to
access the FLDB via that machine.  Eventually, all machines recognize the
current set of Fileset Database machines as a result of routine machine
administration and maintenance.  It is never necessary to restart client or
server machines if you reconfigure a cell's Backup Database machines.
.P
Sections 2.5.3.1 and 2.5.3.2 describe the steps required to add or remove a
database server machine after initial cell configuration.  Recall that each
Fileset Database machine must run the FL Server (\*Lflserver\*O process), and
each Backup Database machine must run the Backup Server (\*Lbakserver\*O
process).  These processes should be controlled by the Basic OverSeer (BOS)
Server (\*Lbosserver\*O process) on their machines, as recommended; if they
are, you can use the appropriate \*Lbos\*O commands to manipulate them.
.P
Also recall that each FL Server must use the same \*Ladmin.fl\*O list, and
that each Backup Server must use the same \*Ladmin.bak\*O list.  In addition,
the abbreviated DFS server principal of each Fileset Database machine must be
included in the \*Ladmin.fl\*O list, and the abbreviated DFS server principal
of each Backup Database machine must be included in the \*Ladmin.bak\*O list.
A DFS server principal can be added directly to a list, or it can be present
as a member of a group included in the list (for example, the group
\*Lsubsys/dce/dfs-fs-servers\*O can be included in the \*Ladmin.fl\*O list).
.PP
.ne 10
Inclusion in the appropriate administrative list allows the database server
process at the synchronization site to distribute changes to the database
server processes at the secondary sites.  The Update Server should be used to
propagate these administrative lists from the System Control machine to their
respective database server machines.
.P
You can use the \*Ludebug\*O command to obtain status information on Ubik
database servers.  The command is useful for diagnosing problems associated
with Ubik. (See Part 2 of this guide and reference for complete information
about the \*Ludebug\*O command and its options.  Refer to the sections at the
beginning of this chapter for more information about the processes that must
run on either type of database server machine and the administrative lists
used to specify who can control them; see Chapter 5 for more information about
\*Lbos\*O commands.)
.H 4 "Adding a Database Server Machine"
.iX "DFS servers" "adding"
.P
To add a database server machine, do the following:
.AL
.LI
\*EIf you intend to configure the machine as a Fileset Database machine\*O and
the machine does not currently have a server entry in the FLDB, use the \*Lfts
crserverentry\*O command to create a server entry in the FLDB for the
abbreviated DFS server principal of the machine.  The machine already has a
server entry in the FLDB if it is configured as a File Server machine. (See
Chapter 6 for more information about using the \*Lfts crserverentry\*O command
to create server entries.)
.LI
Use the \*Ldcecp group add\*O command to add the abbreviated DFS server
principal of the new database server machine to the appropriate security group
(\*Lsubsys/dce/dfs-fs-servers\*O or \*Lsubsys/dce/dfs-bak-servers\*O) in the
Registry Database.
.LI
Use the \*Ldcecp rpcgroup add\*O command to add the name of the RPC binding
of the new database server machine to the appropriate RPC server group
(\*L/.../\*Vcellname\*L/fs\*O or \*L/.../\*Vcellname\*L/subsys/dce/dfs/bak\*O)
in CDS.
.nL
.ne 12
.LI
Use the \*Lbos addadmin\*O command to add the abbreviated DFS server principal
of the new database server machine to the appropriate administrative list
(\*Ladmin.fl\*O or \*Ladmin.bak\*O).  This allows the synchronization site to
propagate changes to the secondary sites.  These administrative lists are
usually updated on the cell's System Control machine, which then distributes
the updated lists via the Update Server.
.P
Alternatively, you can use the \*Ldcecp group add\*O command to add the
abbreviated DFS server principal to a security group included in the list.
Note that, if a group such as \*Lsubsys/dce/dfs-fs-servers\*O is included in
the administrative list, the DFS server principal is already present in the
list as a member of that group.
.LI
Copy the appropriate administrative list (\*Ladmin.fl\*O or \*Ladmin.bak\*O)
to the \*Vdcelocal\*L/var/dfs\*O directory on the new database server machine.
These administrative lists are typically propagated from the cell's System
Control machine via the Update Server.  Modify the Update Server as necessary
if the list is propagated from the cell's System Control machine.
.LI
Stop and restart the appropriate database server process (\*Lflserver\*O or
\*Lbakserver\*O) on each database server machine of that type.  Restarting the
existing database server processes causes the processes to read the updated 
RPC server group.  This ensures that each Ubik coordinator agrees on the 
number and identities of the other database server machines of its type, 
which is vital to Ubik's use of a quorum of database server machines to 
maintain database consistency.
.LI
Start the appropriate database server process (\*Lflserver\*O or
\*Lbakserver\*O) on the new database server machine.
.LE
.H 4 "Removing a Database Server Machine"
.iX "DFS servers" "removing"
.P
To remove a database server machine, do the following:
.AL
.LI
Stop the appropriate database server process (\*Lflserver\*O or
\*Lbakserver\*O) on the database server machine to be removed.
.LI
Use the \*Ldcecp group remove\*O command to remove the abbreviated DFS server
principal of the database server machine to be removed from the appropriate
security group.
.LI
Use the \*Ldcecp rpcgroup remove\*O command to remove the reference to the RPC
binding of the database server machine to be removed from the appropriate RPC
server group.
.LI
Use the \*Ldcecp rpcentry show\*O command on each database server machine of
the appropriate type to update the entry for the appropriate RPC server group
from CDS.  The command forces CDS to update information that it caches from the
entry for the group in the namespace.
.LI
Stop and restart the appropriate database server process (\*Lflserver\*O or
\*Lbakserver\*O) on each database server machine of that type.  Restarting the
existing database server processes causes the processes to read the updated 
RPC server group.  This ensures that each Ubik coordinator agrees on the 
number and identities of the other database server machines of its type, which 
is vital to Ubik's use of a quorum of database server machines to maintain 
database consistency.
.LI
Use the \*Lbos rmadmin\*O command to remove the abbreviated DFS server
principal of the database server machine to be removed from the appropriate
administrative list (\*Ladmin.fl\*O or \*Ladmin.bak\*O).  These administrative
lists are usually updated on the cell's System Control machine, which then
distributes the updated lists via the Update Server.
.P
If you chose instead to add the abbreviated DFS server principal to a security
group included in the list, you can use the \*Ldcecp group remove\*O command
to remove the server principal from the group.  Note that if the DFS server
principal was present in the administrative list as a member of a group such
as \*Lsubsys/dce/dfs-fs-servers\*O, the server principal is already removed
from the list.
.LI
Remove the appropriate administrative list (\*Ladmin.fl\*O or \*Ladmin.bak\*O)
from the \*Vdcelocal\*L/var/dfs\*O directory on the database server machine to
be removed.  Modify the Update Server as necessary if the list is propagated
from the cell's System Control machine.
.LE
.iX "-]" "Ubik" "configuring database server machines"
.iX "-]" "DFS servers" "configuring for Ubik"
.iX "-]" "Ubik"
.iX "-]" "Distributed File Service (DFS)" "configuration"

